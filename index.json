[{"categories":["CS186"],"content":"General Notes issues to consider in any index structure (not just in B+ tree) query support: what class of queries can be supported? choice of search key affects how we write the query data entry storage affect performance of the index variable-length keys tricks affect performance of the index cost model for Index vs Heap vs Sorted File ","date":"2024-08-14","objectID":"/databasel6/:1:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"query support ","date":"2024-08-14","objectID":"/databasel6/:2:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Indexes basic selection \u003ckey\u003e\u003cop\u003e\u003cconstant\u003e ËØ∏Â¶Ç=ÔºåBETWEENÔºå\u003eÔºå\u003cÔºå\u003e=Ôºå\u003c= more selection Áª¥Â∫¶ÁÅæÈöæüò≤ ‰ΩÜÊòØËøôËäÇËØæÊàë‰ª¨Âè™ÊòØÂÖ≥Ê≥®1-d range search, equalityÔºå B+ tree ","date":"2024-08-14","objectID":"/databasel6/:2:1","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Search Key and Ordering Ê≥®ÊÑèlexicographic! ‰ª•‰∏ãÁªôÂá∫‰∫Ü‰∏Ä‰∏™ÂÆö‰πâComposite KeysÔºåÂ§öÂàóÔºåÂâçÁ≠âÔºåÂ∞æÂîØ‰∏Ärange Ê≥®ÊÑèÂØπLexicographic RangeÁöÑÂº∫Ë∞É ","date":"2024-08-14","objectID":"/databasel6/:3:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Data Entry Storage ","date":"2024-08-14","objectID":"/databasel6/:4:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"intro the representation of data? itself or pointers to it? how data is stored? clustered or unclustered? ","date":"2024-08-14","objectID":"/databasel6/:4:1","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"representation alt. 1 index entry: (key, value) alt. 2 index entry: (key, recordID), remember recordID is‚Ä¶‚Ä¶ alt. 3 index entry: (key, list of recordIDs) ","date":"2024-08-14","objectID":"/databasel6/:4:2","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"clustered vs unclustered index clustered is more efficient for IOs ü§î, range search and supports ‚Äúcompression‚Äù ü§î ","date":"2024-08-14","objectID":"/databasel6/:4:3","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Variable-length keys tricks ÈáçÊñ∞ÂÆö‰πâ Occupancy Invariant ÔºàÂΩì‰∏çÊòØÁî®Êï¥Êï∞Êù•indexÊó∂ÂÄôÔºâ get more index entries to shorten the tree (avoiding long-time IOs) prefix key compression (only in leaf level ü§î, slightly change the order of keys?) suffix key compression ","date":"2024-08-14","objectID":"/databasel6/:5:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"B+ Tree Costs ËøôÈáåÂºïÂÖ•Êñ∞ÁöÑÂÅáËÆæÔºö store by ref (see in alt. 2) clustered index with 2/3 full heap file pages clustered -\u003e heapfile is initially sorted fanout is larger ~ $O(Ref)$ assume static index Á¨¶Âè∑Ë°®ËææÂ¶Ç‰∏ãÔºö $ B $ : num of full data blocks (why full? recall previous lecture) $ R $ : num of records per blocks $ D $ : Average time to r/w disk block $ F $ : avg internal node fanout $ E $ : avg num of data entries per leaf side note: Scan all records: $3/2$Êù•Ëá™‰∏éÂç†ÊúâÁéá2/3Ôºå $\\frac{2}{3}B‚Äô = B \\Rightarrow B‚Äô = \\frac{3}{2}B \\Rightarrow B‚ÄôD = \\frac{3}{2}B D$ Equality Search: $1 \\Rightarrow 2$ !! Êù•Ëá™‰∫é‰ªépage‰∏≠ËØªÂèñslot‰ªéËÄåËé∑ÂæóÂÖ∑‰ΩìÁöÑindexÂπ∂‰∏îËØªÂèñÊï∞ÂÄº, $log_F(BR/E)$ ÊòØÊêúÁ¥¢page Range Search: Â∫îËØ•ÊòØ $(log_F(BR/E)+1+3*pages)*D$ Insert\u0026Delete: Â∫îËØ•ÊòØ $(log_F(BR/E)+4)*D$, index 1ÔºåËØªÂèñÊï∞ÂÄº 1ÔºåÊîπÂèòÊï∞ÂÄº 1ÔºåÊîπÂèòindex 1 big-O notation: üò∏ ","date":"2024-08-14","objectID":"/databasel6/:6:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Summary time-stamp: 01h42m07s ","date":"2024-08-14","objectID":"/databasel6/:7:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab05.ipynb\") Lab 5: Modeling, Loss Functions, and Summary Statistics ","date":"2024-08-13","objectID":"/datalab5/:0:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Predicting Restaurant Tips In this lab, you will try to predict restaurant tips from a set of data in several ways: A. Without given any additional information, use a constant model with L2 loss to predict the tip $\\hat{y}$ as a summary statistic, $\\theta$. B. Given one piece of information‚Äîthe total bill $x$‚Äîuse a linear model with L2 loss to predict the tip $\\hat{y}$ as a linear function of $x$. C. See if a constant model with L1 loss changes our predictions. First, let‚Äôs load in the data. # just run this cell import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') sns.set() sns.set_context(\"talk\") tips = sns.load_dataset(\"tips\") tips.head(5) total_bill\rtip\rsex\rsmoker\rday\rtime\rsize\r0\r16.99\r1.01\rFemale\rNo\rSun\rDinner\r2\r1\r10.34\r1.66\rMale\rNo\rSun\rDinner\r3\r2\r21.01\r3.50\rMale\rNo\rSun\rDinner\r3\r3\r23.68\r3.31\rMale\rNo\rSun\rDinner\r2\r4\r24.59\r3.61\rFemale\rNo\rSun\rDinner\r4\rQuick EDA: Note that this dataset is likely from the United States. The below plot graphs the distribution of tips in this dataset, both in absolute amounts ($) and as a fraction of the total bill (post-tax, but pre-tip). # just run this cell fig, ax = plt.subplots(ncols=2, figsize=(10, 4)) sns.histplot(tips['tip'], bins=20, stat=\"proportion\", ax=ax[0]) sns.histplot(tips['tip']/tips['total_bill'], bins=20, stat=\"proportion\", ax=ax[1]) ax[0].set_xlabel(\"Amount ($)\") ax[1].set_xlabel(\"Fraction of total bill\") ax[0].set_ylim((0, 0.35)) ax[1].set_ylim((0, 0.35)) ax[1].set_ylabel(\"\") # for cleaner visualization fig.suptitle(\"Restaurant Tips\") plt.show() In this lab we‚Äôll estimate the tip in absolute amounts ($). The above plot is just to confirm your expectations about the tips dataset. ","date":"2024-08-13","objectID":"/datalab5/:1:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Part A: Tips as a Summary Statistic Let‚Äôs first predict any restaurant tip using one single number: in other words, let‚Äôs try to find the best statistic $\\hat{\\theta}$ to represent (i.e., summarize) the tips from our dataset. Each actual tip in our dataset is $y$, which is what we call the observed value. We want to predict each observed value as $\\hat{y}$. We‚Äôll save the observed tip values in a NumPy array y_tips: # just run this cell y_tips = np.array(tips['tip']) # array of observed tips y_tips.shape (244,)\rRecall the three-step process for modeling as covered in lecture: Define a model. Define a loss function and the associated risk on our training dataset (i.e., average loss). Find the best value of $\\theta$, known as $\\hat{\\theta}$, that minimizes risk. We‚Äôll go through each step of this process next. ","date":"2024-08-13","objectID":"/datalab5/:2:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"A.1: Define the model We will define our model as the constant model: $$\\Large \\hat{y} = \\theta $$ In other words, regardless of any other details (i.e., features) about their meal, we will always predict our tip $\\hat{y}$ as one single value: $\\theta$. $\\theta$ is what we call a parameter. Our modeling goal is to find the value of our parameter(s) that best fit our data. We have choice over which $\\theta$ we pick (using the data at hand), but ultimately we can only pick one to report, so we want to find the optimal parameter(s) $\\hat{\\theta}$. We call the constant model a summary statistic, as we are determining one number that best ‚Äúsummarizes‚Äù a set of values. No code to write here! ","date":"2024-08-13","objectID":"/datalab5/:3:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"A.2: Define the loss function and risk Next, in order to pick our $\\theta$, we need to define a loss function, which is a measure of how well a model is able to predict the expected outcome. In other words, it measures the deviation of a predicted value $\\hat{y}$ from the observed value $y$. We will use squared loss (also known as the $L_2$ loss, pronounced ‚Äúell-two‚Äù). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_2$ loss of: $$\\Large L_2(y, \\hat{y}) = (y - \\hat{y})^2$$ ","date":"2024-08-13","objectID":"/datalab5/:4:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 1 In our constant model $\\hat{y} = \\theta$, we always predict the tip as $\\theta$. Therefore our $L_2$ loss for some actual, observed value $y$ can be rewritten as: $$\\Large L_2(y, \\theta) = (y - \\theta)^2$$ Use the function description below to implement the squared loss function for this single datapoint, assuming the constant model. Your answer should not use any loops. def squared_loss(y_obs, theta): \"\"\" Calculate the squared loss of the observed data and a summary statistic. Parameters ------------ y_obs: an observed value theta : some constant representing a summary statistic Returns ------------ The squared loss between the observation and the summary statistic. \"\"\" return (y_obs - theta)**2 grader.check(\"q1\") We just defined loss for a single datapoint. Let‚Äôs extend the above loss function to our entire dataset by taking the average loss across the dataset. Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = {y_1, \\ldots, y_n}$, where $y_i$ is the $i^{th}$ tip (this is the y_tips array defined at the beginning of Part A). We can define the average loss (aka risk) over the dataset as: $$\\Large R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) $$ If we use $L_2$ loss per datapoint ($L = L_2$), then the risk is also known as mean squared error (MSE). For the constant model $\\hat{y}=\\theta$: $$\\Large R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta)^2 $$ ","date":"2024-08-13","objectID":"/datalab5/:5:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 2 Define the mse_tips_constant function which computes $R(\\theta)$ as the mean squared error on the tips data for a constant model with parameter $\\theta$. Notes/Hints: This function takes in one parameter, theta; data is defined for you as a NumPy array that contains the observed tips values in the data. Use the squared_loss function you defined in the previous question. def mse_tips_constant(theta): data = y_tips return sum(squared_loss(data, theta)) / len(data) mse_tips_constant(5.3) # arbitrarily pick theta = 5.3 np.float64(7.204529508196728)\rgrader.check(\"q2\") ","date":"2024-08-13","objectID":"/datalab5/:6:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"A.3: Find the $\\theta$ that minimizes risk ","date":"2024-08-13","objectID":"/datalab5/:7:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3 Now we can go about choosing our ‚Äúbest‚Äù value of $\\theta$, which we call $\\hat{\\theta}$, that minimizes our defined risk (which we defined as mean squared error). There are several approaches to computing $\\hat{\\theta}$ that we‚Äôll explore in this problem. ","date":"2024-08-13","objectID":"/datalab5/:8:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3a: Visual Solution In the cell below we plot the mean squared error for different thetas: # just run this cell theta_values = np.linspace(0, 6, 100) mse = [mse_tips_constant(theta) for theta in theta_values] plt.plot(theta_values, mse) plt.xlabel(r'$\\theta$') plt.ylabel('average L2 loss') plt.title(r'MSE for different values of $\\theta$'); Find the value of theta that minimizes the mean squared error via observation of the plot above. Round your answer to the nearest integer. min_observed_mse = 3 min_observed_mse 3\rgrader.check(\"q3a\") q3a passed! üíØ ","date":"2024-08-13","objectID":"/datalab5/:8:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Numerically computing $\\hat{\\theta}$ scipy.optimize.minimize is a powerful method that can determine the optimal value of a variety of different functions. In practice, it is used to minimize functions that have no (or difficult to obtain) analytical solutions (it is a numerical method). It is overkill for our simple example, but nonetheless, we will show you how to use it, as it will become useful in the near future. The cell below plots some arbitrary 4th degree polynomial function. # just run this cell x_values = np.linspace(-4, 2.5, 100) def fx(x): return 0.1 * x**4 + 0.2*x**3 + 0.2 * x **2 + 1 * x + 10 plt.plot(x_values, fx(x_values)); plt.title(\"Arbitrary 4th degree polynomial\"); By looking at the plot, we see that the x that minimizes the function is slightly larger than -2. What if we want the exact value? We will demonstrate how to grab the minimum value and the optimal x in the following cell. The function minimize from scipy.optimize will attempt to minimize any function you throw at it. Try running the cell below, and you will see that minimize seems to get the answer correct. Note: For today, we‚Äôll let minimize work as if by magic. We‚Äôll discuss how minimize works later in the course. # just run this cell from scipy.optimize import minimize minimize(fx, x0 = 1.1) message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: 8.728505719866614\rx: [-1.747e+00]\rnit: 6\rjac: [ 1.192e-07]\rhess_inv: [[ 5.088e-01]]\rnfev: 16\rnjev: 8\rNotes: [1] fun: the minimum value of the function. [2] x: the x which minimizes the function. We can index into the object returned by minimize to get these values. We have to add the additional [0] at the end because the minimizing x is returned as an array, but this is not necessarily the case for other attributes (i.e. fun), shown in the cell below. Note [2] means that minimize can also minimize multivariable functions, which we‚Äôll see in the second half of this lab. # just run this cell min_result = minimize(fx, x0 = 1.1) # ËøîÂõû‰∏Ä‰∏™Â≠óÂÖ∏ÔºåÂåÖÊã¨ÊúÄ‰ºòÂÄºfunÂíåÊúÄ‰ºòËß£x min_of_fx = min_result['fun'] x_which_minimizes_fx = min_result['x'][0] min_of_fx, x_which_minimizes_fx (8.728505719866614, np.float64(-1.746827786380178))\rInitial guess: The parameter x0 that we passed to the minimize function is where the minimize function starts looking as it tries to find the minimum. For example, above, minimize started its search at x = 1.1 because that‚Äôs where we told it to start. For the function above, it doesn‚Äôt really matter what x we start at because the function is nice and has only a single local minimum. More technically, the function is nice because it is convex, a property of functions that we will discuss later in the course. Local minima: minimize isn‚Äôt perfect. For example, if we give it a function with many valleys (also known as local minima) it can get stuck. For example, consider the function below: # just run this cell w_values = np.linspace(-2, 10, 100) def fw(w): return 0.1 * w**4 - 1.5*w**3 + 6 * w **2 - 1 * w + 10 plt.plot(w_values, fw(w_values)); plt.title(\"Arbitrary function with local minima\"); If we start the minimization at w = 6.5, we‚Äôll get stuck in the local minimum at w = 7.03. Note that no matter what your actual variable is called in your function (w in this case), the minimize routine still expects a starting point parameter called x0. # just run this cell minimize(fw, x0 = 6.5) # initial w is 6.5 message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: 22.594302881719713\rx: [ 7.038e+00]\rnit: 4\rjac: [-3.815e-06]\rhess_inv: [[ 1.231e-01]]\rnfev: 12\rnjev: 6\r","date":"2024-08-13","objectID":"/datalab5/:8:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3b: Numerical Solution Using the minimize function, find the value of theta that minimizes the mean squared error for our tips dataset. In other words, you want to find the exact minimum of the plot that you saw in the previous part. Notes: You should use the function you defined earlier: mse_tips_constant. For autograding purposes, assign min_scipy to the value of theta that minimizes the MSE according to the minimize function, called with initial x0=0.0. # call minimize with initial x0 = 0.0 min_scipy = minimize(mse_tips_constant, x0=0.0)['x'][0] min_scipy np.float64(2.9982777037277204)\rgrader.check(\"q3b\") ","date":"2024-08-13","objectID":"/datalab5/:8:3","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3c: Analytical Solution In lecture, we used calculus to show that the value of theta that minimizes the mean squared error for the constant model is the average (mean) of the data. Assign min_computed to the mean of the observed y_tips data, and compare this to the values you observed in questions 3a and 3b. min_computed = y_tips.mean() min_computed np.float64(2.99827868852459)\rgrader.check(\"q3c\") Reflecting on the lab so far, we used a 3-step approach to find the ‚Äúbest‚Äù summary statistic $\\theta$: Define the constant model $\\hat{y}=\\theta$. Define ‚Äúbest‚Äù: Define loss per datapoint (L2 loss) and consequently define risk $R(\\theta)$ over a given data array as the mean squared error, or MSE. Find the $\\theta = \\hat{\\theta}$ that minimizes the MSE $R(\\theta)$ in several ways: Visually: Create a plot of $R(\\theta)$ vs. $\\theta$ and eyeball the minimizing $\\hat{\\theta}$. Numerically: Create a function that returns $R(\\theta)$, the MSE for the given data array for a given $\\theta$, and use the scipy minimize function to find the minimizing $\\hat{\\theta}$. Analytically: Simply compute $\\hat{\\theta}$ the mean of the given data array, since this minimizes the defined $R(\\theta)$. (a fourth analytical option) Use calculus to find $\\hat{\\theta}$ that minimizes MSE $R(\\theta)$. At this point, you‚Äôve hopefully convinced yourself that the mean of the data is the summary statistic that minimizes mean squared error. Our prediction for every meal‚Äôs tip: # just run this cell def predict_tip_constant(): return min_computed # do not edit below this line bill = 20 print(f\"\"\"No matter what meal you have, Part A's modeling process predicts that you will pay a tip of ${predict_tip_constant():.2f}.\"\"\") No matter what meal you have, Part A's modeling process\rpredicts that you will pay a tip of $3.00.\r","date":"2024-08-13","objectID":"/datalab5/:8:4","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Part B: Tips as a Linear Function of Total Bill In this section, you will follow the exact same modeling process but instead use total bill to predict tip. We‚Äôll save the observed total bill values (post-tax but pre-tip) and the observed tip values in two NumPy arrays, x_total_bills and y_tips: # just run this cell x_total_bills = np.array(tips['total_bill']) # array of total bill amounts y_tips = np.array(tips['tip']) # array of observed tips print(\"total bills\", x_total_bills.shape) print(\"tips\", y_tips.shape) total bills (244,)\rtips (244,)\r","date":"2024-08-13","objectID":"/datalab5/:9:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"B.1 Define the model We will define our model as the linear model that takes a single input feature, total_bill ($x$): $$\\Large \\hat{y} = a + b x $$ Our ‚Äúparameter‚Äù $\\theta$ is actually two parameters: $a$ and $b$. You may see this written as $\\theta = (a, b)$. Our modeling task is then to pick the best values $a = \\hat{a}$ and $b = \\hat{b}$ from our data. Then, given the total bill $x$, we can predict the tip as $\\hat{y} = \\hat{a} + \\hat{b} x$. No code to write here! ","date":"2024-08-13","objectID":"/datalab5/:10:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"B.2: Define the loss function and risk Next, we‚Äôll define our loss function $L(y, \\hat{y})$ and consequently our risk function $R(\\theta) = R(a, b)$. Similar to our approach to Part A, we‚Äôll use L2 Loss and Mean Squared Error. Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = {(x_1, y_1), \\ldots, (x_n, y_n)}$, where $(x_i, y_i)$ are the $i^{th}$ total bill and tip, respectively, in our dataset. Our L2 Loss and Mean Squared Error are therefore: \\begin{align} \\large L_2(y, \\hat{y}) = \\large (y - \\hat{y})^2 \u0026= \\large (y - (a + bx))^2 \\ \\large R(a, b) = \\large \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) \u0026= \\large \\frac{1}{n} \\sum_{i = 1}^n(y_i - (a + b x_i))^2 \\end{align} Notice that because our model is now the linear model $\\hat{y} = a + bx$, our final expressions for Loss and MSE are different from Part A. ","date":"2024-08-13","objectID":"/datalab5/:11:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 4 Define the mse_tips_linear function which computes $R(a, b)$ as the mean squared error on the tips data for a linear model with parameters $a$ and $b$. Notes: This function takes in two parameters a and b. You should use the NumPy arrays x_total_bills and y_tips defined at the beginning of Part B. We‚Äôve included some skeleton code, but feel free to write your own as well. def mse_tips_linear(a, b): \"\"\" Returns average L2 loss between predicted y_hat values (using x_total_bills and parameters a, b) and actual y values (y_tips) \"\"\" y_hats = a + b * x_total_bills return ((y_hats - y_tips) ** 2).mean() ... mse_tips_linear(0.9, 0.1) # arbitrarily pick a = 0.9, b = 0.1 np.float64(1.052336405737705)\rgrader.check(\"q4\") ","date":"2024-08-13","objectID":"/datalab5/:12:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"B.3: Find the $\\theta$ that minimizes risk Similar to before, we‚Äôd like to try out different approaches to finding the optimal parameters $\\hat{a}$ and $\\hat{b}$ that minimize MSE. ","date":"2024-08-13","objectID":"/datalab5/:13:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5: Analytical Solution In lecture, we derived the following expression for the line of best fit: $$\\Large \\hat{y_i} = \\bar{y} + r \\frac{SD(y)}{SD(x)} (x_i - \\bar{x})$$ where $\\bar{x}$, $\\bar{y}$, $SD(x)$, $SD(y)$ correspond to the means and standard deviations of $x$ and $y$, respectively, and $r$ is the correlation coefficient. ","date":"2024-08-13","objectID":"/datalab5/:14:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5a Assign x_bar, y_bar, std_x, std_y, and r, for our dataset. Note: Make sure to use np.std, and not \u003cSeries name\u003e.std(). Hint: Remember, in our case, y is y_tips, and x is x_total_bills. Hint: You may find np.corrcoef (documentation) handy in computing r. Note that the output of np.corrcoef is a matrix, not a number, so you‚Äôll need to collect the correlation coefficient by indexing into the returned matrix. x_bar = x_total_bills.mean() y_bar = y_tips.mean() std_x = np.std(x_total_bills) std_y = np.std(y_tips) r = np.corrcoef(x_total_bills, y_tips)[0, 1] grader.check(\"q5a\") ","date":"2024-08-13","objectID":"/datalab5/:14:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5b Now, set b_hat and a_hat correctly, in terms of the variables you defined above. Hints: Try and match the slope and intercept in $\\hat{y_i} = \\hat{a} + \\hat{b}x_i$ to the slope and intercept in $\\hat{y_i} = \\bar{y} + r \\frac{SD(y)}{SD(x)} (x_i - \\bar{x})$. You may want to define a_hat in terms of b_hat. b_hat = r*std_y/std_x a_hat = y_bar - b_hat*x_bar grader.check(\"q5b\") ","date":"2024-08-13","objectID":"/datalab5/:14:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5c Now, use a_hat and b_hat to implement the predict_tip_linear function, which predicts the tip for a total bill amount of bill. def predict_tip_linear(bill): return a_hat + b_hat * bill # do not edit below this line bill = 20 print(f\"\"\"If you have a ${bill} bill, Part B's modeling process predicts that you will pay a tip of ${predict_tip_linear(bill):.2f}.\"\"\") If you have a $20 bill, Part B's modeling process\rpredicts that you will pay a tip of $3.02.\rgrader.check(\"q5c\") ","date":"2024-08-13","objectID":"/datalab5/:14:3","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Numerically computing $\\hat{\\theta}$ The minimize function we introduced earlier can also minimize functions of multiple variables (useful for numerically computing $\\hat{a}$ and $\\hat{b}$. There‚Äôs one quirk, however, which is that the function has to accept its parameters as a single list. For example, consider the multivariate $f(u, v) = u^2 - 2 u v - 3 v + 2 v^2$. It turns out this function‚Äôs minimum is at $(1.5, 1.5)$. To minimize this function, we create f. # just run this cell def f(theta): u = theta[0] v = theta[1] return u**2 - 2 * u * v - 3 * v + 2 * v**2 minimize(f, x0 = [0.0, 0.0]) message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: -2.2499999999999982\rx: [ 1.500e+00 1.500e+00]\rnit: 3\rjac: [-5.960e-08 0.000e+00]\rhess_inv: [[ 1.000e+00 5.000e-01]\r[ 5.000e-01 5.000e-01]]\rnfev: 12\rnjev: 4\r","date":"2024-08-13","objectID":"/datalab5/:15:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 6: Numerical Solution ","date":"2024-08-13","objectID":"/datalab5/:16:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 6a Implement the mse_tips_linear_list function, which is exactly like mse_tips_linear defined in Question 4 except that it takes in a single list of 2 variables rather than two separate variables. For example mse_tips_linear_list([2, 3]) should return the same value as mse_tips_linear(2, 3). def mse_tips_linear_list(theta): \"\"\" Returns average L2 loss between predicted y_hat values (using x_total_bills and linear params theta) and actual y values (y_tips) \"\"\" y_hat = theta[0] + theta[1] * x_total_bills mse = sum((y_hat - y_tips)**2) / len(y_tips) return mse grader.check(\"q6a\") ","date":"2024-08-13","objectID":"/datalab5/:16:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 6b Now, set min_scipy_linear to the result of calling minimize to optimize the risk function you just implemented. Hint: Make sure to set x0, say, to [0.0, 0.0]. # call minimize with initial x0 = [0.0, 0.0] min_scipy_linear = minimize(mse_tips_linear_list, x0=[0.0, 0.0]) min_scipy_linear message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: 1.036019442011604\rx: [ 9.203e-01 1.050e-01]\rnit: 3\rjac: [ 1.490e-08 0.000e+00]\rhess_inv: [[ 2.980e+00 -1.253e-01]\r[-1.253e-01 6.335e-03]]\rnfev: 15\rnjev: 5\rBased on the above output from your call to minimize, running the following cell will set and print the values of a_hat and b_hat. # just run this cell a_hat_scipy = min_scipy_linear['x'][0] b_hat_scipy = min_scipy_linear['x'][1] a_hat_scipy, b_hat_scipy (np.float64(0.9202707061277714), np.float64(0.1050244640398299))\rThe following cell will print out the values of a_hat and b_hat computed from both methods (‚Äúmanual‚Äù refers to the analytical solution in Question 5; ‚Äúscipy‚Äù refers to the numerical solution in Question 6). If you‚Äôve done everything correctly, these should be very close to one another. # just run this cell print('a_hat_scipy: ', a_hat_scipy) print('a_hat_manual: ', a_hat) print('\\n') print('b_hat_scipy: ', b_hat_scipy) print('b_hat_manual: ', b_hat) a_hat_scipy: 0.9202707061277714\ra_hat_manual: 0.9202696135546735\rb_hat_scipy: 0.1050244640398299\rb_hat_manual: 0.10502451738435334\r","date":"2024-08-13","objectID":"/datalab5/:16:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"‰∫íÂä®ÂèØËßÜÂåñ Visual Solution (not graded): Feel free to interact with the below plot and verify that the $\\hat{a}$ and $\\hat{b}$ you computed using either method above minimize the MSE. In the cell below we plot the mean squared error for different parameter values. Note that now that we have two parameters, we have a 3D MSE surface plot. Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure. If you get an error that your browser does not support webgl, you may need to restart your kernel and/or browser. # just run this cell import itertools import plotly.graph_objects as go a_values = np.linspace(-1, 1, 80) b_values = np.linspace(-1,1, 80) mse_values = [mse_tips_linear(a, b) \\ for a, b in itertools.product(a_values, b_values)] mse_values = np.reshape(mse_values, (len(a_values), len(b_values)), order='F') fig = go.Figure(data=[go.Surface(x=a_values, y=b_values, z=mse_values)]) fig.update_layout( title=r'MSE for different values of $a, b$', autosize=False, scene = dict( xaxis_title='x=a', yaxis_title='y=b', zaxis_title='z=MSE'), width=500, margin=dict(r=20, b=10, l=10, t=10)) fig.show() ","date":"2024-08-13","objectID":"/datalab5/:17:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Comparing Constant Model vs Linear Model At this point, we can actually compare our two models! Both the linear model and constant model were optimized using the same L2 loss function but predict different values for different tips. Run the cell below: sns.scatterplot(x = x_total_bills, y = y_tips, label='observed'); # the below plot expects you've run all of Question 5 plt.plot(x_total_bills, predict_tip_linear(x_total_bills), label='linear', color='g'); # the below function expects you've run the cell right before part B plt.axhline(y=predict_tip_constant(), label='constant', color='m', ls='--'); plt.legend() plt.xlabel(\"total bill\") plt.ylabel(\"tip\") plt.title(\"Tips: Linear vs Constant Models\"); plt.show() Note that while we plot tip by total bill, the constant model doesn‚Äôt use the total bill in its prediction and therefore shows up as a horizontal line. Thought question: For predicting tip on this data, would you rather use the constant model or the linear model, assuming an L2 loss function for both? This might be more fun with a partner. Note, your answer will not be graded, so don‚Äôt worry about writing a detailed answer. If you want to see our answer, see the very end of this lab notebook. In the not-so-distant future of this class, you will learn more quantitative metrics to compare model performance. Stay tuned! ","date":"2024-08-13","objectID":"/datalab5/:17:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Part C: Using a Different Loss Function In this last (short) section, we‚Äôll consider how the optimal parameters for the constant model would change if we used a different loss function. We will now use absolute loss (also known as the $L_1$ loss, pronounced ‚Äúell-one‚Äù). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_1$ loss of: $$\\Large L_1(y, \\hat{y}) = |y - \\hat{y}|$$ While we still define risk as average loss, since we now use $L_1$ loss per datapoint in our datset $\\mathcal{D} = {y_1, \\ldots, y_n}$, our risk is now known as mean absolute error (MAE). For the constant model $\\hat{y} = \\theta$ (i.e., we predict our tip as a summary statistic): \\begin{align} \\Large R\\left(\\theta\\right) \u0026= \\Large \\frac{1}{n} \\sum_{i=1}^n L_1(y_i, \\hat{y_i}) \\ \u0026= \\Large \\frac{1}{n} \\sum_{i=1}^n |y_i - \\theta| \\end{align} Note: the last line results from using the constant model for $\\hat{y}$. If we decided to use the linear model, we would have a different expression. ","date":"2024-08-13","objectID":"/datalab5/:18:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 7 ","date":"2024-08-13","objectID":"/datalab5/:19:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 7a Define the mae_tips_constant function which computes $R(\\theta)$ as the mean absolute error (MAE) on the tips data for a constant model with parameter $\\theta$. Hint: You may want to check out your solution from Question 2, which computed mean squared error (MSE). def mae_tips_constant(theta): data = y_tips return np.mean(np.abs(data - theta)) mae_tips_constant(5.3) # arbitrarily pick theta = 5.3 np.float64(2.4527868852459016)\rgrader.check(\"q7\") ","date":"2024-08-13","objectID":"/datalab5/:19:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 7b In lecture, we saw that the value of theta that minimizes mean absolute error for the constant model is the median of the data. Assign min_computed_mae to the median of the observed y_tips data. min_computed_mae = np.median(y_tips) min_computed_mae np.float64(2.9)\rgrader.check(\"q7b\") ","date":"2024-08-13","objectID":"/datalab5/:19:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Comparing MAE to MSE Now run the below cell to compare MAE with MSE on the constant model. # just run this cell fig, ax = plt.subplots(nrows=2, figsize=((6, 8))) theta_values = np.linspace(0, 6, 100) mse = [mse_tips_constant(theta) for theta in theta_values] ax[0].plot(theta_values, mse) ax[0].axvline(x=min_computed, linewidth=4, color='k', ls='--', label=r'$\\hat{\\theta}$') ax[0].legend() ax[0].set_ylabel(\"avg L2 loss (MSE)\") mae = [mae_tips_constant(theta) for theta in theta_values] ax[1].plot(theta_values, mae, color='orange') ax[1].axvline(x=min_computed_mae, linewidth=4, color='k', ls='--', label=r'$\\hat{\\theta}$') ax[1].legend() ax[1].set_ylabel(\"avg L1 loss (MAE)\") ax[1].set_xlabel(r'$\\theta$'); fig.suptitle(r\"MAE vs MSE (constant model) for different values of $\\theta$\"); Thought question You should see that the MAE plot (below) looks somewhat similar the MSE plot (above). Try to identify any key differences you observe and write them down below. This might be more fun with a partner. Note, your answer will not be graded, so don‚Äôt worry about writing a detailed answer. If you want to see our answer, see the very end of this lab notebook. Write your answer here, replacing this text. Congratulations! You finished the lab! Extra Notes Our Observations on Constant Model vs Linear Model Earlier in this lab, we said we‚Äôd describe our observations about whether to use Constant Model or Linear Model (both trained with MSE). Here are some thoughts: Recall that $r$ is the correlation coefficient, where values closer to -1 or 1 imply a very linear relationship: # you computed this in Q5a r np.float64(0.6757341092113641)\rThe relationship between $x$ and $y$ is somewhat linear; you can see this more clearly through the scatter plot, where there are many points that don‚Äôt fall close to the linear model line. With this in mind: The linear model seems to work well for most bills. However, as bills get bigger, some datapoints seem to suggest that the constant model works better. In the wild, a tip predictor may use a combination of both the constant and linear models we trained: an average prediction, or a random coin flip to pick the model, or some heuristic decision to choose one model if the total bill exceeds a certain threshold. ÔºàÈõÜÊàêÂ≠¶‰π†ÊÄùÊÉ≥ÔºüÔºâ In the not-so-distant future of this class, you will learn more quantitative metrics to compare model performance. You will have an opportunity to explore your own models in a future assignment! Our Observations on Differences Between MAE vs. MSE Earlier in this lab, we said we‚Äôd describe our observations about the differences between the MAE and MSE. There are three key differences that we identified between the plots of the MSE and MAE. The minimizing $\\theta = \\hat{\\theta}$ is different. The plot for MAE increases linearly instead of quadratically as we move far away from the minimizing $\\theta$. The plot for MAE is piecewise linear instead of smooth. Each change in slope happens at the same $\\theta$ value as a data point in our dataset. ","date":"2024-08-13","objectID":"/datalab5/:19:3","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab06.ipynb\") Lab 6: Linear Regression ","date":"2024-08-13","objectID":"/datalab6/:0:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Objectives In this lab, you will review the details of linear regresison as described in Lectures 10 and 11. In particular: Matrix formulation and solution to Ordinary Least Squares sns.lmplot as a quick visual for simple linear regression scikit-learn, a real world data science tool that is more robust and flexible than analytical/scipy.optimize solutions You will also practice interpreting residual plots (vs. fitted values) and the Multiple $R^2$ metric used in Multiple Linear Regression. For the first part of this lab, you will predict fuel efficiency (mpg) of several models of automobiles using a single feature: engine power (horsepower). For the second part, you will perform feature engineering on multiple features to better predict fuel efficiency. First, let‚Äôs load in the data. # Run this cell import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # Here, we load the fuel dataset, and drop any rows that have missing data vehicle_data = sns.load_dataset('mpg').dropna() vehicle_data = vehicle_data.sort_values('horsepower', ascending=True) vehicle_data.head(5) mpg\rcylinders\rdisplacement\rhorsepower\rweight\racceleration\rmodel_year\rorigin\rname\r102\r26.0\r4\r97.0\r46.0\r1950\r21.0\r73\reurope\rvolkswagen super beetle\r19\r26.0\r4\r97.0\r46.0\r1835\r20.5\r70\reurope\rvolkswagen 1131 deluxe sedan\r325\r44.3\r4\r90.0\r48.0\r2085\r21.7\r80\reurope\rvw rabbit c (diesel)\r326\r43.4\r4\r90.0\r48.0\r2335\r23.7\r80\reurope\rvw dasher (diesel)\r244\r43.1\r4\r90.0\r48.0\r1985\r21.5\r78\reurope\rvolkswagen rabbit custom diesel\rWe have 392 datapoints and 8 potential features (plus our observations, mpg). vehicle_data.shape (392, 9)\rLet us try to fit a line to the below plot, which shows mpg vs. horsepower for several models of automobiles. # just run this cell sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data); ","date":"2024-08-13","objectID":"/datalab6/:1:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1: Ordinary Least Squares Instead of using the SLR formulation, in this lab we will practice linear algebra with Ordinary Least Squares. Recall that the Simple Linear Regression model is written as follows: $$\\hat{y} = \\theta_0 + \\theta_1 x$$ We now use $\\theta = (\\theta_0, \\theta_1)$ so that the formulation more closely matches our multiple linear regression model: $$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_p x_p$$ We can rewrite our multiple linear regression model using matrix notation. Let $\\mathbb{Y}$ be a vector of all $n$ observations in our sample. Then our prediction vector $\\hat{\\mathbb{Y}}$ is $$\\Large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$ where $\\mathbb{X}$ is the design matrix representing the $p$ features for all $n$ datapoints in our sample. Note that for our SLR model, $p = 1$ and therefore the matrix notation seems rather silly. Nevertheless it is valuable to start small and build on our intuition. ","date":"2024-08-13","objectID":"/datalab6/:2:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1a: Construct $\\mathbb{X}$ with an intercept term Because we have an intercept term $\\theta_0$ in our parameter vector $\\theta$, our design matrix $\\mathbb{X}$ for $p$ features actually has dimension $$ \\Large \\mathbb{X} \\in \\mathbb{R}^{n \\times (p + 1)}$$ Therefore, the resulting matrix expression $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$ represents $n$ linear equations, where equation $i$ is $\\hat{y_i} = \\theta_0 \\cdot 1 + \\theta_1 \\cdot x_1 + \\dots + \\theta_p x_p$. The constant all-ones column of $\\mathbb{X}$ is sometimes called the bias feature; $\\theta_0$ is frequently called the bias or intercept term. Below, implement add_intercept, which computes a design matrix such that the first (left-most) column is all ones. The function has two lines: you are responsible for constructing the all-ones column bias_feature using the np.ones function (NumPy documentation). This is then piped into a call to np.concatenate (documentation), which we‚Äôve implemented for you. Note: bias_feature should be a matrix of dimension (n,1), not a vector of dimension (n,). def add_intercept(X): \"\"\" Return X with a bias feature. Parameters ----------- X: a 2D dataframe of p numeric features (may also be a 2D numpy array) of shape n x p Returns ----------- A 2D matrix of shape n x (p + 1), where the leftmost column is a column vector of 1's \"\"\" bias_feature = np.ones((X.shape[0], 1)) return np.concatenate([bias_feature, X], axis=1) # Áü©ÈòµÊãºÊé• # Note the [[ ]] brackets below: the argument needs to be # a matrix (DataFrame), as opposed to a single array (Series). X = add_intercept(vehicle_data[['horsepower']]) X.shape (392, 2)\rgrader.check(\"q1a\") ","date":"2024-08-13","objectID":"/datalab6/:2:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1b: Define the OLS Model The predictions for all $n$ points in our data are (note $\\theta = (\\theta_0, \\theta_1, \\dots, \\theta_p)$) : $$ \\Large \\hat{\\mathbb{Y}} = \\mathbb{X}\\theta $$ Below, implement the linear_model function to evaluate this product. Hint: You can use np.dot, pd.DataFrame.dot, or the @ operator to multiply matrices/vectors. However, while the @ operator can be used to multiply numpy arrays, it generally will not work between two pandas objects, so keep that in mind when computing matrix-vector products! def linear_model(thetas, X): \"\"\" Return the linear combination of thetas and features as defined above. Parameters ----------- thetas: a 1D vector representing the parameters of our model ([theta1, theta2, ...]) X: a 2D dataframe of numeric features (may also be a 2D numpy array) Returns ----------- A 1D vector representing the linear combination of thetas and features as defined above. \"\"\" return np.dot(X, thetas) grader.check(\"q1b\") ","date":"2024-08-13","objectID":"/datalab6/:2:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1c: Least Squares Estimate, Analytically, Â∏∏ËßÅÁöÑÁü©ÈòµÊìç‰Ωú Recall from lecture that Ordinary Least Squares is when we fit a linear model with mean squared error, which is equivalent to the following optimization problem: $$\\Large \\min_{\\theta} ||\\Bbb{X}\\theta - \\Bbb{Y}||^2$$ We showed in Lecture that the optimal estimate $\\hat{\\theta}$ when $X^TX$ is invertible is given by the equation: $$ \\Large \\hat{\\theta} = (\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$$ Below, implement the analytic solution to $\\hat{\\theta}$ using np.linalg.inv (link) to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$. Reminder: To compute the transpose of a matrix, you can use X.T or X.transpose() (link). Note: You can also consider using np.linalg.solve (link) instead of np.linalg.inv because it is more robust (more on StackOverflow here). def get_analytical_sol(X, y): \"\"\" Computes the analytical solution to our least squares problem Parameters ----------- X: a 2D dataframe (or numpy array) of numeric features y: a 1D vector of tip amounts Returns ----------- The estimate for theta (a 1D vector) computed using the equation mentioned above. \"\"\" return np.linalg.inv(X.T @ X) @ X.T @ y Y = vehicle_data['mpg'] analytical_thetas = get_analytical_sol(X, Y) analytical_thetas array([39.93586102, -0.15784473])\rgrader.check(\"q1c\") Now, let‚Äôs analyze our model‚Äôs performance. Your task will be to interpret the model‚Äôs performance using the two visualizations and one performance metric we‚Äôve implemented below. First, we run sns.lmplot, which will both provide a scatterplot of mpg vs horsepower and display the least-squares line of best fit. (If you‚Äôd like to verify the OLS fit you found above is the same line found through Seaborn, change include_OLS to True.) include_OLS = True # change this flag to visualize OLS fit sns.lmplot(x='horsepower', y='mpg', data=vehicle_data); predicted_mpg_hp_only = linear_model(analytical_thetas, X) if include_OLS: # if flag is on, add OLS fit as a dotted red line plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, 'r--') Next, we plot the residuals. While in Simple Linear Regression we have the option to plot residuals vs. the single input feature, in Multiple Linear Regression we often plot residuals vs fitted values $\\hat{\\mathbb{Y}}$. In this lab, we opt for the latter. plt.scatter(predicted_mpg_hp_only, Y - predicted_mpg_hp_only) plt.axhline(0, c='black', linewidth=1) plt.xlabel(r'Fitted Values $\\hat{\\mathbb{Y}}$') plt.ylabel(r'Residuals $\\mathbb{Y} - \\hat{\\mathbb{Y}}$'); Finally, we compute the Multiple $R^2$ metric. As described in Lecture 11 (link), $$R^2 = \\frac{\\text{variance of fitted values}}{\\text{variance of true } y} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$$ $R^2$ can be used in the multiple regression setting, whereas $r$ (the correlation coefficient) is restricted to SLR since it depends on a single input feature. In SLR, $r^{2}$ and Multiple $R^{2}$ are equivalent; the proof is left to you. r2_hp_only = np.var(predicted_mpg_hp_only) / np.var(Y) print('Multiple R^2 using only horsepower: ', r2_hp_only) Multiple R^2 using only horsepower: 0.6059482578894348\r","date":"2024-08-13","objectID":"/datalab6/:2:3","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1d In the cell below, comment on the above visualization and performance metrics, and whether horsepower and mpg have a good linear fit. poor performance ","date":"2024-08-13","objectID":"/datalab6/:2:4","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 2: Transform a Single Feature The Tukey-Mosteller Bulge Diagram tells us to transform our $\\mathbb{X}$ or $\\mathbb{Y}$ to find a linear fit. Let‚Äôs consider the following linear model: $$\\text{predicted mpg} = \\theta_0 + \\theta_1 \\sqrt{\\text{horsepower}}$$ ","date":"2024-08-13","objectID":"/datalab6/:3:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 2a In the cell below, explain why we use the term ‚Äúlinear‚Äù to describe the model above, even though it incorporates a square-root of horsepower as a feature. Ê≥õÂåñÁ∫øÊÄßÊ®°Âûã ","date":"2024-08-13","objectID":"/datalab6/:3:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Introduction to sklearn Yet another way to fit a linear regression model is to use scikit learn, an industry standard package for machine learning applications. Because it is application-specific, sklearn is often faster and more robust than the analytical/scipy-based computation methods we‚Äôve used thus far. To use sklearn: Create an sklearn object fit the object to data Analyze fit or call predict. 1. Create object. We first create a LinearRegression object. Here‚Äôs the sklearn documentation. Note that by default, the object will include an intercept term when fitting. Here, model is like a ‚Äúblank slate‚Äù for a linear model. # 1. just run this cell from sklearn.linear_model import LinearRegression model = LinearRegression(fit_intercept=True) 2. fit the object to data. Now, we need to tell model to ‚Äúfit‚Äù itself to the data. Essentially, this is doing exactly what you did in the previous part of this lab (creating a risk function and finding the parameters that minimize that risk). Note: X needs to be a matrix (or DataFrame), as opposed to a single array (or Series). This is because sklearn.linear_model is robust enough to be used for multiple regression, which we will look at later this lab. # 2. run this cell to add sqrt(hp) column for each car in the dataset vehicle_data['sqrt(hp)'] = np.sqrt(vehicle_data['horsepower']) vehicle_data.head() mpg\rcylinders\rdisplacement\rhorsepower\rweight\racceleration\rmodel_year\rorigin\rname\rsqrt(hp)\r102\r26.0\r4\r97.0\r46.0\r1950\r21.0\r73\reurope\rvolkswagen super beetle\r6.782330\r19\r26.0\r4\r97.0\r46.0\r1835\r20.5\r70\reurope\rvolkswagen 1131 deluxe sedan\r6.782330\r325\r44.3\r4\r90.0\r48.0\r2085\r21.7\r80\reurope\rvw rabbit c (diesel)\r6.928203\r326\r43.4\r4\r90.0\r48.0\r2335\r23.7\r80\reurope\rvw dasher (diesel)\r6.928203\r244\r43.1\r4\r90.0\r48.0\r1985\r21.5\r78\reurope\rvolkswagen rabbit custom diesel\r6.928203\r# 2. run this cell model.fit(X = vehicle_data[['sqrt(hp)']], y= vehicle_data['mpg']) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() 3. Analyze fit. Now that the model exists, we can look at the $\\hat{\\theta_0}$ and $\\hat{\\theta_1}$ values it found, which are given in the attributes intercept and coef, respectively. model.intercept_ np.float64(58.705172037217466)\rmodel.coef_ array([-3.50352375])\r3 (continued). Call predict. To use the scikit-learn linear regression model to make predictions, you can use the model.predict method. Below, we find the estimated mpg for a single datapoint with a sqrt(hp) of 6.78 (i.e., horsepower 46). Note that unlike the linear algebra approach, we do not need to manually add an intercept term, because our model (which was created with fit_intercept=True) will auto-add one. single_datapoint = [[6.78]] # needs to be a 2D array since the X in step 2 was a 2D array. [[ ]] trick model.predict(single_datapoint) d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\rwarnings.warn(\rarray([34.95128104])\r","date":"2024-08-13","objectID":"/datalab6/:3:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 2b Using the model defined above, set predicted_mpg to the predicted mpg for the data below. Running the cell will then compute the multiple $R^2$ value and create a linear regression plot for this new square root feature, overlaid on the original least squares estimate (used in Question 1c). predicted_mpg_hp_sqrt = model.predict(vehicle_data[['sqrt(hp)']]) # do not modify below this line r2_hp_sqrt = np.var(predicted_mpg_hp_sqrt) / np.var(vehicle_data['mpg']) print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt) sns.lmplot(x='horsepower', y='mpg', data=vehicle_data) plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit'); plt.legend(); Multiple R^2 using sqrt(hp): 0.6437035832706468\rThe visualization shows a slight improvement, but note that the underlying pattern is parabolic‚Äìsuggesting that perhaps we should try a quadratic feature. Next, we use the power of multiple linear regression to add an additional feature. ","date":"2024-08-13","objectID":"/datalab6/:3:3","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Add an Additional Feature For the second part of this lab, we move from SLR to multiple linear regression. Until now, we have established relationships between one independent explanatory variable and one response variable. However, with real-world problems you will often want to use multiple features to model and predict a response variable. Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to the observed data. We can consider including functions of existing features as new features to help improve the predictive power of our model. (This is something we will discuss in further detail in the Feature Engineering lecture.) The cell below adds a column which contains the square of the horsepower for each car in the dataset. # just run this cell vehicle_data['hp^2'] = vehicle_data['horsepower'] ** 2 vehicle_data.head() mpg\rcylinders\rdisplacement\rhorsepower\rweight\racceleration\rmodel_year\rorigin\rname\rsqrt(hp)\rhp^2\r102\r26.0\r4\r97.0\r46.0\r1950\r21.0\r73\reurope\rvolkswagen super beetle\r6.782330\r2116.0\r19\r26.0\r4\r97.0\r46.0\r1835\r20.5\r70\reurope\rvolkswagen 1131 deluxe sedan\r6.782330\r2116.0\r325\r44.3\r4\r90.0\r48.0\r2085\r21.7\r80\reurope\rvw rabbit c (diesel)\r6.928203\r2304.0\r326\r43.4\r4\r90.0\r48.0\r2335\r23.7\r80\reurope\rvw dasher (diesel)\r6.928203\r2304.0\r244\r43.1\r4\r90.0\r48.0\r1985\r21.5\r78\reurope\rvolkswagen rabbit custom diesel\r6.928203\r2304.0\r","date":"2024-08-13","objectID":"/datalab6/:4:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3 ","date":"2024-08-13","objectID":"/datalab6/:5:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3a Using scikit learn‚Äôs LinearRegression, create and fit a model that tries to predict mpg from horsepower AND hp^2 using the DataFrame vehicle_data. Name your model model_multi. Hint: We did something very similar in Question 2. model_multi = LinearRegression() # by default, fit_intercept=True model_multi.fit(X = vehicle_data[['horsepower', 'hp^2']], y= vehicle_data['mpg']) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() grader.check(\"q3a\") After fitting, we can see the coefficients and intercept. Note, there are now two elements in model_multi.coef_, since there are two features. model_multi.intercept_ np.float64(56.90009970211301)\rmodel_multi.coef_ array([-0.46618963, 0.00123054])\r","date":"2024-08-13","objectID":"/datalab6/:5:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3b Using the above values, in LaTeX, write out the function that the model is using to predict mpg from horsepower and hp^2. $$ mpg_{predicted} = 56.90 - 0.47 \\times horsepower + 0.001 \\times hp^2 $$ The plot below shows the prediction of our model. It‚Äôs much better! # just run this cell predicted_mpg_multi = model_multi.predict(vehicle_data[['horsepower', 'hp^2']]) r2_multi = np.var(predicted_mpg_multi) / np.var(vehicle_data['mpg']) print('Multiple R^2 using both horsepower and horsepower squared: ', r2_multi) sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data) plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, label='hp only'); plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit'); plt.plot(vehicle_data['horsepower'], predicted_mpg_multi, color = 'gold', linewidth=2, label='hp and hp^2'); plt.legend(); Multiple R^2 using both horsepower and horsepower squared: 0.6875590305127548\r","date":"2024-08-13","objectID":"/datalab6/:5:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3c In the cell below, we assign the mean of the mpg column of the vehicle data dataframe to mean_mpg. Given this information, what is the mean of the mean_predicted_mpg_hp_only, predicted_mpg_hp_sqrt, and predicted_mpg_multi arrays? Hint: You should not have to call np.mean in your code. mean_mpg = np.mean(vehicle_data['mpg']) mean_predicted_mpg_hp_only = mean_mpg # ÊúÄÂ∞è‰∫å‰πòÊÄßË¥®ÂÜ≥ÂÆöÁöÑ y_bar = a + b * x_bar mean_predicted_mpg_hp_sqrt = mean_mpg mean_predicted_mpg_multi = mean_mpg # print(np.mean(predicted_mpg_hp_sqrt)) # print(mean_mpg) # print(np.mean(predicted_mpg_multi)) 23.445918367346934\r23.445918367346938\r23.445918367346938\rgrader.check(\"q3c\") ","date":"2024-08-13","objectID":"/datalab6/:5:3","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Faulty Feature Engineering: Redundant Features Suppose we used the following linear model: \\begin{align} \\text{mpg} \u0026= \\theta_0 + \\theta_1 \\cdot \\text{horsepower} + \\ \u0026\\theta_2 \\cdot \\text{horsepower}^2 + \\theta_3 \\cdot \\text{horsepower} \\end{align} Notice that horsepower appears twice in our model!! We will explore how this redundant feature affects our modeling. ","date":"2024-08-13","objectID":"/datalab6/:6:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 4 ","date":"2024-08-13","objectID":"/datalab6/:7:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 4a: Linear Algebra Construct a matrix X_redundant that uses the vehicle data DataFrame to encode the ‚Äúthree‚Äù features above, as well as a bias feature. Hint: Use the add_intercept term you implemented in Question 1a. X_redundant = add_intercept(vehicle_data[['horsepower', 'hp^2', 'horsepower']]) X_redundant.shape (392, 4)\rgrader.check(\"q4a\") q4a passed! üöÄ Now, run the cell below to find the analytical OLS Estimate using the get_analytical_sol function you wrote in Question 1c. Depending on the machine that you run your code on, you should either see a singular matrix error or end up with thetas that are nonsensical (magnitudes greater than 10^15). This is not good! # just run this cell # the try-except block suppresses errors during submission import traceback try: analytical_thetas = get_analytical_sol(X_redundant, vehicle_data['mpg']) analytical_thetas except Exception as e: print(traceback.format_exc()) ","date":"2024-08-13","objectID":"/datalab6/:7:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 4b In the cell below, explain why we got the error above when trying to calculate the analytical solution to predict mpg. Ëß£ÊûêÊñπÊ≥ï‰∏ç‰∏ÄÂÆöÊ≠£Á°ÆÔºåÊØîËæÉÁêÜÊÉ≥‰ΩÜ‰∏çÁé∞ÂÆûÔºà‰ΩÜÊòØ‰∏äÈù¢Âπ∂Ê≤°ÊúâerrorËØ∂Ôºâ Note: While we encountered errors when using the linear algebra approach, a model fitted with `sklearn` will not encounter matrix singularity errors since it uses numerical methods to find optimums (to be covered in Gradient Descent lecture).\r# just run this cell # sklearn finds optimal parameters despite redundant features model_redundant = LinearRegression(fit_intercept=False) # X_redundant already has an intercept column model_redundant.fit(X = X_redundant, y = vehicle_data['mpg']) model_redundant.coef_ array([ 5.69000997e+01, -2.33094815e-01, 1.23053610e-03, -2.33094815e-01])\r","date":"2024-08-13","objectID":"/datalab6/:7:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Overfitting with Too Many Features Let‚Äôs take what we‚Äôve learned so far and go one step further: introduce even more features. Again, using scikit learn‚Äôs LinearRegression, we fit a model that tries to predict mpg using each of the following as features: horsepower hp^2 model_year acceleration # just run this cell desired_columns = ['horsepower', 'hp^2', 'model_year', 'acceleration'] model_overfit = LinearRegression() model_overfit.fit(X = vehicle_data[desired_columns], y= vehicle_data['mpg']) predicted_mpg_overfit = model_overfit.predict(vehicle_data[['horsepower', 'hp^2', 'model_year', 'acceleration']]) The plot below shows the prediction of our more sophisticated model. Note we arbitrarily plot against horsepower for the ease of keeping our plots 2-dimensional.\r# just run this cell sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data) plt.plot(vehicle_data['horsepower'], predicted_mpg_overfit, color = 'r'); Think about what you see in the above plot. Why is the shape of our prediction curve so jagged? Do you think this is a good model to predict the mpg of some car we don‚Äôt already have information on? This idea ‚Äìthe bias-variance tradeoff‚Äì is an idea we will explore in the coming weeks. ","date":"2024-08-13","objectID":"/datalab6/:8:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 5: Comparing $R^2$ Lastly, set r2_overfit to be the multiple $R^2$ coefficient obtained by using model_overfit. Hint: This is very similar to several pre-computed cells in Questions 1c, 2b, and 3b. r2_overfit = np.var(predicted_mpg_overfit) / np.var(vehicle_data['mpg']) r2_overfit np.float64(0.8163086433998654)\rgrader.check(\"q5\") Comparing this model with previous models: # just run this cell # compares q1, q2, q3, and overfit models (ignores redundant model) print('Multiple R^2 using only horsepower: ', r2_hp_only) print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt) print('Multiple R^2 using both hp and hp^2: ', r2_multi) print('Multiple R^2 using hp, hp^2, model year, and acceleration: ', r2_overfit) Multiple R^2 using only horsepower: 0.6059482578894348\rMultiple R^2 using sqrt(hp): 0.6437035832706468\rMultiple R^2 using both hp and hp^2: 0.6875590305127548\rMultiple R^2 using hp, hp^2, model year, and acceleration: 0.8163086433998654\rIf everything was done correctly, the multiple $R^2$ of our latest model should be substantially higher than that of the previous models. This is because multiple $R^2$ increases with the number of covariates (i.e., features) we add to our model. A Word on Overfitting: We might not always want to use models with large multiple $R^2$ values because these models could be overfitting to our specific sample data, and won‚Äôt generalize well to unseen data from the population. Again, this is an idea we will explore in future lectures and assignments. Congratulations! You finished the lab! ","date":"2024-08-13","objectID":"/datalab6/:9:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab03.ipynb\") Lab 3: Data Cleaning and EDA In this lab you will be working on visualizing a dataset from the City of Berkeley containing data on calls to the Berkeley Police Department. Information about the dataset can be found at this link. ","date":"2024-08-13","objectID":"/datalab3/:0:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Content Warning This lab includes an analysis of crime in Berkeley. If you feel uncomfortable with this topic, please contact your GSI or the instructors. ","date":"2024-08-13","objectID":"/datalab3/:0:1","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Setup In this lab, we‚Äôll perform Exploratory Data Analysis and learn some preliminary tips for working with matplotlib (a Python plotting library). Note that we configure a custom default figure size. Virtually every default aspect of matplotlib can be customized. Collaborators: list names here import pandas as pd import numpy as np import zipfile import matplotlib import matplotlib.pyplot as plt plt.rcParams['figure.figsize'] = (12, 9) fig = plt.figure() plt.show(fig) \u003cFigure size 1200x900 with 0 Axes\u003e\rPart 1: Acquire the Data 1. Obtain data To retrieve the dataset, we will use the ds100_utils.fetch_and_cache utility. # just run this cell import ds100_utils data_dir = 'data' data_url = 'http://www.ds100.org/sp22/resources/assets/datasets/lab03_data_sp22.zip' file_name = 'lab03_data_sp22.zip' dest_path = ds100_utils.fetch_and_cache(data_url=data_url, file=file_name, data_dir=data_dir) print(f'Located at {dest_path}') Using cached version that was downloaded (UTC): Sun Jul 28 01:04:24 2024\rLocated at data\\lab03_data_sp22.zip\r2. Unzip file We will now directly unzip the ZIP archive and start working with the uncompressed files. # just run this cell my_zip = zipfile.ZipFile(dest_path, 'r') my_zip.extractall(data_dir) Note: There is no single right answer regarding whether to work with compressed files in their compressed state or to uncompress them on disk permanently. For example, if you need to work with multiple tools on the same files, or write many notebooks to analyze them‚Äîand they are not too large‚Äîit may be more convenient to uncompress them once. But you may also have situations where you find it preferable to work with the compressed data directly. Python gives you tools for both approaches, and you should know how to perform both tasks in order to choose the one that best suits the problem at hand. 3. View files Now, we‚Äôll use the os package to list all files in the data directory. os.walk() recursively traverses the directory, and os.path.join() creates the full pathname of each file. If you‚Äôre interested in learning more, check out the Python3 documentation pages for os.walk (link) and os.path (link). We use Python 3 format strings to nicely format the printed variables dpath and fpath. # just run this cell # two for loop in the same time... ? not that funny yet? import os for root, directories, filenames in os.walk(data_dir): # first, print out all directories ---\u003e \"secret\" for directory in directories: dpath = os.path.join(root, directory) print(f\"d {dpath}\") # next, print out all files for filename in filenames: fpath = os.path.join(root,filename) print(f\" {fpath}\") d data\\secret\rdata\\ben_kurtovic.py\rdata\\Berkeley_PD_-_Calls_for_Service.csv\rdata\\dummy.txt\rdata\\hello_world.py\rdata\\lab03_data_sp22.zip\rdata\\secret\\do_not_readme.md\rIn this Lab, we‚Äôll be working with the Berkeley_PD_-_Calls_for_Service.csv file. Feel free to check out the other files, though. Part 2: Clean and Explore the Data Let‚Äôs now load the CSV file we have into a pandas.DataFrame object and start exploring the data. # just run this cell calls = pd.read_csv(\"data/Berkeley_PD_-_Calls_for_Service.csv\") calls.head() CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\r0\r21014296\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:58\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\r1\r21014391\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:38\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\r2\r21090494\rTHEFT MISD. (UNDER $950)\r04/19/2021 12:00:00 AM\r12:15\rLARCENY\r1\r06/15/2021 12:00:00 AM\r2100 BLOCK HASTE ST\\nBerkeley, CA\\n(37.864908,...\r2100 BLOCK HASTE ST\rBerkeley\rCA\r3\r21090204\rTHEFT FELONY (OVER $950)\r02/13/2021 12:00:00 AM\r17:00\rLARCENY\r6\r06/15/2021 12:00:00 AM\r2600 BLOCK WARRING ST\\nBerkeley, CA\\n(37.86393...\r2600 BLOCK WARRING ST\rBerkeley\rCA\r4\r21090179\rBURGLARY AUTO\r02/08/2021 12:00:00 AM\r6:20\rBURGLARY - VEHICLE\r1\r06/15/2021 12:00:00","date":"2024-08-13","objectID":"/datalab3/:1:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 1 In the cell below, set answer1 equal to a list of strings corresponding to the possible values (which means unique ü§î ) for OFFENSE when CVLEGEND is ‚ÄúLARCENY‚Äù. You can type the answer manually, or you can create an expression that automatically extracts the names. answer1 = list(calls[calls['CVLEGEND'] == 'LARCENY']['OFFENSE'].unique()) answer1 ['THEFT MISD. (UNDER $950)', 'THEFT FELONY (OVER $950)', 'THEFT FROM PERSON']\rgrader.check(\"q1\") q1 passed! ‚ú® Part 3: Visualize the Data ","date":"2024-08-13","objectID":"/datalab3/:2:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Matplotlib demo You‚Äôve seen some matplotlib in this class already, but now we will explain how to work with the object-oriented plotting API mentioned in this matplotlib.pyplot tutorial useful. In matplotlib, plotting occurs on a set of Axes which are associated with a Figure. An analogy is that on a blank canvas (Figure), you choose a location to plot (Axes) and then fill it in (plot). There are two approaches to labeling and manipulating figure contents, which we‚Äôll discuss below. Approach 1 is closest to the plotting paradigm of MATLAB, the namesake of matplotlib; Approach 2 is also common because many matplotlib-based packages (such as Seaborn) explicitly return the current set of axes after plotting data. Both are essentially equivalent, and at the end of this class you‚Äôll be comfortable with both. Approach 1: matplotlib (or Seaborn) will auto-plot onto the current set of Axes or (if none exists) create a new figure/set of default axes. You can plot data using methods from plt, which is shorthand for the matplotlib.pyplot package. Then subsequent plt calls all edit the same set of default-created axes. Approach 2: After creating the initial plot, you can also use plt.gca() to explicitly get the current set of axes, and then edit those specific axes using axes methods. Note the method naming is slightly different! As an example of the built-in plotting functionality of pandas, the following example uses plot method of the Series class to generate a barh plot type to visually display the value counts for CVLEGEND. There are also many other plots that we will explore throughout the lab. Side note: Pandas also offers basic functionality for plotting. For example, the DataFrame and Series classes both have a plot method, which uses matplotlib under the hood. For now we‚Äôll focus on matplotlib itself so you get used to the syntax, but just know that convenient Pandas plotting methods exist for your own future data science exploration. Below, we show both approaches by generating a horizontal bar plot to visually display the value counts for CVLEGEND. See the barhdocumentation for more details. # DEMO CELL: assign demo to 1 or 2. demo = 1 calls_cvlegend = calls['CVLEGEND'].value_counts() if demo == 1: plt.barh(calls_cvlegend.index, calls_cvlegend) # creates figure and axes (y,x) not (x,y)! print(f\"Demo {demo}: Using plt methods to update plot\") plt.ylabel(\"Crime Category\") # uses most recently plotted axes plt.xlabel(\"Number of Calls\") plt.title(\"Number of Calls by Crime Type\") elif demo == 2: print(f\"Demo {demo}: Using axes methods to update plot\") plt.barh(calls_cvlegend.index, calls_cvlegend) # creates figure and axes ax = plt.gca() ax.set_ylabel(\"Crime Category\") ax.set_xlabel(\"Number of Calls\") ax.set_title(\"Axes methods: Number of Calls by Crime Type\") else: print(\"Error: Please assign the demo variable to 1 or 2.\") plt.show() Demo 1: Using plt methods to update plot\r","date":"2024-08-13","objectID":"/datalab3/:2:1","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"An Additional Note on Plotting in Jupyter Notebooks You may have noticed that many of our plotting code cells end with a semicolon ; or plt.show(). The former prevents any extra output from the last line of the cell; the latter explicitly returns (and outputs) the figure. Try adding this to your own code in the following questions! ","date":"2024-08-13","objectID":"/datalab3/:2:2","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2 Now it is your turn to make a plot using matplotlib. Let‚Äôs start by transforming the data so that it is easier to work with. The CVDOW field isn‚Äôt named helpfully and it is hard to see the meaning from the data alone. According to the website linked at the top of this notebook, CVDOW is actually indicating the day that events happened. 0-\u003eSunday, 1-\u003eMonday ‚Ä¶ 6-\u003eSaturday. ","date":"2024-08-13","objectID":"/datalab3/:3:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2a Add a new column Day into the calls dataframe that has the string weekday (eg. ‚ÄòSunday‚Äô) for the corresponding value in CVDOW. For example, if the first 3 values of CVDOW are [3, 6, 0], then the first 3 values of the Day column should be [\"Wednesday\", \"Saturday\", \"Sunday\"]. Hint: Try using the Series.map function on calls[\"CVDOW\"]. Can you assign this to the new column calls[\"Day\"]? days = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"] day_indices = range(7) indices_to_days_dict = dict(zip(day_indices, days)) # Should look like {0:\"Sunday\", 1:\"Monday\", ..., 6:\"Saturday\"} calls[\"Day\"] = calls[\"CVDOW\"].map(indices_to_days_dict) grader.check(\"q2a\") q2a passed! üåü # just run this example cell ax = calls['CVLEGEND'].value_counts().plot(kind='barh') ax.set_ylabel(\"Crime Category\") ax.set_xlabel(\"Number of Calls\") ax.set_title(\"Number of Calls By Crime Type\"); Challenge (OPTIONAL): You could also accomplish this part as a table left join with pd.merge (documentation), instead of using Series.map. You would need to merge calls with a new dataframe that just contains the days of the week. If you have time, try it out in the below cell! mergeÊìç‰ΩúÂú®Êé•‰∏ãÊù•ÁöÑlabÊúâËØ¶ÁªÜ‰ªãÁªçÔºÅüòâ # scratch space for optional challenge dow_df = pd.DataFrame(days, columns=[\"Day\"]) ... Ellipsis\r","date":"2024-08-13","objectID":"/datalab3/:4:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2b Now let‚Äôs look at the EVENTTM column which indicates the time for events. Since it contains hour and minute information, let‚Äôs extract the hour info and create a new column named Hour in the calls dataframe. You should save the hour as an int. Hint: Your code should only require one line. Hint 2: The vectorized Series.str[ind] performs integer indexing on an array entry. calls[\"Hour\"] = calls[\"EVENTTM\"].str.split(\" \").str[0].str.split(\":\").str[0].astype(int) calls[\"Hour\"] 0 10\r1 10\r2 12\r3 17\r4 6\r..\r2627 12\r2628 15\r2629 0\r2630 18\r2631 2\rName: Hour, Length: 2632, dtype: int64\rgrader.check(\"q2b\") ","date":"2024-08-13","objectID":"/datalab3/:5:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2c Using matplotlib, construct a line plot with the count of the number of calls (entries in the table) for each hour of the day ordered by the time (eg. 12:00 AM, 1:00 AM, ‚Ä¶). Please use the provided variable hours in your answer. Be sure that your axes are labeled and that your plot is titled. Hint: Check out the plt.plot method in the matplotlib tutorial, as well as our demo above. hours = list(range(24)) calls_index = calls['Hour'].value_counts().sort_index() # calls_index plt.plot(hours, calls_index) plt.xlabel(\"Hour\") plt.ylabel(\"Number of Calls\") plt.title(\"Number of Calls per Hour\") # Leave this for grading purposes ax_3d = plt.gca() grader.check(\"q2c\") q2c passed! üåü To better understand the time of day a report occurs we could stratify the analysis by the day of the week. To do this we will use violin plots (a variation of a box plot), which you will learn in more detail next week. For now, just know that a violin plot shows an estimated distribution of quantitative data (e.g., distribution of calls by hour) over a categorical variable (day of the week). More calls occur in hours corresponding to the fatter part of each violin; the median hour of all calls in a particular day is marked by the white dot in the corresponding violin. # for now, just run this cell. # we will learn the seaborn visualization library next week. import seaborn as sns ax = sns.violinplot(data=calls.sort_values(\"CVDOW\"), x=\"Day\", y=\"Hour\", saturation=0.5, palette=\"Set2\") ax.set_title(\"Stratified Analysis of Phone Calls by Day\"); C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_19968\\4044437892.py:5: FutureWarning: Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\rax = sns.violinplot(data=calls.sort_values(\"CVDOW\"),\r","date":"2024-08-13","objectID":"/datalab3/:6:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2d Based on your line plot and our violin plot above, what observations can you make about the patterns of calls? Here are some dimensions to consider: Are there more calls in the day or at night? What are the most and least popular times? Do call patterns vary by day of the week? ","date":"2024-08-13","objectID":"/datalab3/:7:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3 In this last part of the lab, let‚Äôs extract the GPS coordinates (latitude, longitude) from the Block_Location of each record. # an example block location entry calls.loc[4, 'Block_Location'] '2700 BLOCK GARBER ST\\nBerkeley, CA\\n(37.86066, -122.253407)'\r","date":"2024-08-13","objectID":"/datalab3/:8:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3a: Regular Expressions Use regular expressions to create a dataframe calls_lat_lon that has two columns titled Lat and Lon, containing the respective latitude and longitude of each record in calls. You should use the Block_Location column to extract the latitude and longitude coordinates. Hint: Check out the Series.str.extract documentation. calls_lat_lon = calls['Block_Location'].str.extract(r'(\\d+\\.\\d+),\\s([-]*\\d+\\.\\d+)') # Ê≥®ÊÑèÊçïËé∑ÁªÑÊ∂µÁõñËåÉÂõ¥ÔºÅ calls_lat_lon.columns = ['Lat', 'Lon'] calls_lat_lon.head(10) len(calls_lat_lon) 2632\rgrader.check(\"q3a\") q3a passed! üöÄ ","date":"2024-08-13","objectID":"/datalab3/:9:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3b: Join Tables Let‚Äôs include the GPS data into our calls data. In the below cell, use calls_lat_lon to add two new columns called Lat and Lon to the calls dataframe. Hint: pd.merge (documentation) could be useful here. Note that the order of records in calls and calls_lat_lon are the same. import pandas as pd # ÂàõÂª∫‰∏§‰∏™ÈïøÂ∫¶Áõ∏ÂêåÁöÑDataFrame df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}) df2 = pd.DataFrame({'C': [7, 8, 9], 'D': [10, 11, 12]}) # Âü∫‰∫éÁ¥¢ÂºïÂêàÂπ∂ # ‰ΩøÁî®'Key'ÂàóÂêàÂπ∂ merged_df = pd.merge(df1, df2, left_index=True, right_index=True) # Ê†πÊçÆ‰Ω†ÁöÑÈúÄË¶ÅÈÄâÊã©ÂêàÂπ∂Á±ªÂûã print(merged_df) A B C D\r0 1 4 7 10\r1 2 5 8 11\r2 3 6 9 12\r# # Áî®mergeÂêàÂπ∂‰∏§‰∏™dataframeÔºåÊ≥®ÊÑèÊ≠§Êó∂Ê≤°ÊúâÂÖ±ÂêåÂàóÔºÅÂè™ËÉΩÁ¥¢ÂºïÊãºÊé•ÔºÅ # print(calls.shape) # print(calls_lat_lon.shape) calls = pd.merge(calls, calls_lat_lon, left_index=True, right_index=True) # print(calls.shape) calls.sample(5) # random rows CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\rDay\rHour\rLat\rLon\r1173\r21024375\rDISTURBANCE\r06/02/2021 12:00:00 AM\r9:00\rDISORDERLY CONDUCT\r3\r06/15/2021 12:00:00 AM\r2020 KITTREDGE ST\\nBerkeley, CA\\n(37.868356, -...\r2020 KITTREDGE ST\rBerkeley\rCA\rWednesday\r9\r37.868356\r-122.268904\r2573\r21005894\rDISTURBANCE\r02/11/2021 12:00:00 AM\r14:12\rDISORDERLY CONDUCT\r4\r06/15/2021 12:00:00 AM\r2400 BLOCK DWIGHT WAY\\nBerkeley, CA\\n(37.86482...\r2400 BLOCK DWIGHT WAY\rBerkeley\rCA\rThursday\r14\r37.864826\r-122.260719\r990\r21022043\rROBBERY\r05/18/2021 12:00:00 AM\r20:15\rROBBERY\r2\r06/15/2021 12:00:00 AM\r2521 TELEGRAPH AVE\\nBerkeley, CA\\n(37.864705, ...\r2521 TELEGRAPH AVE\rBerkeley\rCA\rTuesday\r20\r37.864705\r-122.258463\r908\r21017272\rTHEFT MISD. (UNDER $950)\r04/19/2021 12:00:00 AM\r20:20\rLARCENY\r1\r06/15/2021 12:00:00 AM\r2800 BLOCK ADELINE ST\\nBerkeley, CA\\n(37.85811...\r2800 BLOCK ADELINE ST\rBerkeley\rCA\rMonday\r20\r37.858116\r-122.268002\r597\r21090359\rBURGLARY AUTO\r03/26/2021 12:00:00 AM\r0:00\rBURGLARY - VEHICLE\r5\r06/15/2021 12:00:00 AM\r2100 BLOCK 5TH ST\\nBerkeley, CA\\n(37.86626, -1...\r2100 BLOCK 5TH ST\rBerkeley\rCA\rFriday\r0\r37.86626\r-122.298335\rgrader.check(\"q3b\") q3b passed! üöÄ ","date":"2024-08-13","objectID":"/datalab3/:10:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3c: Check for Missing Values It seems like every record has valid GPS coordinates: # just run this cell # fraction of valid lat/lon entries (~calls[[\"Lat\", \"Lon\"]].isna()).mean() Lat 1.0\rLon 1.0\rdtype: float64\rHowever, a closer examination of the data reveals something else. Here‚Äôs the first few records of our data again: calls.head(5) CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\rDay\rHour\rLat\rLon\r0\r21014296\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:58\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r1\r21014391\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:38\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r2\r21090494\rTHEFT MISD. (UNDER $950)\r04/19/2021 12:00:00 AM\r12:15\rLARCENY\r1\r06/15/2021 12:00:00 AM\r2100 BLOCK HASTE ST\\nBerkeley, CA\\n(37.864908,...\r2100 BLOCK HASTE ST\rBerkeley\rCA\rMonday\r12\r37.864908\r-122.267289\r3\r21090204\rTHEFT FELONY (OVER $950)\r02/13/2021 12:00:00 AM\r17:00\rLARCENY\r6\r06/15/2021 12:00:00 AM\r2600 BLOCK WARRING ST\\nBerkeley, CA\\n(37.86393...\r2600 BLOCK WARRING ST\rBerkeley\rCA\rSaturday\r17\r37.863934\r-122.250262\r4\r21090179\rBURGLARY AUTO\r02/08/2021 12:00:00 AM\r6:20\rBURGLARY - VEHICLE\r1\r06/15/2021 12:00:00 AM\r2700 BLOCK GARBER ST\\nBerkeley, CA\\n(37.86066,...\r2700 BLOCK GARBER ST\rBerkeley\rCA\rMonday\r6\r37.86066\r-122.253407\rThere is another field that tells us whether we have a valid Block_Location entry per record‚Äîi.e., with GPS coordinates (latitude, longitude) that match the listed block location. What is it? In the below cell, use the field you found to create a new dataframe, missing_lat_lon, that contains only the rows of calls that have invalid latitude and longitude data. Your new dataframe should have all the same columns of calls. missing_lat_lon = calls[calls['Lat'] == '37.869058'] # ÁêÜËÆ∫‰∏äÂ∫îËØ•ÊòØlatÂíålonÔºå‰ΩÜÊòØËøôÈáåÂè™Âèñ‰∫ÜlatÔºåÁÑ∂ÂêéÊ≥®ÊÑèÁ±ªÂûãÈöêÂºèËΩ¨Êç¢ÔºÅ missing_lat_lon.head() # print(missing_lat_lon.shape) CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\rDay\rHour\rLat\rLon\r0\r21014296\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:58\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r1\r21014391\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:38\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r215\r21019124\rBURGLARY RESIDENTIAL\r04/30/2021 12:00:00 AM\r10:00\rBURGLARY - RESIDENTIAL\r5\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rFriday\r10\r37.869058\r-122.270455\r260\r21000289\rVEHICLE STOLEN\r01/01/2021 12:00:00 AM\r12:00\rMOTOR VEHICLE THEFT\r5\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rFriday\r12\r37.869058\r-122.270455\r633\r21013362\rBURGLARY AUTO\r03/27/2021 12:00:00 AM\r4:20\rBURGLARY - VEHICLE\r6\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rSaturday\r4\r37.869058\r-122.270455\rgrader.check(\"q3c\") ","date":"2024-08-13","objectID":"/datalab3/:11:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3d: Check Missing Values Now let us explore if there is a pattern to which types of records have missing latitude and longitude entries. We‚Äôve implemented the plotting code for you below, but read through it and verify you understand what we‚Äôre doing (we‚Äôve thrown in a bonus plt.subplots() call, documentation here). # just run this cell missing_by_time = (pd.to_datetime(missing_lat_lon['EVENTDT']) .value_counts() .sort_index() ) missing_by_crime = (missing_lat_lon['CVLEGEND'] .value_counts() / calls['CVLEGEND'].value_counts() ).dropna().sort_values(ascending=False) fig, ax = plt.subplots(2) ax[0].bar(missing_by_time.index, missing_by_time) ax[0].set_ylabel(\"Calls with Missing Data\") ax[1].barh(missing_by_crime.index, missing_by_crime) ax[1].set_xlabel(\"Fraction of Missing Data per Event Type\") fig.suptitle(\"Characteristics of Missing Lat/Lon Data\") plt.show() C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_19968\\1218153592.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\rmissing_by_time = (pd.to_datetime(missing_lat_lon['EVENTDT'])\rBased on the plots above, are there any patterns among entries that are missing latitude/longitude data? The dataset information linked at the top of this notebook may also give more context. Type your answer here, replacing this text. ","date":"2024-08-13","objectID":"/datalab3/:12:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3d: Explore The below cell plots a map of phonecalls by GPS coordinates (latitude, longitude); we drop missing location data. # just run this cell import folium import folium.plugins SF_COORDINATES = (37.87, -122.28) sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13) locs = calls.drop(missing_lat_lon.index)[['Lat', 'Lon']].astype('float').values heatmap = folium.plugins.HeatMap(locs.tolist(), radius=10) sf_map.add_child(heatmap) Make this Notebook Trusted to load map: File -\u003e Trust Notebook","date":"2024-08-13","objectID":"/datalab3/:13:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Congratulations! Congrats! You are finished with this lab. To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-08-13","objectID":"/datalab3/:14:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export(pdf=False) ","date":"2024-08-13","objectID":"/datalab3/:15:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab04.ipynb\") Lab 4: Visualization, Transformations, and KDEs ","date":"2024-08-13","objectID":"/datalab4/:0:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Objective In this lab you will get some practice plotting, applying data transformations, and working with kernel density estimators (KDEs). We will be working with data from the World Bank containing various statistics for countries and territories around the world. import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import ds100_utils plt.style.use('fivethirtyeight') # Use plt.style.available to see more styles sns.set() sns.set_context(\"talk\") %matplotlib inline ","date":"2024-08-13","objectID":"/datalab4/:0:1","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Loading Data Let us load some World Bank data into a pd.DataFrame object named wb. wb = pd.read_csv(\"data/world_bank_misc.csv\", index_col=0) wb.head() Primary completion rate: Male: % of relevant age group: 2015\rPrimary completion rate: Female: % of relevant age group: 2015\rLower secondary completion rate: Male: % of relevant age group: 2015\rLower secondary completion rate: Female: % of relevant age group: 2015\rYouth literacy rate: Male: % of ages 15-24: 2005-14\rYouth literacy rate: Female: % of ages 15-24: 2005-14\rAdult literacy rate: Male: % ages 15 and older: 2005-14\rAdult literacy rate: Female: % ages 15 and older: 2005-14\rStudents at lowest proficiency on PISA: Mathematics: % of 15 year-olds: 2015\rStudents at lowest proficiency on PISA: Reading: % of 15 year-olds: 2015\r...\rAccess to improved sanitation facilities: % of population: 1990\rAccess to improved sanitation facilities: % of population: 2015\rChild immunization rate: Measles: % of children ages 12-23 months: 2015\rChild immunization rate: DTP3: % of children ages 12-23 months: 2015\rChildren with acute respiratory infection taken to health provider: % of children under age 5 with ARI: 2009-2016\rChildren with diarrhea who received oral rehydration and continuous feeding: % of children under age 5 with diarrhea: 2009-2016\rChildren sleeping under treated bed nets: % of children under age 5: 2009-2016\rChildren with fever receiving antimalarial drugs: % of children under age 5 with fever: 2009-2016\rTuberculosis: Treatment success rate: % of new cases: 2014\rTuberculosis: Cases detection rate: % of new estimated cases: 2015\rAfghanistan\rNaN\rNaN\rNaN\rNaN\r62.0\r32.0\r45.0\r18.0\rNaN\rNaN\r...\r21.0\r32.0\r68.0\r78.0\r62.0\r41.0\r4.6\r11.8\r87.0\r58.0\rAlbania\r108.0\r105.0\r97.0\r97.0\r99.0\r99.0\r98.0\r96.0\r26.0\r7.0\r...\r78.0\r93.0\r98.0\r98.0\r70.0\r63.0\rNaN\rNaN\r88.0\r76.0\rAlgeria\r106.0\r105.0\r68.0\r85.0\r96.0\r92.0\r83.0\r68.0\r51.0\r11.0\r...\r80.0\r88.0\r95.0\r95.0\r66.0\r42.0\rNaN\rNaN\r88.0\r80.0\rAmerican Samoa\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\r61.0\r63.0\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r87.0\rAndorra\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\r100.0\r100.0\r96.0\r97.0\rNaN\rNaN\rNaN\rNaN\r83.0\r87.0\r5 rows √ó 45 columns This table contains some interesting columns. Take a look: list(wb.columns) ['Primary completion rate: Male: % of relevant age group: 2015',\r'Primary completion rate: Female: % of relevant age group: 2015',\r'Lower secondary completion rate: Male: % of relevant age group: 2015',\r'Lower secondary completion rate: Female: % of relevant age group: 2015',\r'Youth literacy rate: Male: % of ages 15-24: 2005-14',\r'Youth literacy rate: Female: % of ages 15-24: 2005-14',\r'Adult literacy rate: Male: % ages 15 and older: 2005-14',\r'Adult literacy rate: Female: % ages 15 and older: 2005-14',\r'Students at lowest proficiency on PISA: Mathematics: % of 15 year-olds: 2015',\r'Students at lowest proficiency on PISA: Reading: % of 15 year-olds: 2015',\r'Students at lowest proficiency on PISA: Science: % of 15 year-olds: 2015',\r'Population: millions: 2016',\r'Surface area: sq. km thousands: 2016',\r'Population density: people per sq. km: 2016',\r'Gross national income, Atlas method: $ billions: 2016',\r'Gross national income per capita, Atlas method: $: 2016',\r'Purchasing power parity gross national income: $ billions: 2016',\r'per capita: $: 2016',\r'Gross domestic product: % growth : 2016',\r'per capita: % growth: 2016',\r'Prevalence of smoking: Male: % of adults: 2015',\r'Prevalence of smoking: Female: % of adults: 2015',\r'Incidence of tuberculosis: per 100,000 people: 2015',\r'Prevalence of diabetes: % of population ages 20 to 79: 2015',\r'Incidence of HIV: Total: % of uninfected population ages 15-49: 2015',\r'Prevalence of HIV: Total: % of population ages 15-49: 2015',\r\"Prevalence of HIV: Women's share of population ages 15+ living with HIV: %: 2015\",\r'Prevalence of HIV: Youth, Male: % of population ages 15-24: 2015',\r'Prevalence of HIV: Youth, Female: % of population ages 15-24: 2015',\r'Antiretroviral therapy coverage: % of people living w","date":"2024-08-13","objectID":"/datalab4/:1:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1a Suppose we wanted to build a histogram of our data to understand the distribution of literacy rates and income per capita individually. We can use countplot in seaborn to create bar charts from categorical data. sns.countplot(x = \"lit\", data = df) plt.xlabel(\"Combined literacy rate: % ages 15 and older: 2005-14\") plt.title('World Bank Combined Adult Literacy Rate') Text(0.5, 1.0, 'World Bank Combined Adult Literacy Rate')\rsns.countplot(x = \"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rIn the cell below, explain why countplot is NOT the right tool for visualizing the distribution of our data. It is so overwhelming! And ugly! ","date":"2024-08-13","objectID":"/datalab4/:2:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1b In the cell below, create a plot of income per capita (the second plot above) using the histplot function. As above, you should have two subplots, where the left subplot is literacy, and the right subplot is income. Don‚Äôt forget to title the plot and label axes! Hint: Copy and paste from above to start. sns.histplot(x = \"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita'); You should see histograms that show the counts of how many data points appear in each bin. distplot uses a heuristic called the Freedman-Diaconis rule to automatically identify the best bin sizes, though it is possible to set the bins yourself (we won‚Äôt). In the cell below, we explore overlaying a rug plot on top of a histogram using rugplot. Note that the rug plot is hard to see. sns.histplot(x=\"inc\", data = df) sns.rugplot(x=\"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rOne way to make it easier to see the difference between the rug plot and the bars is to set a different color, for example: sns.histplot(x=\"inc\", data = df, color = \"lightsteelblue\") sns.rugplot(x=\"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rThere is also another function called kdeplot which plots a Kernel Density Estimate as described in class, and covered in more detail later in this lab. Rather than manually calling histplot, rugplot, and kdeplot to plot histograms, rug plots, and KDE plots, respectively, we can instead use displot, which can simultaneously plot histogram bars, a rug plot, and a KDE plot, and adjust all the colors automatically for visbility. Using the documentation for displot (Link), make a plot of the income data that includes a histogram, rug plot, and KDE plot. Hint: You‚Äôll need to set two parameters to True. sns.displot(x='inc', data=df, kde=True, rug=True) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') # Â§™Èïø‰∫ÜÔºåÊòæÁ§∫‰∏çÂÖ®Ôºü plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rYou should see roughly the same histogram as before. However, now you should see an overlaid smooth line. This is the kernel density estimate discussed in class. Above, the y-axis is labeled by the counts. We can also label the y-axis by the density. An example is given below, this time using the literacy data from the beginning of this lab. sns.displot(x=\"lit\", data = df, rug = True, kde = True, stat = \"density\") plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older: 2005-14\") plt.title('World Bank Combined Adult Literacy Rate') Text(0.5, 1.0, 'World Bank Combined Adult Literacy Rate')\rObservations: You‚Äôll also see that the y-axis value is no longer the count. Instead it is a value such that the total area in the histogram is 1. For example, the area of the last bar is approximately 22.22 * 0.028 = 0.62 The KDE is a smooth estimate of the distribution of the given variable. The area under the KDE is also 1. While it is not obvious from the figure, some of the area under the KDE is beyond the 100% literacy. In other words, the KDE is non-zero for values greater than 100%. This, of course, makes no physical sense. Nonetheless, it is a mathematical feature of the KDE. We‚Äôll talk more about KDEs later in this lab. ","date":"2024-08-13","objectID":"/datalab4/:3:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1c Looking at the income data, it is difficult to see the distribution among low income countries because they are all scrunched up at the left side of the plot. The KDE also has a problem where the density function has a lot of area below 0. Transforming the inc data logarithmically gives us a more symmetric distribution of values. This can make it easier to see patterns. In addition, summary statistics like the mean and standard deviation (square-root of the variance) are more stable with symmetric distributions. In the cell below, make a distribution plot of inc with the data transformed using np.log10 and kde=True. If you want to see the exact counts, just set kde=False. If you don‚Äôt specify the kde parameter, it is by default set to True. Hint: Unlike the examples above, you can pass a series to the displot function, i.e. rather than passing an entire DataFrame as data and a column as x, you can instead pass a series. ax = sns.displot(data=np.log10(df['inc']), kde=True, color='blue') plt.title('World Bank Gross National Income Per Capita') plt.ylabel('Density') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016'); When a distribution has a long right tail, a log-transformation often does a good job of symmetrizing the distribution, as it did here. Long right tails are common with variables that have a lower limit on the values. On the other hand, long left tails are common with distributions of variables that have an upper limit, such as percentages (can‚Äôt be higher than 100%) and GPAs (can‚Äôt be higher than 4). That is the case for the literacy rate. Typically taking a power-transformation such as squaring or cubing the values can help symmetrize the left skew distribution. In the cell below, we will make a distribution plot of lit with the data transformed using a power, i.e., raise lit to the 2nd, 3rd, and 4th power. We plot the transformation with the 4th power below. ax = sns.displot((df['lit']**4), kde = True) # ÁªèÂÖ∏ÂêëÈáèÂåñnumpy plt.ylabel('Density') plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older: 2005-14\") plt.title('World Bank Combined Adult Literacy Rate (4th power)', pad=30); ","date":"2024-08-13","objectID":"/datalab4/:4:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1d If we want to examine the relationship between the female adult literacy rate and the gross national income per capita, we need to make a scatter plot. In the cell below, create a scatter plot of untransformed income per capita and literacy rate using the sns.scatterplot function. Make sure to label both axes using plt.xlabel and plt.ylabel. sns.scatterplot(x=df['lit'], y=df['inc']) plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older\") plt.ylabel('Gross national income per capita (non-log scale)') plt.title('World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate'); We can better assess the relationship between two variables when they have been straightened because it is easier for us to recognize linearity. In the cell below, we see a scatter plot of log-transformed income per capita against literacy rate. sns.scatterplot(x = df['lit'], y = np.log10(df['inc'])) plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older\") plt.ylabel('Gross national income per capita (log scale)') plt.title('World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate'); ","date":"2024-08-13","objectID":"/datalab4/:5:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"ÂèåÂèòÊç¢ÔºåÊÄùË∑ØÊâìÂºÄ This scatter plot looks better. The relationship is closer to linear. We can think of the log-linear relationship between x and y, as follows: a constant change in x corresponds to a percent (scaled) change in y. We can also see that the long left tail of literacy is represented in this plot by a lot of the points being bunched up near 100. Try squaring literacy and taking the log of income. Does the plot look better? plt.figure(figsize=(10,5)) sns.scatterplot(x = (df['lit']**2), y = np.log10(df['inc'])) plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older\") plt.ylabel('Gross national income per capita (log vs. ^2)') plt.title('World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate'); Choosing the best transformation for a relationship is often a balance between keeping the model simple and straightening the scatter plot. Part 2: Kernel Density Estimation In this part of the lab you will develop a deeper understanding of how kernel density estimation works. Explain KDE briefly within the lab ","date":"2024-08-13","objectID":"/datalab4/:6:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Overview Kernel density estimation is used to estimate a probability density function (i.e. a density curve) from a set of data. Just like a histogram, a density function‚Äôs total area must sum to 1. KDE centrally revolves around this idea of a ‚Äúkernel‚Äù. A kernel is a function whose area sums to 1. The three steps involved in building a kernel density estimate are: Placing a kernel at each observation Normalizing kernels so that the sum of their areas is 1 Summing all kernels together üòã The end result is a function, that takes in some value x and returns a density estimate at the point x. When constructing a KDE, there are several choices to make regarding the kernel. Specifically, we need to choose the function we want to use as our kernel, as well as a bandwidth parameter, which tells us how wide or narrow each kernel should be. We will explore these ideas now. Suppose we have 3 data points with values 2, 4, and 9. We can compute the (useless) histogram with a KDE as shown below. data3pts = np.array([2, 4, 9]) sns.displot(data3pts, kde = True, stat = \"density\"); To understand how KDEs are computed, we need to see the KDE outside the given range. The easiest way to do this is to use an old function called distplot. During the Spring 2022 offering of this course, distplot was still a working function in Seaborn, but it will be removed at a future date. If you get an error that says that distplot is not a valid function, sorry, you are too far in the future to do this lab exercise. sns.distplot(data3pts, kde = True); C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_9696\\4279347623.py:1: UserWarning: `distplot` is a deprecated function and will be removed in seaborn v0.14.0.\rPlease adapt your code to use either `displot` (a figure-level function with\rsimilar flexibility) or `histplot` (an axes-level function for histograms).\rFor a guide to updating your code to use the new functions, please see\rhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\rsns.distplot(data3pts, kde = True);\rË∞ÉÊï¥bandwidth One question you might be wondering is how the kernel density estimator decides how ‚Äúwide‚Äù each point should be. It turns out this is a parameter you can set called bw, which stands for bandwith. For example, the code below gives a bandwith value of 0.5 to each data point. You‚Äôll see the resulting KDE is quite different. Try experimenting with different values of bandwidth and see what happens. sns.distplot(data3pts, kde = True, kde_kws = {\"bw\": 0.5}); C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_9696\\942060009.py:1: UserWarning: `distplot` is a deprecated function and will be removed in seaborn v0.14.0.\rPlease adapt your code to use either `displot` (a figure-level function with\rsimilar flexibility) or `histplot` (an axes-level function for histograms).\rFor a guide to updating your code to use the new functions, please see\rhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\rsns.distplot(data3pts, kde = True, kde_kws = {\"bw\": 0.5});\rd:\\miniconda3\\envs\\ds100\\Lib\\site-packages\\seaborn\\distributions.py:2496: UserWarning: The `bw` parameter is deprecated in favor of `bw_method` and `bw_adjust`.\rSetting `bw_method=0.5`, but please see the docs for the new parameters\rand update your code. This will become an error in seaborn v0.14.0.\rkdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\r","date":"2024-08-13","objectID":"/datalab4/:6:1","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 2a As mentioned above, the kernel density estimate (KDE) is just the sum of a bunch of copies of the kernel, each centered on our data points. The default kernel used by the distplot function (as well as kdeplot) is the Gaussian kernel, given by: $$\\Large K_\\alpha(x, z) = \\frac{1}{\\sqrt{2 \\pi \\alpha^2}} \\exp\\left(-\\frac{(x - z)^2}{2 \\alpha ^2} \\right) $$ We‚Äôve implemented the Gaussian kernel for you in Python below. Here, alpha is the smoothing or bandwidth parameter $\\alpha$ for the KDE, z is the center of the Gaussian (i.e. a data point or an array of data points), and x is an array of values of the variable whose distribution we are plotting. def gaussian_kernel(alpha, x, z): return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2)) For example, we can plot the Gaussian kernel centered at 9 with $\\alpha$ = 0.5 as below: xs = np.linspace(-2, 12, 200) alpha=0.5 kde_curve = [gaussian_kernel(alpha, x, 9) for x in xs] plt.plot(xs, kde_curve); In the cell below, plot the 3 kernel density functions corresponding to our 3 data points on the same axis. Use an alpha value of 0.5. Recall that our three data points are 2, 4, and 9. Note: Make sure to normalize your kernels! This means that the area under each of your kernels should be $\\frac{1}{3}$ since there are three data points. You don‚Äôt have to use the following hints, but they might be helpful in simplifying your code. Hint: The gaussian_kernel function can also take a numpy array as an argument for z. Hint: To plot multiple plots at once, you can use plt.plot(xs, y) with a two dimensional array as y. xs = np.linspace(-2, 12, 200) alpha=0.5 kde_curve = [1/3*gaussian_kernel(alpha, x, data3pts) for x in xs] # Ê≥®ÊÑè‚ÄúÊ≠£ÂàôÂåñ‚ÄùÔºÅ plt.plot(xs, kde_curve); In the cell below, we see a plot that shows the sum of all three of the kernels above. The plot resembles the kde shown when you called distplot function with bandwidth 0.5 earlier. The area under the final curve will be 1 since the area under each of the three normalized kernels is $\\frac{1}{3}$. xs = np.linspace(-2, 12, 200) alpha=0.5 kde_curve = np.array([1/3 * gaussian_kernel(alpha, x, data3pts) for x in xs]) plt.plot(xs, np.sum(kde_curve, axis = 1)); # Âè†Âä†Êõ≤Á∫øÔºÅ Recall that earlier we plotted the kernel density estimation for the logarithm of the income data, as shown again below. ax = sns.displot(np.log10(df['inc']), kind = \"kde\", rug = True) plt.title('World Bank Gross National Income Per Capita') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016'); In the cell below, a similar plot is shown using what was done in 2a. Try out different values of alpha in {0.1, 0.2, 0.3, 0.4, 0.5}. You will see that when alpha=0.2, the graph matches the previous graph well, except that the displot function hides the KDE values outside the range of the available data. xs = np.linspace(1, 6, 200) alpha=0.2 kde_curve = np.array([1/len(df['inc']) * gaussian_kernel(alpha, x, np.log10(df['inc'])) for x in xs]) plt.title('World Bank Gross National Income Per Capita') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016') plt.plot(xs, np.sum(kde_curve, axis = 1)); ","date":"2024-08-13","objectID":"/datalab4/:7:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 2b In your answers above, you hard-coded a lot of your work. In this problem, you‚Äôll build a more general kernel density estimator function. Implement the KDE function which computes: $$\\Large f_\\alpha(x) = \\frac{1}{n} \\sum_{i=1}^n K_\\alpha(x, z_i) $$ Where $z_i$ are the data, $\\alpha$ is a parameter to control the smoothness, and $K_\\alpha$ is the kernel density function passed as kernel. def kde(kernel, alpha, x, data): \"\"\" Compute the kernel density estimate for the single query point x. Args: kernel: a kernel function with 3 parameters: alpha, x, data alpha: the smoothing parameter to pass to the kernel x: a single query point (in one dimension) data: a numpy array of data points Returns: The smoothed estimate at the query point x \"\"\" return sum(kernel(alpha, x, zi) for zi in data) / len(data) grader.check(\"q2b\") Assuming you implemented kde correctly, the code below should generate the kde of the log of the income data as before. df['trans_inc'] = np.log10(df['inc']) xs = np.linspace(df['trans_inc'].min(), df['trans_inc'].max(), 1000) curve = [kde(gaussian_kernel, alpha, x, df['trans_inc']) for x in xs] plt.hist(df['trans_inc'], density=True, color='orange') plt.title('World Bank Gross National Income Per Capita') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016'); plt.plot(xs, curve, 'k-'); And the code below should show a 3 x 3 set of plots showing the output of the kde for different alpha values. small to large plt.figure(figsize=(15,15)) alphas = np.arange(0.2, 2.0, 0.2) for i, alpha in enumerate(alphas): plt.subplot(3, 3, i+1) xs = np.linspace(df['trans_inc'].min(), df['trans_inc'].max(), 1000) curve = [kde(gaussian_kernel, alpha, x, df['trans_inc']) for x in xs] plt.hist(df['trans_inc'], density=True, color='orange') plt.plot(xs, curve, 'k-') plt.show() Let‚Äôs take a look at another kernel, the Boxcar kernel. def boxcar_kernel(alpha, x, z): return (((x-z)\u003e=-alpha/2)\u0026((x-z)\u003c=alpha/2))/alpha Run the cell below to enable interactive plots. It should give you a green ‚ÄòOK‚Äô when it‚Äôs finished. from ipywidgets import interact !jupyter nbextension enable --py widgetsnbextension # Ëøô‰∏™ÊòØË¶ÅnotebookÈôçÁ∫ßÂ§ÑÁêÜÁöÑÊèí‰ª∂ Enabling notebook extension jupyter-js-widgets/extension...\r- Validating: ok\rNow, we can plot the Boxcar and Gaussian kernel functions to see what they look like. x = np.linspace(-10,10,1000) def f(alpha): plt.plot(x, boxcar_kernel(alpha,x,0), label='Boxcar') plt.plot(x, gaussian_kernel(alpha,x,0), label='Gaussian') plt.legend(title='Kernel Function') plt.show() interact(f, alpha=(1,10,0.1)); interactive(children=(FloatSlider(value=5.0, description='alpha', max=10.0, min=1.0), Output()), _dom_classes=‚Ä¶\rUsing the interactive plot below compare the the two kernel techniques: (Generating the KDE plot is slow, so you may expect some latency after you move the slider) xs = np.linspace(df['trans_inc'].min(), df['trans_inc'].max(), 1000) def f(alpha_g, alpha_b): plt.hist(df['trans_inc'], density=True, color='orange') g_curve = [kde(gaussian_kernel, alpha_g, x, df['trans_inc']) for x in xs] plt.plot(xs, g_curve, 'k-', label='Gaussian') b_curve = [kde(boxcar_kernel, alpha_b, x, df['trans_inc']) for x in xs] plt.plot(xs, b_curve, 'r-', label='Boxcar') plt.legend(title='Kernel Function') plt.show() interact(f, alpha_g=(0.01,.5,0.01), alpha_b=(0.01,3,0.1)); interactive(children=(FloatSlider(value=0.25, description='alpha_g', max=0.5, min=0.01, step=0.01), FloatSlide‚Ä¶\rBriefly compare and contrast the Gaussian and Boxcar kernels in the cell below. How do the two kernels relate with each other for the same alpha value? ÂúÜÊªëÈóÆÈ¢ò Congrats! You are finished with this assignment. ","date":"2024-08-13","objectID":"/datalab4/:8:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Optional‚ÄìpxÁî®Ê≥ïËÆ≤Ëß£ Below are some examples using plotly. Recall that this is Josh‚Äôs preferred plotting library, though it is not officially covered nor required in this class. This is purely for your future reference if you decide to use plotly on your own. import plotly.express as px px.histogram(df, x = \"lit\") In my opinion, distribution plots are the one place where plotly falls short of seaborn. For example, if we want a rug, KDE, and histogram, the code below does this in plotly. I‚Äôm not personally a fan. import plotly.figure_factory as ff ff.create_distplot([df[\"lit\"]], [\"lit\"]) By contrast, I think many of plotly‚Äôs other features are far superior to seaborn. For example, consider the interactive scatterplot below, where one can mouseover each datapoint in order to see the identity of each country. px.scatter(df, x = \"lit\", y = \"inc\", hover_name = df.index, labels={ \"lit\": \"Adult literacy rate: Combined: % ages 15 and older\", \"inc\": \"Gross national income per capita\" }, title=\"World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate\" ) Naturally there are ways to adjust figure size, text size, marker, etc, but they are not covered here. I just wanted to give you a small taste of plotly. ","date":"2024-08-13","objectID":"/datalab4/:9:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["CS186"],"content":"Index Á´üÁÑ∂ÊòØ‰∏ÄÁßçÊï∞ÊçÆÁªìÊûÑÔºü ","date":"2024-08-11","objectID":"/databasel5/:1:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"search and insertion in ISAM indexed sequential access method Ê≥®ÊÑèÂª∫Á´ã‰∫ÜËÆ∏Â§öÁ¥¢ÂºïÔºåÊ≤øÁî®BSTÁöÑÊÄùÊÉ≥Ôºå‰ΩÜÊòØinsertÁöÑÊó∂ÂÄô‰ºöÂá∫Áé∞overflow pages ÔºàIBM in 1960sÔºâ ","date":"2024-08-11","objectID":"/databasel5/:2:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"B+ Tree ÂíåB TreeÁöÑÂå∫Âà´Âú®‰∫éÔºöB+Âè™ÊúâÂè∂Â≠êÂ≠òÊîæÊï∞ÊçÆÔºåËÄåB TreeÁöÑ‰∏≠Èó¥ËäÇÁÇπ‰πüÂ≠òÊîæÊï∞ÊçÆ„ÄÇ Âá†‰πéÂíå‰∏äÈù¢‰∏ÄÊ†∑Ôºå‰ΩÜÊòØÂ§ö‰∫Ü dynamic tree index always balanced support efficient insertions and deletions grows at root not leaves Ê≥®ÊÑè: Âç†ÊúâÁéáÔºöÂá†‰πéÂçäÊª°ÔºåÈô§‰∫Üroot Â∫ïÈÉ®DLL $max\\ fan\\ out = 2d + 1$ Â∑•‰∏öÂÆûÈôÖÊÉÖÂÜµ ü§î ","date":"2024-08-11","objectID":"/databasel5/:3:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"B+ Tree Operations ","date":"2024-08-11","objectID":"/databasel5/:4:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"search, Âêå‰∏ä ","date":"2024-08-11","objectID":"/databasel5/:4:1","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"insert Ê≤°ÊúâoverflowÔºåÁõ¥Êé•ÊèíÂÖ• Êª°‰∫ÜÔºåÂàÜË£ÇÊàê‰∏§‰∏™ËäÇÁÇπÔºå‰∏≠Èó¥ËäÇÁÇπÂ≠òÊîæ ‰∏≠Èó¥keyÔºàËøáÁ®ã‰∏≠ÂèØËÉΩÊòØÂè≥ËæπÊúÄÂ∞èÁöÑÈÇ£‰∏™keyÔºâÔºåÂ∑¶Âè≥ËäÇÁÇπÂ≠òÊîæÂ∑¶Âè≥key ÈÄíÂΩíÂêë‰∏äÂàÜË£ÇÔºåÁõ¥Âà∞Ê†πËäÇÁÇπ Âè∂Â≠ê $\\xrightarrow{copy}$ Áà∂ËäÇÁÇπ ÔºåÁà∂ËäÇÁÇπ $\\xrightarrow{push}$ Êñ∞Áà∂ËäÇÁÇπ ","date":"2024-08-11","objectID":"/databasel5/:4:2","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"delete üòâ ","date":"2024-08-11","objectID":"/databasel5/:4:3","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"bulk loading ÊâπÈáèÂä†ËΩΩ Á≤æÈ´ìÂú®‰∫éÂÖàÊéíÂ∫èÔºåÂêéÊûÑÂª∫ time-stamp: 01h09min02s ","date":"2024-08-11","objectID":"/databasel5/:4:4","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"cost model and analysis Âü∫Êú¨ÈáèÂåñÊåáÊ†áÂÆö‰πâ Âü∫Êú¨ÂÅáËÆæ single record insert and delete equality selection - exactly one match for heap files: insert always appends for sorted files: packed: files compacted after deletions sorted according to search key ‰ª•‰∏ãÊòØËÆ°ÁÆóÊó∂Èó¥ÔºåÁ±ª‰ººÊ∏êËøõËÆ∞Ê≥ïÔºå‰ΩÜÊòØÊúâÁªÜËäÇ üòé side note: $\\times D$ ÊòØÁÆÄÂåñ‰∫ÜÊØèÊ¨°‚ÄúÊìç‰Ωú‚ÄùÁöÑÊó∂Èó¥Ôºå‚ÄúÊìç‰Ωú‚ÄùÊåáÁöÑÊòØ‚ÄúËØª‚Äù‰∏é‚ÄúÂÜô‚Äù ËÄÉËôëÈöèÊú∫ÂèòÈáè Êìç‰ΩúÊ¨°Êï∞ $N$ ÂèäÂÖ∂ $\\mathbb{E}(N)$ Equality SearchÂØπ‰∫ésorted files: $$ \\begin{equation} \\begin{aligned} \\mathbb{E}(N) \u0026= \\sum_{i=1}^{log_2B} i \\times \\frac{2^{i-1}}{B} \\notag\\ \u0026= log_2B - \\frac{B-1}{B}\\ \u0026\\approx log_2B \\end{aligned} \\end{equation} $$ Range SearchÂÄüÈâ¥Equality SearchÁöÑÊÄùÊÉ≥ÔºåÂØπ‰∫éheap filesÔºåalways go through to find allÔºõÂØπ‰∫ésorted filesÔºå‰∫åÂàÜÊü•Êâæ‰∏ãÁïåÁÑ∂Âêé scan right InsertÂØπ‰∫éheap files: ÂÅáËÆæÁü•ÈÅìfree spaceÔºå‰∏Ä‰∏™ËØªÔºå‰∏Ä‰∏™ÂÜôÔºõÂØπ‰∫ésorted files: ÂÅáËÆæ‰∏≠Èó¥ÊèíÂÖ•ÔºåËØªÂÜôÂêÑÂç† $B/2$ DeleteÂêåÊ†∑ÂÄüÈâ¥Equality SearchÁöÑÊÄùÊÉ≥ÔºåÂØπ‰∫éheap filesÔºå $+1$ ÊòØÂà†Èô§Ôºà‚ÄúÂÜô‚ÄùÔºâÔºõÂØπ‰∫ésorted filesÔºå ËøáÁ®ãÂíåInsert‰∏ÄÊ†∑ time-stamp: 01h13min26s ","date":"2024-08-11","objectID":"/databasel4/:1:0","tags":null,"title":"CS186-L4: Disks, Buffers, Files II","uri":"/databasel4/"},{"categories":["CS186"],"content":"big picture sql client -\u003e DBMS -\u003e database ü§ì ","date":"2024-08-11","objectID":"/databasel3/:1:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"DBMS parsing \u0026 optimization ÊâßË°åSQLËØ≠Âè•Êó∂ÔºåDBMSÈúÄË¶ÅËß£ÊûêSQLËØ≠Âè•ÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ÊâßË°åËÆ°Âàí„ÄÇ‰ºòÂåñÂô®‰ºöÊ†πÊçÆÁªüËÆ°‰ø°ÊÅØ„ÄÅÊü•ËØ¢Ê®°Âºè„ÄÅÁ¥¢ÂºïÁ≠âÂõ†Á¥†ÔºåÈÄâÊã©ÊúÄ‰ºòÁöÑÊâßË°åËÆ°Âàí„ÄÇ relational operators Â§ÑÁêÜÊï∞ÊçÆÊµÅ or ÂÖ≥Á≥ªËøêÁÆóÁ¨¶Ôºü files and index management buffer management disk space management ‰∫ãÂÆû‰∏äÁ∫µÂêëËøòÊúâ‰∏§‰∏™Ê®°ÂùóÔºöconcurrency controlÂíårecovery„ÄÇ ÁúÅÊµÅÔºö‰ªéRAM \u0026 DISKËé∑ÂèñÊï∞ÊçÆÈùûÂ∏∏ÊÖ¢Ôºå Áõ∏ÂØπ‰∫éCPU ","date":"2024-08-11","objectID":"/databasel3/:1:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"Disk Ê≥®ÊÑèsector, disk headÔºå ÂÖ∂‰∏≠ÂêéËÄÖ‰ºº‰πéÂè™ËÉΩÂçïÊ¨°ËØªÂÜô ","date":"2024-08-11","objectID":"/databasel3/:1:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"access time ","date":"2024-08-11","objectID":"/databasel3/:1:3","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"flash SSD Ê≥®ÊÑèÔºö readÂæàÂø´ÔºåÈöèÁùÄÊï∞ÊçÆÂèòÂ§ßÔºåÂèØ‰ª•È¢ÑÊµã writeÂæàÊÖ¢Ôºåslower for randomÔºåÂÜôÂÖ•ÊîæÂ§ß ","date":"2024-08-11","objectID":"/databasel3/:2:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"disk space management ","date":"2024-08-11","objectID":"/databasel3/:3:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"block level storage block: unit of transfer for disk read/write (64~128KB in 2018) page: a common synonym for block, in some contexts, it means in RAM ","date":"2024-08-11","objectID":"/databasel3/:3:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"implementation talk to hardware directly ü§î use file system (FS) üòÄ always remember: next is fast ","date":"2024-08-11","objectID":"/databasel3/:3:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"files and representation important! üòé tables stored in files consist of pages pages contain a collection of records ","date":"2024-08-11","objectID":"/databasel3/:4:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"DB files unordered heap files DLLÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑÂÆûÁé∞heap fileÁöÑÊñπÊ°àÔºå‰ΩÜÊòØinsertÊïàÁéá‰∏çÈ´ò üòû better: a page directory Ê≥®ÊÑèheader page is SLL ","date":"2024-08-11","objectID":"/databasel3/:4:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"layout of a page page header nums of records free space maybe a next/last pointer bitmaps, slot table (what is that? ü§î) Ê≥®ÊÑèÔºåÊòØÂê¶recordsÂÆöÈïø‰ª•ÂèäÊòØÂê¶Êúâfree spaceÂÜ≥ÂÆö‰∫Üpage layout ","date":"2024-08-11","objectID":"/databasel3/:5:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"fixed-length packed records now take a look at a fixed length records, packed page: ","date":"2024-08-11","objectID":"/databasel3/:5:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"fixed-length unpacked records same as before, but with unpacked records: ","date":"2024-08-11","objectID":"/databasel3/:5:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"variable-length records records can have different lengths ü§Ø header -\u003e footer footer has a slot directory (read from right to left, has a pointer to the start of the free space) slot directory save slot, each slot has a pointer to the start of the record and the length of the record growing slot directory, ÂâçÂêéÂ§πÂáª ","date":"2024-08-11","objectID":"/databasel3/:5:3","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"layout of records each record has a fixed type system catalog stores the SCHEMA no need to store the type of records catalog just a table Ê≥®ÊÑèÔºå‰ª•‰∏ãËÆ®ËÆ∫Âú®Â≠óÊÆµÔºàfieldÔºâÁ∫ßÂà´Ôºå‰∏çÊòØrecordÁ∫ßÂà´„ÄÇ ","date":"2024-08-11","objectID":"/databasel3/:6:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"fixed-length records Á±ªÊØîÊï∞ÁªÑÔºåÊ≥®ÊÑènullÂ≠òÂú®Â∞±ÊòØÁ©∫ÁùÄÔºå‰∏çÊòØÂæàcompact ","date":"2024-08-11","objectID":"/databasel3/:6:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"variable-length records ÂÜó‰ΩôÈÖçÁΩÆÊåâÁÖßÂÆöÈïøÂ§ÑÁêÜ ÔºàpaddingÔºâ Á±ªÊØîCSVÔºåÊØèË°åËÆ∞ÂΩï‰∏çÂêåÈïøÂ∫¶ÔºåÁî®ÂàÜÈöîÁ¨¶ÂàÜÈöî a record header way ","date":"2024-08-11","objectID":"/databasel3/:6:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["TOOLS"],"content":"Âéü‰ΩúËÄÖÂ∏ñÂ≠êÔºü COSTARÊèêÁ§∫ËØçÊ°ÜÊû∂ÊòØ‰∏Ä‰∏™Áî®‰∫éÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇGPT-3/4ÔºâÁîüÊàêÁªìÊûúË¥®ÈáèÁöÑÂ∑•ÂÖ∑„ÄÇËøô‰∏™Ê°ÜÊû∂ÁöÑÁõÆÊ†áÊòØÈÄöËøáÊèê‰æõËØ¶ÁªÜÂíåÂÖ∑‰ΩìÁöÑÊèêÁ§∫ÔºåÊåáÂØºÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÊõ¥ÂáÜÁ°ÆÂíåÊúâÁî®ÁöÑÂÜÖÂÆπ„ÄÇ‰ª•‰∏ãÊòØÂØπCoSTARÊèêÁ§∫ËØçÊ°ÜÊû∂ÁöÑËØ¶ÁªÜ‰ªãÁªçÔºö COSTARÊèêÁ§∫ËØçÊ°ÜÊû∂ÊòØ‰∏ÄÁßçÁî®‰∫é‰ºòÂåñÂíåÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇGPT-4ÔºâÁîüÊàêÊïàÊûúÁöÑÊñπÊ≥ï„ÄÇÈÄöËøáÂ∫îÁî®Ëøô‰∏™Ê°ÜÊû∂ÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊåáÂØºÊ®°ÂûãÁîüÊàêÊõ¥Âä†Áõ∏ÂÖ≥„ÄÅÂáÜÁ°ÆÂíåÊúâÁî®ÁöÑÂÜÖÂÆπ„ÄÇCOSTAR ÊòØ‰∏Ä‰∏™È¶ñÂ≠óÊØçÁº©Áï•ËØçÔºåÊØè‰∏™Â≠óÊØç‰ª£Ë°®‰∫ÜÊèêÁ§∫Ê°ÜÊû∂‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊñπÈù¢Ôºö ContextÔºà‰∏ä‰∏ãÊñáÔºâÔºö Êèê‰æõËØ¶ÁªÜÁöÑËÉåÊôØ‰ø°ÊÅØÂíåÂÖ∑‰ΩìÊÉÖÂ¢ÉÔºåÂ∏ÆÂä©Ê®°ÂûãÁêÜËß£ÈóÆÈ¢òÁöÑÂÖ∑‰ΩìÈúÄÊ±Ç„ÄÇ Á§∫‰æãÔºöÊèê‰æõÊõ¥Â§öÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºå‰ΩøÁîüÊàêÁöÑÂÜÖÂÆπÊõ¥Âä†Á¨¶ÂêàÈ¢ÑÊúü„ÄÇ ObjectiveÔºàÁõÆÊ†áÔºâÔºö ÊòéÁ°ÆÁîüÊàêÂÜÖÂÆπÁöÑÁõÆÊ†áÂíåÈ¢ÑÊúüÁªìÊûúÔºåÂ∏ÆÂä©Ê®°ÂûãËÅöÁÑ¶‰∫éÁâπÂÆö‰ªªÂä°„ÄÇ Á§∫‰æãÔºöÊ∏ÖÊô∞Âú∞ÈôàËø∞Â∏åÊúõÊ®°ÂûãÁîüÊàê‰ΩïÁßçÁ±ªÂûãÁöÑÂÜÖÂÆπÔºàÂ¶ÇËß£Èáä„ÄÅÊÄªÁªì„ÄÅÂàõÂª∫ÊïÖ‰∫ãÁ≠âÔºâ„ÄÇ StyleÔºàÈ£éÊ†ºÔºâÔºö ËÆæÂÆöÁîüÊàêÂÜÖÂÆπÁöÑËØ≠Ê∞î„ÄÅÈ£éÊ†ºÂíåÊ†ºÂºèÔºåÁ°Æ‰øùËæìÂá∫Á¨¶ÂêàÈ¢ÑÊúüÁöÑÈ£éÊ†º„ÄÇ Á§∫‰æãÔºöÊåáÂÆöÊ≠£Âºè„ÄÅÈùûÊ≠£Âºè„ÄÅÂπΩÈªò„ÄÅÂ≠¶ÊúØÁ≠â‰∏çÂêåÁöÑÈ£éÊ†º„ÄÇ ToneÔºàËØ≠Ê∞îÔºâÔºö ÊåáÂÆöÁîüÊàêÂÜÖÂÆπÁöÑÊÉÖÊÑüÂü∫Ë∞ÉÔºåÂ¶ÇÂèãÂ•Ω„ÄÅÈºìÂä±„ÄÅË≠¶ÂëäÁ≠âÔºå‰ª•Á¨¶ÂêàÈ¢ÑÊúüÁöÑ‰∫§ÊµÅÊïàÊûú„ÄÇ Á§∫‰æãÔºöÁ°ÆÂÆöÈúÄË¶ÅÊ∏©Âíå„ÄÅ‰∏•ËÇÉ„ÄÅËΩªÊùæÁ≠â‰∏çÂêåÁöÑËØ≠Ê∞î„ÄÇ AudienceÔºàÂèó‰ºóÔºâÔºö ÊòéÁ°ÆÁîüÊàêÂÜÖÂÆπÁöÑÁõÆÊ†áËØªËÄÖÊàñËßÇ‰ºóÔºåÂ∏ÆÂä©Ê®°ÂûãË∞ÉÈÄÇÂÜÖÂÆπÁöÑÂ§çÊùÇÂ∫¶ÂíåÈÄÇÁî®ÊÄß„ÄÇ Á§∫‰æãÔºöÂå∫ÂàÜÊòØÈù¢Âêë‰∏ì‰∏ö‰∫∫Â£´ËøòÊòØÊôÆÈÄöËØªËÄÖÔºå‰ΩøÂÜÖÂÆπÊõ¥ÂÖ∑ÈíàÂØπÊÄß„ÄÇ RelevanceÔºàÁõ∏ÂÖ≥ÊÄßÔºâÔºö Âº∫Ë∞ÉÁîüÊàêÂÜÖÂÆπ‰∏é‰∏ªÈ¢òÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÈÅøÂÖç‰∏çÂøÖË¶ÅÁöÑÂÅèÁ¶ªÂíåÊó†ÂÖ≥ÂÜÖÂÆπ„ÄÇ Á§∫‰æãÔºöÁ°Æ‰øùÊ®°ÂûãÁîüÊàêÁöÑÂÜÖÂÆπÁõ¥Êé•ÂõûÁ≠îÈóÆÈ¢òÔºåÈÅøÂÖçÂÅèÈ¢ò„ÄÇ ","date":"2024-07-24","objectID":"/tools/costar/:0:0","tags":null,"title":"COSTARÊèêÁ§∫ËØçÊ°ÜÊû∂Á¨îËÆ∞","uri":"/tools/costar/"},{"categories":["TOOLS"],"content":"Â¶Ç‰ΩïÂ∫îÁî®COSTARÊèêÁ§∫ËØçÊ°ÜÊû∂ ‰∏ä‰∏ãÊñáÔºàContextÔºâÔºö Êèê‰æõË∂≥Â§üÁöÑËÉåÊôØ‰ø°ÊÅØ„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÈúÄË¶ÅÁîüÊàêÂÖ≥‰∫éÁâπÂÆöÂéÜÂè≤‰∫ã‰ª∂ÁöÑÂÜÖÂÆπÔºåÂèØ‰ª•ÂÖàÊèê‰æõ‰∏Ä‰∫õÁõ∏ÂÖ≥ÁöÑÂéÜÂè≤ËÉåÊôØ„ÄÇ ËØ∑Êèê‰æõÂÖ≥‰∫é1969Âπ¥ÈòøÊ≥¢ÁΩó11Âè∑ÁôªÊúà‰ªªÂä°ÁöÑËØ¶ÁªÜÊèèËø∞„ÄÇÈòøÊ≥¢ÁΩó11Âè∑ÊòØÁæéÂõΩÂÆáËà™Â±ÄÁöÑ‰∏ÄÊ¨°‰ªªÂä°ÔºåÁõÆÁöÑÊòØÂ∞Ü‰∫∫Á±ªÈ¶ñÊ¨°ÈÄÅ‰∏äÊúàÁêÉ„ÄÇ ÁõÆÊ†áÔºàObjectiveÔºâÔºö ÊòéÁ°ÆÁîüÊàêÂÜÖÂÆπÁöÑÁõÆÊ†á„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÈúÄË¶ÅÊ®°ÂûãÁîüÊàê‰∏ÄÁØá‰ªãÁªçÊñáÁ´†ÔºåÂèØ‰ª•ÊòéÁ°ÆËØ¥ÊòéËøô‰∏ÄÁÇπ„ÄÇ ËØ∑ÂÜô‰∏ÄÁØáÂÖ≥‰∫éÂèØÊåÅÁª≠ÂèëÂ±ïÁöÑ‰ªãÁªçÊñáÁ´†ÔºåÈáçÁÇπ‰ªãÁªçÂÖ∂ÈáçË¶ÅÊÄßÂíå‰∏ªË¶ÅÁ≠ñÁï•„ÄÇ È£éÊ†ºÔºàStyleÔºâÔºö ÊåáÂÆöÁîüÊàêÂÜÖÂÆπÁöÑÈ£éÊ†º„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÈúÄË¶ÅÁîüÊàêÂ≠¶ÊúØÈ£éÊ†ºÁöÑÊñáÁ´†ÔºåÂèØ‰ª•ËøôÊ†∑ÊèêÁ§∫„ÄÇ ËØ∑‰ª•Â≠¶ÊúØÈ£éÊ†ºÂÜô‰∏ÄÁØáÂÖ≥‰∫é‰∫∫Â∑•Êô∫ËÉΩÂú®ÂåªÁñóÈ¢ÜÂüüÂ∫îÁî®ÁöÑËÆ∫Êñá„ÄÇ ËØ≠Ê∞îÔºàToneÔºâÔºö ÊåáÂÆöÂÜÖÂÆπÁöÑËØ≠Ê∞î„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÂ∏åÊúõÂÜÖÂÆπÂÖ∑ÊúâÊøÄÂä±ÊÄßÔºåÂèØ‰ª•ÊòéÁ°ÆËØ¥Êòé„ÄÇ ËØ∑Áî®ÈºìÂä±ÁöÑËØ≠Ê∞îÂÜô‰∏ÄÁØáÂÖ≥‰∫éÂ¶Ç‰ΩïÂÖãÊúçÂõ∞ÈöæÁöÑÊñáÁ´†„ÄÇ Âèó‰ºóÔºàAudienceÔºâÔºö ÊòéÁ°ÆÂÜÖÂÆπÁöÑÂèó‰ºó„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÂÜÖÂÆπÊòØÈù¢ÂêëÂ≠¶ÁîüÁöÑÔºåÂèØ‰ª•ËøôÊ†∑ÊèêÁ§∫„ÄÇ ËØ∑‰∏∫È´ò‰∏≠ÁîüÂÜô‰∏ÄÁØáÂÖ≥‰∫éÊ∞îÂÄôÂèòÂåñÁöÑ‰ªãÁªçÊñáÁ´†„ÄÇ Áõ∏ÂÖ≥ÊÄßÔºàRelevanceÔºâÔºö Âº∫Ë∞ÉÂÜÖÂÆπÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÈúÄË¶ÅÂÜÖÂÆπËÅöÁÑ¶Âú®Êüê‰∏™‰∏ªÈ¢òÔºåÂèØ‰ª•ËøôÊ†∑ÊèêÁ§∫„ÄÇ ËØ∑ÂÜô‰∏ÄÁØáÂÖ≥‰∫éÁîµÂä®ËΩ¶‰ºòÂäøÁöÑÊñáÁ´†ÔºåÁâπÂà´ÂÖ≥Ê≥®ÂÖ∂ÂØπÁéØÂ¢ÉÁöÑÁßØÊûÅÂΩ±Âìç„ÄÇ ","date":"2024-07-24","objectID":"/tools/costar/:0:1","tags":null,"title":"COSTARÊèêÁ§∫ËØçÊ°ÜÊû∂Á¨îËÆ∞","uri":"/tools/costar/"},{"categories":["CS186"],"content":"ÊÄé‰πàËØªÊáÇSQLËØ≠Âè•Ôºü FROM WHERE, to eliminate rows SELECT GROUP BY HAVING, to eliminate groups DISTINCT ORDER BY, LIMIT, OFFSETÁ≠âÁ≠âÊ†ºÂºèÂåñËæìÂá∫ Join Queries ","date":"2024-07-22","objectID":"/databasel2/:0:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"cross product ÁîüÊàêÊâÄÊúâÁöÑÁªÑÂêàÔºåÁÑ∂ÂêéËøáÊª§Êéâ‰∏çÁ¨¶ÂêàÊù°‰ª∂ÁöÑÁªÑÂêàÔºå‰ΩÜÊòØ‰ΩéÊïà ËÄÉËôë‰ª•‰∏ãsqlÔºåÊõ¥Âä†ÁÆÄÊ¥Å SELECT S.sid, sname, bid FROM Sailors AS S, Reserves AS R WHERE S.sid = R.sid ","date":"2024-07-22","objectID":"/databasel2/:1:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"self join and more aliases SELECT x.sname, x.age, y.sname AS sname2, y.age AS age2 FROM Sailors AS x, Sailors AS y WHERE x.age \u003e y.age ","date":"2024-07-22","objectID":"/databasel2/:2:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"inner/natural join where clause üÜô ü§ì Áúã‰∏ãÈù¢ select s.*, r.bid from sailors as s inner join reserves as r on s.sid = r.sid ","date":"2024-07-22","objectID":"/databasel2/:3:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"left outer join returns all matched rows, and preserves all unmatched rows from the left table of the join clause NULLÂá∫Áé∞ FULL OUTER JOINÁ≠âÁ≠âÂêåÁêÜ select r.sid, r.bid, b.bname from reserves as r full outer join boats as b on r.bid = b.bid Arithmetic Expressions Ê≥®ÊÑèSELECTÂíåWHERE SELECT salary * 1.1 AS new_salary FROM Employees WHERE 2*salary \u003e 10000 use sql as calculator ü§ì SELECT log(1000) as three, exp(ln(2)) as two, cos(pi()) as zero, ln(2*3) = ln(2) + ln(3) as sanity; string functions old way ü§® SELECT s.sname FROM Sailors AS s WHERE s.sname LIKE 'a_%' new way üòé use regular expressions! SELECT s.sname FROM Sailors AS s WHERE s.sname ~ 'a.*' bool and combining SELECT r.sid FROM boats as b, reserves as r WHERE b.bid = r.bid AND (b.color = 'blue' OR b.color = 'green') ‰ª•‰∏ä‰∏§ËÄÖÁ≠â‰ª∑ Áúã‰∏ãÈù¢‰∏§‰∏™ SELECT r.sid FROM boats as b, reserves as r WHERE b.bid = r.bid AND (b.color = 'blue' AND b.color = 'green') return nothing ü§Ø ËøîÂõûÂç≥È¢ÑÂÆö‰∫ÜÁ∫¢ËàπÂèàÈ¢ÑÂÆö‰∫ÜÁªøËàπÁöÑ‰∫∫ ü§ì Set Operations ","date":"2024-07-22","objectID":"/databasel2/:4:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"only set union, intersect, except, ËøîÂõûÁöÑÈÉΩÊòØÈõÜÂêàÔºåÊ≤°ÊúâÈáçÂ§ç ","date":"2024-07-22","objectID":"/databasel2/:5:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"multiset UNION ALL: ËøîÂõûÊâÄÊúâÂÖÉÁ¥†ÔºåÂåÖÊã¨ÈáçÂ§çÂÖÉÁ¥† sum INTERSECT ALL: ËøîÂõû‰∫§ÈõÜÔºåÂåÖÊã¨ÈáçÂ§çÂÖÉÁ¥† min EXCEPT ALL: ËøîÂõûÂ∑ÆÈõÜÔºåÂåÖÊã¨ÈáçÂ§çÂÖÉÁ¥† minus Nested Queries subquery üòÄ select s.sname from sailors as s where s.sid in (select r.sid from reserves as r where r.bid = 6767) NOT IN ÂêåÁêÜÂç≥ÂèØ ËÄÉËôëEXISTSÔºå ÈùûÁ©∫Âç≥ÂèØËøîÂõû Âè¶‰∏Ä‰∏™‰æãÂ≠ê select s.sname from sailors as s where exists (select * from reserves as r where r.sid = s.sid and r.bid = 6767) ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊü•ËØ¢‰∏≠ÂåÖÂê´‰∫Ü‰∏Ä‰∏™EXISTSÊù°‰ª∂ÔºåËøô‰∏™Êù°‰ª∂‰∏≠ÁöÑÂ≠êÊü•ËØ¢ÊòØ‰∏é‰∏ªÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑ„ÄÇÊØèÂΩì‰∏ªÊü•ËØ¢Â§ÑÁêÜSailorsË°®‰∏≠ÁöÑ‰∏ÄË°åÊó∂ÔºåÈÉΩ‰ºöÂ∞ÜËØ•Ë°å‰∏≠ÁöÑsidÂÄºÂ∏¶ÂÖ•Â≠êÊü•ËØ¢‰∏≠ËøõË°åËÆ°ÁÆóÔºå‰ª•Á°ÆÂÆöËøôË°åÊï∞ÊçÆÊòØÂê¶Êª°Ë∂≥Êù°‰ª∂ÔºàÂç≥ÊòØÂê¶Â≠òÂú®‰∏ÄÊù°ÂØπÂ∫îÁöÑReservesËÆ∞ÂΩïÔºâ„ÄÇËøôÊ†∑ÔºåÂ≠êÊü•ËØ¢ÁöÑËÆ°ÁÆó‰ºöÈöèÁùÄSailorsË°®‰∏≠Ë°åÁöÑ‰∏çÂêåËÄåÂèòÂåñÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏∫ÊØè‰∏ÄË°åÈáçÊñ∞ËÆ°ÁÆó„ÄÇ ËøôÊÑèÂë≥ÁùÄÔºåÂ¶ÇÊûúSailorsË°®‰∏≠ÊúâÂæàÂ§öË°åÔºåÂ≠êÊü•ËØ¢‰πü‰ºöË¢´ÊâßË°åÂæàÂ§öÊ¨°ÔºåËøôÂèØËÉΩ‰ºöÂΩ±ÂìçÊü•ËØ¢ÁöÑÊÄßËÉΩ„ÄÇ ËÄÉËôëANYÔºå ALL SELECT s.sname FROM sailors AS s WHERE s.age \u003e ANY (SELECT AVG(age) FROM sailors) ÂÖ≥Á≥ªÈô§Ê≥ï Ëøô‰∏™PPTÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™ÂÖ≥‰∫é‚ÄúÂÖ≥Á≥ªÈô§Ê≥ï‚ÄùÔºàRelational DivisionÔºâÁöÑSQLÊü•ËØ¢ÁöÑ‰æãÂ≠êÔºåÁõÆÁöÑÊòØÂØªÊâæÈÇ£‰∫õÂ∑≤ÁªèÈ¢ÑËÆ¢‰∫ÜÊâÄÊúâËàπÂè™ÁöÑÊ∞¥Êâã„ÄÇÈÄöËøáËøôÁßçÊü•ËØ¢ÔºåÊàë‰ª¨ÂèØ‰ª•ÊâæÂà∞ÈÇ£‰∫õÊ≤°ÊúâÊºèÊéâ‰ªª‰Ωï‰∏ÄËâòËàπÂè™È¢ÑËÆ¢ÁöÑÊ∞¥Êâã„ÄÇ ","date":"2024-07-22","objectID":"/databasel2/:6:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"ÁêÜËß£Ê≠•È™§Ôºö ÂÖ≥Á≥ªÈô§Ê≥ïÁöÑÂÆö‰πâÔºö ÂÖ≥Á≥ªÈô§Ê≥ïÊòØ‰∏ÄÁßçÂ§çÊùÇÁöÑSQLÊü•ËØ¢Êìç‰ΩúÔºåÁî®‰∫éÊâæÂà∞ÈÇ£‰∫õÂú®‰∏Ä‰∏™ÈõÜÂêà‰∏≠ÂØπÊâÄÊúâÂÖÉÁ¥†ÈÉΩÊª°Ë∂≥Êüê‰∏™Êù°‰ª∂ÁöÑËÆ∞ÂΩï„ÄÇ Âú®Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåÊàë‰ª¨ÊÉ≥ÊâæÂà∞ÈÇ£‰∫õÈ¢ÑËÆ¢‰∫ÜÊâÄÊúâËàπÂè™ÁöÑÊ∞¥Êâã„ÄÇ Êü•ËØ¢ÁöÑÈÄªËæëÔºö Â§ñÂ±ÇÊü•ËØ¢ÔºöSELECT S.sname FROM Sailors SÔºöÈÄâÊã©ÊâÄÊúâÊ∞¥ÊâãÁöÑÂêçÂ≠ó„ÄÇ NOT EXISTSÂ≠êÊü•ËØ¢ÔºöËøô‰∏™ÈÉ®ÂàÜÊòØÂÖ≥ÈîÆÔºö WHERE NOT EXISTS ( SELECT B.bid FROM Boats B WHERE NOT EXISTS ( SELECT R.bid FROM Reserves R WHERE R.bid = B.bid AND R.sid = S.sid ) ) ÈÄªËæëËß£ÈáäÔºö È¶ñÂÖàÔºåÊü•ËØ¢‰∫ÜÊâÄÊúâÁöÑËàπÂè™ (Boats B)„ÄÇ ÂØπ‰∫éÊØèËâòËàπÔºåÂè™Ë¶ÅÂ≠òÂú®‰∏ÄËâòËàπ (B.bid)ÔºåÂΩìÂâçÊ∞¥Êâã (S.sid) Ê≤°ÊúâÈ¢ÑËÆ¢ (R.sid = S.sid AND R.bid = B.bid)ÔºåÈÇ£‰πàËøô‰∏™Ê∞¥ÊâãÂ∞±‰ºöË¢´ÊéíÈô§„ÄÇ Â¶ÇÊûúÂØπ‰∫éÊüê‰∏™Ê∞¥ÊâãÔºå‰∏çÂ≠òÂú®ËøôÊ†∑‰∏ÄËâò‰ªñÊ≤°ÊúâÈ¢ÑËÆ¢ÁöÑËàπÔºàÂç≥NOT EXISTSÁöÑÁªìÊûú‰∏∫ÁúüÔºâÔºåÈÇ£‰πàËøô‰∏™Ê∞¥ÊâãÂ∞±Êª°Ë∂≥È¢ÑËÆ¢‰∫ÜÊâÄÊúâËàπÁöÑÊù°‰ª∂„ÄÇ ÁªìËÆ∫Ôºö ÊúÄÁªàÁöÑÊü•ËØ¢Â∞Ü‰ºöËøîÂõûÈÇ£‰∫õÂêçÂ≠óÊòØÊ∞¥ÊâãÂπ∂‰∏îÈ¢ÑËÆ¢‰∫ÜÊØè‰∏ÄËâòËàπÁöÑ‰∫∫„ÄÇ ARGMAX find the sailor with the highest rating select * from sailors as s where s.rating \u003e= ALL (SELECT sailors.rating FROM sailors) select * from sailors as s where s.rating = (SELECT MAX(sailors.rating) FROM sailors) Ê≥®ÊÑè‰∏ãÈù¢Ëøô‰∏™ ‚òï select * from sailors as s order by s.rating desc limit 1 ","date":"2024-07-22","objectID":"/databasel2/:6:1","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"ARGMAX GROUP BY ÊèêÁ§∫ÔºöÂÄüÂä©ËßÜÂõæÁ≠õÈÄâ Creating Views ÊúâÊó∂ÂÄô‰∏çÈúÄË¶ÅÂª∫Á´ãÊòæÂºèÁöÑviews select b, c from boats as b, (select b.bid, count(*) from reserves as r, boats as b where r.bid = b.bid and b.color = 'blue' group by b.bid) as Reds(bid, c) where b.bid = Reds.bid ÊúâÊó∂ÂÄôCTEÔºàcommon table expressionÔºâË°®Á§∫Ê≥ïÊõ¥Âä†ÁÆÄÊ¥Å Ê≥®ÊÑèWITH‰ªéÂè•ÂêéÈù¢ËÉΩÂª∫Á´ãÂ§ö‰∏™ËßÜÂõæÔºåËÆ∞ÂæóÂä†‰∏äÈÄóÂè∑ÔºÅ NULL ","date":"2024-07-22","objectID":"/databasel2/:7:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"NULLÁöÑÊØîËæÉ ‰∏çË¶Å‰ΩøÁî®=ÔºåËÄåÊòØ‰ΩøÁî®IS NULLÊàñIS NOT NULL IS NULL : Â∑¶ËæπÊòØNULL IS NOT NULL : Â∑¶Ëæπ‰∏çÊòØNULL ","date":"2024-07-22","objectID":"/databasel2/:8:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"NULL in boolean expressions È¶ñÂÖàÔºå ÂΩ¢Â¶ÇWHERE NULLÊòØ‰∏çÂêàÊ≥ïÁöÑÔºÅ ‰∏âÂÄºÈÄªËæëË°®Â¶Ç‰∏ã ","date":"2024-07-22","objectID":"/databasel2/:9:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"NULL in aggregation functions Êé•‰∏ãÊù•Â∞±ÊòØimplement! üòÄ ","date":"2024-07-22","objectID":"/databasel2/:10:0","tags":null,"title":"CS186-L2: SQL‚Ö°","uri":"/databasel2/"},{"categories":["CS186"],"content":"Â§ßÁ∫≤ËøõÁ®ãÔºö sheet SQL I ","date":"2024-07-22","objectID":"/databasel1/:0:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["CS186"],"content":"pros and cons ","date":"2024-07-22","objectID":"/databasel1/:1:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["CS186"],"content":"relational Terminology and concepts database: set of name relations relation(table): schema: descriptions ‚Äúmetadata‚Äù fixed, unique attribute names, atomic types instance: set of data Á¨¶Âêàdescription often changed, can duplicate multiset of tuples or ‚Äúrows‚Äù attribute (column,field) tuple (row,record),ÊÄÄÁñë‰∏Ä‰∫õpythonÊ¶ÇÂøµ‰πüÊù•Ëá™‰∫éÊ≠§ ","date":"2024-07-22","objectID":"/databasel1/:2:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["CS186"],"content":"DDL (Data Definition Language) CREATE TABLE myTable ( ID INTEGER, myName CHAR(50), Age INTEGER, Salary FLOAT, PRIMARY KEY (ID, myName), FOREIGN KEY (ID) REFERENCES myOtherTable(ID), FOREIGN KEY (myName) REFERENCES myOtherTable(myName) ); SELECT [DISTINCT] \u003ccolumn expression list\u003e FROM \u003csingle_table\u003e [WHERE \u003cpredicate\u003e] ORDER BY Lexicographic order by default Â≠óÂÖ∏Â∫è LIMIT Aggregation functions AVG: average COUNT: count the number of rows MAX: maximum value MIN: minimum value SUM: sum of values SELECT AVG(Salary) FROM myTable; GROUP BY HAVING SELECT AVG(Salary) FROM myTable GROUP BY Age HAVING AVG(Salary) \u003e 50000; ‰∏çÂêåÁöÑDISTINCT‰ΩçÁΩÆÊïàÊûú‰∏çÂêå, ÂÖ∂‰∏≠Á¨¨‰∫å‰∏™ÂéãÊ†πÊ≤°Áî® ","date":"2024-07-22","objectID":"/databasel1/:3:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["DATA100"],"content":"mapreduce MapReduceÊòØ‰∏ÄÁßçÁºñÁ®ãÊ®°Âûã,Áî®‰∫éÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÁöÑÂπ∂Ë°åËøêÁÆó[1][2][3]„ÄÇÂÆÉÂ∞ÜÂ§çÊùÇÁöÑÂπ∂Ë°åËÆ°ÁÆóËøáÁ®ãÊäΩË±°‰∏∫‰∏§‰∏™ÂáΩÊï∞:MapÂíåReduce[4]„ÄÇ MapÂáΩÊï∞Â∞ÜËæìÂÖ•Êï∞ÊçÆÈõÜÊãÜÂàÜÊàêÁã¨Á´ãÁöÑÂùó,Âπ∂ÂØπÊØè‰∏™ÂùóÂ∫îÁî®Êò†Â∞ÑÊìç‰Ωú,ÁîüÊàê‰∏ÄÁªÑ‰∏≠Èó¥ÈîÆÂÄºÂØπ[1][2][3]„ÄÇReduceÂáΩÊï∞‰ºöÂØπÊâÄÊúâMapÁöÑËæìÂá∫ËøõË°åÂêàÂπ∂Êìç‰Ωú,ÁîüÊàêÊúÄÁªàÁªìÊûú[1][2][3]„ÄÇ MapReduceÁöÑ‰∏ªË¶ÅÁâπÁÇπÂåÖÊã¨[4][5]: Êòì‰∫éÁºñÁ®ã:Á®ãÂ∫èÂëòÂè™ÈúÄÊèèËø∞ÂÅö‰ªÄ‰πà,ÂÖ∑‰ΩìÊÄé‰πàÂÅöÁî±Á≥ªÁªüÁöÑÊâßË°åÊ°ÜÊû∂Â§ÑÁêÜ ËâØÂ•ΩÁöÑÊâ©Â±ïÊÄß:ÂèØÈÄöËøáÊ∑ªÂä†ËäÇÁÇπÊâ©Â±ïÈõÜÁæ§ËÉΩÂäõ È´òÂÆπÈîôÊÄß:ÈÄöËøáËÆ°ÁÆóËøÅÁßªÊàñÊï∞ÊçÆËøÅÁßªÁ≠âÁ≠ñÁï•ÊèêÈ´òÈõÜÁæ§ÁöÑÂèØÁî®ÊÄß‰∏éÂÆπÈîôÊÄß MapReduceÈááÁî®\"ÂàÜËÄåÊ≤ª‰πã\"Á≠ñÁï•,Â∞ÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂàáÂàÜÊàêÂ§ö‰∏™Áã¨Á´ãÁöÑÂàÜÁâá,Ëøô‰∫õÂàÜÁâáÂèØ‰ª•Ë¢´Â§ö‰∏™Map‰ªªÂä°Âπ∂Ë°åÂ§ÑÁêÜ[4]„ÄÇÂÆÉËÆæËÆ°ÁöÑ‰∏Ä‰∏™ÁêÜÂøµÊòØ\"ËÆ°ÁÆóÂêëÊï∞ÊçÆÈù†Êã¢\",ÁßªÂä®Êï∞ÊçÆÈúÄË¶ÅÂ§ßÈáèÁöÑÁΩëÁªú‰º†ËæìÂºÄÈîÄ[4]„ÄÇ ÊÄª‰πã,MapReduceÊòØ‰∏ÄÁßçÁÆÄÂçï„ÄÅÂèØÊâ©Â±ïÁöÑÂπ∂Ë°åËÆ°ÁÆóÊ®°Âûã,ÈÄöËøáÊäΩË±°MapÂíåReduceÂáΩÊï∞,‰ΩøÂæóÁ®ãÂ∫èÂëòÂèØ‰ª•ËΩªÊùæÁºñÂÜôÂ§ßËßÑÊ®°Âπ∂Ë°åÂ∫îÁî®Á®ãÂ∫è,ËÄåÊó†ÈúÄÂÖ≥Ê≥®Â∫ïÂ±ÇÁöÑÂàÜÂ∏ÉÂºèÁªÜËäÇ[1][2][3][4][5]„ÄÇ Citations: [1] https://baike.baidu.com/item/MapReduce/133425 [2] https://zh.wikipedia.org/zh-hans/MapReduce [3] https://www.ibm.com/cn-zh/topics/mapreduce [4] https://cshihong.github.io/2018/05/11/MapReduce%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/ [5] https://cloud.tencent.com/developer/article/1778549 apache spark lazy strategy: Âª∂ËøüËÆ°ÁÆóÁ≠ñÁï•,SparkÈªòËÆ§ÈááÁî®ËøôÁßçÁ≠ñÁï•,Âç≥Âè™ÊúâÂΩìÊï∞ÊçÆÁúüÊ≠£Ë¢´‰ΩøÁî®Êó∂Êâç‰ºöËÆ°ÁÆó„ÄÇ ÁºñËØë‰ºòÂåñËØ≠Âè•ÊâßË°åÈ°∫Â∫èÔºÅ Conclusion Â∑•ÂÖ∑Èìæ What is next? ÊúâÁî®ÁöÑdata scienceÈìæÊé• http://kaggle.com https://github.com/awesomedata/awesome-public-datasets http://toolbox.google.com/datasetsearch https://towardsdatascience.com https://www.reddit.com/r/dataisbeautiful/ https://fivethirtyeight.com ","date":"2024-07-19","objectID":"/datal26/:0:0","tags":null,"title":"DATA100-L26: Parallel Data Analytics; Conclusion","uri":"/datal26/"},{"categories":["DATA100"],"content":"introduction to clustering no label at all üò¢ K-means clustering ÁÆóÊ≥ïÂä®ÁîªÊºîÁ§∫ K-Means vs KNN minimizing inertia convex?? ÊçüÂ§±ÂáΩÊï∞‰∏ç‰∏ÄÂÆöÂá∏ÔºåÊ¢ØÂ∫¶‰∏ãÈôçÈöæÈ°∂ how to see which one is better ‚ùì ‰ΩÜÊòØÊâæÂà∞ÂÖ®Â±ÄÊúÄ‰ºòËß£ÈùûÂ∏∏Âõ∞Èöæ agglomerative clustering ÊºîÁ§∫ËßÅ‰∏äÈù¢ÈìæÊé•‰ª•Âèälec codeÔºÅ ÂíåCS61BÁöÑminimum spanning treeÁ±ª‰ººÔºåÊØèÊ¨°ÂêàÂπ∂‰∏§‰∏™ÊúÄËøëÁöÑÁÇπÔºåÁõ¥Âà∞ÁªàÊ≠¢Êù°‰ª∂ outlier ÊúâÊó∂ÂøΩÁï•Â§ÑÁêÜÊàñËÄÖËá™Êàê‰∏ÄÁ±ª picking K SmaxÔºü can s be negative? ","date":"2024-07-19","objectID":"/datal24/:0:0","tags":null,"title":"DATA100-L24: Clustering","uri":"/datal24/"},{"categories":["DATA100"],"content":"summary ","date":"2024-07-19","objectID":"/datal24/:1:0","tags":null,"title":"DATA100-L24: Clustering","uri":"/datal24/"},{"categories":["DATA100"],"content":"impetus for regulation why ‚Äúyou‚Äù should care Because you are gonna to be a data scientist and product owner! regulations: Privacy laws GDPR (General Data Protection Regulation) CCPA (California Consumer Privacy Act) Cyber Security Law in China deletion can be more difficult than you think üòè ‰º†Ëæì‰πüË¶ÅÁõëÁÆ° fully take advantage of the ‚Äúregulations‚Äù take care of gray areas ü§î work with dear NGO and GO other regulations/ regulatory bodies ","date":"2024-07-19","objectID":"/datal25/:0:0","tags":null,"title":"DATA100-L25: Data Regulations","uri":"/datal25/"},{"categories":["DATA100"],"content":"Multiclass Classification Â§öÂàÜÁ±ªÈóÆÈ¢ò ‰ΩÜÊòØÊ≤°Êúâsoftmax üò¢ Decision Trees (conceptually) Decision Tree Demo ","date":"2024-07-19","objectID":"/datal23/:0:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Creating Decision Trees in sklearn ÂèØËßÜÂåñ‰ª£Á†ÅËßÅlecture code ","date":"2024-07-19","objectID":"/datal23/:1:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Evaluating Tree Accuracy ","date":"2024-07-19","objectID":"/datal23/:2:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Overfit Decision Tree Example tree is too complex to generalize well to new data too tall and narrow ÊúâÁî®ÁöÑÁâπÂæÅË∂äÂ§öÔºåÊ†ëÁöÑÁªìÊûÑÂèØËÉΩÊØîËæÉÁÆÄÂçïü§î The Decision Tree Generation Algorithm ","date":"2024-07-19","objectID":"/datal23/:3:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Intuitively Evaluating Split Quality ÂàÜÂâ≤ÊÄé‰πàÊ†∑‚ÄúÊõ¥ÊòéÊòæ‚ÄùÔºü ","date":"2024-07-19","objectID":"/datal23/:4:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Entropy Ê≤øÁùÄÊ†ëÂêë‰∏ãÔºå‰ø°ÊÅØÁÜµË∂äÂ∞èÔºüÂèØËÉΩÂèòÂ§ßÔºÅ ","date":"2024-07-19","objectID":"/datal23/:5:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Generating Trees Using Entropy Weighted entropy can decrease! Traditional decision tree generation algorithm: All of the data starts in the root node. Repeat until every node is either pure or unsplittable: Pick the best feature x and split value Œ≤ such that the ŒîWS is maximized, e.g. x = petal_width, Œ≤ = 0.8 has ŒîWS = 0.91. Split data into two nodes, one where x \u003c Œ≤, and one where x ‚â• Œ≤. Notes: A node that has only one samples from one class is called a ‚Äúpure‚Äù node. A node that has overlapping data points from different classes and thus that cannot be split is called ‚Äúunsplittable‚Äù. Avoiding Overfitting Ê≠£ÂàôÂåñÂú®ËøôÈáå‰∏çËµ∑‰ΩúÁî®Ôºü ","date":"2024-07-19","objectID":"/datal23/:6:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Heuristically Restricting Decision Tree ComplexityÔºàÂêØÂèëÂºèÁÆóÊ≥ïÔºâ Approach2: allow full growth of the tree, but Prune the tree. ","date":"2024-07-19","objectID":"/datal23/:7:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Embracing Decision Tree Complexity with Random Forests ü™µ üòã Bagging: Short for Bootstrap AGGregatING. Generate bootstrap resamples of training data. Fit one model for each resample. Final model = average predictions of each small model. Invented by Leo Breiman in 1994 (Berkeley Statistics!). ","date":"2024-07-19","objectID":"/datal23/:8:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"ÂêØÂèëÂºèÁöÑÁâπÁÇπ These ideas are generally ‚Äúheuristic‚Äù Not provably best or mathematically optimal. Instead, they are just ideas that somebody thought sounded good, implemented, then found to work in practice acceptably well. Summary and Context Decision trees provide an alternate non-linear framework for classification and regression. The underlying principle is fundamentally different. Decision boundaries can be more complex. Danger of overfitting is high. Small decision trees are very easy to interpret. Doing regression with a tree is straightforward. See statquest video. Keeping complexity under control is not nearly as mathematically elegant and relies on heuristic rules. Hard constraints. Pruning rules. Random forests: Generate multiple trees using bootstrap. Have the trees vote on the outcome. ","date":"2024-07-19","objectID":"/datal23/:8:1","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Regression vs. Classification ÂÖ®ÊîªÁï•üòã intuition: the coin flip ÈáçÊñ∞ÂÆö‰πâÊ¶ÇÁéáÔºåÂè™ÈúÄË¶ÅÊª°Ë∂≥‰∏Ä‰∫õÊÄßË¥®Âç≥ÂèØ„ÄÇÂèÇËÄÉ Ê¶ÇÁéáËÆ∫‰∏éÊï∞ÁêÜÁªüËÆ° deriving the logistic regression model knn‰∏ÄÁû• ËøôËØ¥ÊòéÂèØ‰ª•‰ªéÊüê‰∫õÂèòÂåñËΩ¨Êç¢‰∏∫Á∫øÊÄßÊÄßË¥® ËÄÉËôë probability $p$ ËÄÉËôë odds $\\frac{p}{1-p}$ ËÄÉËôë log odds Âπø‰πâÁ∫øÊÄßÁî±Ê≠§ÂèØËßÅ ","date":"2024-07-19","objectID":"/datal21/:0:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"Graph of Averages ","date":"2024-07-19","objectID":"/datal21/:1:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"the sigmoid function $$ \\sigma(t)=\\frac{1}{1+e^{-t}} $$ the logistic regression model ","date":"2024-07-19","objectID":"/datal21/:2:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"comparison to linear regression parameter estimation ","date":"2024-07-19","objectID":"/datal21/:3:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"pitfalls of squared loss non-convex bounded, MSE ‚àà[0Ôºå1] conceptually questionable, not matching the ‚ÄúProbability and 0/1 labels‚Äù ","date":"2024-07-19","objectID":"/datal21/:4:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"cross-entropy loss $$ -\\frac{1}{N}\\sum_{i=1}^N[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)] $$ Loss function should penalize well! ","date":"2024-07-19","objectID":"/datal21/:5:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"maximum likelihood estimation see extra in L22! ","date":"2024-07-19","objectID":"/datal21/:6:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"logistic regression model continued ","date":"2024-07-19","objectID":"/datal22/:0:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"sklearn demo go to see lec code! ","date":"2024-07-19","objectID":"/datal22/:1:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"MLE: high-level, detailed (recorded) linear separability and regularization Á∫øÊÄßÂèØÂàÜÊÄßÔºöÂ¶ÇÊûúÂ≠òÂú®‰∏Ä‰∏™ Ë∂ÖÂπ≥Èù¢ÔºàhyperplaneÔºâ ÂèØ‰ª•Â∞ÜÊï∞ÊçÆÈõÜÂàÜÂâ≤Êàê‰∏§ÈÉ®ÂàÜÔºåÈÇ£‰πàËøôÂ∞±ÊòØÁ∫øÊÄßÂèØÂàÜÁöÑ„ÄÇ Ë∂ÖÂπ≥Èù¢ÁöÑÁª¥Â∫¶ÂíåÊï∞ÊçÆÈõÜÁöÑÁª¥Â∫¶Áõ∏Âêå $$ C $$ Ê≥®ÊÑèÂØπ‚Äúpush‚ÄùÁöÑÁêÜËß£ÔºÅ Âè¶‰∏ÄÁßçÁêÜËß£Ê≠£ÂàôÂåñÁöÑËßíÂ∫¶ ËøôÈáåÊòØÈÅøÂÖçlossÂá∫Áé∞Êó†ÈôêÂ§ßÁöÑÊÉÖÂÜµÔºàÊ¢ØÂ∫¶ÁàÜÁÇ∏ÔºüÔºâÔºåÈÅøÂÖçÂá∫Áé∞‰ΩøÂâçÈù¢ÊÉÖÂÜµÂèëÁîüÁöÑÂèÇÊï∞Ôºàinfinite thetaÔºâÂá∫Áé∞ÔºåÊâÄ‰ª•Âú®lossÈáåÈù¢È¢ÑÂÖàÂä†ÂÖ•Ê≠£ÂàôÂåñÈ°π„ÄÇ performance metrics ","date":"2024-07-19","objectID":"/datal22/:2:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"accuracy # using sklearn model.score(X_test, y_test) ","date":"2024-07-19","objectID":"/datal22/:3:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"imbalanced data, precision, recall Acc is not a good metric for imbalanced data, use precision and recall instead!!! $$ acc= \\frac{TP+TN}{n}\\ precision(Á≤æÁ°ÆÁéá)=\\frac{TP}{TP+FP}\\ recall(Âè¨ÂõûÁéá)=\\frac{TP}{TP+FN} $$ adjusting the classification threshold(ÈòàÂÄºÁïåÈôê) ","date":"2024-07-19","objectID":"/datal22/:4:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"a case study ÂèòÁïåÈôêÂèØËÉΩÊòØÂõ†‰∏∫imbalanced dataÂØºËá¥ÁöÑ ","date":"2024-07-19","objectID":"/datal22/:5:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"ROC curves and AUC ÊÄé‰πàÈÄâÊã©ÈòàÂÄºÔºü [extra] detailed MLE, gradient descent, PR curves ","date":"2024-07-19","objectID":"/datal22/:6:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"Why cross-entropy? KLÊï£Â∫¶: https://www.textbook.ds100.org/ch/24/classification_cost_justification.html?highlight=divergence MLE ‰ª•‰∏ãËÆ®ËÆ∫MLEÔºå‰∫åÂàÜÁ±ªÁöÑËØù‰ª• ‰ºØÂä™Âà© ‰∏æ‰æã ","date":"2024-07-19","objectID":"/datal22/:7:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"PR curves false positiveÂú®TÂèòÂ§ßÁöÑÊó∂ÂÄôÂ¢ûÂä†ÂæóÊõ¥Âø´ÔºåÊâÄ‰ª•ÂèØËÉΩslightly decrease ËÄÉËôëPR ","date":"2024-07-19","objectID":"/datal22/:8:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"ÊèíÊõ≤ ‰ºº‰πéËá™ÁÑ∂ÁßëÂ≠¶ÊâÄÊúâÂ≠¶ÁßëÈÉΩÂèØ‰ª•Ë¢´Ëß£ÊûÑ‰∏∫ ‚ÄúËßÇÊµãÂà∞ÁöÑÁü•ËØÜÁÇπÔºàcontextÔºâ‚Äù + ‰ø°ÊÅØÊï∞ÁêÜÂåñÔºàmath \u0026 computer scienceÔºâ Ôºü Êç¢Ë®Ä‰πãÂè™ÈúÄË¶Å‰∏ÄÊñπÈù¢‰∏çÊñ≠Êâ©ÂÖÖÊï∞ÊçÆ/Áü•ËØÜÁÇπÔºåÂè¶‰∏ÄÊñπÈù¢ÊèêÂá∫È´òÊòéÁöÑ‰ø°ÊÅØÊï∞ÁêÜÂåñÂàÜÊûêÊñπÊ≥ïÔºåÂ∞±ÂèØ‰ª•Êé®Âä®ÁßëÂ≠¶ÁöÑËøõÊ≠•Ôºüü§î ü§î ‚ùì https://docs.google.com/presentation/d/1YsxPERhul760_0TrLhawljbWWqDbtIp5tUm05irfkmw/edit#slide=id.g12444cd4007_0_537 ","date":"2024-07-19","objectID":"/datal22/:9:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"SQL II ","date":"2024-07-19","objectID":"/datal19/:0:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"sql and pandas how to connect sql to python import pandas as pd import sqlalchmey engine = sqlalchemy.create_engine('sqlite:///mydatabase.db') connection = engine.connect() pd.read_sql(\"\"\" SELECT * FROM mytable GROUP BY column1, column2 \"\"\", connection) ","date":"2024-07-19","objectID":"/datal19/:1:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"LIKE and CAST LIKE: search for a pattern in a column SELECT * FROM mytable WHERE column1 LIKE '%value%' CAST: convert data type ","date":"2024-07-19","objectID":"/datal19/:2:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"SQL Joins ","date":"2024-07-19","objectID":"/datal19/:3:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"Cross Join SELECT * FROM table1 CROSS JOIN table2 ","date":"2024-07-19","objectID":"/datal19/:3:1","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"Inner Join SELECT * FROM table1 INNER JOIN table2 ON table1.column1 = table2.column1 SELECT * FROM t1, t2 WHERE t1.id = t2.id ","date":"2024-07-19","objectID":"/datal19/:3:2","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"left/right/full outer join ","date":"2024-07-19","objectID":"/datal19/:3:3","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"other join conditions PCA ","date":"2024-07-19","objectID":"/datal19/:3:4","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"dimensionality and rank of data dimensionality \u003c===\u003e rank ","date":"2024-07-19","objectID":"/datal19/:4:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"two interpretations of matrix multiplication matrices as linear operations ~coordinate transformation ","date":"2024-07-19","objectID":"/datal19/:5:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"matrix decomposition and rank Â∞ΩÂèØËÉΩÁöÑ‰øùÁïô‰∏ªÊàêÂàÜÔºåËÄåËàçÂºÉÊó†ÂÖ≥ÁöÑÊàêÂàÜ ===\u003e rank ","date":"2024-07-19","objectID":"/datal19/:6:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"manual matrix decomposition exercise lin algÊúâÁöÑÊó∂ÂÄô‰∏çËÉΩÂàÜÊûêÂá∫ÁúüÊ≠£ÁöÑ‚Äúrank\",ÈúÄË¶Å ","date":"2024-07-19","objectID":"/datal19/:7:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"singular value decomposition (high level look) ","date":"2024-07-19","objectID":"/datal19/:8:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"recap and Goals ","date":"2024-07-19","objectID":"/datal20/:0:0","tags":null,"title":"DATA100-L20: PCA II","uri":"/datal20/"},{"categories":["DATA100"],"content":"approximate factorization $W\\ L\\rightarrow (W+L)/ 2$ rank ‰∏ãÈôç‰ΩøÂæó‰ø°ÊÅØÁº∫Â§±‰∫Ü ÊâÄ‰ª• $M_{100 \\times 4} = N_{100 \\times P} \\times Q_{P \\times 4}$ PÁöÑÂÄºÂ∞ΩÈáè‰∏çË¶ÅÂ∞è‰∫éÂéüÊù•ÁöÑ\"Áß©\" singular value decomposition (SVD) low rank approximation no bad! seem good! SVD theory È™åËØÅorthonormal set V@V.T = I ÂΩìÁõ∏‰πòÁöÑÊó∂ÂÄôÊú¨Ë¥®‰∏äÊòØÊóãËΩ¨Ôºå‰∏ç‰ºöÊãâ‰º∏ Principal Components Èõ∂‰∏≠ÂøÉÂåñÂÜçÊù•ÁúãPCA Principal Components and Variance PCA example Why is useful? ü§î ","date":"2024-07-19","objectID":"/datal20/:1:0","tags":null,"title":"DATA100-L20: PCA II","uri":"/datal20/"},{"categories":["DATA100"],"content":"sample statistics (from last time) ÂèÇËÄÉ Ê¶ÇÁéáËÆ∫‰∏éÊï∞ÁêÜÁªüËÆ° prediction vs. inference ","date":"2024-07-19","objectID":"/datal17/:0:0","tags":null,"title":"DATA100-L17: Estimators, Bias, and Variance","uri":"/datal17/"},{"categories":["DATA100"],"content":"modeling: assumptions of randomness the bias-variance tradeoff $$ model\\ risk = observation\\ variance + (model\\ bias)^2+model\\ variance $$ $$ \\mathbb{E}[(Y-\\hat{Y}(x))^2] = \\sigma^2+(\\mathbb{E}[\\hat{Y}(x)]-g(x))^2+Var(\\hat{Y}(x)) $$ interpreting slopes slope == 0? ÂÅáËÆæÊ£ÄÈ™åËØÅÊòéÊòØÂê¶Êó†ÂÖ≥ [Extra]review of the Bootstrap [Extra]derivation of Bias-Variance decomposition https://docs.google.com/presentation/d/1gzgxGO_nbCDajYs7qIpjzjQfJqKadliBOat7Es10Ll8/edit#slide=id.g11df3da7bd7_0_467 ","date":"2024-07-19","objectID":"/datal17/:1:0","tags":null,"title":"DATA100-L17: Estimators, Bias, and Variance","uri":"/datal17/"},{"categories":["DATA100"],"content":"why databases structured query language (SQL) üòã DBMS: database management system sql example type INT for integer REAL for decimal TEXT for string BLOB for ARBITRARY data DATETIME for date and time different implementations of sql support different types sql table use singular, CamelCase for SQL tables! basic sql queries ÈÄöÈÖç SELECT * FROM table_name; ÈÄâÂÆöÂ≠êÈõÜ SELECT column1, column2 FROM table_name; AS rename columns SELECT cute AS cuteness, smart AS intelligence FROM table_name; WHERE filter rows SELECT * FROM table_name WHERE column1 = 'value1' AND column2 = 'value2'; ORDER BY sort rows DESC for descending order, ASC for ascending order SELECT * FROM table_name ORDER BY column1 DESC; LIMIT restrict number of rows returned SELECT * FROM table_name LIMIT 10; OFFSET 5; basic GROUP BY Operations SELECT column1 FROM table_name GROUP BY column1; SELECT column1, SUM(column2), MAX(column3), MIN(column4) FROM table_name GROUP BY column1; SUM, AVG, COUNT, MAX, MIN, etc. SELECT column1, COUNT(*) FROM table_name GROUP BY column1; COUNT(*) counts the number of rows in each group.(even null values) SELECT column1, column2 FROM table_name GROUP BY column1, column2; HAVING COUNT(*) \u003e 5; Generate a group for each unique combination of column1 and column2 values, but only include groups with more than 5 rows. To filter groups, HAVING, to filter rows, WHERE (before HAVING). trickier GROUP BY Operations DISTINCT see in lecture 18 SELECT type, AVG(DISTINCT cost) FROM Dish GROUP BY type; SELECT DISTINCT type, cost FROM Dish WHERE cost \u003c 9; ","date":"2024-07-19","objectID":"/datal18/:0:0","tags":null,"title":"DATA100-L18: SQL I","uri":"/datal18/"},{"categories":["DATA100"],"content":"Cross Validation ","date":"2024-07-19","objectID":"/datal15/:0:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"the holdout method from sklearn.utils import shuffle training_set, dev_set = np.split(shuffle(data), [int(.8*len(data))]) ÊØîËæÉvalidation errorÂíåtraining errorÔºåÈÄâÊã©ÊúÄ‰ºòÁöÑÊ®°Âûã„ÄÇ ","date":"2024-07-19","objectID":"/datal15/:1:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"K-fold cross validation K=1 is equivalent to holdout method. ","date":"2024-07-19","objectID":"/datal15/:2:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"Test sets provide an unbiased estimate of the model‚Äôs performance on new, unseen data. Regularization ","date":"2024-07-19","objectID":"/datal15/:3:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"L2 regularization (Ridge) the small the ball, the simpler the model ÊãâÊ†ºÊúóÊó•ÊÄùÊÉ≥Ôºå$\\alpha$ Ë∂äÂ§ßÔºåÁ∫¶ÊùüË∂äÂº∫ÔºåÊ®°ÂûãË∂äÁÆÄÂçï„ÄÇ Â≤≠ÂõûÂΩí ","date":"2024-07-19","objectID":"/datal15/:4:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"scaling data for regularization Ê†áÂáÜÂåñÊï∞ÊçÆÔºåbe on the same scale ","date":"2024-07-19","objectID":"/datal15/:5:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"L1 regularization (Lasso) ","date":"2024-07-19","objectID":"/datal15/:6:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"summary ","date":"2024-07-19","objectID":"/datal15/:6:1","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"ÈöèÊú∫ÂèòÈáèÂèäÂÖ∂ÂàÜÂ∏É ÂèÇËÄÉ Ê¶ÇÁéáËÆ∫‰∏éÊï∞ÁêÜÁªüËÆ° ËØæÁ®ãÂç≥ÂèØ ÊúüÊúõ‰∏éÊñπÂ∑Æ $$ \\mathbb{E}[X] Âíå Var(X) $$ ÂèÇËÄÉ Ê¶ÇÁéáËÆ∫‰∏éÊï∞ÁêÜÁªüËÆ° ËØæÁ®ãÂç≥ÂèØ ÈöèÊú∫ÂèòÈáèÁöÑÂíå ","date":"2024-07-19","objectID":"/datal16/:0:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"equality vs Identically distributed vs IID ÊúâÊÑèÊÄùÁöÑÂàÜÊûê üòè ","date":"2024-07-19","objectID":"/datal16/:1:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"ÊúüÊúõÂíåÊñπÂ∑ÆÁöÑÊÄßË¥® ","date":"2024-07-19","objectID":"/datal16/:2:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"ÂçèÊñπÂ∑Æ‰∏éÁõ∏ÂÖ≥Á≥ªÊï∞ ‰ºØÂä™Âà©Ôºà0-1ÔºâÂíåBinomialÂàÜÂ∏É Sample Statistics ","date":"2024-07-19","objectID":"/datal16/:3:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"sample mean ","date":"2024-07-19","objectID":"/datal16/:4:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"central limit theorem ","date":"2024-07-19","objectID":"/datal16/:5:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":" $\\downarrow{shuffle}$ SGD: Stochastic Gradient Descent(but size == 1) ","date":"2024-07-19","objectID":"/datal13/:0:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"convexity (ÂáπÂá∏ÊÄß) feature engineering Âú®‰∫éÊÄé‰πà‰ΩøÁî®transforming ","date":"2024-07-19","objectID":"/datal13/:1:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"feature function see website code ","date":"2024-07-19","objectID":"/datal13/:2:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"non-numeric features one-hot encoding ","date":"2024-07-19","objectID":"/datal13/:2:1","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"concat ","date":"2024-07-19","objectID":"/datal13/:2:2","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"high order polynomials ","date":"2024-07-19","objectID":"/datal13/:3:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"detect overfitting collect more data more see next lecture ","date":"2024-07-19","objectID":"/datal13/:4:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":" Accuracy is a necessary, but not sufficient, condition for fair system. Fairness and transparency are context-dependent. Learn to work with contexts, and consider how your data analysis will reshape them. Keep in mind the power, and limits, of data analysis. ","date":"2024-07-19","objectID":"/datal14/:0:0","tags":null,"title":"DATA100-L14: Case Study (HCE): Fairness in Housing Appraisal","uri":"/datal14/"},{"categories":["DATA100"],"content":"ÂºÄÂßãË∞ÉÂåÖÔºÅüòè from sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(df[[\"total_bill\"]], df[\"tip\"]) df[\"predicted_tip\"] = model.predict(df[[\"total_bill\"]]) ÊâÄÊúâÁöÑÊú∫Âô®Â≠¶‰π†‰ºº‰πéÈÉΩÂú®ÊúÄÂ∞èÂåñloss functionÔºåËÄåÊ¢ØÂ∫¶‰∏ãÈôçÂ∞±ÊòØ‰∏ÄÁßç‰ºòÂåñÁÆóÊ≥ïÔºåÂÆÉÈÄöËøáËø≠‰ª£ÁöÑÊñπÂºè‰∏çÊñ≠Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞Ôºå‰ΩøÂæóloss functionÁöÑÂÄº‰∏çÊñ≠ÂáèÂ∞è„ÄÇ ËØ¶ÊÉÖËßÅNNDLÊ†èÁõÆ ","date":"2024-07-19","objectID":"/datal12/:0:0","tags":null,"title":"DATA100-L12: Gradient Descent, sklearn","uri":"/datal12/"},{"categories":["DATA100"],"content":"linear in theta linear combination of parameters $\\theta$ ","date":"2024-07-19","objectID":"/datal11/:1:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"define multiple linear regression OLS problem formulation ordinary least squares (OLS) Áî®Á∫øÊÄß‰ª£Êï∞ÈáçÂÜô‰πã $$ \\mathbb{\\hat{Y}} = \\mathbb{X}\\theta $$ ","date":"2024-07-19","objectID":"/datal11/:2:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"multiple linear regression model ","date":"2024-07-19","objectID":"/datal11/:3:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"MSE $$ R(\\theta) = \\frac{1}{n}||\\mathbb{Y}-\\hat{\\mathbb{Y}}||_2^2 $$ geometric derivation ","date":"2024-07-19","objectID":"/datal11/:4:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"lin alg review: orthogonality, span $$ span(\\mathbb{A})ÊòØ‰∏Ä‰∏™Áî±ÂàóÂêëÈáèÁªÑÊàêÁöÑspace $$ Ê≠£‰∫§ ","date":"2024-07-19","objectID":"/datal11/:5:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"least squares estimate proof performance: residuals, multiple R-squared lec11.ipynb $$ R^2‚àà[0,1] $$ Ë∂äÂ§ßÊãüÂêàÊïàÊûúË∂äÂ•Ω OLS properties ","date":"2024-07-19","objectID":"/datal11/:6:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"residuals ","date":"2024-07-19","objectID":"/datal11/:7:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"the bias/intercept term ","date":"2024-07-19","objectID":"/datal11/:8:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"existence of a unique solution ","date":"2024-07-19","objectID":"/datal11/:9:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"constant model + MSE ÂæÆÁßØÂàÜÊòØÊ±ÇÊúÄ‰ºòÂåñÁöÑ‰∏ÄÁßçÊñπÊ≥ï ‰∏§ÁßçËÆ∞Ê≥ï constant model + MAE ÁªùÂØπÂÄºÊ±ÇÂØºÊñ∞ËßÜËßí $$ \\sum_{\\theta \u003cy_i} 1=\\sum_{\\theta \u003ey_i} 1 $$ ÊòØËÆ°Êï∞ÔºÅ==\u003e‰∏≠‰ΩçÊï∞ lossÁöÑÊïèÊÑüÊÄßÈóÆÈ¢ò revisiting SLR evaluation ÁîªÂõæbefore modelingÔºÅÔºÅÔºÅ transformations to fit linear model ÁªèÈ™å‰πãË∞à introducing notation for multiple linear regression ","date":"2024-07-18","objectID":"/datal10/:0:0","tags":null,"title":"DATA100-L10: Constant Model, Loss, and Transformations","uri":"/datal10/"},{"categories":["DATA100"],"content":"regression line, correlation È´ò‰∏≠ÊúÄÂ∞è‰∫å‰πòÊ≥ï(least squares regression)ÔºåÁ∫øÊÄßÂõûÂΩí model $‚Äúall\\ models\\ are\\ wrong,\\ but\\ some\\ are\\ useful‚Äù$ trade between interpretability and accuracy Áâ©ÁêÜorÁªüËÆ°Ê®°Âûã the modeling process: definitions SLR: Simple Linear Regression ÊòéÁ°ÆinputÂíåparameterÁöÑÂå∫Âà´ Êúâ‰∫õÁªüËÆ°Ê®°ÂûãÂèØ‰ª•Ê≤°ÊúâÂèÇÊï∞ÔºÅ loss functions metric for good or bad minimizing average loss (Empirical Risk ÊúüÊúõÈ£éÈô©Ôºü) ÊúÄ‰ºòÂåñÔºÅ interpreting SLR: slope, Anscombe‚Äôs quartet Ëß£ÈáäÂèÇÊï∞ÊÑè‰πâ È¢ÑÊµãÊú™Áü•Êï∞ÊçÆ evaluating the model: RMSE, Residual Plot ","date":"2024-07-18","objectID":"/datal9/:0:0","tags":null,"title":"DATA100-L9: Introduction to Modeling, Simple Linear Regression","uri":"/datal9/"},{"categories":["DATA100"],"content":"Kernel Density Functions ","date":"2024-07-16","objectID":"/datal8/:0:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"KDE Mechanics ","date":"2024-07-16","objectID":"/datal8/:1:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"smoothing in 1DÔºàhistogramsÔºâ rug ‚Äî\u003e histogram ","date":"2024-07-16","objectID":"/datal8/:1:1","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"smoothing in 2DÔºàheatmaps/Hex PlotÔºâ ","date":"2024-07-16","objectID":"/datal8/:1:2","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"KDEs ‰ª£Á†ÅÂÆûÁé∞Ôºö sns.distplot(data, kde=True) ","date":"2024-07-16","objectID":"/datal8/:1:3","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"Kernel Functions and Bandwidth $\\alpha$ Ë∂äÂ§ßÔºåÊõ≤Á∫øË∂äÂπ≥Êªë ÂΩìÁÑ∂‰πüÊúâÂÖ∂‰ªñÁöÑkernelÂáΩÊï∞ÔºåÊØîÂ¶ÇÔºö triangular kernel epanechnikov kernel boxcar kernel Visualization Theory Ê≥®ÊÑèÂèØËßÜÂåñÁöÑÁõÆÁöÑÔºÅ ‰ªÖ‰ªÖÈù†ÁªüËÆ°ÊñπÊ≥ï‰∏çÂ§üÁõ¥ËßÇÂπ∂‰∏î‰∏çÂ§üÂáÜÁ°ÆÔºÅ ","date":"2024-07-16","objectID":"/datal8/:2:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"Information Channels color, shape, size, position (coordinate), and orientation ","date":"2024-07-16","objectID":"/datal8/:3:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing X/Y do not use different scales for x and y in the same visualization! ÊØî‰æãÈÄÇ‰∏≠ ","date":"2024-07-16","objectID":"/datal8/:4:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Color ÈÄâÈ¢úËâ≤Ôºåjet, viridis‰∏ªÈ¢òÁ≠âÁ≠â ÊúÄÂ•ΩÈÄâÊã©perceptually uniformÁöÑÈ¢úËâ≤ÔºÅËÄåjet‰∏çÊòØÔºÅInfernoÔºå TurboÂèØ‰ª• ","date":"2024-07-16","objectID":"/datal8/:5:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Markings ‰∫∫Êõ¥ÂÄæÂêë‰∫éÊØîËæÉÊï¥ÈΩêÁöÑÁõ¥ÊñπÂõæÔºà‰∏ÄÁª¥ÈïøÂ∫¶Ôºâ ÈÅøÂÖçÁßªÂä®Ë∞ÉÊï¥Âü∫Á∫øÔºÅ ÂèñÂÜ≥‰∫éËÆ≤‰ªÄ‰πàÊïÖ‰∫ã ","date":"2024-07-16","objectID":"/datal8/:6:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Conditioning ","date":"2024-07-16","objectID":"/datal8/:7:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Context Transformations linearizeÁ∫øÊÄßÂåñÂ§ÑÁêÜ log transformÂØπÊï∞ÂèòÊç¢ Êõ¥Â§öÁöÑ‰ª£Á†ÅÂèÇËÄÉjupyter notebook ","date":"2024-07-16","objectID":"/datal8/:8:0","tags":null,"title":"DATA100-L8: Visualizations ‚Ö°","uri":"/datal8/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab02.ipynb\") Pandas is one of the most widely used Python libraries in data science. In this lab, you will review commonly used data wrangling operations/tools in Pandas. We aim to give you familiarity with: Creating DataFrames Slicing DataFrames (i.e. selecting rows and columns) Filtering data (using boolean arrays and groupby.filter) Aggregating (using groupby.agg) In this lab you are going to use several pandas methods. Reminder from lecture that you may press shift+tab on method parameters to see the documentation for that method. For example, if you were using the drop method in pandas, you couold press shift+tab to see what drop is expecting. Pandas is very similar to the datascience library that you saw in Data 8. This conversion notebook may serve as a useful guide! This lab expects that you have watched the pandas lectures. If you have not, this lab will probably take a very long time. Note: The Pandas interface is notoriously confusing for beginners, and the documentation is not consistently great. Throughout the semester, you will have to search through Pandas documentation and experiment, but remember it is part of the learning experience and will help shape you as a data scientist! import numpy as np import matplotlib.pyplot as plt import pandas as pd import plotly.express as px %matplotlib inline ","date":"2024-07-15","objectID":"/datalab2/:0:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Creating DataFrames \u0026 Basic Manipulations Recall that a DataFrame is a table in which each column has a specific data type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). Usually you‚Äôll create DataFrames by using a function like pd.read_csv. However, in this section, we‚Äôll discuss how to create them from scratch. The documentation for the pandas DataFrame class provides several constructors for the DataFrame class. Syntax 1: You can create a DataFrame by specifying the columns and values using a dictionary as shown below. The keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries. fruit_info = pd.DataFrame( data = {'fruit': ['apple', 'orange', 'banana', 'raspberry'], 'color': ['red', 'orange', 'yellow', 'pink'], 'price': [1.0, 0.75, 0.35, 0.05] }) fruit_info fruit color price 0 apple red 1.00 1 orange orange 0.75 2 banana yellow 0.35 3 raspberry pink 0.05 Syntax 2: You can also define a DataFrame by specifying the rows as shown below. Each row corresponds to a distinct tuple, and the columns are specified separately. ËøôÈáåÂèØ‰ª•ÁúãÂá∫columnsÊòØ‰∏™ÂèÇÊï∞ fruit_info2 = pd.DataFrame( [(\"red\", \"apple\", 1.0), (\"orange\", \"orange\", 0.75), (\"yellow\", \"banana\", 0.35), (\"pink\", \"raspberry\", 0.05)], columns = [\"color\", \"fruit\", \"price\"]) fruit_info2 color fruit price 0 red apple 1.00 1 orange orange 0.75 2 yellow banana 0.35 3 pink raspberry 0.05 You can obtain the dimensions of a DataFrame by using the shape attribute DataFrame.shape. fruit_info.shape (4, 3) You can also convert the entire DataFrame into a two-dimensional NumPy array. fruit_info.values array([['apple', 'red', 1.0], ['orange', 'orange', 0.75], ['banana', 'yellow', 0.35], ['raspberry', 'pink', 0.05]], dtype=object) There are other constructors but we do not discuss them here. ","date":"2024-07-15","objectID":"/datalab2/:1:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"REVIEW: Selecting Rows and Columns in Pandas As you‚Äôve seen in lecture and discussion, there are two verbose operators in Python for selecting rows: loc and iloc. Let‚Äôs review them briefly. Approach 1: loc The first of the two verbose operators is loc, which takes two arguments. The first is one or more row labels, the second is one or more column labels. The desired rows or columns can be provided individually, in slice notation, or as a list. Some examples are given below. Note that slicing in loc is inclusive on the provided labels. #get rows 0 through 2 and columns fruit through price fruit_info.loc[0:2, 'fruit':'price'] # Èó≠Âå∫Èó¥ fruit color price 0 apple red 1.00 1 orange orange 0.75 2 banana yellow 0.35 # get rows 0 through 2 and columns fruit and price. # Note the difference in notation and result from the previous example. fruit_info.loc[0:2, ['fruit', 'price']] # Á¶ªÊï£ÁöÑ fruit price 0 apple 1.00 1 orange 0.75 2 banana 0.35 # get rows 0 and 2 and columns fruit and price. fruit_info.loc[[0, 2], ['fruit', 'price']] # Êõ¥Âä†Á¶ªÊï£ÁöÑ fruit price 0 apple 1.00 2 banana 0.35 # get rows 0 and 2 and column fruit fruit_info.loc[[0, 2], ['fruit']] fruit 0 apple 2 banana Note that if we request a single column but don‚Äôt enclose it in a list, the return type of the loc operator is a Series rather than a DataFrame. Ê≥®ÊÑè[ ]ÂåÖË£πÈóÆÈ¢ò # get rows 0 and 2 and column fruit, returning the result as a Series fruit_info.loc[[0, 2], 'fruit'] 0 apple 2 banana Name: fruit, dtype: object If we provide only one argument to loc, it uses the provided argument to select rows, and returns all columns.ÂèØ‰ª•Âè™ÁªôË°åÔºå ‰∏çÂèØ‰ª•Âè™ÁªôÂàó fruit_info.loc[0:1] fruit color price 0 apple red 1.00 1 orange orange 0.75 Note that if you try to access columns without providing rows, loc will crash. # uncomment, this code will crash # fruit_info.loc[[\"fruit\", \"price\"]] # uncomment, this code works fine: fruit_info.loc[:, [\"fruit\", \"price\"]] fruit price 0 apple 1.00 1 orange 0.75 2 banana 0.35 3 raspberry 0.05 Approach 2: iloc iloc is very similar to loc except that its arguments are row numbers and column numbers, rather than row labels and labels names. A usueful mnemonic is that the i stands for ‚Äúinteger‚Äù. In addition, slicing for iloc is exclusive on the provided integer indices. Some examples are given below: ËÄÉËôëÊ≠§Êó∂ÂèòÊàêpythonÁªèÂÖ∏Á¥¢Âºï # get rows 0 through 3 (exclusive) and columns 0 through 2 (exclusive) fruit_info.iloc[0:3, 0:3] fruit color price 0 apple red 1.00 1 orange orange 0.75 2 banana yellow 0.35 # get rows 0 through 3 (exclusive) and columns 0 and 2. fruit_info.iloc[0:3, [0, 2]] fruit price 0 apple 1.00 1 orange 0.75 2 banana 0.35 # get rows 0 and 2 and columns 0 and 2. fruit_info.iloc[[0, 2], [0, 2]] fruit price 0 apple 1.00 2 banana 0.35 #get rows 0 and 2 and column fruit fruit_info.iloc[[0, 2], [0]] fruit 0 apple 2 banana # get rows 0 and 2 and column fruit fruit_info.iloc[[0, 2], 0] # return a Series! 0 apple 2 banana Name: fruit, dtype: object Note that in these loc and iloc examples above, the row label and row number were always the same. Let‚Äôs see an example where they are different. If we sort our fruits by price, we get: fruit_info_sorted = fruit_info.sort_values(\"price\") fruit_info_sorted fruit color price 3 raspberry pink 0.05 2 banana yellow 0.35 1 orange orange 0.75 0 apple red 1.00 Observe that the row number 0 now has index 3, row number 1 now has index 2, etc. These indices are the arbitrary numerical index generated when we created the DataFrame. For example, banana was originally in row 2, and so it has row label 2. If we request the rows in positions 0 and 2 using iloc, we‚Äôre indexing using the row NUMBERS, not labels. ËøôÈáå‰ºº‰πéÂπ∂‰∏çÊòØÊåâÁÖßlabÊâÄËØ¥ÁöÑÈÇ£Ê†∑Ôºü fruit_info_sorted.iloc[[0, 2], 0] # Âà´ÂíåÊï∞Â≠¶Ë°®ËææÊ∑∑Ê∑ÜÔºÅ 3 raspberry 1 orange Name: fruit, dtype: object Lastly, similar to with loc, the second argument to iloc is optional. That is, if you provide only one argument to iloc, it treats the argument you provide as a set of desired row numbers, not column numbers. ÂèØ‰ª•Âè™ÁªôË°åÔºå‰∏çÂèØÁªôÂàó fruit_info.iloc[[0, 2]] fruit color ","date":"2024-07-15","objectID":"/datalab2/:1:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 1(a) For a DataFrame d, you can add a column with d['new column name'] = ... and assign a list or array of values to the column. Add a column of integers containing 1, 2, 3, and 4 called rank1 to the fruit_info table which expresses your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). fruit_info['rank1'] = [1,2,3,4] fruit_info fruit color price rank1 0 apple red 1.00 1 1 orange orange 0.75 2 2 banana yellow 0.35 3 3 raspberry pink 0.05 4 grader.check(\"q1a\") q1a passed! üíØ ","date":"2024-07-15","objectID":"/datalab2/:1:2","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 1(b) You can also add a column to d with d.loc[:, 'new column name'] = .... As above, the first parameter is for the rows and second is for columns. The : means change all rows and the 'new column name' indicates the name of the column you are modifying (or in this case, adding). Add a column called rank2 to the fruit_info table which contains the same values in the same order as the rank1 column. fruit_info.loc[:, 'rank2'] = [1,2,3,4] fruit_info fruit color price rank1 rank2 0 apple red 1.00 1 1 1 orange orange 0.75 2 2 2 banana yellow 0.35 3 3 3 raspberry pink 0.05 4 4 grader.check(\"q1b\") q1b passed! üçÄ ","date":"2024-07-15","objectID":"/datalab2/:1:3","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 2 Use the .drop() method to drop both the rank1 and rank2 columns you created. Make sure to use the axis parameter correctly. Note that drop does not change a table, but instead returns a new table with fewer columns or rows unless you set the optional inplace parameter. Hint: Look through the documentation to see how you can drop multiple columns of a Pandas DataFrame at once using a list of column names. fruit_info_original = fruit_info.drop(labels=['rank1','rank2'],axis=1) fruit_info_original fruit color price 0 apple red 1.00 1 orange orange 0.75 2 banana yellow 0.35 3 raspberry pink 0.05 grader.check(\"q2\") q2 passed! üíØ ","date":"2024-07-15","objectID":"/datalab2/:1:4","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 3 Use the .rename() method to rename the columns of fruit_info_original so they begin with capital letters. Set this new DataFrame to fruit_info_caps. For an example of how to use rename, see the linked documentation above. fruit_info_caps = fruit_info_original.rename(columns={'fruit':'Fruit', 'color':'Color', 'price':'Price'}) fruit_info_caps Fruit Color Price 0 apple red 1.00 1 orange orange 0.75 2 banana yellow 0.35 3 raspberry pink 0.05 grader.check(\"q3\") q3 passed! üéâ ","date":"2024-07-15","objectID":"/datalab2/:1:5","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Babynames Dataset For the new few questions of this lab, let‚Äôs move on to a real world dataset. We‚Äôll be using the babynames dataset from Lecture 1. The babynames dataset contains a record of the given names of babies born in the United States each year. First let‚Äôs run the following cells to build the DataFrame baby_names. The cells below download the data from the web and extract the data into a DataFrame. There should be a total of 6215834 records. ","date":"2024-07-15","objectID":"/datalab2/:1:6","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"fetch_and_cache Helper The following function downloads and caches data in the data/ directory and returns the Path to the downloaded file. The cell below the function describes how it works. You are not expected to understand this code, but you may find it useful as a reference as a practitioner of data science after the course. import requests from pathlib import Path def fetch_and_cache(data_url, file, data_dir=\"data\", force=False): \"\"\" Download and cache a url and return the file object. data_url: the web address to download file: the file in which to save the results. data_dir: (default=\"data\") the location to save the data force: if true the file is always re-downloaded return: The pathlib.Path to the file. \"\"\" data_dir = Path(data_dir) data_dir.mkdir(exist_ok=True) file_path = data_dir/Path(file) if force and file_path.exists(): file_path.unlink() if force or not file_path.exists(): print('Downloading...', end=' ') resp = requests.get(data_url) with file_path.open('wb') as f: f.write(resp.content) print('Done!') else: import time created = time.ctime(file_path.stat().st_ctime) print(\"Using cached version downloaded at\", created) return file_path In Python, a Path object represents the filesystem paths to files (and other resources). The pathlib module is effective for writing code that works on different operating systems and filesystems. To check if a file exists at a path, use .exists(). To create a directory for a path, use .mkdir(). To remove a file that might be a symbolic link, use .unlink(). This function creates a path to a directory that will contain data files. It ensures that the directory exists (which is required to write files in that directory), then proceeds to download the file based on its URL. The benefit of this function is that not only can you force when you want a new file to be downloaded using the force parameter, but in cases when you don‚Äôt need the file to be re-downloaded, you can use the cached version and save download time. Below we use fetch_and_cache to download the namesbystate.zip zip file, which is a compressed directory of CSV files. This might take a little while! Consider stretching. data_url = 'https://www.ssa.gov/oact/babynames/state/namesbystate.zip' namesbystate_path = fetch_and_cache(data_url, 'namesbystate.zip') Using cached version downloaded at Fri Jul 12 20:04:41 2024 The following cell builds the final full baby_names DataFrame. It first builds one DataFrame per state, because that‚Äôs how the data are stored in the zip file. Here is documentation for pd.concat if you want to know more about its functionality. As before, you are not expected to understand this code. import zipfile zf = zipfile.ZipFile(namesbystate_path, 'r') column_labels = ['State', 'Sex', 'Year', 'Name', 'Count'] def load_dataframe_from_zip(zf, f): with zf.open(f) as fh: return pd.read_csv(fh, header=None, names=column_labels) states = [ load_dataframe_from_zip(zf, f) for f in sorted(zf.filelist, key=lambda x:x.filename) if f.filename.endswith('.TXT') ] baby_names = states[0] for state_df in states[1:]: baby_names = pd.concat([baby_names, state_df]) baby_names = baby_names.reset_index().iloc[:, 1:] len(baby_names) 6215834 baby_names.head() State Sex Year Name Count 0 AK F 1910 Mary 14 1 AK F 1910 Annie 12 2 AK F 1910 Anna 10 3 AK F 1910 Margaret 8 4 AK F 1910 Helen 7 ","date":"2024-07-15","objectID":"/datalab2/:1:7","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Selection Examples on Baby Names As with our synthetic fruit dataset, we can use loc and iloc to select rows and columns of interest from our dataset. baby_names.loc[2:5, 'Name']# Series 2 Anna 3 Margaret 4 Helen 5 Elsie Name: Name, dtype: object Notice the difference between the following cell and the previous one, just passing in 'Name' returns a Series while ['Name'] returns a DataFrame. baby_names.loc[2:5, ['Name']] #df Name 2 Anna 3 Margaret 4 Helen 5 Elsie The code below collects the rows in positions 1 through 3, and the column in position 3 (‚ÄúName‚Äù). baby_names.iloc[1:4, [3]] Name 1 Annie 2 Anna 3 Margaret ","date":"2024-07-15","objectID":"/datalab2/:1:8","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 4 Use .loc to select Name and Year in that order from the baby_names table. name_and_year = baby_names.loc[:, ['Name', 'Year']] name_and_year[:5] # ÁâàÊú¨ÈóÆÈ¢ò Name Year 0 Mary 1910 1 Annie 1910 2 Anna 1910 3 Margaret 1910 4 Helen 1910 grader.check(\"q4\") Now repeat the same selection using the plain [] notation. Êé•Âèó‰∏Ä‰∏™list of columns name_and_year = baby_names[['Name','Year']] name_and_year[:5] Name Year 0 Mary 1910 1 Annie 1910 2 Anna 1910 3 Margaret 1910 4 Helen 1910 ","date":"2024-07-15","objectID":"/datalab2/:1:9","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Filtering Data ","date":"2024-07-15","objectID":"/datalab2/:2:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Review: Filtering with boolean arrays Filtering is the process of removing unwanted material. In your quest for cleaner data, you will undoubtedly filter your data at some point: whether it be for clearing up cases with missing values, for culling out fishy outliers, or for analyzing subgroups of your data set. Example usage looks like df[df['column name'] \u003c 5]. For your reference, some commonly used comparison operators are given below. Symbol Usage Meaning == a == b Does a equal b? \u003c= a \u003c= b Is a less than or equal to b? \u003e= a \u003e= b Is a greater than or equal to b? \u003c a \u003c b Is a less than b? \u003e a \u003e b Is a greater than b? ~ ~p Returns negation of p | p | q p OR q \u0026 p \u0026 q p AND q ^ p ^ q p XOR q (exclusive or) In the following we construct the DataFrame containing only names registered in CaliforniaÊ≥®ÊÑèËøôÈáåÂçÅÂàÜÈáçË¶ÅÔºÅ ca = baby_names[baby_names['State'] == 'CA'] ca.head(5) State Sex Year Name Count 390635 CA F 1910 Mary 295 390636 CA F 1910 Helen 239 390637 CA F 1910 Dorothy 220 390638 CA F 1910 Margaret 163 390639 CA F 1910 Frances 134 ","date":"2024-07-15","objectID":"/datalab2/:2:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 5 Using a boolean array, select the names in Year 2000 (from baby_names) that have larger than 3000 counts. Keep all columns from the original baby_names DataFrame. Note: Note that compound expressions have to be grouped with parentheses. That is, any time you use p \u0026 q to filter the DataFrame, make sure to use df[(df[p]) \u0026 (df[q])] or df.loc[(df[p]) \u0026 (df[q])]. You may use either [] or loc. Both will achieve the same result. For more on [] vs. loc see the stack overflow links from the intro portion of this lab. result = baby_names[(baby_names['Year'] == 2000) \u0026 (baby_names['Count'] \u003e 3000)] result.head() State Sex Year Name Count 725638 CA M 2000 Daniel 4342 725639 CA M 2000 Anthony 3839 725640 CA M 2000 Jose 3804 725641 CA M 2000 Andrew 3600 725642 CA M 2000 Michael 3572 grader.check(\"q5\") # ‰æùÊóßÊòØÁâàÊú¨ÈóÆÈ¢ò Query Review Recall that pandas also has a query command. For example, we can get California baby names with the code below. ca = baby_names.query('State == \"CA\"') ca.head(5) State Sex Year Name Count 390635 CA F 1910 Mary 295 390636 CA F 1910 Helen 239 390637 CA F 1910 Dorothy 220 390638 CA F 1910 Margaret 163 390639 CA F 1910 Frances 134 Using the query command, select the names in Year 2000 (from baby_names) that have larger than 3000 counts. result_using_query = baby_names.query(\"Count \u003e 3000 and Year == 2000\") result_using_query.head(5) State Sex Year Name Count 725638 CA M 2000 Daniel 4342 725639 CA M 2000 Anthony 3839 725640 CA M 2000 Jose 3804 725641 CA M 2000 Andrew 3600 725642 CA M 2000 Michael 3572 ","date":"2024-07-15","objectID":"/datalab2/:2:2","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Groupby Let‚Äôs now turn to using groupby from lecture 4. Note: This slide provides a visual picture of how groupby.agg works if you‚Äôd like a reference. ","date":"2024-07-15","objectID":"/datalab2/:3:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6: Elections Review: Let‚Äôs start by reading in the election dataset from the pandas lectures. # run this cell elections = pd.read_csv(\"data/elections.csv\") elections.head(5) Year Candidate Party Popular vote Result % 0 1824 Andrew Jackson Democratic-Republican 151271 loss 57.210122 1 1824 John Quincy Adams Democratic-Republican 113142 win 42.789878 2 1828 Andrew Jackson Democratic 642806 win 56.203927 3 1828 John Quincy Adams National Republican 500897 loss 43.796073 4 1832 Andrew Jackson Democratic 702735 win 54.574789 As we saw, we can groupby a specific column, e.g. ‚ÄúParty‚Äù. It turns out that using some syntax we didn‚Äôt cover in lecture, we can print out the subframes that result. This isn‚Äôt something you‚Äôll do for any practical purpose. However, it may help you get an understanding of what groupby is actually doing. An example is given below for elections since 1980. # run this cell for n, g in elections.query(\"Year \u003e= 1980\").groupby(\"Party\"): print(f\"Name: {n}\") # by the way this is an \"f string\", a relatively new and great feature of Python display(g) Name: Citizens Year Candidate Party Popular vote Result % 127 1980 Barry Commoner Citizens 233052 loss 0.270182 Name: Constitution Year Candidate Party Popular vote Result % 160 2004 Michael Peroutka Constitution 143630 loss 0.117542 164 2008 Chuck Baldwin Constitution 199750 loss 0.152398 172 2016 Darrell Castle Constitution 203091 loss 0.149640 Name: Democratic Year Candidate Party Popular vote Result % 129 1980 Jimmy Carter Democratic 35480115 loss 41.132848 134 1984 Walter Mondale Democratic 37577352 loss 40.729429 137 1988 Michael Dukakis Democratic 41809074 loss 45.770691 140 1992 Bill Clinton Democratic 44909806 win 43.118485 144 1996 Bill Clinton Democratic 47400125 win 49.296938 151 2000 Al Gore Democratic 50999897 loss 48.491813 158 2004 John Kerry Democratic 59028444 loss 48.306775 162 2008 Barack Obama Democratic 69498516 win 53.023510 168 2012 Barack Obama Democratic 65915795 win 51.258484 176 2016 Hillary Clinton Democratic 65853514 loss 48.521539 178 2020 Joseph Biden Democratic 81268924 win 51.311515 Name: Green Year Candidate Party Popular vote Result % 149 1996 Ralph Nader Green 685297 loss 0.712721 155 2000 Ralph Nader Green 2882955 loss 2.741176 156 2004 David Cobb Green 119859 loss 0.098088 165 2008 Cynthia McKinney Green 161797 loss 0.123442 170 2012 Jill Stein Green 469627 loss 0.365199 177 2016 Jill Stein Green 1457226 loss 1.073699 181 2020 Howard Hawkins Green 405035 loss 0.255731 Name: Independent Year Candidate Party Popular vote Result % 130 1980 John B. Anderson Independent 5719850 loss 6.631143 143 1992 Ross Perot Independent 19743821 loss 18.956298 161 2004 Ralph Nader Independent 465151 loss 0.380663 167 2008 Ralph Nader Independent 739034 loss 0.563842 174 2016 Evan McMullin Independent 732273 loss 0.539546 Name: Libertarian Year Candidate Party Popular vote Result % 128 1980 Ed Clark Libertarian 921128 loss 1.067883 132 1984 David Bergland Libertarian 228111 loss 0.247245 138 1988 Ron Paul Libertarian 431750 loss 0.472660 139 1992 Andre Marrou Libertarian 290087 loss 0.278516 146 1996 Harry Browne Libertarian 485759 loss 0.505198 153 2000 Harry Browne Libertarian 384431 loss 0.365525 159 2004 Michael Badnarik Libertarian 397265 loss 0.325108 163 2008 Bob Barr Libertarian 523715 loss 0.399565 169 2012 Gary Johnson Libertarian 1275971 loss 0.992241 175 2016 Gary Johnson Libertarian 4489235 loss 3.307714 180 2020 Jo Jorgensen Libertarian 1865724 loss 1.177979 Name: Natural Law Year Candidate Party Popular vote Result % 148 1996 John Hagelin Natural Law 113670 loss 0.118219 Name: New Alliance Year Candidate Party Popular vote Result % 136 1988 Lenora Fulani New Alliance 217221 loss 0.237804 Name: Populist Year Candidate Party Popular vote Result % 141 1992 Bo Gritz Populist 106152 loss 0.101918 Name: Reform Year Candidate Party Popular vote Result % 150 1996 Ross Perot Reform 8085294 loss 8.408844 154 2000 Pat Buchanan Reform 448895 l","date":"2024-07-15","objectID":"/datalab2/:3:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6a Using groupby.agg or one of the shorthand methods (groupby.min, groupby.first, etc.), create a Series best_result_percentage_only that returns a Series showing the entire best result for every party, sorted in decreasing order. Your Series should include only parties which have earned at least 10% of the vote in some election. Your result should look like this: Party Democratic 61.344703 Republican 60.907806 Democratic-Republican 57.210122 National Union 54.951512 Whig 53.051213 Liberal Republican 44.071406 National Republican 43.796073 Northern Democratic 29.522311 Progressive 27.457433 American 21.554001 Independent 18.956298 Southern Democratic 18.138998 American Independent 13.571218 Constitutional Union 12.639283 Free Soil 10.138474 Name: %, dtype: float64 A list of named groupby.agg shorthand methods is here (you‚Äôll have to scroll down about one page). best_result_percentage_only = elections[elections['%']\u003e=10].groupby('Party')['%'].agg(max).sort_values(ascending=False) # put your code above this line best_result_percentage_only C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_61736\\687541662.py:1: FutureWarning: The provided callable \u003cbuilt-in function max\u003e is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead. best_result_percentage_only = elections[elections['%']\u003e=10].groupby('Party')['%'].agg(max).sort_values(ascending=False) Party Democratic 61.344703 Republican 60.907806 Democratic-Republican 57.210122 National Union 54.951512 Whig 53.051213 Liberal Republican 44.071406 National Republican 43.796073 Northern Democratic 29.522311 Progressive 27.457433 American 21.554001 Independent 18.956298 Southern Democratic 18.138998 American Independent 13.571218 Constitutional Union 12.639283 Free Soil 10.138474 Name: %, dtype: float64 grader.check(\"q6a\") ","date":"2024-07-15","objectID":"/datalab2/:3:2","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6b Repeat Question 6a. However, this time, your result should be a DataFrame showing all available information rather than only the percentage as a series. This question is trickier than Question 6a. Make sure to check the Lecture 4 slides if you‚Äôre stuck! It‚Äôs very easy to make a subtle mistake that shows Woodrow Wilson and Howard Taft both winning the 2020 election. For example, the first 3 rows of your table should be: Party Year Candidate Popular Vote Result % Democratic 1964 Lyndon Johnson 43127041 win 61.344703 Republican 1972 Richard Nixon 47168710 win 60.907806 Democratic-Republican 1824 Andrew Jackson 151271 loss 57.210122 Note that the index is Party. In other words, don‚Äôt use reset_index. best_result = elections[elections['%']\u003e=10].sort_values(by='%',ascending=False).groupby(['Party']).agg(lambda x: x.iloc[0]).sort_values(by='%',ascending=False) # @ 52:03 in the video of Lecture 4 # put your code above this line best_result Year Candidate Popular vote Result % Party Democratic 1964 Lyndon Johnson 43127041 win 61.344703 Republican 1972 Richard Nixon 47168710 win 60.907806 Democratic-Republican 1824 Andrew Jackson 151271 loss 57.210122 National Union 1864 Abraham Lincoln 2211317 win 54.951512 Whig 1840 William Henry Harrison 1275583 win 53.051213 Liberal Republican 1872 Horace Greeley 2834761 loss 44.071406 National Republican 1828 John Quincy Adams 500897 loss 43.796073 Northern Democratic 1860 Stephen A. Douglas 1380202 loss 29.522311 Progressive 1912 Theodore Roosevelt 4122721 loss 27.457433 American 1856 Millard Fillmore 873053 loss 21.554001 Independent 1992 Ross Perot 19743821 loss 18.956298 Southern Democratic 1860 John C. Breckinridge 848019 loss 18.138998 American Independent 1968 George Wallace 9901118 loss 13.571218 Constitutional Union 1860 John Bell 590901 loss 12.639283 Free Soil 1848 Martin Van Buren 291501 loss 10.138474 grader.check(\"q6b\") ","date":"2024-07-15","objectID":"/datalab2/:3:3","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6c Our DataFrame contains a number of parties which have never had a successful presidential run. For example, the 2020 elections included candiates from the Libertarian and Green parties, neither of which have elected a president. # just run this cell elections.tail(5) Year Candidate Party Popular vote Result % 177 2016 Jill Stein Green 1457226 loss 1.073699 178 2020 Joseph Biden Democratic 81268924 win 51.311515 179 2020 Donald Trump Republican 74216154 loss 46.858542 180 2020 Jo Jorgensen Libertarian 1865724 loss 1.177979 181 2020 Howard Hawkins Green 405035 loss 0.255731 Suppose we were conducting an analysis trying to focus our attention on parties that had elected a president. The most natural approach is to use groupby.filter. This is an incredibly powerful but subtle tool for filtering data. As a reminder of how filter works, see this slide. The code below accomplishes the task at hand. It does this by creating a function that returns True if and only if a sub-dataframe (a.k.a. group) contains at least one winner. This function in turn uses the Pandas function ‚Äúany‚Äù. # just run this cell def at_least_one_candidate_in_the_frame_has_won(frame): \"\"\"Returns df with rows only kept for parties that have won at least one election \"\"\" return (frame[\"Result\"] == 'win').any() winners_only = ( elections .groupby(\"Party\") .filter(at_least_one_candidate_in_the_frame_has_won) ) winners_only.tail(5) Year Candidate Party Popular vote Result % 171 2012 Mitt Romney Republican 60933504 loss 47.384076 173 2016 Donald Trump Republican 62984828 win 46.407862 176 2016 Hillary Clinton Democratic 65853514 loss 48.521539 178 2020 Joseph Biden Democratic 81268924 win 51.311515 179 2020 Donald Trump Republican 74216154 loss 46.858542 Alternately we could have used a lambda function instead of explicitly defining a named function using def. # just run this cell (alternative) winners_only = ( elections .groupby(\"Party\") .filter(lambda x : (x[\"Result\"] == \"win\").any()) ) winners_only.tail(5) Year Candidate Party Popular vote Result % 171 2012 Mitt Romney Republican 60933504 loss 47.384076 173 2016 Donald Trump Republican 62984828 win 46.407862 176 2016 Hillary Clinton Democratic 65853514 loss 48.521539 178 2020 Joseph Biden Democratic 81268924 win 51.311515 179 2020 Donald Trump Republican 74216154 loss 46.858542 For your exercise, you‚Äôll do a less restrictive filtering of the elections data. Exercise: Using filter, create a DataFrame major_party_results_since_1988 that includes all election results starting in 1988, but only show a row if the Party it belongs to has earned at least 1% of the popular vote in ANY election since 1988. For example, in 1988, you should not include the New Alliance candidate, since this party has not earned 1% of the vote since 1988. However, you should include the Libertarian candidate from 1988 despite only having 0.47 percent of the vote in 1988, because in 2016 and 2020, the Libertarian candidates Gary Johnson and Jo Jorgensen exceeded 1% of the vote. For example, the first three rows of the table you generate should look like: Year Candidate Party Popular vote Result % 135 1988 George H. W. Bush Republican 48886597 win 53.5188 137 1988 Michael Dukakis Democratic 41809074 loss 45.7707 138 1988 Ron Paul Libertarian 431750 loss 0.47266 major_party_results_since_1988 = elections[(elections['Year']\u003e=1988)].groupby('Party').filter(lambda x: (x['%'] \u003e= 1).any()) major_party_results_since_1988.head() Year Candidate Party Popular vote Result % 135 1988 George H. W. Bush Republican 48886597 win 53.518845 137 1988 Michael Dukakis Democratic 41809074 loss 45.770691 138 1988 Ron Paul Libertarian 431750 loss 0.472660 139 1992 Andre Marrou Libertarian 290087 loss 0.278516 140 1992 Bill Clinton Democratic 44909806 win 43.118485 grader.check(\"q6c\") ","date":"2024-07-15","objectID":"/datalab2/:3:4","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 7 Pandas provides special purpose functions for working with specific common data types such as strings and dates. For example, the code below provides the length of every Candidate‚Äôs name from our elections dataset. elections[\"Candidate\"].str.len() 0 14 1 17 2 14 3 17 4 14 .. 177 10 178 12 179 12 180 12 181 14 Name: Candidate, Length: 182, dtype: int64 Exercise: Using .str.split. Create a new DataFrame called elections_with_first_name with a new column First Name that is equal to the Candidate‚Äôs first name. See the Pandas str documentation for documentation on using str.split. Hint: Use [0] somewhere in your code. elections_with_first_name = elections.copy() # your code here elections_with_first_name['First Name'] = elections_with_first_name['Candidate'].str.split(' ').str[0].to_frame() # end your code elections_with_first_name Year Candidate Party Popular vote Result % First Name 0 1824 Andrew Jackson Democratic-Republican 151271 loss 57.210122 Andrew 1 1824 John Quincy Adams Democratic-Republican 113142 win 42.789878 John 2 1828 Andrew Jackson Democratic 642806 win 56.203927 Andrew 3 1828 John Quincy Adams National Republican 500897 loss 43.796073 John 4 1832 Andrew Jackson Democratic 702735 win 54.574789 Andrew ... ... ... ... ... ... ... ... 177 2016 Jill Stein Green 1457226 loss 1.073699 Jill 178 2020 Joseph Biden Democratic 81268924 win 51.311515 Joseph 179 2020 Donald Trump Republican 74216154 loss 46.858542 Donald 180 2020 Jo Jorgensen Libertarian 1865724 loss 1.177979 Jo 181 2020 Howard Hawkins Green 405035 loss 0.255731 Howard 182 rows √ó 7 columns grader.check(\"q7\") q7 passed! üôå ","date":"2024-07-15","objectID":"/datalab2/:3:5","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 8 The code below creates a table with the frequency of all names from 2020. # just run this cell baby_names_2020 = ( baby_names.query('Year == 2020') .groupby(\"Name\") .sum()[[\"Count\"]] .reset_index() ) baby_names_2020 Name Count 0 Aaden 15 1 Aadhira 6 2 Aadhvik 5 3 Aadhya 186 4 Aadi 14 ... ... ... 8697 Zymere 6 8698 Zymir 74 8699 Zyon 130 8700 Zyra 33 8701 Zyrah 5 8702 rows √ó 2 columns Exercise: Using the pd.merge function described in lecture, combine the baby_names_2020 table with the elections_with_first_name table you created earlier to form presidential_candidates_and_name_popularity. presidential_candidates_and_name_popularity = pd.merge(elections_with_first_name,baby_names_2020, left_on='First Name', right_on='Name') presidential_candidates_and_name_popularity Year Candidate Party Popular vote Result % First Name Name Count 0 1824 Andrew Jackson Democratic-Republican 151271 loss 57.210122 Andrew Andrew 5991 1 1824 John Quincy Adams Democratic-Republican 113142 win 42.789878 John John 8180 2 1828 Andrew Jackson Democratic 642806 win 56.203927 Andrew Andrew 5991 3 1828 John Quincy Adams National Republican 500897 loss 43.796073 John John 8180 4 1832 Andrew Jackson Democratic 702735 win 54.574789 Andrew Andrew 5991 ... ... ... ... ... ... ... ... ... ... 148 2016 Hillary Clinton Democratic 65853514 loss 48.521539 Hillary Hillary 20 149 2020 Joseph Biden Democratic 81268924 win 51.311515 Joseph Joseph 8349 150 2020 Donald Trump Republican 74216154 loss 46.858542 Donald Donald 407 151 2020 Jo Jorgensen Libertarian 1865724 loss 1.177979 Jo Jo 6 152 2020 Howard Hawkins Green 405035 loss 0.255731 Howard Howard 131 153 rows √ó 9 columns grader.check(\"q8\") ","date":"2024-07-15","objectID":"/datalab2/:3:6","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Bonus Exercises The following exercises are optional and use the ca_baby_names dataset defined below. perhaps next time! üòé # just run this cell ca_baby_names = baby_names.query('State == \"CA\"') ca_baby_names State Sex Year Name Count 390635 CA F 1910 Mary 295 390636 CA F 1910 Helen 239 390637 CA F 1910 Dorothy 220 390638 CA F 1910 Margaret 163 390639 CA F 1910 Frances 134 ... ... ... ... ... ... 784809 CA M 2020 Ziaan 5 784810 CA M 2020 Ziad 5 784811 CA M 2020 Ziaire 5 784812 CA M 2020 Zidan 5 784813 CA M 2020 Zymir 5 394179 rows √ó 5 columns Sorted Female Name Counts Create a Series female_name_since_2000_count which gives the total number of occurrences of each name for female babies born in California from the year 2000 or later. The index should be the name, and the value should be the total number of births. Your Series should be ordered in decreasing order of count. For example, your first row should have index ‚ÄúEmily‚Äù and value 52334, because 52334 Emilys have been born since the year 2000 in California. female_name_since_2000_count = female_name_since_2000_count Counts for All Names Using groupby, create a Series count_for_names_2020 listing all baby names from 2020 in California, in decreasing order of popularity. The result should not be broken down by sex! If a name is used by both male and female babies, the number you provide should be the total. Note: In this question we are now computing the number of registered babies with a given name. For example, count_for_names_2020[\"Noah\"] should be the number 2631 because in 2018 there were 2631 Noahs born (23 female and 2608 male). ... count_for_names_2020 ","date":"2024-07-15","objectID":"/datalab2/:4:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Extra: Explore the Data Set The popularity of some baby names may be influenced by cultural phenomena, such as a political figure coming to power. Below, we plot the popularity of name Hillary for female babies in Calfiornia over time. What do you notice about this plot? What real-world events in the U.S. occurred when there was a steep drop in babies named Hillary? hillary_baby_name = baby_names[(baby_names['Name'] == 'Hillary') \u0026 (baby_names['State'] == 'CA') \u0026 (baby_names['Sex'] == 'F')] plt.plot(hillary_baby_name['Year'], hillary_baby_name['Count']) plt.title(\"Hillary Popularity Over Time\") plt.xlabel('Year') plt.ylabel('Count'); The code above is hard coded to generate a dataframe representing the popularity of the female name Hillary in the state of California. While this approach works, it‚Äôs inelegant. Here we‚Äôll use a more elegant approach that builds a dataframe such that: It contains ALL names. The counts are summed across all 50 states, not just California. To do this, we use groupby, though here we‚Äôre grouping on two columns (‚ÄúName‚Äù and ‚ÄúYear‚Äù) instead of just one. After grouping, we use the sum aggregation function. # just run this cell counts_aggregated_by_name_and_year = baby_names.groupby([\"Name\", \"Year\"]).sum() counts_aggregated_by_name_and_year Note that the resulting DataFrame is multi-indexed, i.e. it has two indices. The outer index is the Name, and the inner index is the Year. In order to visualize this data, we‚Äôll use reset_index in order to set the index back to an integer and transform the Name and Year back into columnar data. # just run this cell counts_aggregated_by_name_and_year = counts_aggregated_by_name_and_year.reset_index() counts_aggregated_by_name_and_year Similar to before, we can plot the popularity of a given name by selecting the name we want to visualize. The code below is very similar to the plotting code above, except that we use query to get the name of interest instead of using a boolean array. Note: Here we use a special syntax @name_of_interest to tell the query command to use the python variable name_of_interest. Try out some other names and see what trends you observe. Note that since this is the American social security database, international names are not well represented. # just run this cell name_of_interest = 'Hillary' chosen_baby_name = counts_aggregated_by_name_and_year.query(\"Name == @name_of_interest\") plt.plot(chosen_baby_name['Year'], chosen_baby_name['Count']) plt.title(f\"Popularity Of {name_of_interest} Over Time\") plt.xlabel('Year') plt.ylabel('Count'); To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-07-15","objectID":"/datalab2/:4:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export(pdf=False) ","date":"2024-07-15","objectID":"/datalab2/:5:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"distribution ÂÆö‰πâ ","date":"2024-07-15","objectID":"/datal7/:1:0","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"bar plots for distribution ","date":"2024-07-15","objectID":"/datal7/:2:0","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"data8 example ","date":"2024-07-15","objectID":"/datal7/:2:1","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"compound way ","date":"2024-07-15","objectID":"/datal7/:2:2","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"seaborn example import seaborn as sns sns.countplot(x='variable', data=df) # rug plot sns.rugplot(x='variable', data=df, color='black') ","date":"2024-07-15","objectID":"/datal7/:2:3","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"plotly example ","date":"2024-07-15","objectID":"/datal7/:2:4","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"Â§ÑÁêÜÂºÇÂ∏∏ÂÄºÔºàoutliersÔºâÂíåÂ≥∞ÂÄºÔºàmodeÔºâ ","date":"2024-07-15","objectID":"/datal7/:3:0","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"density curve ÂØÜÂ∫¶Êõ≤Á∫øÁúãÂ≥∞ ÁÆ±ÂûãÂõæ import seaborn as sns sns.boxplot(x='variable', data=df) violin plot ÂíåÁÆ±ÂûãÂõæÂØπÊØîÊù•ÁúãÔºåviolin plotÂÆΩÂ∫¶ÊúâÊÑè‰πâ import seaborn as sns sns.violinplot(x='variable', data=df) Â§ÑÁêÜoverplotting random jitter ","date":"2024-07-15","objectID":"/datal7/:3:1","tags":null,"title":"DATA100-L7: Visualization ‚Ö†","uri":"/datal7/"},{"categories":["DATA100"],"content":"text data ","date":"2024-07-15","objectID":"/datal6/:1:0","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"python string methods .replace(str1, str2) .split('/') ","date":"2024-07-15","objectID":"/datal6/:1:1","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"regex ÂèÇËÄÉToolsÊ≠£ÂàôË°®ËææÂºèÁ¨îËÆ∞ | ‰ºòÂÖàÁ∫ßËæÉ‰Ωé ÊÑüÂÖ¥Ë∂£ÁöÑÁªÉ‰π†‚Üì https://alf.nu/RegexGolf ","date":"2024-07-15","objectID":"/datal6/:2:0","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"python re .sub() pattern(r\"......\") is a raw string, which means that backslashes are not interpreted as escape characters. eg: ‚Äú\\\\section‚Äù in regular str, ‚Äú\\section‚Äù in raw str. .findall(pattern, string) import re pattern = r'\\b\\w{3}\\b' string = 'The quick brown fox jumps over the lazy dog' matches = re.findall(pattern, string) print(matches) Output: ['The', 'fox', 'dog'] extract() extractall() ÊúâÊó∂‰ºöÊúâser.str.extract()ÂΩ¢ÂºèÔºÅ ","date":"2024-07-15","objectID":"/datal6/:2:1","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"Discussion 2: Pandas Practice We will begin our discussion of Pandas. You will practice: Selecting columns Filtering with boolean conditions Counting with value_counts import pandas as pd import numpy as np ","date":"2024-07-15","objectID":"/datadisc02/:0:0","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Pandas Practise In the first Pandas question, we will be working with the elections dataset from lecture. elections = pd.read_csv(\"elections.csv\") # read in the elections data into a pandas dataframe! elections.head(5) Year Candidate Party Popular vote Result % 0 1824 Andrew Jackson Democratic-Republican 151271 loss 57.210122 1 1824 John Quincy Adams Democratic-Republican 113142 win 42.789878 2 1828 Andrew Jackson Democratic 642806 win 56.203927 3 1828 John Quincy Adams National Republican 500897 loss 43.796073 4 1832 Andrew Jackson Democratic 702735 win 54.574789 ","date":"2024-07-15","objectID":"/datadisc02/:1:0","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 5 We want to select the ‚ÄúPopular vote‚Äù column as a pd.Series. Which of the following lines of code will error? elections['Popular vote'] elections.iloc['Popular vote'] elections.loc['Popular vote'] elections.loc[:, 'Popular vote'] elections.iloc[:, 'Popular vote'] Run each line in the cell below and see for yourself! # elections.iloc['Popular vote'] # wrong # elections.iloc[:, 'popular votes'] # wrong # elections['Popular vote'] # right # elections.loc['Popular vote'] # ket error # elections.loc[:,'Popular vote'] # right 0 151271 1 113142 2 642806 3 500897 4 702735 ... 173 62984828 174 732273 175 4489235 176 65853514 177 1457226 Name: Popular vote, Length: 178, dtype: int64 ","date":"2024-07-15","objectID":"/datadisc02/:1:1","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 6 Write one line of Pandas code that returns a pd.DataFrame that only contains election results from the 1900s. elections[(elections['Year'] \u003e= 1900) \u0026 (elections['Year'] \u003c 2000)] # Ê≥®ÊÑèÊòØ \u0026 Year Candidate Party Popular vote Result % 54 1900 John G. Woolley Prohibition 210864 loss 1.526821 55 1900 William Jennings Bryan Democratic 6370932 loss 46.130540 56 1900 William McKinley Republican 7228864 win 52.342640 57 1904 Alton B. Parker Democratic 5083880 loss 37.685116 58 1904 Eugene V. Debs Socialist 402810 loss 2.985897 ... ... ... ... ... ... ... 146 1996 Harry Browne Libertarian 485759 loss 0.505198 147 1996 Howard Phillips Taxpayers 184656 loss 0.192045 148 1996 John Hagelin Natural Law 113670 loss 0.118219 149 1996 Ralph Nader Green 685297 loss 0.712721 150 1996 Ross Perot Reform 8085294 loss 8.408844 97 rows √ó 6 columns ","date":"2024-07-15","objectID":"/datadisc02/:1:2","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 7 Write one line of Pandas code that returns a pd.Series, where the index is the Party, and the values are how many times that party won an election. Hint: use value_counts. # Your answer here elections['Party'].value_counts() Party Democratic 46 Republican 40 Prohibition 11 Libertarian 11 Socialist 10 Independent 6 Whig 6 Green 6 Progressive 4 Populist 3 Constitution 3 American Independent 3 American 2 National Republican 2 Democratic-Republican 2 Reform 2 Free Soil 2 Anti-Masonic 1 National Union 1 Constitutional Union 1 National Democratic 1 Union Labor 1 Greenback 1 Anti-Monopoly 1 Liberal Republican 1 Southern Democratic 1 Northern Democratic 1 Farmer‚ÄìLabor 1 Dixiecrat 1 States' Rights 1 Communist 1 Union 1 Taxpayers 1 New Alliance 1 Citizens 1 Natural Law 1 Name: count, dtype: int64 ","date":"2024-07-15","objectID":"/datadisc02/:1:3","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Grading Assistance (Bonus) Fernando is writing a grading script to compute grades for students in Data 101. Recall that many factors go into computing a student‚Äôs final grade, including homework, discussion, exams, and labs. In this question, we will help Fernando compute the homework grades for all students using a DataFrame, hw_grades, provided by Gradescope. The Pandas DataFrame hw_grades contains homework grades for all students for all homework assignments, with one row for each combination of student and homework assignment. Any assignments that are incomplete are denoted by NaN (missing) values, and any late assignments are denoted by a True boolean value in the Late column. You may assume that the names of students are unique. Below is a sample of hw_grades. hw_grades = pd.read_csv(\"hw_grades.csv\") hw_grades.sample(5, random_state = 0) Name Assignment Grade Late 28 Sid Homework 9 82.517998 True 11 Ash Homework 2 78.264844 True 10 Ash Homework 1 98.421049 False 41 Emily Homework 2 62.900313 False 2 Meg Homework 3 89.785619 False ","date":"2024-07-15","objectID":"/datadisc02/:2:0","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8a Assuming there is a late penalty that causes a 10% grade reduction to the student‚Äôs current score (i.e. a 65% score would become a 65% - 6.5% = 58.5%), write a line of Pandas code to calculate all the homework grades, including the late penalty if applicable, and store it in a column named ‚ÄôLPGrade‚Äô. # Your answer here hw_grades['LPGrade'] = hw_grades['Grade'] * (1 - hw_grades['Late'] * 0.1) # Áî®‰∏™ÈöêÂºèËΩ¨Êç¢ hw_grades.head() Name Assignment Grade Late LPGrade 0 Meg Homework 1 NaN False NaN 1 Meg Homework 2 64.191844 False 64.191844 2 Meg Homework 3 89.785619 False 89.785619 3 Meg Homework 4 74.420033 False 74.420033 4 Meg Homework 5 74.372434 True 66.935190 ","date":"2024-07-15","objectID":"/datadisc02/:2:1","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8b Which of the following expressions outputs the students‚Äô names and number of late assignments, from least to greatest number of late assignments? hw_grades.groupby([‚ÄôName‚Äô]).sum().sort_values() hw_grades.groupby([‚ÄôName‚Äô, ‚ÄôLate‚Äô]).sum().sort_values() hw_grades.groupby([‚ÄôName‚Äô]).sum()[‚ÄôLate‚Äô].sort_values() hw_grades.groupby([‚ÄôName‚Äô]).sum().sort_values()[‚ÄôLate‚Äô] # Your answer here # hw_grades.groupby(['Name']).sum().sort_values() # \u003c---- Try to sort on df, but have to give 'by=...' into sort_values() hw_grades.groupby(['Name']).sum()['Late'].sort_values() Name Sid 1 Emily 2 Meg 2 Ash 3 Smith 3 Name: Late, dtype: int64 ","date":"2024-07-15","objectID":"/datadisc02/:2:2","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8c If each assignment is weighted equally, fill in the blanks below to calculate each student‚Äôs overall homework grade, including late penalties for any applicable assignments. Hint: Recall that incomplete assignments have NaN values. How can we use fillna to replace these null values? hw_grades._________(_______) \\ .groupby(___________)[____________] \\ .agg(____________) # Your answer here hw_grades.fillna(0)\\ .groupby(['Name'])['LPGrade']\\ .agg('mean') # Python‰∏≠ÔºåÂèçÊñúÊù† \\ Áî®‰ΩúË°åÁª≠Â≠óÁ¨¶ÔºåÂÆÉÂÖÅËÆ∏‰Ω†Â∞Ü‰∏ÄË°å‰ª£Á†ÅÂàÜÂâ≤ÊàêÂ§öË°åÔºå‰ª•ÊèêÈ´ò‰ª£Á†ÅÁöÑÂèØËØªÊÄß„ÄÇËøôÂú®ÁºñÂÜôËæÉÈïøÁöÑ‰∏ÄË°å‰ª£Á†ÅÊó∂ÁâπÂà´ÊúâÁî®ÔºåÂèØ‰ª•ÈÅøÂÖç‰ª£Á†ÅËøá‰∫éÊã•Êå§Ôºå‰ΩøÂæó‰ª£Á†ÅÊõ¥Êòì‰∫éÈòÖËØªÂíåÁª¥Êä§„ÄÇ Name Ash 80.830657 Emily 84.297725 Meg 69.218137 Sid 63.020729 Smith 58.332233 Name: LPGrade, dtype: float64 ","date":"2024-07-15","objectID":"/datadisc02/:2:3","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8d Of all the homework assignments, which are the most difficult in terms of the median grade? Order by the median grade, from lowest to greatest. Do not consider incomplete assignments or late penalties in this calculation. Fill in the blanks below to answer this question. Hint: Recall that incomplete assignments have NaN values. How can we use dropna to remove these null values? hw_grades._________() \\ .groupby(___________)[____________] \\ .agg(____________) \\ .sort_values() # Your answer here hw_grades.dropna()\\ .groupby('Assignment')['Grade']\\ .agg('median')\\ .sort_values() Assignment Homework 2 64.160918 Homework 10 66.366211 Homework 5 74.372434 Homework 8 76.362904 Homework 4 78.207572 Homework 3 78.348163 Homework 9 82.517998 Homework 6 84.369535 Homework 1 85.473281 Homework 7 92.200688 Name: Grade, dtype: float64 ","date":"2024-07-15","objectID":"/datadisc02/:2:4","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"EDA: Exploratory Data Analysis ","date":"2024-07-14","objectID":"/datal5/:0:0","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"Infinite loop DW \u0026 EDA‚Ä¶‚Ä¶ DW: raw data -\u003e clean data -\u003e usable data ","date":"2024-07-14","objectID":"/datal5/:1:0","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"key data properties to consider in EDA ","date":"2024-07-14","objectID":"/datal5/:2:0","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"structure file format rectangular data: tables and matrices CSV, TabSV/TSV, json(is a dict) txt, XML pd.read_csv('filename.tsv',delimiter='\\t') turn to lec jupyter notebook to see more details ÂèòÈáèÁßçÁ±ª Ê≥®ÊÑèÔºö‰∏çÂîØ‰∏ÄÔºå‰∏çÂÖ®Èù¢ multiple files ‰∏ªÈîÆÔºü ","date":"2024-07-14","objectID":"/datal5/:2:1","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"granularity(È¢óÁ≤íÂ∫¶Ôºü) scope and temporality È¢óÁ≤íÂ∫¶ scope: sampling frame temporality: time-series data unix time posix time ","date":"2024-07-14","objectID":"/datal5/:2:2","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"faithfulness missing data? ","date":"2024-07-14","objectID":"/datal5/:2:3","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["UCB-CS61B"],"content":"Exceptions throw statement: throws an exception public V get(K key) { int location = findKey(key); if (location \u003c 0) { throw new IllegalArgumentException(\"Key \" + key + \" does not exist in map.\"); } return values[findKey(key)]; } ÊòæÂºèÊäõÂá∫ÂºÇÂ∏∏ public static void main(String[] args) { System.out.println(\"ayyy lmao\"); throw new RuntimeException(\"For no reason.\"); } ","date":"2024-07-14","objectID":"/61b-14/:0:0","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"What has been Thrown, can be Caught Dog d = new Dog(\"Lucy\", \"Retriever\", 80); d.becomeAngry(); try { d.receivePat(); } catch (Exception e) { System.out.println( \"Tried to pat: \" + e); } System.out.println(d); Áî±callstackÈ°∫Â∫è exceptionÊòØ‰∏ÄÁßçÂØπË±°ÔºåÊúâÊó∂ÂèØËÉΩËßÅÂà∞ÁöÑÈîôËØØ‚Üì ÊúÄÂ•ΩÊòéÁ°ÆÊÄé‰πàÂ§ÑÁêÜexception public static void gulgate() throws IOException { ... throw new IOException(\"hi\"); ... } ÊúâÊó∂ÈúÄË¶ÅËÄÉËôëmainÊÉÖÂÜµ ‰∏äÈù¢Ê≤°ÊúâÊòéÁ°ÆÂ§ÑÁêÜ checked‰∏éÂê¶ËßÅÁßçÁ±ª Iteration ÂàõÂª∫ËÉΩÊîØÊåÅfor (Item i : someIterable)ÁöÑÊÉÖÂÜµ ","date":"2024-07-14","objectID":"/61b-14/:1:0","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"The Iterable Interface public interface Iterable\u003cT\u003e { Iterator\u003cT\u003e iterator(); } package java.util; public interface Iterator\u003cT\u003e { boolean hasNext(); T next(); } ","date":"2024-07-14","objectID":"/61b-14/:2:0","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"ÂÅáÂ¶ÇË¶ÅÈÅçÂéÜArrayMapÔºåÈúÄË¶ÅÂØπkeyËøõË°åÊìç‰Ωú Âà∞Ê≠§ÂÆåÊàêÔºåÂèØ‰ª•ÊâßË°åÈÅçÂéÜ ","date":"2024-07-14","objectID":"/61b-14/:2:1","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"Packages and JAR Files ","date":"2024-07-14","objectID":"/61b-15/:0:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"ÂàõÂª∫ÂåÖ At the top of every file in the package, put the package name. Make sure that the file is stored in a folder with the appropriate folder name. For a package with name ug.joshh.animal, use folder ug/joshh/animal. Ë¶ÅÁî®ÁöÑÊó∂ÂÄôimportÂç≥ÂèØ ","date":"2024-07-14","objectID":"/61b-15/:1:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"default package ","date":"2024-07-14","objectID":"/61b-15/:2:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"JAR Files Access Control Object Methods: Equals and toString( ) ","date":"2024-07-14","objectID":"/61b-15/:3:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"toString( ) ","date":"2024-07-14","objectID":"/61b-15/:4:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"== vs equals( ) == compares references equals( ) compares values, but pay attention to the type! public class Date { private final int month; private final int day; private final int year; public Date(int m, int d, int y) { month = m; day = d; year = y; } public boolean equals(Object x) { if (this == x) return true; if (x == null) return false; if (this.getClass() != x.getClass()) { return false; } Date that = (Date) x; if (this.day != that.day) { return false; } if (this.month != that.month) { return false; } if (this.year != that.year) { return false; } return true; } } ","date":"2024-07-14","objectID":"/61b-15/:5:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"Rules for Equals in Java ÂèçË∫´ÊÄßÔºöx.equals(x) == true ÂØπÁß∞ÊÄß ‰º†ÈÄíÊÄß Ê≥®ÊÑèÂÆûÁé∞equalsÊñπÊ≥ïÊó∂Ôºå‰∏çË¶ÅËøùËÉåËøô‰∫õÊÄßË¥®ÔºÅ ","date":"2024-07-14","objectID":"/61b-15/:6:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"61B: Writing Efficient Programs Programming cost. How long does it take to develop your programs? How easy is it to read, modify, and maintain your code? More important than you might think! Majority of cost is in maintenance, not development! Ëá™È°∂Âêë‰∏ãÔºåÈÄêÂ±ÇÊäΩË±°ÔºåÂàÜËÄåÊ≤ª‰πãÔºåÂåñÊï¥‰∏∫Èõ∂ ADT Implementations Designing ADTs ËôΩÁÑ∂extensionÁÆÄÂçïÔºå‰ΩÜÊòØÂßîÊâòdelegationÊõ¥Âä†ÁÅµÊ¥ª Views ËßÜÂõæ Âú®Java‰∏≠Ôºå‚Äúview\"ÈÄöÂ∏∏ÊåáÁöÑÊòØ‰∏ÄÁßçÊï∞ÊçÆÁªìÊûÑÁöÑËßÜÂõæÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁßçËÆøÈóÆÂíåÊìç‰ΩúÂ∫ïÂ±ÇÊï∞ÊçÆÁöÑÊñπÂºèÔºåËÄå‰∏çÈúÄË¶ÅÂ§çÂà∂Êï¥‰∏™Êï∞ÊçÆÈõÜ„ÄÇËßÜÂõæÁöÑ‰∏ªË¶Å‰ºòÁÇπÊòØÂÆÉ‰ª¨Êèê‰æõ‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÊñπÂºèÊù•Êìç‰ΩúÊï∞ÊçÆÂ≠êÈõÜÔºåËÄå‰∏çÈúÄË¶ÅÂ§çÂà∂Êï∞ÊçÆÔºå‰ªéËÄåËäÇÁúÅÂÜÖÂ≠òÂíåÊèêÈ´òÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåËßÜÂõæ‰πüÊúâ‰∏Ä‰∫õÈôêÂà∂Ôºå‰æãÂ¶ÇÂõ∫ÂÆöÂ§ßÂ∞èÁöÑËßÜÂõæ‰∏çËÉΩÊ∑ªÂä†ÊàñÂà†Èô§ÂÖÉÁ¥†„ÄÇ Occasionally, implementation details may allow for views that are too difficult to implement for an abstract type. ","date":"2024-07-14","objectID":"/61b-16/:1:0","tags":null,"title":"61B-16: Encapsulation, Lists, Delegation vs. Extension","uri":"/61b-16/"},{"categories":["UCB-CS61B"],"content":"Programming in the Real World ÂØπÊäÄÊúØË¶ÅÊï¨Áïè midterm review Comparing strings for equality using == vs .equals ‚Äî\u003e see in autoboxing lecture Âú®Java‰∏≠Ôºåthis ÊòØ‰∏Ä‰∏™ÊåáÂêëÂΩìÂâçÂØπË±°ÂÆû‰æãÁöÑÂºïÁî®„ÄÇÂÆÉÈÄöÂ∏∏Áî®‰∫éÂºïÁî®ÂΩìÂâçÁ±ªÁöÑÂÆû‰æãÊàêÂëòÔºåÊàñËÄÖÂú®ÊñπÊ≥ï‰∏≠Âå∫ÂàÜÊàêÂëòÂèòÈáèÂíåÂ±ÄÈÉ®ÂèòÈáè„ÄÇÁÑ∂ËÄåÔºå‰Ω†‰∏çËÉΩÂ∞Ü this ÈáçÊñ∞ËµãÂÄº‰∏∫Âè¶‰∏Ä‰∏™ÂØπË±°ÁöÑÂºïÁî®ÔºåÂõ†‰∏∫ this ÊòØ‰∏Ä‰∏™Âõ∫ÂÆöÁöÑÊ¶ÇÂøµÔºåÂÆÉ‰ª£Ë°®ÂΩìÂâçÂØπË±°Êú¨Ë∫´„ÄÇ ‰Ω†Êèê‰æõÁöÑ‰ª£Á†ÅÁ§∫‰æã‰∏≠ÔºåÂ∞ùËØïÂ∞Ü this ËµãÂÄº‰∏∫‰∏Ä‰∏™Êñ∞ÁöÑ Dog ÂØπË±°ÔºåËøôÊòØ‰∏çÂÖÅËÆ∏ÁöÑ„ÄÇJava ÁºñËØëÂô®‰ºöÊä•ÈîôÔºåÂõ†‰∏∫ÂÆÉËøùÂèç‰∫Ü this ÁöÑ‰ΩøÁî®ËßÑÂàô„ÄÇ public class Dog { public void f() { this = new Dog(); // ËøôË°å‰ª£Á†Å‰ºöÂØºËá¥ÁºñËØëÈîôËØØÔºåÂõ†‰∏∫‰∏çËÉΩÈáçÊñ∞ËµãÂÄºthis } } Dog d = new Dog(); d.f(); // Ë∞ÉÁî®f()ÊñπÊ≥ïÔºå‰ΩÜÁî±‰∫é‰∏äÈù¢ÁöÑÈîôËØØÔºåËøôË°å‰ª£Á†ÅÂÆûÈôÖ‰∏äÊó†Ê≥ïÊâßË°å Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑ Dog ÂØπË±°Âπ∂Â∞ÜÂÖ∂ÂºïÁî®ËµãÁªô thisÔºå‰Ω†ÈúÄË¶Å‰ΩøÁî®Âè¶‰∏Ä‰∏™ÂèòÈáèÔºåÊØîÂ¶Ç anotherDog„ÄÇ‰∏ãÈù¢ÊòØ‰øÆÊîπÂêéÁöÑ‰ª£Á†ÅÁ§∫‰æãÔºö public class Dog { public void f() { Dog anotherDog = new Dog(); // ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑDogÂØπË±° // ËøôÈáå‰Ω†ÂèØ‰ª•‰ΩøÁî®anotherDogÊù•ÂºïÁî®Êñ∞ÂàõÂª∫ÁöÑDogÂØπË±° } } Dog d = new Dog(); d.f(); // Áé∞Âú®f()ÊñπÊ≥ïÂèØ‰ª•Ê≠£Â∏∏ÊâßË°åÔºåÊ≤°ÊúâÁºñËØëÈîôËØØ Âú®ËøôÊÆµ‰øÆÊîπÂêéÁöÑ‰ª£Á†Å‰∏≠ÔºåanotherDog ÂèòÈáèÁî®‰∫éÂ≠òÂÇ®Êñ∞ÂàõÂª∫ÁöÑ Dog ÂØπË±°ÁöÑÂºïÁî®ÔºåËÄå this ‰ªçÁÑ∂‰øùÊåÅÂÖ∂ÂéüÂßãÂê´‰πâÔºåÂç≥ÊåáÂêëÂΩìÂâçÁöÑ Dog ÂØπË±°ÂÆû‰æã„ÄÇ ","date":"2024-07-14","objectID":"/61b-12/:0:0","tags":null,"title":"61B-12:  Coding in the Real World, Review","uri":"/61b-12/"},{"categories":["UCB-CS61B"],"content":"13-16Âá†‰πéÊòØjavaËØ≠Ê≥ïËÆ≤Ëß£üòÑ ","date":"2024-07-14","objectID":"/61b-13/:0:0","tags":null,"title":"61B-13: Generics, Autoboxing","uri":"/61b-13/"},{"categories":["UCB-CS61B"],"content":"Primitives Cannot Be Used as Actual Type Arguments ","date":"2024-07-14","objectID":"/61b-13/:1:0","tags":null,"title":"61B-13: Generics, Autoboxing","uri":"/61b-13/"},{"categories":["UCB-CS61B"],"content":"Autoboxing Wrapper Types Are (Mostly) Just Like Any Class 8ÁßçÂü∫Êú¨Á±ªÂûã‰πãÈó¥ËΩ¨Êç¢‰πüÂ≠òÂú®widening Immutability Á±ª‰ºº‰∫éconstÂú®cpp public class Date { public final int month; public final int day; public final int year; private boolean contrived = true; public Date(int m, int d, int y) { month = m; day = d; year = y; } } Warning: Declaring a reference as Final does not make object immutable. Example: public final ArrayDeque d = new ArrayDeque(); The d variable can never change, but the referenced deque can! ËßÅÊåáÈíàÂ∏∏Èáè‰∏éÂ∏∏ÈáèÊåáÈíàÁöÑÂå∫Âà´in C++ Defining Generic Classes Goal 1: Create a class ArrayMap with the following methods: put(key, value): Associate key with value. If -1, adds k and v to the last position of the arrays. containsKey(key): Checks to see if arraymap contains the key. get(key): Returns value, assuming key exists.. keys(): Returns a list of all keys. size(): Returns number of keys. public class ArrayMap\u003cK, V\u003e { private K[] keys; private V[] values; private int size; public ArrayMap() { keys = (K[]) new Object[100]; values = (V[]) new Object[100]; size = 0; } private int findKey(K key) { for (int i = 0; i \u003c size; i++) { if (keys[i].equals(key)) { return i; } } return -1; } private int getKeyIndex(K key) { for (int i = 0; i \u003c size; i++) { if (keys[i].equals(key)) { return i; } return -1; } } public void put(K key, V value) { int i = getKeyIndex(key); if (i \u003e -1) { values[i] = value; return; } keys[size] = key; values[size] = value; size += 1; } public V get(K key) { return values[findKey(key)]; } public boolean containsKey(K key) { int i = findKey(key); return (i \u003e -1); } public List\u003cK\u003e keys() { List\u003cK\u003e list = new ArrayList\u003cK\u003e(); for (int i = 0; i \u003c size; i++) { list.add(keys[i]); } return list; } public int size() { return size; } } Generic Methods Goal: Create a class MapHelper with two methods: get(Map61B, key): Returns the value corresponding to the given key in the map if it exists, otherwise null. Unlike the ArrayMap‚Äôs get method, which crashes if the key doesn‚Äôt exist. maxKey(Map61B): Returns the maximum of all keys in the given ArrayMap. Works only if keys can be compared. public class MapHelper { public static \u003cX,Z\u003e Z get(ArrayMap\u003cX,Z\u003e map, X key) { if (map.containsKey(key)) { return map.get(key); } else { return null; } } public static \u003cX extends Comparable\u003cX\u003e,Z\u003e X maxKey(ArrayMap\u003cX,Z\u003e map) { X max = null; for (X key : map.keys()) { if (max == null || key.compareTo(max) \u003e 0) { max = key; } } return max; } } ","date":"2024-07-14","objectID":"/61b-13/:2:0","tags":null,"title":"61B-13: Generics, Autoboxing","uri":"/61b-13/"},{"categories":["UCB-CS61B"],"content":"Subtype Polymorphism ÊåáÁöÑÊòØÂèØ‰ª•‰ΩøÁî®Áà∂Á±ªÁ±ªÂûãÁöÑÂØπË±°Êù•ÂºïÁî®Â≠êÁ±ªÁ±ªÂûãÁöÑÂÆû‰æã„ÄÇ DIY Comparison ÊØîËæÉObjectÁ±ªÂØπË±°Êó∂‰∫ßÁîüÈóÆÈ¢òÔºåÂ¶Ç‰ΩïÊØîËæÉÔºüÔºüÔºü ËÄÉËôëÂÜô‰∏Ä‰∏™ÊØîËæÉÂô®ÔºåÊØîËæÉ‰∏§‰∏™ObjectÂØπË±° Âä†Ê∑±ÁºñËØëÁêÜËß£ Comparable Interface public interface Comparable\u003cT\u003e { public int compareTo(T obj); } Comparator Interface public interface Comparator\u003cT\u003e { public int compare(T obj1, T obj2); } ","date":"2024-07-14","objectID":"/61b-10/:0:0","tags":null,"title":"61B-10: Subtype Polymorphism vs. HoFs","uri":"/61b-10/"},{"categories":["UCB-CS61B"],"content":"‰∏§ËÄÖÁöÑÂÖ≥Á≥ª‚Üì ÊÄªÁªì ","date":"2024-07-14","objectID":"/61b-10/:0:1","tags":null,"title":"61B-10: Subtype Polymorphism vs. HoFs","uri":"/61b-10/"},{"categories":["UCB-CS61B"],"content":"Java Libraries ","date":"2024-07-14","objectID":"/61b-11/:0:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"Collections Collections is a package in Java that provides various utility classes for working with collections. ","date":"2024-07-14","objectID":"/61b-11/:1:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"TasksÂºïÂÖ• 3 tasks, given the text of a book: Create a list of all words in the book. Count the number of unique words. Keep track of the number of times that specific words are mentioned. #1 way set public static int countUniqueWords(List\u003cString\u003e words) { Set\u003cString\u003e ss = new HashSet\u003c\u003e(); for (String s : words) { ss.add(s); } return ss.size(); } public static int countUniqueWords(List\u003cString\u003e words) { Set\u003cString\u003e ss = new HashSet\u003c\u003e(); ss.addAll(words); return ss; } #2 way map public static Map\u003cString, Integer\u003e collectWordCount(List\u003cString\u003e words, List\u003cString\u003e targets) { Map\u003cString, Integer\u003e wordCounts = new HashMap\u003c\u003e(); for (String s : targets) { wordCounts.put(s, 0); } for (String s : words) { if (wordCounts.containsKey(s)) { int oldCount = wordCounts.get(s); wordCounts.put(s, oldCount + 1); } } return wordCounts; } PythonÈáåÈù¢ÂàôÊòØdictÂÆûÁé∞ÔºåÂíã‰∏ÄÁúã‰ºº‰πéÊõ¥Â•ΩÔºü‰ΩÜÊòØÔºü Java9Êñ∞ÁâπÁÇπÔºö Interfaces and Abstract Classes More interface details: Can provide variables, but they are public static final. final means the value can never change. A class can implement multiple interfaces. ","date":"2024-07-14","objectID":"/61b-11/:2:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"interface summary ","date":"2024-07-14","objectID":"/61b-11/:3:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"abstract class intro ","date":"2024-07-14","objectID":"/61b-11/:4:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"‰∏§ËÄÖÂØπÊØî Packages A package is a namespace that organizes classes and interfaces. ","date":"2024-07-14","objectID":"/61b-11/:5:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"Implementation Inheritance: Extends extends Because of extends, RotatingSLList inherits all members of SLList: All instance and static variables. ÔºàÊ≥®ÊÑèpublic, private, protectedÁöÑÂå∫Âà´Ôºâ All methods. All nested classes. Constructors are not inherited. super public class VengefulSLList\u003cItem\u003e extends SLList\u003cItem\u003e { private SLList\u003cItem\u003e deletedItems; public VengefulSLList() { deletedItems = new SLList\u003cItem\u003e(); } @Override public Item removeLast() { Item oldBack = super.removeLast(); deletedItems.addLast(oldBack); return oldBack; } public void printLostItems() { deletedItems.print(); } } Ê≥®ÊÑèÊ≤°Êúâsuper.superÁöÑÊÉÖÂÜµ ","date":"2024-07-14","objectID":"/61b-9/:0:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"constructor ","date":"2024-07-14","objectID":"/61b-9/:1:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"Object class Encapsulation ","date":"2024-07-14","objectID":"/61b-9/:2:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"Module Module: A set of methods that work together as a whole to perform some task or set of related tasks. Implementation Inheritance Breaks Encapsulation Ê≥®ÊÑèprivate ËøòÊúâ ÂèçÂ§çËá™ÊàëË∞ÉÁî®‚Üì Type Checking and Casting Â≠êÁ±ªËµãÂÄºÁªôÂü∫Á±ªÂèØ‰ª•ÔºåÂèç‰πã‰∏çË°å Dynamic Method Selection and Casting Puzzle Higher Order Functions (A First Look) ÊâßË°åÁ±ª‰ººf(f(x))ÁöÑÊìç‰Ωú Java7Âèä‰πãÂâç‰∏çËÉΩ‰ΩøÁî®ÂáΩÊï∞ÊåáÈíàÔºåËÄÉËôëÂÆû‰ΩìÂåñ‰∏Ä‰∏™ÂáΩÊï∞ÂØπË±° Java8ÂèäÂÖ∂‰ª•ÂêéÔºö ‰∏ÄÂº†ÂõæÊÄªÁªìÁªßÊâø ","date":"2024-07-14","objectID":"/61b-9/:3:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"Ad Hoc Testing vs. JUnit public class TestSort { /** Tests the sort method of the Sort class. */ public static testSort() { String[] input = {\"cows\", \"dwell\", \"above\", \"clouds\"}; String[] expected = {\"above\", \"cows\", \"clouds\", \"dwell\"}; Sort.sort(input); org.junit.Assert.assertArrayEquals(expected, input); } public static void main(String[] args) { testSort(); } } Selection Sort ÁÆÄÂçï‰ªãÁªç‰∏Ä‰∏ã‰∫ÜÔºåÂÖ≥Ê≥®ÁÇπÂú®junit Simpler JUnit Tests ADD, TDD, Integration Testing More On JUnit (Extra) ","date":"2024-07-14","objectID":"/61b-7/:0:0","tags":null,"title":"61B-7: Testing","uri":"/61b-7/"},{"categories":["UCB-CS61B"],"content":" but hard to maintain! Hypernyms, Hyponyms, and Interface Inheritance ","date":"2024-07-14","objectID":"/61b-8/:0:0","tags":null,"title":"61B-8: Inheritance, Implements","uri":"/61b-8/"},{"categories":["UCB-CS61B"],"content":"interface public interface List61B\u003cItem\u003e { public void addFirst(Item x); public void addLast(Item y); public Item getFirst(); public Item getLast(); public Item removeLast(); public Item get(int i); public void insert(Item x, int position); public int size(); } Overriding vs. Overloading overrideÊ≥®ÊÑèÂä†‰∏ä@Override!!! Interface Inheritance Âü∫Á±ªÂ≠òÊîæÊåáÈíàÈóÆÈ¢ò Answer: If X is a superclass of Y, then memory boxes for X may contain Y. An AList is-a List. Therefore List variables can hold ALList addresses. Implementation Inheritance: Default Methods public interface List61B\u003cItem\u003e { public void addFirst(Item x); public void addLast(Item y); public Item getFirst(); public Item getLast(); public Item removeLast(); public Item get(int i); public void insert(Item x, int position); public int size(); default public void print() { for (int i = 0; i \u003c size(); i += 1) { System.out.print(get(i) + \" \"); } System.out.println(); } } Static and Dynamic Type, Dynamic Method Selection ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è More Dynamic Method Selection, Overloading vs. Overriding ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è Á¥ßÁ¥ßÁõØ‰ΩèÁªÜËäÇÔºÅ Interface vs. Implementation Inheritance ","date":"2024-07-14","objectID":"/61b-8/:1:0","tags":null,"title":"61B-8: Inheritance, Implements","uri":"/61b-8/"},{"categories":["UCB-CS61B"],"content":"From IntList to SLList ‰∫ãÂÆûÊòØÂú®SLListÈáåÈù¢Ê∑ªÂä†‰∏Ä‰∏™IntlistÊï∞ÊçÆÊàêÂëò Public vs. Private or Nested Classes ‰ªãÁªç‰∫Üprivate ","date":"2024-07-14","objectID":"/61b-4/:0:0","tags":null,"title":"61B-4: SLLists, Nested Classes, Sentinel Nodes","uri":"/61b-4/"},{"categories":["UCB-CS61B"],"content":"Nested Classes public class SLList { public class IntNode { public int item; public IntNode next; public IntNode(int i, IntNode n) { item = i; next = n; } } private IntNode first; public SLList(int x) { first = new IntNode(x, null); } ... } addLast() and size() ËÆ®ËÆ∫recursionÂíåiteration‰∏§ÁßçÊÄùË∑ØÔºåÁï•Ëøá Á©∫Èó¥Êç¢Êó∂Èó¥ÂàùËßÅ Sentinel Nodes ÂØπËæπÁïåÁé∞Ë±°ÔºàÁ©∫ÊåáÈíàÔºâÁöÑËÆ®ËÆ∫Âíå‰øùÊä§ ÁÆÄÂåñ‰ª£Á†Å.addLast(x) ","date":"2024-07-14","objectID":"/61b-4/:1:0","tags":null,"title":"61B-4: SLLists, Nested Classes, Sentinel Nodes","uri":"/61b-4/"},{"categories":["UCB-CS61B"],"content":"Doubly Linked Lists Ê≥®ÊÑèÂè™ÊúâsentinelÊó∂Ë¶ÅËÆ®ËÆ∫‰∏Ä‰∫õÁâπÊÆäÊÉÖÂÜµÔºåÁâπÂà´ÊòØÁéØÁä∂ÈìæË°®„ÄÇ Generic Lists Ê≥õÂûãÂàóË°® public class SLList\u003cBleepBlorp\u003e { private IntNode sentinel; private int size; public class IntNode { public BleepBlorp item; public IntNode next; ... } ... } SLList\u003cInteger\u003e s1 = new SLList\u003c\u003e(5); s1.insertFront(10); SLList\u003cString\u003e s2 = new SLList\u003c\u003e(\"hi\"); s2.insertFront(\"apple\"); Arrays, AList ‰ªãÁªç‰∫ÜSystem.arraycopy()Áî®Êù•resize 2D Arrays Arrays vs. Classes arrayÁöÑruntimeÂä®ÊÄÅÁ¥¢ÂºïÔºàÂíåcpp‰∏ç‰∏ÄÊ†∑Ôºâ class runtime ","date":"2024-07-14","objectID":"/61b-5/:0:0","tags":null,"title":"61B-5: DLLists, Arrays","uri":"/61b-5/"},{"categories":["UCB-CS61B"],"content":"A Last Look at Linked Lists Naive Array Lists public class AList { private int[] items; private int size; public AList() { items = new int[100]; size = 0; } public void addLast(int x) { items[size] = x; size += 1; } public int getLast() { return items[size - 1]; } public int get(int i) { return items[i]; } public int size() { return size; } } Resizing Arrays public void addLast(int x) { if (size == items.length) { int[] a = new int[size + 1]; System.arraycopy(items, 0, a, 0, size); items = a; } items[size] = x; size += 1; } private void resize(int capacity) { int[] a = new int[capacity]; System.arraycopy(items, 0, a, 0, size); items = a; } public void addLast(int x) { if (size == items.length) { resize(size + 1); } items[size] = x; size += 1; } Âá†‰Ωïresize ","date":"2024-07-14","objectID":"/61b-6/:0:0","tags":null,"title":"61B-6: ALists, Resizing, vs. SLists","uri":"/61b-6/"},{"categories":["UCB-CS61B"],"content":"memory usage ‰ªîÁªÜËÄÉÈáè Generic ALists public class AList\u003cGlorp\u003e { private Glorp[] items; private int size; public AList() { items = (Glorp []) new Object[8]; size = 0; } private void resize(int cap) { Glorp[] a = (Glorp []) new Object[cap]; // reinterprets as Glorp[] System.arraycopy(items, 0, a, 0, size); items = a; } public Glorp get(int i) { return items[i]; } public Glorp deleteBack() { Glorp returnItem = getBack(); items[size - 1] = null; // to help garbage collection size -= 1; return returnItem; } ... ","date":"2024-07-14","objectID":"/61b-6/:1:0","tags":null,"title":"61B-6: ALists, Resizing, vs. SLists","uri":"/61b-6/"},{"categories":["UCB-CS61B"],"content":"java oop Java is an object oriented language with strict requirements: Every Java file must contain a class declaration*. All code lives inside a class*, even helper functions, global constants, etc. To run a Java program, you typically define a main method using public static void main(String[] args) *: This is not completely true, e.g. we can also declare ‚Äúinterfaces‚Äù in .Java files that may contain code. We‚Äôll cover these later. ","date":"2024-07-14","objectID":"/61b-1/:1:0","tags":null,"title":"61B-1: Intro, Hello World Java","uri":"/61b-1/"},{"categories":["UCB-CS61B"],"content":"Java and Static Typing The compiler checks that all the types in your program are compatible before the program ever runs! This is unlike a language like Python, where type checks are performed DURING execution. ","date":"2024-07-14","objectID":"/61b-1/:2:0","tags":null,"title":"61B-1: Intro, Hello World Java","uri":"/61b-1/"},{"categories":["UCB-CS61B"],"content":"ÁºñËØëÂàùÁúã ","date":"2024-07-14","objectID":"/61b-2/:1:0","tags":null,"title":"61B-2: Defining and Using Classes","uri":"/61b-2/"},{"categories":["UCB-CS61B"],"content":"Defining and Instantiating Classes Static vs. Instance Members ","date":"2024-07-14","objectID":"/61b-2/:2:0","tags":null,"title":"61B-2: Defining and Using Classes","uri":"/61b-2/"},{"categories":["UCB-CS61B"],"content":"Static vs. Non-static public static void main(String[] args) Using Libraries ","date":"2024-07-14","objectID":"/61b-2/:3:0","tags":null,"title":"61B-2: Defining and Using Classes","uri":"/61b-2/"},{"categories":["UCB-CS61B"],"content":"Primitive Types 8 primitive types in Java: byte, short, int, long, float, double, boolean, char Everything else, including arrays, is a reference type. ","date":"2024-07-14","objectID":"/61b-3/:0:0","tags":null,"title":"61B-3: References, Recursion, and Lists","uri":"/61b-3/"},{"categories":["UCB-CS61B"],"content":"The Golden Rule of Equals (GRoE) Given variables y and x: y = x copies all the bits from x into y. Reference Types Everything else, including arrays, is a reference type. ÂíåcppÁöÑÂå∫Âà´‰πã‰∏Ä‰∏çÊòæÂºè‰ΩøÁî®ÊåáÈíà Parameter Passing pass by value üòã pass by referenceÔºàÊüêÁßçÊÑè‰πâ‰∏äjavaÁ∫ØÁ∫Øpass by value üòèÔºâ Instantiation of Arrays IntList and Linked Data Structures ÂçïÈìæË°®Ôºå‰∏çËµòËø∞ ","date":"2024-07-14","objectID":"/61b-3/:1:0","tags":null,"title":"61B-3: References, Recursion, and Lists","uri":"/61b-3/"},{"categories":null,"content":"CS61ABC DATA100(DS100) Ëß¶Âä®ÁöÑÁû¨Èó¥üòã ","date":"2024-07-14","objectID":"/beyondcode/thinking1/:0:0","tags":null,"title":"Thinking1","uri":"/beyondcode/thinking1/"},{"categories":null,"content":"ËÆ∞ÂΩï2024ÂØíÂÅáÊó∂ÂÄôÁöÑÊÄùËÄÉ ","date":"2024-07-14","objectID":"/beyondcode/thinking0/:0:0","tags":null,"title":"Thinking0","uri":"/beyondcode/thinking0/"},{"categories":["DATA100"],"content":"lambda function lambda x: x**2 ÈùûÊòæÂºèÂÆö‰πâÂáΩÊï∞ÔºåÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®lambdaË°®ËææÂºèÊù•ÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞„ÄÇ This is a lambda function that takes in one argument x and returns the square of x. ÂÜçÁúãsort_values() Ê≥®ÊÑè‰º†ÈÄíkey df.sort_values(by='column_name', keys=lambda x: x.str.lower(), ascending=True) ","date":"2024-07-14","objectID":"/datal4/:1:0","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":"add move modify and so on sort by length of string approach1: create a new column and add to original df newdf = df[\"Name\"].str.len() # create a new column with length of each string df['length'] = newdf df.sort_values(by='length', ascending=True) drop column ","date":"2024-07-14","objectID":"/datal4/:2:0","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":"groupby.agg ","date":"2024-07-14","objectID":"/datal4/:3:0","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":"never use loops in this class! df.groupby('column_name').agg(f) f is a dictionary of functions to apply to each column. The function can be a lambda function or a named function. ","date":"2024-07-14","objectID":"/datal4/:3:1","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":"groupby type: pandas.core.groupby.generic.DataFrameGroupBy ","date":"2024-07-14","objectID":"/datal4/:4:0","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":"filter df.groupby('column_name').filter(lambda x: x['column_name'].mean() \u003e 10) This will return a new DataFrame with only the groups that have a mean value greater than 10. ","date":"2024-07-14","objectID":"/datal4/:4:1","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":"multi index Â§öÁª¥Á¥¢Âºï using pivot_table() df.pivot_table(index=['column1', 'column2'], columns='column3', values='column4', aggfunc='mean') This will create a multi-index pivot table with the specified index and columns, and the mean of column4 for each group. using groupby() df.groupby(['column1', 'column2']).agg({'column3': 'mean', 'column4':'sum'}) This will group the DataFrame by column1 and column2, and calculate the mean and sum of column3 and column4 for each group. ","date":"2024-07-14","objectID":"/datal4/:4:2","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":"joining tables left.merge(right, on='column_name', how='inner', lefton='column_name_left', righton='column_name_right') ","date":"2024-07-14","objectID":"/datal4/:5:0","tags":["pandas"],"title":"DATA100-L4: Pandas ‚Ö°","uri":"/datal4/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"hw01.ipynb\") HW 1: Math Review and Plotting ÈáçÁÇπÂú®codingÔºåÁêÜËÆ∫‰æõÂèÇËÄÉ ","date":"2024-07-14","objectID":"/datahw01/:0:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Due Date: Thursday Jan 27, 11:59 PM ","date":"2024-07-14","objectID":"/datahw01/:1:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Collaboration Policy Data science is a collaborative activity. While you may talk with others about the homework, we ask that you write your solutions individually. If you do discuss the assignments with others please include their names at the top of your notebook. Collaborators: list collaborators here ","date":"2024-07-14","objectID":"/datahw01/:2:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"This Assignment The purpose of this assignment is for you to combine Python, math, and the ideas in Data 8 to draw some interesting conclusions. The methods and results will help build the foundation of Data 100. ","date":"2024-07-14","objectID":"/datahw01/:3:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Score Breakdown Question Points 1a 1 1b 2 2a 1 2b 1 2c 2 2d 2 2e 1 3a 2 3b 2 3c 1 3d 2 3e 2 4a 1 4b 1 4c 1 4d 1 5a 1 5b 1 5d 3 6a 2 6b(i) 2 6b(ii) 2 6c 2 Total 36 ","date":"2024-07-14","objectID":"/datahw01/:4:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Before You Start For each question in the assignment, please write down your answer in the answer cell(s) right below the question. We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, NEVER add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file. Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder. Please be sure to check your results carefully. ","date":"2024-07-14","objectID":"/datahw01/:5:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Initialize your environment This cell should run without error if you‚Äôre using the course Jupyter Hub or you have set up your personal computer correctly. import numpy as np import matplotlib import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') ","date":"2024-07-14","objectID":"/datahw01/:5:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: Jupyter Shortcuts Here are some useful Jupyter notebook keyboard shortcuts. To learn more keyboard shortcuts, go to Help -\u003e Keyboard Shortcuts in the menu above. Here are a few we like: ctrl+return : Evaluate the current cell shift+return: Evaluate the current cell and move to the next esc : command mode (may need to press before using any of the commands below) a : create a cell above b : create a cell below dd : delete a cell m : convert a cell to markdown y : convert a cell to code ","date":"2024-07-14","objectID":"/datahw01/:5:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: NumPy You should be able to understand the code in the following cells. If not, review the following: The Data 8 Textbook Chapter on NumPy DS100 NumPy Review Condensed NumPy Review The Official NumPy Tutorial Jupyter pro-tip: Pull up the docs for any function in Jupyter by running a cell with the function name and a ? at the end: np.arange? \u001b[1;31mDocstring:\u001b[0m arange([start,] stop[, step,], dtype=None, *, device=None, like=None) Return evenly spaced values within a given interval. ``arange`` can be called with a varying number of positional arguments: * ``arange(stop)``: Values are generated within the half-open interval ``[0, stop)`` (in other words, the interval including `start` but excluding `stop`). * ``arange(start, stop)``: Values are generated within the half-open interval ``[start, stop)``. * ``arange(start, stop, step)`` Values are generated within the half-open interval ``[start, stop)``, with spacing between values given by ``step``. For integer arguments the function is roughly equivalent to the Python built-in :py:class:`range`, but returns an ndarray rather than a ``range`` instance. When using a non-integer step, such as 0.1, it is often better to use `numpy.linspace`. See the Warning sections below for more information. Parameters ---------- start : integer or real, optional Start of interval. The interval includes this value. The default start value is 0. stop : integer or real End of interval. The interval does not include this value, except in some cases where `step` is not an integer and floating point round-off affects the length of `out`. step : integer or real, optional Spacing between values. For any output `out`, this is the distance between two adjacent values, ``out[i+1] - out[i]``. The default step size is 1. If `step` is specified as a position argument, `start` must also be given. dtype : dtype, optional The type of the output array. If `dtype` is not given, infer the data type from the other input arguments. device : str, optional The device on which to place the created array. Default: None. For Array-API interoperability only, so must be ``\"cpu\"`` if passed. .. versionadded:: 2.0.0 like : array_like, optional Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as ``like`` supports the ``__array_function__`` protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument. .. versionadded:: 1.20.0 Returns ------- arange : ndarray Array of evenly spaced values. For floating point arguments, the length of the result is ``ceil((stop - start)/step)``. Because of floating point overflow, this rule may result in the last element of `out` being greater than `stop`. Warnings -------- The length of the output might not be numerically stable. Another stability issue is due to the internal implementation of `numpy.arange`. The actual step value used to populate the array is ``dtype(start + step) - dtype(start)`` and not `step`. Precision loss can occur here, due to casting or due to using floating points when `start` is much larger than `step`. This can lead to unexpected behaviour. For example:: \u003e\u003e\u003e np.arange(0, 5, 0.5, dtype=int) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \u003e\u003e\u003e np.arange(-3, 3, 0.5, dtype=int) array([-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8]) In such cases, the use of `numpy.linspace` should be preferred. The built-in :py:class:`range` generates :std:doc:`Python built-in integers that have arbitrary size \u003cpython:c-api/long\u003e`, while `numpy.arange` produces `numpy.int32` or `numpy.int64` numbers. This may result in incorrect results for large integer values:: \u003e\u003e\u003e power = 40 \u003e\u003e\u003e modulo = 10000 \u003e\u003e\u003e x1 = [(n ** power) % modulo for n in range(8)] \u003e\u003e\u003e x2 = [(n ** power) % modulo for n in np.arange(8)] \u003e\u003e\u003e print(x1) [0, 1, 7776, 8801, 6176, 625, 6576, 4001] # correct \u003e\u003e\u003e print(x2) [0, 1, 7776, 7185, 0, 5969, 4816, 3361] # incorrect See Also -------- numpy","date":"2024-07-14","objectID":"/datahw01/:5:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: LaTeX You should use LaTeX to format math in your answers. If you aren‚Äôt familiar with LaTeX, not to worry. It‚Äôs not hard to use in a Jupyter notebook. Just place your math in between dollar signs within Markdown cells: $ f(x) = 2x $ becomes $ f(x) = 2x $. If you have a longer equation, use double dollar signs to place it on a line by itself: $$ \\sum_{i=0}^n i^2 $$ becomes: $$ \\sum_{i=0}^n i^2$$ You can align multiple lines using the \u0026 anchor, \\\\ newline, in an align block as follows: \\begin{align} f(x) \u0026= (x - 1)^2 \\\\ \u0026= x^2 - 2x + 1 \\end{align} becomes \\begin{align} f(x) \u0026= (x - 1)^2 \\ \u0026= x^2 - 2x + 1 \\end{align} This PDF has some handy LaTeX. For more about basic LaTeX formatting, you can read this article. ","date":"2024-07-14","objectID":"/datahw01/:5:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: Sums Here‚Äôs a recap of some basic algebra written in sigma notation. The facts are all just applications of the ordinary associative and distributive properties of addition and multiplication, written compactly and without the possibly ambiguous ‚Äú‚Ä¶‚Äù. But if you are ever unsure of whether you‚Äôre working correctly with a sum, you can always try writing $\\sum_{i=1}^n a_i$ as $a_1 + a_2 + \\cdots + a_n$ and see if that helps. You can use any reasonable notation for the index over which you are summing, just as in Python you can use any reasonable name in for name in list. Thus $\\sum_{i=1}^n a_i = \\sum_{k=1}^n a_k$. $\\sum_{i=1}^n (a_i + b_i) = \\sum_{i=1}^n a_i + \\sum_{i=1}^n b_i$ $\\sum_{i=1}^n d = nd$ $\\sum_{i=1}^n (ca_i + d) = c\\sum_{i=1}^n a_i + nd$ These properties may be useful in the Least Squares Predictor question. To see the LaTeX we used, double-click this cell. Evaluate the cell to exit. ","date":"2024-07-14","objectID":"/datahw01/:5:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 1: Calculus In this question we will review some fundamental properties of the sigmoid function, which will be discussed when we talk more about logistic regression in the latter half of the class. The sigmoid function is defined to be $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ ","date":"2024-07-14","objectID":"/datahw01/:6:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 1a Show that $\\sigma(-x) = 1 - \\sigma(x)$. Note, again: In this class, you must always put your answer in the cell that immediately follows the question. DO NOT create any cells between this one and the one that says Type your answer here, replacing this text. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:6:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 1b Show that the derivative of the sigmoid function can be written as: $$\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$ This PDF has some handy LaTeX. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:6:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2: Probabilities and Proportions Much of data analysis involves interpreting proportions ‚Äì lots and lots of related proportions. So let‚Äôs recall the basics. It might help to start by reviewing the main rules from Data 8, with particular attention to what‚Äôs being multiplied in the multiplication rule. ","date":"2024-07-14","objectID":"/datahw01/:7:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2a The Pew Research Foundation publishes the results of numerous surveys, one of which is about the trust that Americans have in groups such as the military, scientists, and elected officials to act in the public interest. A table in the article summarizes the results. Pick one of the options (i) and (ii) to answer the question below; if you pick (i), fill in the blank with the percent. Then, explain your choice. The percent of surveyed U.S. adults who had a great deal of confidence in both scientists and religious leaders (i) is equal to ______________________. (ii) cannot be found with the information in the article. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:7:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2b In a famous (or infamous) survey, members of the Harvard medical school were asked to consider a scenario in which ‚Äúa test to detect a disease whose prevalence is 1/1,000 has a false positive rate of 5 percent‚Äù. The terminology, the specific question asked in the survey, and the answer, are discussed in detail in a Stat 88 textbook section that you are strongly encouraged to read. As Stat 88 is a Data 8 connector course, the section is another look at the same ideas as in the corresponding Data 8 textbook section. The corresponding tree diagram is copied below for your reference. The survey did not provide the true positive rate. The respondents and Stat 88 were allowed to assume that the true positive rate is 1, but we will not do so here. Let the true positive rate be some unknown proportion $p$. Suppose a person is picked at random from the population. Let $N$ be the event that the person doesn‚Äôt have the disease and let $T_N$ be the event that the person‚Äôs test result is negative. Fill in Blanks 1 and 2 with options chosen from (1)-(9). The proportion $P(N \\mid T_N)$ is the number of people who $\\underline{1}$ relative to the total number of people who $\\underline{2}$. (1) are in the population (2) have the disease (3) don‚Äôt have the disease (4) test positive (5) test negative (6) have the disease and test positive (7) have the disease and test negative (8) don‚Äôt have the disease and test positive (9) don‚Äôt have the disease and test negative Assign the variable q4bi to your answer to the first blank and q4bii to your answer to the second blank. q4bi = ... q4bii = ... q4bi, q4bii grader.check(\"q2b\") ","date":"2024-07-14","objectID":"/datahw01/:7:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2c (This is a continuation of the previous part.) Define a function no_disease_given_negative that takes $p$ as its argument and returns $P(N \\mid T_N)$. def no_disease_given_negative(p): ... grader.check(\"q4c\") ","date":"2024-07-14","objectID":"/datahw01/:7:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2d (This part is a continuation of the previous two.) Pick all of the options (i)-(iv) that are true for all values of $p$. Explain by algebraic or probailistic reasoning; you are welcome to use your function no_disease_given_negative to try a few cases numerically. Your explanation should include the reasons why you didn‚Äôt choose some options. $P(N \\mid T_N)$ is (i) equal to $0.95$. (ii) equal to $0.999 \\times 0.95$. (iii) greater than $0.999 \\times 0.95$. (iv) greater than $0.95$. Type your answer here, replacing this text. # Use this cell for experimenting if you wish, but your answer should be written in the cell above. ","date":"2024-07-14","objectID":"/datahw01/:7:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2e Suzuki is one of most commonly owned makes of cars in our county (Alameda). A car heading from Berkeley to San Francisco is pulled over on the freeway for speeding. Suppose I tell you that the car is either a Suzuki or a Lamborghini, and you have to guess which of the two is more likely. What would you guess, and why? Make some reasonable assumptions and explain them (data scientists often have to do this), justify your answer, and say how it‚Äôs connected to the previous parts. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:7:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3: Distributions Visualizing distributions, both categorical and numerical, helps us understand variability. In Data 8 you visualized numerical distributions by drawing histograms, which look like bar charts but represent proportions by the areas of the bars instead of the heights or lengths. In this exercise you will use the hist function in matplotlib instead of the corresponding Table method to draw histograms. To start off, suppose we want to plot the probability distribution of the number of spots on a single roll of a die. That should be a flat histogram since the chance of each of the values 1 through 6 is 1/6. Here is a first attempt at drawing the histogram. faces = range(1, 7) plt.hist(faces) (array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1.]), array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]), \u003cBarContainer object of 10 artists\u003e) This default plot is not helpful. We have to choose some arguments to get a visualization that we can interpret. Note that the second printed line shows the left ends of the default bins, as well as the right end of the last bin. The first line shows the counts in the bins. If you don‚Äôt want the printed lines you can add a semi-colon at the end of the call to plt.hist, but we‚Äôll keep the lines for now. Let‚Äôs redraw the histogram with bins of unit length centered at the possible values. By the end of the exercise you‚Äôll see a reason for centering. Notice that the argument for specifying bins is the same as the one for the Table method hist. unit_bins = np.arange(0.5, 6.6) plt.hist(faces, bins = unit_bins) (array([1., 1., 1., 1., 1., 1.]), array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]), \u003cBarContainer object of 6 artists\u003e) We need to see the edges of the bars! Let‚Äôs specify the edge color ec to be white. Here are all the colors you could use, but do try to drag yourself away from the poetic names. plt.hist(faces, bins = unit_bins, ec='white') (array([1., 1., 1., 1., 1., 1.]), array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]), \u003cBarContainer object of 6 artists\u003e) That‚Äôs much better, but look at the vertical axis. It is not drawn to the density scale defined in Data 8. We want a histogram of a probability distribution, so the total area should be 1. We just have to ask for that. plt.hist(faces, bins = unit_bins, ec='white', density=True) (array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667]), array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]), \u003cBarContainer object of 6 artists\u003e) That‚Äôs the probability histogram of the number of spots on one roll of a die. The proportion is $1/6$ in each of the bins. Note: You may notice that running the above cells also displayed the return value of the last function call of each cell. This was intentional on our part to show you how plt.hist() (documentation) returned different values per plot. Note 2: Going forward, you can use a semicolon ; on the last line to suppress additional display, as below. plt.hist(faces, bins = unit_bins, ec='white', density=True); ","date":"2024-07-14","objectID":"/datahw01/:8:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3a Define a function integer_distribution that takes an array of integers and draws the histogram of the distribution using unit bins centered at the integers and white edges for the bars. The histogram should be drawn to the density scale. The left-most bar should be centered at the smallest integer in the array, and the right-most bar at the largest. Your function does not have to check that the input is an array consisting only of integers. The display does not need to include the printed proportions and bins. If you have trouble defining the function, go back and carefully read all the lines of code that resulted in the probability histogram of the number of spots on one roll of a die. Pay special attention to the bins. def integer_distribution(x): bins = np.arange(min(x) - 0.5, max(x) + 1.5) plt.hist(x, bins=bins, ec='white',density=True) integer_distribution(faces) ","date":"2024-07-14","objectID":"/datahw01/:8:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3b (Note: You can complete this part with just prerequisite knowledge for Data 100. That being said, Lecture 2 provides additional historical context and definitions for probability sample, sampling bias, and chance error). One way to use probability samples is to quantify sampling bias and chance error. Put briefly, if we assume that a sample distribution was selected at random from a known population, then we can quantify how likely that sample is to have arisen due to random chance (chance error). If the difference in sample and population distributions is too great, then we suspect that the given sample has bias in how it was selected from the population. Let‚Äôs see this process in a post-analysis of pre-election polling of the 1936 U.S. Presidential Election. Through the U.S. electoral college process (we‚Äôll ignore it in this question, but read more here), Franklin D. Roosevelt won the election by an overwhelming margin. The popular vote results were approximately 61% Roosevelt (Democrat, incumbent), 37% Alf Landon (Republican), and 2% other candidates. For this problem, this is our population distribution. You can use np.random.multinomial to simulate drawing at random with replacement from a categorical distribution. The arguments are the sample size n and an array pvals of the proportions in all the categories. The function simulates n independent random draws from the distribution and returns the observed counts in all the categories. Read the documentation to see how this is described formally; we will use the formal terminology and notation in future assignments after we have discussed them in class. You will see that the function also takes a third argument size, which for our purposes will be an integer that specifies the number of times to run the entire simulation. All the runs are independent of each other. Write one line of code that uses np.random.multinomial to run 10 independent simulations of drawing 100 times at random with replacement from a population in which 61% of the people vote for Roosevelt, 37% for Landon, and 2% for other candidatdes. The output should be an array containing the counts in the Roosevelt category in the 10 simulations. It will help to recall how to slice NumPy arrays. Assign your answer to the variable sample. sample = np.random.multinomial(100, [0.61, 0.37, 0.02], size=10)[:, 0] sample array([54, 65, 58, 53, 66, 58, 61, 56, 60, 57], dtype=int32) grader.check(\"q3b\") q3b passed! üôå ","date":"2024-07-14","objectID":"/datahw01/:8:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3c Replace the ‚Äú‚Ä¶‚Äù in the code cell below with a Python expression so that the output of the cell is an empirical histogram of 500,000 simulated counts of voters for Roosevelt in 100 draws made at random with replacement from the voting population. After you have drawn the histogram, you might want to take a moment to recall the conclusion reached by the Literary Digest, a magazine that‚Äîwhile having successfully predicted the outcome of many previous presidential elections‚Äîfailed to correctly predict the winner of the 1936 presidential election. In their survey of 10 million individuals, they predicted the popular vote as just 43% for Roosevelt and 57% for Landon. Based on our simulation, there was most definitely sampling bias in the Digest‚Äôs sampling process. simulated_counts = np.random.multinomial(100, [0.61, 0.37, 0.02], size=500000)[:, 0] integer_distribution(simulated_counts) ","date":"2024-07-14","objectID":"/datahw01/:8:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3d As you know, the count of Roosevelt voters in a sample of 100 people drawn at random from the eligible population is expected to be 61. Just by looking at the histogram in Part c, and no other calculation, pick the correct option and explain your choice. You might want to refer to the Data 8 textbook again. The SD of the distribution of the number of Roosevelt voters in a random sample of 100 people drawn from the eligible population is closest to (i) 1.9 (ii) 4.9 (iii) 10.9 (iv) 15.9 Type your answer here. ÂÅáËÆæ‰∏äÈù¢ÂõæÁâáÊòØÂØπÁöÑÔºåÁî®3œÉÂéüÂàôÁ≤óÁï•‰º∞ËÆ°5Â∑¶Âè≥„ÄÇ ","date":"2024-07-14","objectID":"/datahw01/:8:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3e The normal curve with mean $\\mu$ and SD $\\sigma$ is defined by $$ f(x) ~ = ~ \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}}, ~~~ -\\infty \u003c x \u003c \\infty $$ Redraw your histogram from Part c and overlay the normal curve with $\\mu = 61$ and $\\sigma$ equal to the choice you made in Part d. You just have to call plt.plot after integer_distribution. Use np.e for $e$. For the curve, use 2 as the line width, and any color that is easy to see over the blue histogram. It‚Äôs fine to just let Python use its default color. Now you can see why centering the histogram bars over the integers was a good idea. The normal curve peaks at 26, which is the center of the corresponding bar. mu = 61 sigma = 4.9 x = np.linspace(40, 80, 200) f_x = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp(-(x - mu)**2 / (2 * sigma**2)) plt.plot(x, f_x) integer_distribution(simulated_counts) ","date":"2024-07-14","objectID":"/datahw01/:8:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4: Linear Algebra A common representation of data uses matrices and vectors, so it is helpful to familiarize ourselves with linear algebra notation, as well as some simple operations. Define a vector $\\vec{v}$ to be a column vector. Then, the following properties hold: $c\\vec{v}$ with $c$ some constant $c \\in \\mathbb{R}$, is equal to a new vector where every element in $c\\vec{v}$ is equal to the corresponding element in $\\vec{v}$ multiplied by $c$. For example, $2 \\begin{bmatrix} 1 \\ 2 \\ \\end{bmatrix} = \\begin{bmatrix} 2 \\ 4 \\ \\end{bmatrix}$ $\\vec{v}_1 + \\vec{v}_2$ is equal to a new vector with elements equal to the elementwise addition of $\\vec{v}_1$ and $\\vec{v}_2$. For example, $\\begin{bmatrix} 1 \\ 2 \\ \\end{bmatrix} + \\begin{bmatrix} -3 \\ 4 \\ \\end{bmatrix} = \\begin{bmatrix} -2 \\ 6 \\ \\end{bmatrix}$. The above properties form our definition for a linear combination of vectors. $\\vec{v}_3$ is a linear combination of $\\vec{v}_1$ and $\\vec{v}_2$ if $\\vec{v}_3 = a\\vec{v}_1 + b\\vec{v}_2$, where $a$ and $b$ are some constants. Oftentimes, we stack column vectors to form a matrix. Define the rank of a matrix $A$ to be equal to the maximal number of linearly independent columns in $A$. A set of columns is linearly independent if no column can be written as a linear combination of any other column(s) within the set. For example, let $A$ be a matrix with 4 columns. If three of these columns are linearly independent, but the fourth can be written as a linear combination of the other three, then $\\text{rank}(A) = 3$. For each part below, you will be presented with a set of vectors, and a matrix consisting of those vectors stacked in columns. State the rank of the matrix, and whether or not the matrix is full rank. If the matrix is not full rank, state a linear relationship among the vectors‚Äîfor example: $\\vec{v}_1 = 2\\vec{v}_2$. ","date":"2024-07-14","objectID":"/datahw01/:9:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4a $$ \\vec{v}_1 = \\begin{bmatrix} 1 \\ 0 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} 1 \\ 1 \\ \\end{bmatrix} , A = \\begin{bmatrix} \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \\ \\vert \u0026 \\vert \\end{bmatrix}$$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4b $$ \\vec{v}_1 = \\begin{bmatrix} 3 \\ -4 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} 0 \\ 0 \\ \\end{bmatrix} , B = \\begin{bmatrix} \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \\ \\vert \u0026 \\vert \\end{bmatrix} $$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4c $$ \\vec{v}_1 = \\begin{bmatrix} 0 \\ 1 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} 5 \\ 0 \\ \\end{bmatrix} , \\vec{v}_3 = \\begin{bmatrix} 10 \\ 10 \\ \\end{bmatrix} , C = \\begin{bmatrix} \\vert \u0026 \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \u0026 \\vec{v}_3 \\ \\vert \u0026 \\vert \u0026 \\vert \\end{bmatrix} $$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4d $$ \\vec{v}_1 = \\begin{bmatrix} 0 \\ 2 \\ 3 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} -2 \\ -2 \\ 5 \\ \\end{bmatrix} , \\vec{v}_3 = \\begin{bmatrix} 2 \\ 4 \\ -2 \\ \\end{bmatrix} , D = \\begin{bmatrix} \\vert \u0026 \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \u0026 \\vec{v}_3 \\ \\vert \u0026 \\vert \u0026 \\vert \\end{bmatrix} $$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5: A Least Squares Predictor Let the list of numbers $(x_1, x_2, \\ldots, x_n)$ be data. You can think of each index $i$ as the label of a household, and the entry $x_i$ as the annual income of Household $i$. Define the mean or average $\\mu$ of the list to be $$\\mu ~ = ~ \\frac{1}{n}\\sum_{i=1}^n x_i.$$ ","date":"2024-07-14","objectID":"/datahw01/:10:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5a The $i$ th deviation from average is the difference $x_i - \\mu$. In Data 8 you saw in numerical examples that the sum of all these deviations is 0. Now prove that fact. That is, show that $\\sum_{i=1}^n (x_i - \\mu) = 0$. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:10:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5b Recall that the variance of a list is defined as the mean squared deviation from average, and that the standard deviation (SD) of the list is the square root of the variance. The SD is in the same units as the data and measures the rough size of the deviations from average. Denote the variance of the list by $\\sigma^2$. Write a math expression for $\\sigma^2$ in terms of the data ($x_{1} \\dots x_{n}$) and $\\mu$. We recommend building your expression by reading the definition of variance from right to left. That is, start by writing the notation for ‚Äúaverage‚Äù, then ‚Äúdeviation from average‚Äù, and so on. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:10:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Mean Squared Error Suppose you have to predict the value of $x_i$ for some $i$, but you don‚Äôt get to see $i$ and you certainly don‚Äôt get to see $x_i$. You decide that whatever $x_i$ is, you‚Äôre just going to use some number $c$ as your predictor. The error in your prediction is $x_i - c$. Thus the mean squared error (MSE) of your predictor $c$ over the entire list of $n$ data points can be written as: $$MSE(c) = \\frac{1}{n}\\sum_{i=1}^n (x_i - c)^2.$$ You may already see some similarities to your definition of variance from above! You then start to wonder‚Äîif you picked your favorite number $c = \\mu$ as the predictor, would it be ‚Äúbetter‚Äù than other choices $c \\neq \\mu$? ","date":"2024-07-14","objectID":"/datahw01/:10:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5c One common approach to defining a ‚Äúbest‚Äù predictor is as predictor that minimizes the MSE on the data $(x_1, \\dots, x_n)$. In this course, we commonly use calculus to find the predictor $c$ as follows: Define $MSE$ to be a function of $c$, i.e., $MSE(c)$ as above. Assume that the data points $x_1, x_2, ‚Ä¶, x_n$ are fixed, and that $c$ is the only variable. Determine the value of $c$ that minimizes $MSE(c)$. Justify that this is indeed a minimum, not a maximum. Step 1 is done for you in the problem statement; follow steps 2 and 3 to show that $\\mu$ is the value of $c$ that minimizes $MSE(c)$. You must do both steps. Type your answer here, replacing this text. Your proof above shows that $\\mu$ is the least squares constant predictor. ","date":"2024-07-14","objectID":"/datahw01/:10:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6: A More Familiar Least Squares Predictor In Data 8 you found (numerically) the least squares linear predictor of a variable $y$ based on a related variable $x$. In this course, we will prove your findings using a generalization of your calculation in the previous question. When we get to this proof later in this course, you will need to be comfortable with vector operations. For now, you will get familiar with this notation by rewriting your least squares findings from Data 8 (and the previous question) using vector notation. This question won‚Äôt require you to write LaTeX, so just focus on the mathematical notation we‚Äôre presenting. ","date":"2024-07-14","objectID":"/datahw01/:11:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"The Dot Product (1) We start by defining the dot product of two real vectors $x = \\begin{bmatrix} x_1 \\ x_2 \\ \\dots \\ x_n \\end{bmatrix}$ and $y = \\begin{bmatrix} y_1 \\ y_2 \\ \\dots \\ y_n \\end{bmatrix}$ as follows: $$x^T y = \\sum_{i=1}^n x_i y_i $$ Given the above definition, the dot product is (1) a scalar, not another vector; and (2) only defined for two vectors of the same length. Note: In this course we often opt for $x$ instead of $\\vec{x}$ to simplify notation; $x$ as a vector is inferred from its use in the dot product. Then $x_i$ is the $i$-th element of the vector $x$. Detail: In this course, we prefer the notation $x^Ty$ to illustrate a dot product, defined as matrix multiplication of $x^T$ and $y$. In the literature you may also see $x \\cdot y$, but we avoid this notation since the dot ($\\cdot$) notation is occasionally used for scalar values. Detail: The dot product is a special case of an inner product, where $x, y \\in \\mathbb{R}^n$. (2) We introduce a special vector, $\\mathbb{1}$, to write the mean $\\bar{x}$ of data $(x_1, x_2, \\dots, x_n)$ as a dot product: \\begin{align} \\bar{x} \u0026= \\frac{1}{n}\\sum_{i=1}^n x_i = \\frac{1}{n}\\sum_{i=1}^n 1x_i \\ \u0026= \\frac{1}{n}(x^T\\mathbb{1}). \\end{align} The data $(x_1, \\dots, x_n)$ have been defined as an $n$-dimensional column vector $x$, where $x = \\begin{bmatrix} x_1 \\ x_2 \\ \\dots \\ x_n \\end{bmatrix}$. The special vector $\\mathbb{1}$ is a vector of ones, whose length is defined by the vector operation in which it is used. So with $n$-dimensional column vector $x$, the dot product $x^T\\mathbb{1}$ implies that $\\mathbb{1}$ is an $n$-dimensional column vector where every element is $1$. Because dot products produce scalars, the multiplication of two scalars $\\frac{1}{n}$ and $x^T\\mathbb{1}$ produces another scalar, $\\bar{x}$. Note: We use bar notation for the mean ($\\bar{x}$ instead of $\\mu$) in this problem to differentiate $\\bar{x}$ from $\\bar{y}$, the latter of which is the mean of data $(y_1, \\dots, y_n)$. (3) We can further use this definition of $\\bar{x}$ to additionally write the variance $\\sigma_x^2$ of the data $(x_1, \\dots, x_n)$ as a dot product. Verify for yourself that the below operation defines $\\sigma_x^2$ as a scalar: \\begin{align} \\sigma_x^2 \u0026= \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 \\ \u0026= \\frac{1}{n}(x - \\bar{x})^T(x - \\bar{x}). \\end{align} ","date":"2024-07-14","objectID":"/datahw01/:11:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6a To verify your understanding of the dot product as defined above, suppose you are working with $n$ datapoints ${(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)}$. Define the $x$ data as $(x_1, \\dots, x_n)$ and the $y$ data as $(y_1, \\dots, y_n)$, and define $x$ and $y$ as two $n$-dimensional column vectors, where the $i$-th elements of $x$ and $y$ are $x_i$ and $y_i$, respectively. Define $\\bar{x}$ and $\\bar{y}$ as the means of the $x$ data and $y$ data, respectively. Define $\\sigma_x^2$ and $\\sigma_y^2$ as the variances of the $x$ data and $y$ data, respectively. Therefore $\\sigma_x = \\sqrt{\\sigma_x^2}$ and $\\sigma_y = \\sqrt{\\sigma_y^2}$ are the standard deviations of the $x$ data and $y$ data, respectively. Suppose $n = 32$. What is the dimension of each of the following expressions? Expression (i). Note there are two ways it is written in the literature. $$\\dfrac{1}{\\sigma_x} (x - \\bar{x}) = \\dfrac{x - \\bar{x}}{\\sigma_x} $$ Expression (ii). $$\\dfrac{1}{n} \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)^T \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)$$ Assign the variables q6a_i and q6a_ii to an integer representing the dimension of the above expressions (i) and (ii), respectively. q6a_i = ... q6a_ii = ... # do not modify these lines print(f\"Q6a(i) is {q6a_i}-dimensional\") print(f\"Q6a(ii) is {q6a_ii}-dimensional\") grader.check(\"q6a\") ","date":"2024-07-14","objectID":"/datahw01/:11:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Dot Products in NumPy Next, we‚Äôll use NumPy‚Äôs matrix multiplication operators to compute expressions for the regression line, which you learned in Data 8 was the unique line that minimizes the mean squared error of estimation among all straight lines. At this time, it may be helpful to review the Data 8 section. Before we continue, let‚Äôs contextualize our computation by loading in a dataset you saw in Data 8: the relation between weight lifted and shot put distance among surveyed female collegiate athletes. We‚Äôve plotted the point using matplotlib‚Äôs scatter function, which you will see in more detail in two weeks. # Run this cell to plot the data. weight_lifted = np.array([ 37.5, 51.5, 61.3, 61.3, 63.6, 66.1, 70. , 92.7, 90.5, 90.5, 94.8, 97. , 97. , 97. , 102. , 102. , 103.6, 100.4, 108.4, 114. , 115.3, 114.9, 114.7, 123.6, 125.8, 119.1, 118.9, 141.1]) shot_put_distance = np.array([ 6.4, 10.2, 12.4, 13. , 13.2, 13. , 12.7, 13.9, 15.5, 15.8, 15.8, 16.8, 17.1, 17.8, 14.8, 15.5, 16.1, 16.2, 17.9, 15.9, 15.8, 16.7, 17.6, 16.8, 17. , 18.2, 19.2, 18.6]) plt.scatter(weight_lifted, shot_put_distance) plt.xlabel(\"Weight Lifted\") plt.ylabel(\"Shot Put Distance\") Looks pretty linear! Let‚Äôs try to fit a regression line to this data. Define the vectors $x$ as the weight lifted data vector and $y$ as the shot put distance data vector, respectively, of the college athletes. Then the regression line uses the weight lifted $x$ to predict $\\hat{y}$, which is the linear estimate of the actual value shot put distance $y$ as follows: \\begin{align} \\hat{y} \u0026= \\hat{a} + \\hat{b}{x}\\text{, where} \\ \\hat{a} \u0026= \\bar{y} - \\hat{b}\\bar{x} \\ \\hat{b} \u0026= r \\dfrac{\\sigma_y}{\\sigma_x} \\end{align} $\\bar{x}, \\bar{y}$ and $\\sigma_x, \\sigma_y$ are the means and standard deviations, respectively of the data $x$ and $y$, respectively. Here, $r$ is the correlation coefficient as defined in Data 8! Note: We use the hat $\\hat{}$ notation to indicate values we are estimating: $\\hat{y}$, the predicted shot put distance, as well as $\\hat{a}$ and $\\hat{b}$, the respective estimated intercept and slope parameters we are using to model the ‚Äúbest‚Äù linear predictor of $y$ from $x$. We‚Äôll dive into this later in the course. Note: Remember how we dropped the $\\vec{}$ vector notation? These linear regression equations therefore represent both the scalar case (predict a single value $\\hat{y}$ from a single $x$) and the vector case (predict a vector $\\hat{y}$ element-wise from a vector $x$). How convenient!! In this part, instead of using NumPy‚Äôs built-in statistical functions like np.mean() and np.std(), you are going to use NumPy‚Äôs matrix operations to create the components of the regression line from first principles. The @ operator multiplies NumPy matrices or arrays together (documentation). We can use this operator to write functions to compute statistics on data, using the expressions that we defined in part (a). Check it out: # Just run this cell. def dot_mean(arr): n = len(arr) all_ones = np.ones(n) # creates n-dimensional vector of ones return (arr.T @ all_ones)/n def dot_var(arr): n = len(arr) mean = dot_mean(arr) zero_mean_arr = arr - mean return (zero_mean_arr.T @ zero_mean_arr)/n def dot_std(arr): return np.sqrt(dot_var(arr)) print(\"np.mean(weight_lifted) =\", np.mean(weight_lifted), \"\\tdot_mean(weight_lifted) =\", dot_mean(weight_lifted)) print(\"np.var(weight_lifted) =\", np.std(weight_lifted), \"\\tdot_var(weight_lifted =\", dot_var(weight_lifted)) print(\"np.std(weight_lifted) =\", np.std(weight_lifted), \"\\tdot_std(weight_lifted =\", dot_std(weight_lifted)) Now, you will write code to define the expressions you explored in part (a) of this question. ","date":"2024-07-14","objectID":"/datahw01/:11:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6b (i) Use the NumPy @ operator to compute expression (i) from part (a). For convenience, we‚Äôve rewritten the expression below. Note that this expression is also referred to as $x$ in standard units (Data 8 textbook section). $$\\dfrac{x - \\bar{x}}{\\sigma_x} $$ Write the body of the function dot_su which takes in a 1-D NumPy array arr and returns arr in standard units. Do not use np.mean(), np.std(), np.var(), np.sum() nor any Python loops. You should only use a subset of @, /, +, -, len(), the dot_mean(), dot_var(), and dot_std() functions defined above. def dot_su(arr): ... # do not edit below this line q6bi_su = dot_su(weight_lifted) q6bi_su grader.check(\"q6bi\") ","date":"2024-07-14","objectID":"/datahw01/:11:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6b (ii) Next use the NumPy @ operator to compute the correlation coefficient $r$, which is expression (ii) from part (a). For convenience, we‚Äôve rewritten the expression below. $$r = \\dfrac{1}{n} \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)^T \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)$$ Write the body of the function dot_corr_coeff which takes in two 1-D NumPy arrays x and y and returns the correlation coefficient of x and y. As before, Do not use np.mean(), np.std(), np.var(), np.sum() nor any Python loops. As before, you should only use a subset of @, /, +, -, len(), the dot_mean(), dot_var(), and dot_std() functions defined above. You may also use the dot_su() function that you defined in the previous part. def dot_corr_coeff(x, y): ... # do not edit below this line q6bii_r = dot_corr_coeff(weight_lifted, shot_put_distance) q6bii_r grader.check(\"q6bii\") ","date":"2024-07-14","objectID":"/datahw01/:11:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6c We‚Äôre ready to put everything together! Finally, use the dot_-prefixed functions in this question to compute the regression line. For convenience, we‚Äôve rewritten the expressions below. $\\hat{y}$ is the linear estimate of the value $y$ based on $x$. \\begin{align} \\hat{y} \u0026= \\hat{a} + \\hat{b}{x}\\text{, where} \\ \\hat{a} \u0026= \\bar{y} - \\hat{b}\\bar{x} \\ \\hat{b} \u0026= r \\dfrac{\\sigma_y}{\\sigma_x} \\end{align} Define the functions compute_a_hat and compute_b_hat which return the intercept and slope, respectively, of the regression line defind above for a linear estimator of y using x. Verify how the functions are used to plot the linear regression line (implemented for you). As before, Do not use np.mean(), np.std(), np.var(), np.sum(), or any for loops. You may use a subset of @, /, +, -, len(), dot_mean(), dot_var(), dot_std(), dot_su(), dot_corr_coeff(). Hint: You may want to define a_hat in terms of b_hat. def compute_a_hat(x, y): ... def compute_b_hat(x, y): ... # do not edit below this line a_hat = compute_a_hat(weight_lifted, shot_put_distance) b_hat = compute_b_hat(weight_lifted, shot_put_distance) shot_put_hats = a_hat + b_hat * weight_lifted plt.scatter(weight_lifted, shot_put_distance) # the actual data plt.plot(weight_lifted, shot_put_hats, color='g', alpha=0.5) # the prediction line, transparent green plt.xlabel(\"Weight Lifted\") plt.ylabel(\"Shot Put Distance\") display(compute_a_hat(weight_lifted, shot_put_distance)) display(compute_b_hat(weight_lifted, shot_put_distance)) grader.check(\"q6c\") To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-07-14","objectID":"/datahw01/:11:6","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export() ","date":"2024-07-14","objectID":"/datahw01/:12:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"DataFrames: a data structure for tabular data ","date":"2024-07-13","objectID":"/datal3/:1:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"API always remember to turn to GPT/google/doc ","date":"2024-07-13","objectID":"/datal3/:1:1","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"indexing and loc/iloc generate subsets: loc: an operator select items by labels df.loc[row_indexer, column_indexer] row_indexer: can be a single label, a list of labels, sliceÔºàÈó≠Âå∫Èó¥Ôºâ, single value column_indexer: same as row_indexer returns a DataFrame or Series iloc: an operator select items by positions df.iloc[row_indexer, column_indexer] row_indexer: numeric index or a list of numeric indicesÔºåÊ≠§Êó∂ÂõûÂà∞pythonÁªèÂÖ∏Á¥¢Âºï Â∑¶Èó≠Âè≥ÂºÄ column_indexer: same as row_indexer returns a DataFrame or Series ÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨‰ΩøÁî® loc ËøõË°åÁ¥¢Âºï .head(6) and .tail() to get the first or last few rows of a DataFrame(syntactic sugar) ","date":"2024-07-13","objectID":"/datal3/:2:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"[ ]: context sensitive operator ","date":"2024-07-13","objectID":"/datal3/:3:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"series: a data structure for 1D labeled data ","date":"2024-07-13","objectID":"/datal3/:4:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"index: a array-like object that labels the rows and columns of a DataFrame columns: usually do not have same name. ËΩ¨Êç¢Ôºö ","date":"2024-07-13","objectID":"/datal3/:5:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"conditional selection ","date":"2024-07-13","objectID":"/datal3/:6:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"Á±ªÂûãÊÑèËØÜ! ‰∏âÁßçÊï∞ÊçÆÁ±ªÂûã‰∏≠ÁöÑÂì™‰∏Ä‰∏™ÔºüÔºüÔºü ","date":"2024-07-13","objectID":"/datal3/:7:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"describe() ","date":"2024-07-13","objectID":"/datal3/:7:1","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"sample() df.sample(n=5, replace=True) # randomly select 5 rows ","date":"2024-07-13","objectID":"/datal3/:7:2","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"value_counts() df['column_name'].value_counts() # count the frequency of each value in a column return a Series with the count of each value in the column. ","date":"2024-07-13","objectID":"/datal3/:7:3","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"unique() df['column_name'].unique() # get all unique values in a column return a numpy array with the unique values in the column. ","date":"2024-07-13","objectID":"/datal3/:7:4","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"sort_values() df.sort_values(by='column_name', ascending=False) # sort the DataFrame by values in a column return a new DataFrame with the rows sorted by values in a column. ","date":"2024-07-13","objectID":"/datal3/:7:5","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["DATA100"],"content":"reference https://www.textbook.ds100.org/ch/a04/ref_pandas.html https://pandas.pydata.org/pandas-docs/stable/reference/index.html ","date":"2024-07-13","objectID":"/datal3/:8:0","tags":["pandas"],"title":"DATA100-L3: Pandas ‚Ö†","uri":"/datal3/"},{"categories":["UCB-CS61B"],"content":"definitions A sort is a permutation (re-arrangement) of a sequence of elements that brings them into order according to some total order. A total order ‚âº is: Total: x ‚âº y or y ‚âº x for all x, y Reflexive: x ‚âº x Antisymmetric: x ‚âº y and y ‚âº x iff x = y (x and y are equivalent). Transitive: x ‚âº y and y ‚âº z implies x ‚âº z. In Java, total order is typically specified by compareTo or compare methods. May be inconsistent with equals! For example sorting an array of Strings by length has items that are equivalent, but not equal, e.g. ‚Äúcat‚Äù and ‚Äúdog‚Äù. Goal of sorting: Given a sequence of elements with Z inversions. Perform a sequence of operations that reduces inversions to 0. ","date":"2024-07-13","objectID":"/61b-32/:1:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"Performance definition Characterizations of the runtime efficiency are sometimes called the time complexity of an algorithm. Examples: DFS has time complexity Œò(V+E). Characterizations of the ‚Äúextra‚Äù memory usage of an algorithm is sometimes called the space complexity of an algorithm. DFS has space complexity Œò(V). Note that the graph takes up space Œò(V+E), but we don‚Äôt count this as part of the runtime of DFS, since we‚Äôre only accounting for the extra space that DFS uses. Selection Sort and Heapsort selection Sort Properties: Áõ¥Êé•ÈÄâÊã©Ê≥ï in Chinese $Œò(N^2)$ time if we use an array (or similar data structure). ","date":"2024-07-13","objectID":"/61b-32/:2:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"Naive Heapsort: Leveraging a Max-Oriented Heap ÂàÜÊûê ","date":"2024-07-13","objectID":"/61b-32/:3:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"in-place Heapsort demo ","date":"2024-07-13","objectID":"/61b-32/:4:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"performance analysis Merge Sort demo Time complexity, analysis from previous lecture: Œò(N log N runtime) Space complexity with aux array: Costs Œò(N) memory. Also possible to do in-place merge sort, but algorithm is very complicated, and runtime performance suffers by a significant constant factor. Insertion Sort General strategy: Starting with an empty output sequence. Add each item from input, inserting into output at right point. Naive approach, build entirely new output: Demo ","date":"2024-07-13","objectID":"/61b-32/:4:1","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"in-place Insertion Sort Demo ÊúâÁÇπÂÉèÂÜíÊ≥°Ôºü Áúã‰∏äÂéªÊ≤°‰ªÄ‰πàÂ•ΩÁöÑË°®Áé∞Ôºü Shell‚Äôs Sort (Extra) (Not on Exam) so far summary ","date":"2024-07-13","objectID":"/61b-32/:5:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"Backstory, Partitioning Quick Sort Partition Sort, a.k.a. Quicksort Quicksort Runtime Theoretical analysis: Best case: Œò(N log N) Worst case: Œò(N2) Compare this to Mergesort. Best case: Œò(N log N) Worst case: Œò(N log N) Recall that Œò(N log N) vs. Œò(N2) is a really big deal. So how can Quicksort be the fastest sort empirically? Because on average it is Œò(N log N). Rigorous proof requires probability theory + calculus, but intuition + empirical analysis will hopefully convince you. Argument #2: Quicksort is BST Sort ü§î ","date":"2024-07-13","objectID":"/61b-33/:0:0","tags":null,"title":"61B-33: Quick Sort","uri":"/61b-33/"},{"categories":["UCB-CS61B"],"content":"so far summary Avoiding the Quicksort Worst Case ","date":"2024-07-13","objectID":"/61b-33/:0:1","tags":null,"title":"61B-33: Quick Sort","uri":"/61b-33/"},{"categories":["UCB-CS61B"],"content":"summary so far ","date":"2024-07-13","objectID":"/61b-33/:1:0","tags":null,"title":"61B-33: Quick Sort","uri":"/61b-33/"},{"categories":["UCB-CS61B"],"content":"Shortest Paths in a DAG (Directed Acyclic Graphs) Dynamic Programming ","date":"2024-07-13","objectID":"/61b-31/:0:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"the DAG SPT algorithm The DAG SPT algorithm can be thought of as solving increasingly large subproblems: Distance from source to source is very easy, and is just zero. We then tackle distances to vertices that are a bit farther to the right. We repeat this until we get all the way to the end of the graph. Problems grow larger and larger. By ‚Äúlarge‚Äù we informally mean depending on more and more of the earlier subproblems. This approach of solving increasingly large subproblems is sometimes called dynamic programming. a simple and powerful idea for solving ‚Äúbig problems‚Äù: Identify a collection of subproblems. Solve subproblems one by one, working from smallest to largest. Use the answers to the smaller problems to help solve the larger ones. Identification of the ‚Äúright‚Äù subproblems is often quite tricky. Largely beyond scope of CS61B. You‚Äôll study this in much more detail in CS170. Longest Increasing Subsequence The Longest Increasing Subsequence (LIS) problem is a classic dynamic programming problem. ","date":"2024-07-13","objectID":"/61b-31/:1:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"LLIS Related problem: Find the length of the longest increasing subsequence (LLIS). Âä®Áî®Ë¥üÊï∞Â§ßÂèòÂ∞è LIS Using Dynamic Programming ËøòÊòØÂØπLLISËøõË°åËÄÉÈáè Can think of the Q values as memoized answers to shorter subproblems. Q(K) is the length of the longest subsequence ending at K. Thus, length of the longest subsequence is just the maximum of all Q. no DAG version ","date":"2024-07-13","objectID":"/61b-31/:2:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"summary of LLIS solutions ","date":"2024-07-13","objectID":"/61b-31/:3:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"#1 ","date":"2024-07-13","objectID":"/61b-31/:3:1","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"#2a ","date":"2024-07-13","objectID":"/61b-31/:3:2","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"#2b Runtime for Approach 1 (Extra) https://docs.google.com/presentation/d/1RlGUoB0bvKlHkxZXogmWGEzsI9ZXRWlg3ngf1FJkpac/edit#slide=id.g1298bbb99e_0_1920 ÂèñÂÜ≥‰∫éÊÄé‰πàÂª∫Á´ãDAG ","date":"2024-07-13","objectID":"/61b-31/:3:3","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"pros and cons ","date":"2024-07-13","objectID":"/61b-29/:0:1","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"Graph Problem so far Punchline: DFS and BFS both traverse entire graphs, just in a different order (like preorder, inorder, postorder, and level order for trees). Solving graph problems is often a question of identifying the right traversal. Many traversals may work. Example: DFS for topological sort. BFS for shortest paths. Example: DFS or BFS about equally good for checking existence of path. Dijkstra‚Äôs Algorithm problem restatement: Find the shortest paths from source vertex s to some target vertex t. another problem restatement: Find the shortest path from source vertex s to all other vertices. Ê∑±Â±ÇËß£Èáä ","date":"2024-07-13","objectID":"/61b-29/:1:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"SPTÔºàShortest Path TreeÔºâ Edge Count If G is a connected edge-weighted graph with V vertices and E edges, how many edges are in the Shortest Paths Tree of G? [assume every vertex is reachable] Always V-1: For each vertex, there is exactly one input edge (except source). Insert all vertices into fringe PQ, storing vertices in order of distance from source. Repeat: Remove (closest) vertex v from PQ, and relax all edges pointing from v. Dijkstra‚Äôs Algorithm Demo Link. Ë¥™ÂøÉÊÄùÊÉ≥ËØÅÊòé ","date":"2024-07-13","objectID":"/61b-29/:2:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"‰º™‰ª£Á†Å http://algs4.cs.princeton.edu/44sp/DijkstraSP.java.html ","date":"2024-07-13","objectID":"/61b-29/:3:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"runtime analysis ","date":"2024-07-13","objectID":"/61b-29/:4:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"so far graph problems ","date":"2024-07-13","objectID":"/61b-29/:5:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"The Problem with Dijkstra‚Äôs A* (CS188 Preview) We have only a single target in mind, so we need a different algorithm. How can we do better? A* Demo Link How do we get our estimate? Estimate is an arbitrary heuristic h(v). heuristic: ‚Äúusing experience to learn and improve‚Äù Doesn‚Äôt have to be perfect! /** h(v) DOES NOT CHANGE as algorithm runs. */ public method h(v) { return computeLineDistance(v.latLong, NYC.latLong); } ÂèØËßÜÂåñ‰∏§ËÄÖÂØπÊØî http://qiao.github.io/PathFinding.js/visual/ ","date":"2024-07-13","objectID":"/61b-29/:5:1","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"Summary ","date":"2024-07-13","objectID":"/61b-29/:6:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"graph problems so so far A* Tree Search vs. A* Graph Search Admissibility vs. Consistency (Extra: See CS188 for more) see Iterative DFS (Extra) see ","date":"2024-07-13","objectID":"/61b-29/:7:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"warm up MST, Cut Property, Generic MST Algorithm ","date":"2024-07-13","objectID":"/61b-30/:0:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"MST vs SPT A shortest paths tree depends on the start vertex: Because it tells you how to get from a source to EVERYTHING. There is no source for a MST. Nonetheless, the MST sometimes happens to be an SPT for a specific vertex. ‰∏§ËÄÖÂÖ≥Á≥ª‰∏çÂ§ßÔºü ","date":"2024-07-13","objectID":"/61b-30/:1:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Cut Property ÁÆÄÂçïËØÅÊòé cross bridge ‰∏ÄÂÆöÂú® MST ‰∏≠„ÄÇ ","date":"2024-07-13","objectID":"/61b-30/:2:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Generic MST Algorithm Start with no edges in the MST. Find a cut that has no crossing edges in the MST. Add smallest crossing edge to the MST. Repeat until V-1 edges. This should work, but we need some way of finding a cut with no crossing edges! Random isn‚Äôt a very good idea. Prim‚Äôs Algorithm https://docs.google.com/presentation/d/1NFLbVeCuhhaZAM1z3s9zIYGGnhT4M4PWwAc-TLmCJjc/edit#slide=id.g9a60b2f52_0_0 https://docs.google.com/presentation/d/1GPizbySYMsUhnXSXKvbqV4UhPCvrt750MiqPPgU-eCY/edit#slide=id.g9a60b2f52_0_0 ","date":"2024-07-13","objectID":"/61b-30/:3:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Prim‚Äôs vs. Dijkstra‚Äôs Prim‚Äôs and Dijkstra‚Äôs algorithms are exactly the same, except Dijkstra‚Äôs considers ‚Äúdistance from the source‚Äù, and Prim‚Äôs considers ‚Äúdistance from the tree.‚Äù Visit order: Dijkstra‚Äôs algorithm visits vertices in order of distance from the source. Prim‚Äôs algorithm visits vertices in order of distance from the MST under construction. Relaxation: Relaxation in Dijkstra‚Äôs considers an edge better based on distance to source. Relaxation in Prim‚Äôs considers an edge better based on distance to tree. ","date":"2024-07-13","objectID":"/61b-30/:4:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"pseudocode ","date":"2024-07-13","objectID":"/61b-30/:5:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"runtime Kruskal‚Äôs Algorithm conceptual real ","date":"2024-07-13","objectID":"/61b-30/:6:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Kruskal‚Äôs Algorithm Pseudocode ","date":"2024-07-13","objectID":"/61b-30/:7:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"runtime ","date":"2024-07-13","objectID":"/61b-30/:8:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Summary https://docs.google.com/presentation/d/1I8GSEL0CgT09_JjSUF7MfoRMJkyzPjo8lKRd8XdOaRA/edit#slide=id.g772f8a8e2_0_117 SOTA of compare-based MST algorithms üÜô ","date":"2024-07-13","objectID":"/61b-30/:9:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Depth First Paths ‚ÄúDepth First Search‚Äù is a more general term for any graph search algorithm that traverses a graph as deep as possible before backtracking. The term is used for several slightly different algorithms. For example: DFS may refer to the version from the previous lecture that doesn‚Äôt use any marks (and thus can get caught in a loop). DFS may refer to the version where vertices are marked. DFS may refer to a version where vertices are marked and source edges recorded (as in Depth First Paths). DFS may refer to other algorithms like the ‚Äútopological sort algorithm‚Äù well discuss later in lecture. And more! space ÂàÜÊûêÂèØËÉΩÊòØË¶ÅËÄÉËôëruntimeÁöÑÂÜÖÂ≠òÂç†Áî®ÔºåÂõ†‰∏∫DFSÁÆóÊ≥ïÈúÄË¶ÅÈÄíÂΩíË∞ÉÁî®ÔºåÊØè‰∏ÄÊ¨°ÈÄíÂΩíË∞ÉÁî®ÈÉΩ‰ºöÊ∂àËÄó‰∏ÄÂÆöÁöÑÂÜÖÂ≠ò„ÄÇ Graph Traversals Â∫îËØ•ËÆ≤ÁöÑÊòØBFS Topological Sorting DFS, then reverse the order of the vertices. public class DepthFirstOrder { private boolean[] marked; private Stack\u003cInteger\u003e reversePostorder; public DepthFirstOrder(Digraph G) { reversePostorder = new Stack\u003cInteger\u003e(); marked = new boolean[G.V()]; for (int v = 0; v \u003c G.V(); v++) { if (!marked[v]) { dfs(G, v); } /** * Perform DFS of all unmarked vertices. * Note: Our algorithm earlier started at vertices with indegree zero. It turns out this * algorithm works no matter where you start! * */ } private void dfs(Digraph G, int v) { marked[v] = true; for (int w : G.adj(v)) { if (!marked[w]) { dfs(G, w); } } reversePostorder.push(v); // After each DFS is done, ‚Äòvisit‚Äô vertex by putting on a stack. } public Iterable\u003cInteger\u003e reversePostorder() { return reversePostorder; } } ","date":"2024-07-13","objectID":"/61b-28/:0:0","tags":null,"title":"61B-28:  Graph Traversals","uri":"/61b-28/"},{"categories":["UCB-CS61B"],"content":"now graph problem summary Êú¨Ë¥®‰∏äËøòÊòØ DFS Breadth First Search ‚Äúfringe‚ÄùÔºàËæπÁºòÔºâÈÄöÂ∏∏ÊåáÁöÑÊòØ‰∏Ä‰∏™Êï∞ÊçÆÁªìÊûÑÔºåÁî®‰∫éÂ≠òÂÇ®Á≠âÂæÖÂ§ÑÁêÜÁöÑËäÇÁÇπ„ÄÇsee public class BreadthFirstPaths { private boolean[] marked; private int[] edgeTo; ... private void bfs(Graph G, int s) { Queue\u003cInteger\u003e fringe = new Queue\u003cInteger\u003e(); fringe.enqueue(s); // set up starting vertex marked[s] = true; while (!fringe.isEmpty()) { int v = fringe.dequeue(); /** * for freshly dequeued vertex v, for each neighbor that is unmarked: * Enqueue that neighbor to the fringe. * Mark it. * Set its edgeTo to v. */ for (int w : G.adj(v)) { if (!marked[w]) { fringe.enqueue(w); marked[w] = true; edgeTo[w] = v; } } } } ","date":"2024-07-13","objectID":"/61b-28/:1:0","tags":null,"title":"61B-28:  Graph Traversals","uri":"/61b-28/"},{"categories":["UCB-CS61B"],"content":"For BSTs, the most inefficient way to add is in to put it in order. Âú®‰∫åÂèâÊêúÁ¥¢Ê†ëÔºàBSTÔºâ‰∏≠ÔºåÊúÄ‰ΩéÊïàÁöÑÊñπÂºèÊòØÊåâÂçáÂ∫èÊàñÈôçÂ∫èÊèíÂÖ•ËäÇÁÇπÔºåÂõ†‰∏∫ËøôÁßçÊñπÂºè‰ºöÂØºËá¥Ê†ëÂèòÊàê‰∏ÄÊù°ÈìæË°®Ôºå‰ªéËÄå‰ΩøÂÖ∂ÊÄßËÉΩÈÄÄÂåñÂà∞O(n)ÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇ ËøôÈáåÊòØ‰∏Ä‰∏™Áî®JavaÂÆûÁé∞ÁöÑ‰æãÂ≠êÔºåÂ±ïÁ§∫‰∫ÜÊåâÂçáÂ∫èÊèíÂÖ•ËäÇÁÇπÁöÑ‰∫åÂèâÊêúÁ¥¢Ê†ëÔºàBSTÔºâ‰ºöÂèòÊàêÈìæË°®ÁöÑÊÉÖÂÜµÔºö // ÂÆö‰πâÊ†ëÁöÑËäÇÁÇπ class TreeNode { int value; TreeNode left; TreeNode right; TreeNode(int value) { this.value = value; this.left = null; this.right = null; } } // ÂÆö‰πâ‰∫åÂèâÊêúÁ¥¢Ê†ë class BinarySearchTree { private TreeNode root; // ÊèíÂÖ•ËäÇÁÇπ public void insert(int value) { root = insertRec(root, value); } private TreeNode insertRec(TreeNode root, int value) { if (root == null) { root = new TreeNode(value); return root; } if (value \u003c root.value) { root.left = insertRec(root.left, value); } else { root.right = insertRec(root.right, value); } return root; } // ÊâìÂç∞Ê†ëÁöÑ‰∏≠Â∫èÈÅçÂéÜ public void inorderTraversal() { inorderRec(root); System.out.println(); } private void inorderRec(TreeNode root) { if (root != null) { inorderRec(root.left); System.out.print(root.value + \" \"); inorderRec(root.right); } } // ÊâìÂç∞Ê†ëÁöÑÁªìÊûÑÔºàÁî®‰∫éË∞ÉËØïÔºâ public void printTree() { printTreeRec(root, \"\", true); } private void printTreeRec(TreeNode root, String indent, boolean last) { if (root != null) { System.out.print(indent); if (last) { System.out.print(\"R----\"); indent += \" \"; } else { System.out.print(\"L----\"); indent += \"| \"; } System.out.println(root.value); printTreeRec(root.left, indent, false); printTreeRec(root.right, indent, true); } } } // ÊµãËØïÁ±ª public class Main { public static void main(String[] args) { BinarySearchTree bst = new BinarySearchTree(); // ÊåâÂçáÂ∫èÊèíÂÖ•ËäÇÁÇπÔºà1, 2, 3, 4, 5Ôºâ bst.insert(1); bst.insert(2); bst.insert(3); bst.insert(4); bst.insert(5); System.out.println(\"Inorder Traversal of BST:\"); bst.inorderTraversal(); System.out.println(\"Tree Structure:\"); bst.printTree(); } } ","date":"2024-07-13","objectID":"/61b-26/:0:0","tags":null,"title":"61B-26: Midterm 2 Review","uri":"/61b-26/"},{"categories":["UCB-CS61B"],"content":"‰ª£Á†ÅËß£Êûê TreeNodeÁ±ªÔºöÂÆö‰πâ‰∫ÜÊ†ëÁöÑËäÇÁÇπÔºåÂåÖÊã¨ËäÇÁÇπÁöÑÂÄº‰ª•ÂèäÂ∑¶Â≠êËäÇÁÇπÂíåÂè≥Â≠êËäÇÁÇπ„ÄÇ BinarySearchTreeÁ±ªÔºöÂÆö‰πâ‰∫Ü‰∫åÂèâÊêúÁ¥¢Ê†ëÔºåÂåÖÊã¨ÊèíÂÖ•ËäÇÁÇπÁöÑÈÄªËæëÂíå‰∏≠Â∫èÈÅçÂéÜÊâìÂç∞Ê†ëÁöÑÁªìÊûÑ„ÄÇ insertÊñπÊ≥ïÔºöÂ∞ÜËäÇÁÇπÊåâÂçáÂ∫èÊèíÂÖ•„ÄÇÁî±‰∫éÊØè‰∏™Êñ∞ÊèíÂÖ•ÁöÑËäÇÁÇπÈÉΩÊòØÊØîÂΩìÂâçËäÇÁÇπÂ§ßÁöÑÔºåÂõ†Ê≠§ÊâÄÊúâÊñ∞ËäÇÁÇπÈÉΩ‰ºöË¢´ÊèíÂÖ•Âà∞Âè≥Â≠êÊ†ë‰∏äÔºåÂØºËá¥Ê†ëÁªìÊûÑÂÉèÈìæË°®„ÄÇ printTreeÊñπÊ≥ïÔºöÁî®‰∫é‰ª•ÁªìÊûÑÂåñÊñπÂºèÊâìÂç∞Ê†ëÔºåÁî®‰∫éË∞ÉËØïÊ†ëÁöÑÁªìÊûÑ„ÄÇ ","date":"2024-07-13","objectID":"/61b-26/:0:1","tags":null,"title":"61B-26: Midterm 2 Review","uri":"/61b-26/"},{"categories":["UCB-CS61B"],"content":"ËøêË°åÁªìÊûú ÊèíÂÖ•ÂçáÂ∫èËäÇÁÇπÂêéÔºåÊ†ëÁöÑÁªìÊûÑÂ∞ÜÂèòÊàêÈìæË°®Ê†∑ÂºèÔºåÂç≥ÊØè‰∏™ËäÇÁÇπÂè™ÊúâÂè≥Â≠êËäÇÁÇπÔºåÊ≤°ÊúâÂ∑¶Â≠êËäÇÁÇπ„ÄÇ Inorder Traversal of BST: 1 2 3 4 5 Tree Structure: R----1 R----2 R----3 R----4 R----5 ÂèØ‰ª•ÁúãÂà∞ÔºåÊ†ëÁöÑÁªìÊûÑÂèòÊàê‰∫Ü‰∏Ä‰∏™ÂêëÂè≥ÂÄæÊñúÁöÑÈìæË°®ÔºåËØ¥ÊòéÊØè‰∏™ËäÇÁÇπÂè™Êúâ‰∏Ä‰∏™Âè≥Â≠êËäÇÁÇπÔºåÊ≤°ÊúâÂ∑¶Â≠êËäÇÁÇπ„ÄÇËøôÁßçÁªìÊûÑ‰ΩøÂæóÊ†ëÁöÑÊÄßËÉΩÈÄÄÂåñ‰∏∫O(n)ÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶„ÄÇ more to see https://docs.google.com/presentation/d/1TA-xr-z7df4vnJz6oo4s7OkpGLoVy1EYYTwR_8AVhxA/edit#slide=id.g35a9240b53_0_150 ","date":"2024-07-13","objectID":"/61b-26/:0:2","tags":null,"title":"61B-26: Midterm 2 Review","uri":"/61b-26/"},{"categories":["UCB-CS61B"],"content":"intro ","date":"2024-07-13","objectID":"/61b-27/:0:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"types of graph ","date":"2024-07-13","objectID":"/61b-27/:1:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"terminology ","date":"2024-07-13","objectID":"/61b-27/:2:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"ÂõæËÆ∫ÈóÆÈ¢ò s-t Path. Is there a path between vertices s and t? Shortest s-t Path. What is the shortest path between vertices s and t? Cycle. Does the graph contain any cycles? Euler Tour. Is there a cycle that uses every edge exactly once? Hamilton Tour. Is there a cycle that uses every vertex exactly once? Connectivity. Is the graph connected, i.e. is there a path between all vertex pairs? Biconnectivity. Is there a vertex whose removal disconnects the graph? Planarity. Can you draw the graph on a piece of paper with no crossing edges? Isomorphism. Are two graphs isomorphic (the same graph in disguise)? Graph problems: Unobvious which are easy, hard, or computationally intractable. Graph Representations Common Simplification: Integer Vertices ","date":"2024-07-13","objectID":"/61b-27/:3:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"API public class Graph { public Graph(int V): // Create empty graph with v vertices public void addEdge(int v, int w): // add an edge v-w Iterable\u003cInteger\u003e adj(int v): // vertices adjacent to v int V(): // number of vertices int E(): // number of edges ... ","date":"2024-07-13","objectID":"/61b-27/:4:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"#1 Adjacency Matrix ","date":"2024-07-13","objectID":"/61b-27/:5:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"#2 edge set ","date":"2024-07-13","objectID":"/61b-27/:6:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"#3 Adjacency List $\\Theta(V) \\rightarrow \\Theta(V^2) $ ‚àö $$ \\Theta(V+E) $$ ÊâçÊòØÊúâÊÑèÊÄùÁöÑÁ≠îÊ°à ","date":"2024-07-13","objectID":"/61b-27/:7:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"ÊÄªÁªì public class Graph { private final int V; private List\u003cInteger\u003e[] adj; public Graph(int V) { this.V = V; adj = (List\u003cInteger\u003e[]) new ArrayList[V]; // cast! for (int v = 0; v \u003c V; v++) { adj[v] = new ArrayList\u003cInteger\u003e(); } } public void addEdge(int v, int w) { adj[v].add(w); adj[w].add(v); } public Iterable\u003cInteger\u003e adj(int v) { return adj[v]; } } Depth First Traversal https://docs.google.com/presentation/d/1IJyC4cAogU2x3erW7E3hDz8jWDTLoaaot8u2ebHtpto/pub?start=false\u0026loop=false\u0026delayms=3000 ","date":"2024-07-13","objectID":"/61b-27/:8:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"implementation public class Paths { public Paths(Graph G, int s): Find all paths from G boolean hasPathTo(int v): is there a path from s to v? Iterable\u003cInteger\u003e pathTo(int v): path from s to v (if any) } ","date":"2024-07-13","objectID":"/61b-27/:9:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"summary ","date":"2024-07-13","objectID":"/61b-27/:10:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"interface /** (Min) Priority Queue: Allowing tracking and removal of the * smallest item in a priority queue. */ public interface MinPQ\u003cItem\u003e { /** Adds the item to the priority queue. */ public void add(Item x); /** Returns the smallest item in the priority queue. */ public Item getSmallest(); /** Removes the smallest item from the priority queue. */ public Item removeSmallest(); /** Returns the size of the priority queue. */ public int size(); } ÂÖ®ËÆ∞ÂΩï‰∏ãÊù•ÁÑ∂ÂêéÊéíÂ∫èÔºünaive way! better way heaps ","date":"2024-07-13","objectID":"/61b-24/:0:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"bst way see https://goo.gl/wBKdFQ Given a heap, how do we implement PQ operations? getSmallest() - return the item in the root node. add(x) - place the new employee in the last position, and promote as high as possible. removeSmallest() - assassinate the president (of the company), promote the rightmost person in the company to president. Then demote repeatedly, always taking the ‚Äòbetter‚Äô successor. Tree Representations ","date":"2024-07-13","objectID":"/61b-24/:1:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"How do we Represent a Tree in Java? ","date":"2024-07-13","objectID":"/61b-24/:2:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"1a ","date":"2024-07-13","objectID":"/61b-24/:2:1","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"1b ","date":"2024-07-13","objectID":"/61b-24/:2:2","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"1c ","date":"2024-07-13","objectID":"/61b-24/:2:3","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"2 Store keys in an array. Store parentIDs in an array. Similar to what we did with disjointSets. ","date":"2024-07-13","objectID":"/61b-24/:2:4","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"3 Store keys in an array. Don‚Äôt store structure anywhere. ‰ªéÂ∑¶Âà∞Âè≥ÂàÜÂ±ÇÁ∫ßÁºñÁ†ÅËøõÂÖ•Êï∞ÁªÑ public class Tree3\u003cKey\u003e { Key[] keys; ... public void swim(int k) { if (keys[parent(k)] ‚âª keys[k]) { swap(k, parent(k)); swim(parent(k)); } } public int parent(int k) { return (k - 1) / 2; } // ËßÇÂØüÊ≥ï 3b ÂØπÊØî Data Structures Summary ","date":"2024-07-13","objectID":"/61b-24/:2:5","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"Search Data Structures (The particularly abstract ones) ","date":"2024-07-13","objectID":"/61b-24/:3:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"Traversals Level Order Traverse top-to-bottom, left-to-right (like reading in English): We say that the nodes are ‚Äòvisited‚Äô in the given order. Depth First Traversals Preorder, Inorder, Postorder Basic (rough) idea: Traverse ‚Äúdeep nodes‚Äù (e.g. A) before shallow ones (e.g. F). ","date":"2024-07-13","objectID":"/61b-25/:0:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"preorder preOrder(BSTNode x) { if (x == null) return; print(x.key) preOrder(x.left) preOrder(x.right) } D B A C F E G ","date":"2024-07-13","objectID":"/61b-25/:0:1","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"inorder inOrder(BSTNode x) { if (x == null) return; inOrder(x.left) print(x.key) inOrder(x.right) } A B C D E F G ","date":"2024-07-13","objectID":"/61b-25/:0:2","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"postorder postOrder(BSTNode x) { if (x == null) return; postOrder(x.left) postOrder(x.right) print(x.key) } A C B E G F D ","date":"2024-07-13","objectID":"/61b-25/:0:3","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"trick to think about ","date":"2024-07-13","objectID":"/61b-25/:1:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"visitor pattern interface Action\u003cLabel\u003e { void visit(Tree\u003cLabel\u003e T); } class FindPig implements Action\u003cString\u003e { boolean found = false; @Override void visit(Tree\u003cString\u003e T) { if (\"pig\".equals(T.label)) { found = true; } } } void preorderTraverse(Tree\u003cLabel\u003e T, Action\u003cLabel\u003e whatToDo) { if (T == null) { return; } whatToDo.visit(T); /* before we hard coded a print */ preorderTraverse(T.left, whatToDo); preorderTraverse(T.right, whatToDo); } preorderTraverse(someTree, new FindPig()); What is the runtime of a preorder traversal in terms of N, the number of nodes? (in code below, assume the visit action takes constant time) Œò(N) : Every node visited exactly once. Constant work per visit. Level Order Traversal ","date":"2024-07-13","objectID":"/61b-25/:2:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"Iterative Deepening public void levelOrder(Tree T, Action toDo) { for (int i = 0; i \u003c T.height(); i += 1) { visitLevel(T, i, toDo); } } // Run visitLevel H times, one for each level. public void visitLevel(Tree T, int level, Action toDo) { if (T == null) { return; } if (lev == 0) { toDo.visit(T.key); } else { visitLevel(T.left(), lev - 1, toDo); visitLevel(T.right(), lev - 1, toDo); } } For algorithms whose runtime depends on height, difference between bushy tree and spindly tree can be huge! Range Finding ÈóÆÈ¢òÊèèËø∞ Easy approach, just do a traversal of the whole tree, and use visitor pattern to collect matching items. $\\Theta(N)$ better way: Pruning, Restricting our search to only nodes that might contain the answers we seek. $\\Theta(log N+R)$ ÔºåÂÖ∂‰∏≠RÊòØÂåπÈÖçÁöÑÁöÑ‰∏™Êï∞„ÄÇ Spatial Trees 2D Range Finding, Building Trees of Two Dimensional Data Optional: Tree Iterators https://docs.google.com/presentation/d/14pqGRZAN_Q60xqYGR3c8XTqHdQT8BVWGPJPC_G_INJQ/edit#slide=id.g75c09ac94_0284 ","date":"2024-07-13","objectID":"/61b-25/:3:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"ordered linked list‚Äî\u003e binary search tree or skip list (out of course) BST Definitions A tree consists of: A set of nodes. A set of edges that connect those nodes. Constraint: There is exactly one path between any two nodes. In a rooted tree, we call one node the root. Every node N except the root has exactly one parent, defined as the first node on the path from N to the root. Unlike (most) real trees, the root is usually depicted at the top of the tree. A node with no child is called a leaf. In a rooted binary tree, every node has either 0, 1, or 2 children (subtrees). ","date":"2024-07-13","objectID":"/61b-21/:0:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Properties of BSTs A binary search tree is a rooted binary tree with the BST property. BST Property. For every node X in the tree: Every key in the left subtree is less than X‚Äôs key. Every key in the right subtree is greater than X‚Äôs key. One consequence of these rules: No duplicate keys allowed! BST Operations ","date":"2024-07-13","objectID":"/61b-21/:1:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Finding a searchKey in a BST static BST find(BST T, Key sk) { if (T == null) return null; if (sk.keyequals(T.label())) return T; else if (sk ‚â∫ T.label()) return find(T.left, sk); else return find(T.right, sk); } runtime: $O(h)$, where $h$ is the height of the tree, i.e., $O(log (N))$, where $N$ is the number of nodes in the tree. ","date":"2024-07-13","objectID":"/61b-21/:2:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Inserting a new key into a BST static BST insert(BST T, Key ik) { if (T == null) return new BST(ik); if (ik ‚â∫ T.label()) T.left = insert(T.left, ik); else if (ik ‚âª T.label()) T.right = insert(T.right, ik); return T; } ","date":"2024-07-13","objectID":"/61b-21/:3:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Deleting a key from a BST no child: simply remove the node. one child: replace the node with its child. two children: find the inorder successor (smallest in the right subtree) and replace the node with it. Then recursively delete the inorder successor from the right subtree. BST Performance ","date":"2024-07-13","objectID":"/61b-21/:4:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Tree Height Performance of spindly trees can be just as bad as a linked list! usually, the height of a BST is $O(log(N))$, where $N$ is the number of nodes in the tree. ","date":"2024-07-13","objectID":"/61b-21/:5:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Insertion demo Random inserts take on average only Œò(log N) each. ","date":"2024-07-13","objectID":"/61b-21/:6:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Deletion demo https://docs.google.com/presentation/d/1rEHpAx8Xu2LnJBWsRPWy8blL20qb96Q5UhdZtQYFkBI/edit#slide=id.g75707c75c_0224 ","date":"2024-07-13","objectID":"/61b-21/:7:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"To avoid the worst-case time complexity of O(n), we need to use a balanced binary search tree. Tree Rotation Non-obvious fact: Rotation allows balancing of any BST. One way to achieve balance: Rotate after each insertion and deletion to maintain balance. ‚Ä¶ the mystery is to know which rotations. We‚Äôll come back to this. B-trees / 2-3 trees / 2-3-4 trees ","date":"2024-07-13","objectID":"/61b-22/:0:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"search tree ","date":"2024-07-13","objectID":"/61b-22/:1:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"weird solution to achieve balance add leaf overstuff leaf, but can not be too juicy! ","date":"2024-07-13","objectID":"/61b-22/:2:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"Performance of B-trees Splitting tree is a better name, but I didn‚Äôt invent them, so we‚Äôre stuck with their real name: B-trees. A B-tree of order M=4 (like we used today) is also called a 2-3-4 tree or a 2-4 tree. The name refers to the number of children that a node can have, e.g. a 2-3-4 tree node may have 2, 3, or 4 children. A B-tree of order M=3 (like in the textbook) is also called a 2-3 tree. Red-Black Trees There are many types of search trees: Binary search trees: Require rotations to maintain balance. There are many strategies for rotation. Coming up with a strategy is hard. 2-3 trees: No rotations required. Clever (and strange idea): Build a BST that is isometric (structurally identical) to a 2-3 tree. Use rotations to ensure the isometry. Since 2-3 trees are balanced, rotations on BST will ensure balance. Maintaining Isometry Through Rotations (Optional) Violations for LLRBs: Two red children. Two consecutive red links. Right red child. Operations for Fixing LLRB Tree Violations: Tree rotations and Color Flips! when insert , use red link if right-insert happens, rotateLeft two red children? two reds in a row? Left-Red-Right-Red? ","date":"2024-07-13","objectID":"/61b-22/:3:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"summary ","date":"2024-07-13","objectID":"/61b-22/:4:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"Ëµ∑Âõ†ÔºåÊó†Â∫èarray ","date":"2024-07-13","objectID":"/61b-23/:0:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"Using data as an Index One extreme approach: All data is really just bits. Use data itself as an array index. Store true and false in the array. Extremely wasteful of memory. To support checking presence of all positive integers, we need 2 billion booleans. Need some way to generalize beyond integers. public class DataIndexedIntegerSet { boolean[] present; public DataIndexedIntegerSet() { present = new boolean[16]; } public insert(int i) { present[i] = true; } public contains(int i) { return present[i]; } } Binary Representations DataIndexedSet insert a cat ‰ΩÜÊòØÊúâÂº±ÁÇπ‚Üì collision handling \u0026 computing a hashCodeÔºÅ Handling Collisions ÊäΩÂ±âÂéüÁêÜÂëäËØâÊàë‰ª¨Ôºå‰∏çÂèØ‰ª•Âè™Èù†Êâ©Â±ïÊï∞ÁªÑÂÆπÈáèÊù•Â§ÑÁêÜ‰πã„ÄÇ Suppose N items have the same hashcode h: Instead of storing true in position h, store a list of these N items at position h. ","date":"2024-07-13","objectID":"/61b-23/:1:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"external chaining Depends on the number of items in the ‚Äòbucket‚Äô. If N items are distributed across M buckets, average time grows with N/M = L, also known as the load factor. Average runtime is Œò(L). Whenever L=N/M exceeds some number, increase M by resizing. Ë¥üÊï∞Á¥¢Âºï Hash Functions ","date":"2024-07-13","objectID":"/61b-23/:2:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"str example @Override public int hashCode() { int hashCode = 1; for (Object o : this) { hashCode = hashCode * 31; hashCode = hashCode + o.hashCode(); } return hashCode; } ","date":"2024-07-13","objectID":"/61b-23/:3:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"recursive example ","date":"2024-07-13","objectID":"/61b-23/:4:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"default hashCode() All Objects have hashCode() function. Default: returns this (i.e. address of object). Can have strange consequences: ‚Äúhello‚Äù.hashCode() is not the same as (‚Äúh‚Äù + ‚Äúello‚Äù).hashCode() Can override for your type. Hash tables (HashSet, HashMap, etc.) are so important that Java requires that all objects implement hashCode(). ","date":"2024-07-13","objectID":"/61b-23/:5:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"HashSets and HashMaps Java provides a hash table based implementation of sets and maps. Idea is very similar to what we‚Äôve done in lecture. Warning: Never store mutable objects in a HashSet or HashMap! Warning #2: Never override equals without also overriding hashCode. extra ","date":"2024-07-13","objectID":"/61b-23/:6:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"Big-O Notation ","date":"2024-07-13","objectID":"/61b-19/:1:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"ÁªÜËäÇÂàÜÊûê ","date":"2024-07-13","objectID":"/61b-19/:2:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"Â±ÄÈôêÊÄß ","date":"2024-07-13","objectID":"/61b-19/:3:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"ÂØπÊØî Important: Big O does not mean ‚Äúworst case‚Äù! Often abused to mean this. ","date":"2024-07-13","objectID":"/61b-19/:4:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"Â§ßOËÆ∞Âè∑ÁöÑÁî®Â§Ñ ","date":"2024-07-13","objectID":"/61b-19/:5:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"Big-Omega Notation $\\Omega(f(n))$ ","date":"2024-07-13","objectID":"/61b-19/:6:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"Â§ßŒ©ËÆ∞Âè∑ÁöÑÁî®Â§Ñ ","date":"2024-07-13","objectID":"/61b-19/:7:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"Èùû‰∏•Ê†ºËØÅÊòéÂíå‰∏•Ê†ºËØÅÊòé ","date":"2024-07-13","objectID":"/61b-19/:8:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"‰∏çÁõ∏‰∫§ÈõÜÈóÆÈ¢ò public interface DisjointSets { /** Connects two items P and Q. */ void connect(int p, int q); /** Checks to see if two items are connected. */ boolean isConnected(int p, int q); } ","date":"2024-07-13","objectID":"/61b-20/:0:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"naive implementation ÁúüÁöÑÈìæÊé•‰∏§‰∏™ÂÖÉÁ¥†ÔºåÁÑ∂ÂêéËÄÉËôëÈÅçÂéÜÊï¥‰∏™ÈõÜÂêàÔºåÂà§Êñ≠ÊòØÂê¶Êúâ‰∏§‰∏™ÂÖÉÁ¥†ÊòØËøûÈÄöÁöÑ„ÄÇ ","date":"2024-07-13","objectID":"/61b-20/:0:1","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"better implementation Better approach: Model connectedness in terms of sets. How things are connected isn‚Äôt something we need to know.üòâ ","date":"2024-07-13","objectID":"/61b-20/:0:2","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"quick-find public class QuickFindDS implements DisjointSets { private int[] id; // really fast public boolean isConnected(int p, int q) { return id[p] == id[q]; } public void connect(int p, int q) { int pid = id[p]; int qid = id[q]; for (int i = 0; i \u003c id.length; i++) { if (id[i] == pid) { id[i] = qid; } }... } // constructor public QuickFindDS(int N) { id = new int[N]; for (int i = 0; i \u003c N; i++) id[i] = i; } } ","date":"2024-07-13","objectID":"/61b-20/:1:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"quick-union ËÄÉËôë‰∏çÁî®Êï∞ÁªÑÔºåÁî® üå≥üòã tree can not be too tall: Ê†ë‰∏çËÉΩÂ§™È´òÔºåÂê¶Âàô‰ºöÈÄÄÂåñÊàêÈìæË°®‚ö†Ô∏è public class QuickUnionDS implements DisjointSets { private int[] parent; public QuickUnionDS(int N) { parent = new int[N]; for (int i = 0; i \u003c N; i++) parent[i] = i; } // linear time to create N trees private int find(int p) { while (p != parent[p]) p = parent[p]; // p[i] and i ÂæàÈáçË¶ÅÔºÅ return p; } public boolean isConnected(int p, int q) { return find(p) == find(q); } public void connect(int p, int q) { int i = find(p); int j = find(q); parent[i] = j; // ÂêàÂπ∂‰∏§‰∏™Ê†ë } } ","date":"2024-07-13","objectID":"/61b-20/:2:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"weighted quick-union Â∏åÊúõÂπ≥Ë°°ÊùÉÈáç ÊùÉÈáçÂèØ‰ª•ÊòØÊ†ëÁöÑÂ§ßÂ∞èÔºå‰πüÂèØ‰ª•ÊòØÊ†ëÁöÑÊ∑±Â∫¶„ÄÇ ‰ª•‰∏ãËÄÉËôëÂÖÉÁ¥†‰∏™Êï∞ÔºàÊ†ëÁöÑÂ§ßÂ∞èÔºâ New ruleÔºàÁõÆÂâçÊòØ‰∏çÂä†ËØÅÊòéÁöÑÁªèÈ™åÂÖ¨ÂºèÔºâ: Always link root of smaller tree to larger tree. public class WeightedQuickUnionDS implements DisjointSets { private int[] parent; private int[] size; // size of each tree public WeightedQuickUnionDS(int N) { parent = new int[N]; size = new int[N]; // Â¢ûÂä†‰∫Üsize arrayËÆ∞ÂΩï for (int i = 0; i \u003c N; i++) { parent[i] = i; size[i] = 1; // each tree is of size 1 } } // find and isConnected are the same as before! private int find(int p) { while (p != parent[p]) p = parent[p]; // p[i] and i ÂæàÈáçË¶ÅÔºÅ return p; } public boolean isConnected(int p, int q) { return find(p) == find(q); } public void connect(int p, int q) { int i = find(p); int j = find(q); if (size[i] \u003c size[j]) { parent[i] = j; size[j] += size[i]; // add size of i to j } else { parent[j] = i; size[i] += size[j]; // add size of j to i } } } ","date":"2024-07-13","objectID":"/61b-20/:3:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"path compressionÔºàUCB-CS170üòãÔºâ Ë∑ØÂæÑÂéãÁº©ÔºöÂ∞ÜÊ†ëÁöÑÊ†πËäÇÁÇπÊåáÂêëÊ†ëÁöÑÊ†πËäÇÁÇπÔºåÂáèÂ∞ëÊ†ëÁöÑÈ´òÂ∫¶„ÄÇ Ë∑ØÂæÑÂéãÁº©ÁöÑÂ•ΩÂ§ÑÔºö ÂáèÂ∞ëÊ†ëÁöÑÈ´òÂ∫¶Ôºå‰ΩøÂæófindÂíåisConnectedÁöÑÊïàÁéáÊõ¥È´ò„ÄÇ ÂáèÂ∞ëÂÜÖÂ≠òÊ∂àËÄó„ÄÇ log*(n) is the iterated log - it‚Äôs the number of times you need to apply log to n to go below 1. Note that 2^65536 is higher than the number of atoms in the universe. ","date":"2024-07-13","objectID":"/61b-20/:4:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"‰∏çÂä†ËØÅÊòéÁªôÂá∫ÁõÆÂâçÊúÄÊûÅÈôêÁöÑÊÉÖÂÜµ $\\alpha(N)$ public class WeightedQuickUnionDSWithPathCompression implements DisjointSets { private int[] parent; private int[] size; public WeightedQuickUnionDSWithPathCompression(int N) { parent = new int[N]; size = new int[N]; for (int i = 0; i \u003c N; i++) { parent[i] = i; size[i] = 1; } } // find Âπ∂‰∏ç‰ºöÂ§™Èöæ ‰πê private int find(int p) { if (p == parent[p]) { return p; } else { parent[p] = find(parent[p]); return parent[p]; } } public boolean isConnected(int p, int q) { return find(p) == find(q); } public void connect(int p, int q) { int i = find(p); int j = find(q); if (i == j) return; if (size[i] \u003c size[j]) { parent[i] = j; size[j] += size[i]; } else { parent[j] = i; size[i] += size[j]; } } } ","date":"2024-07-13","objectID":"/61b-20/:4:1","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"references Nazca Lines: http://redicecreations.com/ul_img/24592nazca_bird.jpg Implementation code adapted from Algorithms, 4th edition and Professor Jonathan Shewchuk‚Äôs lecture notes on disjoint sets, where he presents a faster one-array solution. I would recommend taking a look. (http://www.cs.berkeley.edu/~jrs/61b/lec/33) The proof of the inverse ackermann runtime for disjoint sets is given here: http://www.uni-trier.de/fileadmin/fb4/prof/INF/DEA/Uebungen_LVA-Ankuendigungen/ws07/KAuD/effi.pdf as originally proved by Tarjan here at UC Berkeley in 1975. ","date":"2024-07-13","objectID":"/61b-20/:5:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"Ê≤°ÊúâÊç∑ÂæÑÔºåÂÖ®Èù†‰ªîÁªÜ for loops ","date":"2024-07-13","objectID":"/61b-18/:1:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"recursion ÂΩ¢Â¶Ç$\\Theta(n^k)$, $k$ ÊòØÈÄíÂΩíÁöÑÊ∑±Â∫¶ ","date":"2024-07-13","objectID":"/61b-18/:2:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"binary search ÂΩ¢Â¶Ç$\\Theta(\\log n)$ C(N) = ‚åälog2(N)‚åã+1 ","date":"2024-07-13","objectID":"/61b-18/:3:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"merge sort ÂΩ¢Â¶Ç$\\Theta(n\\log n)$ ","date":"2024-07-13","objectID":"/61b-18/:4:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"Runtime Characterizations In most cases, we care only about asymptotic behavior, i.e. what happens for very large N. ","date":"2024-07-13","objectID":"/61b-17/:1:0","tags":null,"title":"61B-17: Asymptotics I","uri":"/61b-17/"},{"categories":["UCB-CS61B"],"content":"Intuitive Simplification Consider only the worst case. Restrict Attention to One Operation. ÊâæÂæóÂ•ΩÂπ∂‰∏îÂ∑ßÁöÑËØùÂèØÂæàÂø´ÁúãÂá∫ÔºåÈÄÄËÄåÊ±ÇÂÖ∂Ê¨°ÁöÑËØùÂèØ‰ª•ËÄÉËôëÁîªÂõæÂàÜÊûê Eliminate low order terms. Eliminate multiplicative constants. ","date":"2024-07-13","objectID":"/61b-17/:2:0","tags":null,"title":"61B-17: Asymptotics I","uri":"/61b-17/"},{"categories":["UCB-CS61B"],"content":"Big-Theta Notation $\\Theta(f(n))$ The only difference is that we use the Œò symbol anywhere we would have said ‚Äúorder of growth‚Äù. ","date":"2024-07-13","objectID":"/61b-17/:3:0","tags":null,"title":"61B-17: Asymptotics I","uri":"/61b-17/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab01.ipynb\") Lab 01 Welcome to the first lab of Data 100! This lab is meant to help you familiarize yourself with JupyterHub, review Python and numpy, and introduce you to matplotlib, a Python visualization library. ","date":"2024-07-13","objectID":"/datalab1/:0:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Part 1: Jupyter Tips ","date":"2024-07-13","objectID":"/datalab1/:1:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Viewing Documentation To output the documentation for a function, use the help function. # help(print) ?print You can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work. Below, click your mouse anywhere on the print block below and use Shift + Tab to view the function‚Äôs documentation. ","date":"2024-07-13","objectID":"/datalab1/:1:1","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Importing Libraries and Magic Commands import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') %matplotlib inline %matplotlib inline is a Jupyter magic command that configures the notebook so that Matplotlib displays any plots that you draw directly in the notebook rather than to a file, allowing you to view the plots upon executing your code. (Note: In practice, this is no longer necessary, but we‚Äôre showing it to you now anyway.) Another useful magic command is %%time, which times the execution of that cell. You can use this by writing it as the first line of a cell. (Note that %% is used for cell magic commands that apply to the entire cell, whereas % is used for line magic commands that only apply to a single line.) %%time lst = [] for i in range(100): lst.append(i) CPU times: total: 0 ns Wall time: 0 ns ","date":"2024-07-13","objectID":"/datalab1/:1:2","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Keyboard Shortcuts To learn about keyboard shortcuts, go to Help ‚Äì\u003e Keyboard Shortcuts in the menu above. Here are a few that we like: Ctrl + Return (or Cmd + Return on Mac): Evaluate the current cell Shift + Return: Evaluate the current cell and move to the next ESC : command mode (may need to press before using any of the commands below) a : create a cell above b : create a cell below dd : delete a cell z : undo the last cell operation m : convert a cell to markdown y : convert a cell to code ","date":"2024-07-13","objectID":"/datalab1/:1:3","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Part 2: Prerequisites ","date":"2024-07-13","objectID":"/datalab1/:2:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Python Python is the main programming language we‚Äôll use in the course. We expect that you‚Äôve taken CS 61A, Data 8, or an equivalent class, so we will not be covering general Python syntax. If any of the following exercises are challenging (or if you would like to refresh your Python knowledge), please review one or more of the following materials. Python Tutorial: Introduction to Python from the creators of Python. Composing Programs Chapter 1: This is more of a introduction to programming with Python. Advanced Crash Course: A fast crash course which assumes some programming background. ","date":"2024-07-13","objectID":"/datalab1/:2:1","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"NumPy NumPy is the numerical computing module introduced in Data 8, which is a prerequisite for this course. Here‚Äôs a quick recap of NumPy. For more review, read the following materials. NumPy Quick Start Tutorial DS100 NumPy Review Stanford CS231n NumPy Tutorial The Data 8 Textbook Chapter on NumPy ","date":"2024-07-13","objectID":"/datalab1/:2:2","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 1 The core of NumPy is the array. Like Python lists, arrays store data; however, they store data in a more efficient manner. In many cases, this allows for faster computation and data manipulation. In Data 8, we used make_array from the datascience module, but that‚Äôs not the most typical way. Instead, use np.array to create an array. It takes a sequence, such as a list or range. Below, create an array arr containing the values 1, 2, 3, 4, and 5 (in that order). arr = np.array([1,2,3,4,5]) grader.check(\"q1\") q1 passed! üåü In addition to values in the array, we can access attributes such as shape and data type. A full list of attributes can be found here. arr[3] np.int64(4) arr[2:4] # Â∑¶Èó≠Âè≥ÂºÄ array([3, 4]) arr.shape # ‰∏ÄÁª¥ÈïøÂ∫¶‰∏∫5 (5,) arr.dtype dtype('int64') Arrays, unlike Python lists, cannot store items of different data types. # A regular Python list can store items of different data types [1, '3'] [1, '3'] # Arrays will convert everything to the same data type np.array([1, '3']) array(['1', '3'], dtype='\u003cU21') # Another example of array type conversion np.array([5, 8.3]) array([5. , 8.3]) Arrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays. For example, observe the following: # Python list addition will concatenate the two lists [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6] # NumPy array addition will add them element-wise np.array([1, 2, 3]) + np.array([4, 5, 6]) array([5, 7, 9]) ","date":"2024-07-13","objectID":"/datalab1/:2:3","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 2 Question 2a Write a function summation that evaluates the following summation for $n \\geq 1$: $$\\sum_{i=1}^{n} i^3 + 3 i^2$$ Note: You should not use for loops in your solution. Check the NumPy documentation. If you‚Äôre stuck, try a search engine! Searching the web for examples of how to use modules is very common in data science. # Áî®Â•ΩnpÂêëÈáèÂåñ def summation(n): \"\"\"Compute the summation i^3 + 3 * i^2 for 1 \u003c= i \u003c= n.\"\"\" arr = np.arange(1, n+1) newArr = arr**3 + 3 * arr**2 return int(np.sum(newArr)) grader.check(\"q2a\") q2a passed! üíØ Question 2b Write a function elementwise_array_sum that computes the square of each value in list_1, the cube of each value in list_2, then returns a list containing the element-wise sum of these results. Assume that list_1 and list_2 have the same number of elements, do not use for loops. The input parameters will both be python lists, so you may need to convert the lists into arrays before performing your operations. The output should be a numpy array. def elementwise_array_sum(list_1, list_2): \"\"\"Compute x^2 + y^3 for each x, y in list_1, list_2. Assume list_1 and list_2 have the same length. Return a NumPy array. \"\"\" assert len(list_1) == len(list_2), \"both args must have the same number of elements\" # create a NumPy array from the two lists arr1 = np.array(list_1) arr2 = np.array(list_2) arr_sum = arr1 ** 2 + arr2 ** 3 return arr_sum grader.check(\"q2b\") q2b passed! üåü You might have been told that Python is slow, but array arithmetic is carried out very fast, even for large arrays. Below is an implementation of the above code that does not use NumPy arrays. def elementwise_list_sum(list_1, list_2): \"\"\"Compute x^2 + y^3 for each x, y in list_1, list_2. Assume list_1 and list_2 have the same length. \"\"\" return [x ** 2 + y ** 3 for x, y in zip(list_1, list_2)] For ten numbers, elementwise_list_sum and elementwise_array_sum both take a similar amount of time. sample_list_1 = list(range(10)) sample_array_1 = np.arange(10) %%time elementwise_list_sum(sample_list_1, sample_list_1) CPU times: total: 0 ns Wall time: 0 ns [0, 2, 12, 36, 80, 150, 252, 392, 576, 810] %%time elementwise_array_sum(sample_array_1, sample_array_1) CPU times: total: 0 ns Wall time: 0 ns array([ 0, 2, 12, 36, 80, 150, 252, 392, 576, 810]) The time difference seems negligible for a list/array of size 10; depending on your setup, you may even observe that elementwise_list_sum executes faster than elementwise_array_sum! However, we will commonly be working with much larger datasets: sample_list_2 = list(range(100000)) sample_array_2 = np.arange(100000) %%time elementwise_list_sum(sample_list_2, sample_list_2) ; # The semicolon hides the output CPU times: total: 15.6 ms Wall time: 33.2 ms %%time elementwise_array_sum(sample_array_2, sample_array_2) CPU times: total: 0 ns Wall time: 557 Œºs array([ 0, 2, 12, ..., 999920002099982, 999950000799996, 999980000100000]) With the larger dataset, we see that using NumPy results in code that executes over 50 times faster! Throughout this course (and in the real world), you will find that writing efficient code will be important; arrays and vectorized operations are the most common way of making Python programs run quickly. Question 2c Recall the formula for population variance below: $$\\sigma^2 = \\frac{\\sum_{i=1}^N (x_i - \\mu)^2}{N}$$ Complete the functions below to compute the population variance of population, an array of numbers. For this question, do not use built in NumPy functions, such as np.var. Again, avoid using for loops! def mean(population): \"\"\" Returns the mean of population (mu) Keyword arguments: population -- a numpy array of numbers \"\"\" # Calculate the mean of a population return sum(population)/float(len(population)) def variance(population): \"\"\" Returns the variance of population (sigma squared) Keyword arguments: population -- a numpy array of numbers \"\"\" # Calculate the variance of a population mu = mean(population) return sum((population-mu) ** 2)/float(len(populati","date":"2024-07-13","objectID":"/datalab1/:2:4","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Part 3: Plotting Here we explore plotting using matplotlib and numpy. ","date":"2024-07-13","objectID":"/datalab1/:3:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 3 Consider the function $f(x) = x^2$ for $-\\infty \u003c x \u003c \\infty$. Question 3a Find the equation of the tangent line to $f$ at $x = 0$. Use LaTeX to type your solution, such that it looks like the serif font used to display the math expressions in the sentences above. HINT: You can click any text cell to see the raw Markdown syntax. $tangent line: ÂàáÁ∫ø$ $y = 0$ Question 3b Find the equation of the tangent line to $f$ at $x = 8$. Please use LaTeX to type your solution. $y = 16x$ Question 3c Write code to plot the function $f$, the tangent line at $x=8$, and the tangent line at $x=0$. Set the range of the x-axis to (-15, 15) and the range of the y-axis to (-100, 300) and the figure size to (4,4). Your resulting plot should look like this (it‚Äôs okay if the colors in your plot don‚Äôt match with ours, as long as they‚Äôre all different colors): You should use the plt.plot function to plot lines. You may find the following functions useful: plt.plot(..) plt.figure(figsize=..) plt.ylim(..) plt.axhline(..) def f(x): return x ** 2 def df(x): return 2*x def plot(f, df): plt.figure(figsize=(4, 4)) x = np.array([-15, 15]) plt.plot(x,f(x),color='blue') plt.plot(x,df(x),color='green') plt.axhline(0,color='red') plt.ylim(-100, 300) plot(f, df) ","date":"2024-07-13","objectID":"/datalab1/:3:1","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 4 (Ungraded) Data science is a rapidly expanding field and no degree program can hope to teach you everything that will be helpful to you as a data scientist. So it‚Äôs important that you become familiar with looking up documentation and learning how to read it. Below is a section of code that plots a three-dimensional ‚Äúwireframe‚Äù plot. You‚Äôll see what that means when you draw it. Replace each # Your answer here with a description of what the line above does, what the arguments being passed in are, and how the arguments are used in the function. For example, np.arange(2, 5, 0.2) # This returns an array of numbers from 2 to 5 with an interval size of 0.2 Hint: The Shift + Tab tip from earlier in the notebook may help here. Remember that objects must be defined in order for the documentation shortcut to work; for example, all of the documentation will show for method calls from np since we‚Äôve already executed import numpy as np. However, since z is not yet defined in the kernel, z.reshape(x.shape) will not show documentation until you run the line z = np.cos(squared). from mpl_toolkits.mplot3d import axes3d u = np.linspace(1.5 * np.pi, -1.5 * np.pi, 100) # Your answer here [x, y] = np.meshgrid(u, u) # Your answer here squared = np.sqrt(x.flatten() ** 2 + y.flatten() ** 2) z = np.cos(squared) # Your answer here z = z.reshape(x.shape) # Your answer here fig = plt.figure(figsize = (6, 6)) ax = fig.add_subplot(111, projection = '3d') # Your answer here ax.plot_wireframe(x, y, z, rstride = 5, cstride = 5, lw = 2) # Your answer here ax.view_init(elev = 60, azim = 25) # Your answer here plt.savefig(\"figure1.png\") # Your answer here ","date":"2024-07-13","objectID":"/datalab1/:3:2","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 5 (Ungraded) Do you think that eating french fries with mayonnaise is a crime? Tell us what you think in the following Markdown cell. :) To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-07-13","objectID":"/datalab1/:3:3","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export(pdf=False) ","date":"2024-07-13","objectID":"/datalab1/:4:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"two common errors chance error: randomness can vary bias error: systematic error in one direction ","date":"2024-07-13","objectID":"/datal2/:1:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"bias ","date":"2024-07-13","objectID":"/datal2/:2:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"common non-random samples convenience samples: samples that are easy to obtain but may not be representative of the population quota samples: samples that are drawn from a limited number of individuals or groups ","date":"2024-07-13","objectID":"/datal2/:3:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"random samples random can produce biases but we can estimate the bias and chance error properties of random samples: ÊòéÁ°ÆÊ¶ÇÁéá no need to be same chance üòã scheme of random sampling: ‚ÄúRandom sample with replacement‚Äù ÊòØÁªüËÆ°Â≠¶‰∏≠ÁöÑ‰∏Ä‰∏™ÊúØËØ≠ÔºåÊåáÁöÑÊòØÂú®ËøõË°åÊäΩÊ†∑Êó∂ÔºåÊØèÊ¨°ÊäΩÂèñÁöÑÊ†∑Êú¨Âú®ÊîæÂõûÂéüÊÄª‰Ωì‰πãÂêéÔºåÂÜçËøõË°å‰∏ã‰∏ÄÊ¨°ÊäΩÂèñ„ÄÇËøôÊÑèÂë≥ÁùÄÂêå‰∏Ä‰∏™‰∏™‰ΩìÊàñÂÖÉÁ¥†ÊúâÂèØËÉΩË¢´Â§öÊ¨°ÊäΩÂèñ„ÄÇ ËøôÁßçÊäΩÊ†∑ÊñπÊ≥ïÁöÑÁâπÁÇπÂåÖÊã¨Ôºö ÊØèÊ¨°ÊäΩÂèñÈÉΩÊòØÁã¨Á´ãÁöÑÔºåÂç≥Ââç‰∏ÄÊ¨°ÁöÑÊäΩÂèñÁªìÊûú‰∏ç‰ºöÂΩ±ÂìçÂêé‰∏ÄÊ¨°ÁöÑÊäΩÂèñ„ÄÇ ÊÄª‰Ωì‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†Âú®ÊØèÊ¨°ÊäΩÂèñ‰∏≠Ë¢´ÈÄâ‰∏≠ÁöÑÊ¶ÇÁéáÊòØÁõ∏ÂêåÁöÑ„ÄÇ Áî±‰∫éÊ†∑Êú¨Ë¢´ÊîæÂõûÔºåÊ†∑Êú¨ÁöÑÂ§ßÂ∞èÂèØ‰ª•Á≠â‰∫éÊàñÂ∞è‰∫éÊÄª‰ΩìÁöÑÂ§ßÂ∞è„ÄÇ ‰∏é‰πãÁõ∏ÂØπÁöÑÊòØ ‚ÄúRandom sample without replacement‚ÄùÔºåÂç≥‰∏çÊîæÂõûÊäΩÊ†∑ÔºåËøôÁßçÊÉÖÂÜµ‰∏ãÔºå‰∏ÄÊó¶‰∏Ä‰∏™ÂÖÉÁ¥†Ë¢´ÊäΩÂèñÔºåÂÆÉÂ∞±‰∏ç‰ºöÂÜçÊ¨°Ë¢´ÊäΩÂèñÔºåÂõ†Ê≠§ÊäΩÂèñÁöÑÊ†∑Êú¨ÈáèÊÄªÊòØÂ∞è‰∫éÊÄª‰ΩìÁöÑÂ§ßÂ∞è„ÄÇüòâ SRSÔºöü§îÊ≥®ÊÑèÊòØÊØè‰∏™\"pair\"ÔºÅ ","date":"2024-07-13","objectID":"/datal2/:4:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"Â§öÈ°πÂºèÂíå‰∫åÈ°πÂºèÂàÜÂ∏ÉÈááÊ†∑ ","date":"2024-07-13","objectID":"/datal2/:5:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["NNDL"],"content":"neuronal networks and deep learning‚Ä¶ coming soon ","date":"2024-07-12","objectID":"/nndl/:0:0","tags":null,"title":"NNDL0","uri":"/nndl/"},{"categories":["CSIEC"],"content":"COLLEGE STUDENTS INNOVATION AND ENTREPRENEURSHIP COMPETITION (CSIEC) ","date":"2024-07-12","objectID":"/experiences/csiec/csiec0/:0:0","tags":null,"title":"CSIEC0","uri":"/experiences/csiec/csiec0/"},{"categories":["DATA100"],"content":"cycle of data science ","date":"2024-07-12","objectID":"/datal1/:1:0","tags":null,"title":"DATA100-L1: course overview","uri":"/datal1/"},{"categories":["UCB-CS61B"],"content":"math problems $$ N! ‚àà \\Omega (N^{N}) ? $$ ‚àö $$ log(N!) ‚àà \\Omega (NlogN) ? $$ ‚àö $$ NlogN‚àà \\Omega (log(N!)) ? $$ ‚àö ÊâÄ‰ª•ÂèØ‰ª•Êé®Âá∫Ôºö $$ NlogN ‚àà \\Theta (logN!) $$ $$ log(N!) ‚àà \\Theta (NlogN) $$ ","date":"2024-07-11","objectID":"/61b-35/:1:0","tags":null,"title":"61B-35: Sorting and Algorithmic Bounds","uri":"/61b-35/"},{"categories":["UCB-CS61B"],"content":"TUCSÁî®Êó∂ ‰∏ä‰∏ãÁïåÔºü the ultimate comparison sort run time $$ \\Omega(NlogN) $$ $$ O(NlogN) $$ ‰∏ãÈù¢ÂºÄÂßãËØÅÊòéÔºö ËÄÉËôë‰∏ãÁïåÔºåÂØπn‰∏™Áâ©‰ΩìËøõË°åÊéíÂ∫èÔºåÊúâNÔºÅÁßçÂèØËÉΩÔºåÁî®‰∏§‰∏§ÊØîÂ§ßÂ∞èÔºåËÄÉËôëÂÜ≥Á≠ñÊ†ëÁöÑÈ´òÂ∫¶$$ H = \\log_2 N! $$ Âõ†Ê≠§‰∏ãÁïå‰∏∫ $$ \\Omega (log(N!)) $$ ÊàñËÄÖ $$ \\Omega (NlogN) $$ ‰∏äÁïåÈÄöËøáTUCSÁöÑÊÄßË¥®ÂèØ‰ª•ÈÄöËøáÂÖ∑‰ΩìÁ§∫‰æãÂèçËØÅÂæóÂà∞ÔºåÊØîÂ¶ÇÁî®merge sort ","date":"2024-07-11","objectID":"/61b-35/:2:0","tags":null,"title":"61B-35: Sorting and Algorithmic Bounds","uri":"/61b-35/"},{"categories":["UCB-CS61B"],"content":"More quick sort, Stability, Shuffling ","date":"2024-07-11","objectID":"/61b-34/:0:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"quick sort VS merge sort QuicksortL3S = left + 3-scan + shuffle Quicksort_LTHS: Tony Hoare partition scheme: L ptr ‰ªÖ‰ªÖÊåáÂêëÂ∞èÁöÑ G ptr ‰ªÖ‰ªÖÊåáÂêëÂ§ßÁöÑ ptr walk towards to each other, stopping on a hated item ‰∏§‰∏™ÈÉΩÂÅú‰∏ãÊù•ÁöÑËØùÔºå ‰∫§Êç¢‰∏Ä‰∏ãÔºå ÁÑ∂ÂêéÁßªÂä®ÂÖ∂‰∏≠‰∏Ä‰∏™ when ptrs cross, done. ÂíåG‰∫§Êç¢pivot ","date":"2024-07-11","objectID":"/61b-34/:1:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"Not random smarter pivot selection: median Quicksort_PickTH ËÄÉËôë‰∫ÜÂ¶Ç‰ΩïËÆ°ÁÆóÊï∞ÁªÑÂú∞ÂùÄÁöÑÂ§çÊùÇÂ∫¶Ôºå ‰ª•ÂèäÂ¶Ç‰ΩïÈÄâÊã©pivotÁöÑÂ§çÊùÇÂ∫¶„ÄÇ worst case: $$ \\Theta(NlogN) $$ ‰ΩÜÂÆûÈôÖ‰∏äÂπ∂Ê≤°ÊúâÈÇ£‰πàÂ•ΩÔºåÂõ†‰∏∫ËÆ°ÁÆó‰∏≠‰ΩçÊï∞ÁöÑÂ§çÊùÇÂ∫¶ÊòØ$$\\Theta(N)$$„ÄÇËÄóË¥π‰∫ÜÊõ¥Â§öÊó∂Èó¥„ÄÇ ","date":"2024-07-11","objectID":"/61b-34/:2:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"quick select‚Äìusing partitioning worst case: a sorted array $$ \\Theta(N^2) $$ on average: $$ N + N/2 + N/4 +‚Ä¶ + 1 = \\Theta(N) $$ ","date":"2024-07-11","objectID":"/61b-34/:3:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"stability for stable sort, we need to keep the relative order of equal elements Is insertion sort stable? yes, it is stable. Is quick sort stable? depends on the partitioning scheme. ","date":"2024-07-11","objectID":"/61b-34/:4:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"adaptive array.sort() is adaptive Êü•ÁúãjavaÂÆòÊñπÊñáÊ°£ ","date":"2024-07-11","objectID":"/61b-34/:5:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"shuffling random number and then sort ","date":"2024-07-11","objectID":"/61b-34/:6:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"‰ø°ÊÅØÊó†ÊçüÊÄß Ê®°Á≥äÊÄß ","date":"2024-07-11","objectID":"/61b-38/:0:0","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"prefix-free codes ","date":"2024-07-11","objectID":"/61b-38/:1:0","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"Huffman codes ","date":"2024-07-11","objectID":"/61b-38/:1:1","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"shannon-fano codes using tries to convert compressed data into a original data longest prefix matching ","date":"2024-07-11","objectID":"/61b-38/:1:2","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"self-extracting bits ","date":"2024-07-11","objectID":"/61b-38/:2:0","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"Overview ","date":"2024-07-11","objectID":"/61b-37/:1:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"Tries‚Äî‚ÄîÂâçÁºÄÊ†ë/Â≠óÂÖ∏Ê†ë usages: prefix matching approximate matching keysWithPrefix(String prefix) // returns all keys in the trie that start with the given prefix longestPrefixOf(String query) // returns the longest key in the trie that is a prefix of the query ","date":"2024-07-11","objectID":"/61b-37/:2:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"implementation private class Node{ boolean exists; Map\u003cCharacter, Node\u003e links; public Node(){ links = new TreeMap\u003c\u003e(); exists = false; } } ","date":"2024-07-11","objectID":"/61b-37/:3:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"T9 keyboard ","date":"2024-07-11","objectID":"/61b-37/:4:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"Ternary search Tries public class TSTSet\u003cValue\u003e{ private Node\u003cValue\u003e root; private static class Node\u003cValue\u003e{ private char c; private Node\u003cValue\u003e lo, mid, hi; } } ‰ΩÜÊòØËøôÁßçÂÆûÁé∞ÊñπÂºèË°®Áé∞‰∏ç‰Ω≥ ","date":"2024-07-11","objectID":"/61b-37/:5:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"radix sort ‰∏çÁî®comparisonsÁöÑÊéíÂ∫èÁÆóÊ≥ïÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶O(dn)Ôºåd‰∏∫ÊúÄÂ§ßÊï∞ÁöÑ‰ΩçÊï∞Ôºån‰∏∫ÂæÖÊéíÂ∫èÊï∞ÁöÑ‰∏™Êï∞„ÄÇ Á©∫Èó¥Êç¢Êó∂Èó¥ bucket sort counting sort: ÊâæÂá∫ÂæÖÊéíÂ∫èÊï∞ÁöÑÊúÄÂ§ßÂÄºmaxÔºåÁ°ÆÂÆöËÆ°Êï∞Êï∞ÁªÑÁöÑÈïøÂ∫¶‰∏∫max+1„ÄÇ ÈÅçÂéÜÂæÖÊéíÂ∫èÊï∞ÔºåÂ∞ÜÊØè‰∏™Êï∞ÁöÑ‰∏™‰ΩçÊï∞ÂÄº‰Ωú‰∏∫Á¥¢ÂºïÔºåÂ∞ÜËØ•Á¥¢ÂºïÂØπÂ∫îÁöÑËÆ°Êï∞Êï∞ÁªÑÂÖÉÁ¥†Âä†1„ÄÇ ÈÅçÂéÜËÆ°Êï∞Êï∞ÁªÑÔºåÂ∞ÜÊØè‰∏™ÂÖÉÁ¥†ÁöÑÂÄº‰Ωú‰∏∫Á¥¢ÂºïÔºåÂ∞ÜËØ•Á¥¢ÂºïÂØπÂ∫îÁöÑÂÖÉÁ¥†ÂÄºËæìÂá∫Âà∞ÁªìÊûúÊï∞ÁªÑ‰∏≠„ÄÇ runtime: O(n+k) LSD radix sort: least significant digit radix sort ÊâæÂá∫ÂæÖÊéíÂ∫èÊï∞ÁöÑÊúÄÂ§ßÂÄºmaxÔºåÁ°ÆÂÆöËÆ°Êï∞Êï∞ÁªÑÁöÑÈïøÂ∫¶‰∏∫10„ÄÇ ÈÅçÂéÜÂæÖÊéíÂ∫èÊï∞ÔºåÂ∞ÜÊØè‰∏™Êï∞ÁöÑ‰∏™‰ΩçÊï∞ÂÄº‰Ωú‰∏∫Á¥¢ÂºïÔºåÂ∞ÜËØ•Á¥¢ÂºïÂØπÂ∫îÁöÑËÆ°Êï∞Êï∞ÁªÑÂÖÉÁ¥†Âä†1„ÄÇ LSD sort vs merge sort: similar strings:LSD sort is better dissimilar strings:merge sort is better MSD radix sort: most significant digit radix sort ÊâæÂá∫ÂæÖÊéíÂ∫èÊï∞ÁöÑÊúÄÂ§ßÂÄºmaxÔºåÁ°ÆÂÆöËÆ°Êï∞Êï∞ÁªÑÁöÑÈïøÂ∫¶‰∏∫10„ÄÇ ÈÅçÂéÜÂæÖÊéíÂ∫èÊï∞ÔºåÂ∞ÜÊØè‰∏™Êï∞ÁöÑ‰∏™‰ΩçÊï∞ÂÄº‰Ωú‰∏∫Á¥¢ÂºïÔºåÂ∞ÜËØ•Á¥¢ÂºïÂØπÂ∫îÁöÑËÆ°Êï∞Êï∞ÁªÑÂÖÉÁ¥†Âä†1„ÄÇ ÈÅçÂéÜËÆ°Êï∞Êï∞ÁªÑÔºåÂ∞ÜÊØè‰∏™ÂÖÉÁ¥†ÁöÑÂÄº‰Ωú‰∏∫Á¥¢ÂºïÔºåÂ∞ÜËØ•Á¥¢ÂºïÂØπÂ∫îÁöÑÂÖÉÁ¥†ÂÄºËæìÂá∫Âà∞ÁªìÊûúÊï∞ÁªÑ‰∏≠„ÄÇ runtime: O(n+k) ","date":"2024-07-10","objectID":"/61b-36/:0:0","tags":null,"title":"61B-36: Radix Sorts","uri":"/61b-36/"},{"categories":["TOOLS"],"content":"Êñá‰ª∂Áä∂ÊÄÅ Êú™Ë∑üË∏™-Êú™‰øÆÊîπ-Â∑≤‰øÆÊîπ-ÊöÇÂ≠ò git add \u003cname\u003e - *-\u003eÊöÇÂ≠ò git commit -m \"message\" - ÊöÇÂ≠ò-\u003eÊú™‰øÆÊîπ git rm \u003cname\u003e - Êú™‰øÆÊîπ-\u003eÊú™Ë∑üË∏™ ","date":"2024-06-29","objectID":"/tools/git/git/:1:0","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Êü•ÁúãÁä∂ÊÄÅ git status Êõ¥Âä†ÁªÜËá¥Âá†Ë°åÂá†Âàó git diff Êü•ÁúãÂéÜÂè≤Êó•Âøó git log --pretty=oneline git log --graph --oneline --decorate ","date":"2024-06-29","objectID":"/tools/git/git/:1:1","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Âü∫Êú¨Êìç‰Ωú ","date":"2024-06-29","objectID":"/tools/git/git/:2:0","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Âü∫Á°ÄÈÖçÁΩÆ git config --global user.name \"your name\" git config --global user.email \"your email\" ","date":"2024-06-29","objectID":"/tools/git/git/:2:1","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"ÂàõÂª∫ÁâàÊú¨Â∫ì mkdir myproject cd myproject git init ","date":"2024-06-29","objectID":"/tools/git/git/:2:2","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"ÂÖãÈöÜÁâàÊú¨Â∫ì git clone https://github.com/username/repository.git ","date":"2024-06-29","objectID":"/tools/git/git/:2:3","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Ë∑üË∏™Êñá‰ª∂orÊñá‰ª∂Â§π git add \u003cfilename\u003e git rm \u003cfilename\u003e git rm --cache \u003cfilename\u003e ","date":"2024-06-29","objectID":"/tools/git/git/:2:4","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"ËÆæÁΩÆÁºìÂ≠òÁä∂ÊÄÅ git add git reset HEAD \u003cfilename\u003e ","date":"2024-06-29","objectID":"/tools/git/git/:2:5","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Êèê‰∫§‰øÆÊîπ git commit -m \"commit message str\" Êí§ÈîÄÈùûÈ¶ñÊ¨°‰øÆÊîπ git reset head~ --soft ","date":"2024-06-29","objectID":"/tools/git/git/:2:6","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"ÂíågithubËÅîÁ≥ª git remote add origin https://github.com/username/repository.git git remote git remote rename origin old_name Êé®Âà∞ËøúÁ®ã‰ªìÂ∫ì git push origin master sshËøûÊé•Ôºü ","date":"2024-06-29","objectID":"/tools/git/git/:2:7","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"ÂàÜÊîØÁÆ°ÁêÜ ÂàõÂª∫ÂàÜÊîØ git branch --list git branch hhzz git checkout hhzz git checkout -b hhzz ÂêàÂπ∂ÂàÜÊîØ git merge hhzz Âà†Èô§ÂàÜÊîØ git branch -d hhzz ","date":"2024-06-29","objectID":"/tools/git/git/:2:8","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Ë¥ÆËóèÂäüËÉΩ stash ÂæÖÊñΩÂ∑• ","date":"2024-06-29","objectID":"/tools/git/git/:2:9","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"ÈáçÁΩÆ„ÄÅÊç¢Âü∫ÂäüËÉΩ ÂæÖÊñΩÂ∑• ","date":"2024-06-29","objectID":"/tools/git/git/:2:10","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Regular Expressions ","date":"2024-06-29","objectID":"/tools/reg/:0:0","tags":null,"title":"Ê≠£ÂàôË°®ËææÂºèÁ¨îËÆ∞","uri":"/tools/reg/"},{"categories":["TOOLS"],"content":"Ê≥®ÊÑèÁâàÊú¨ÂíåÊñáÊ°£ÔºÅ ","date":"2024-06-29","objectID":"/tools/reg/:1:0","tags":null,"title":"Ê≠£ÂàôË°®ËææÂºèÁ¨îËÆ∞","uri":"/tools/reg/"},{"categories":["TOOLS"],"content":"Â∏∏Áî®Â∑•ÂÖ∑ https://regex101.com/ https://regexr.com/ python reÊ®°Âùó Â≠óÁ¨¶ . ÂåπÈÖç‰ªªÊÑè‰∏Ä‰∏™Â≠óÁ¨¶ [] ÂåπÈÖçÊã¨Âè∑‰∏≠ÁöÑ‰ªªÊÑè‰∏Ä‰∏™Â≠óÁ¨¶,Â¶Ç [a-zA-Z1-3] ÂåπÈÖçÂ§ßÂÜôÂ≠óÊØçÊàñÂ∞èÂÜôÂ≠óÊØçÊàñÊï∞Â≠ó1-3, [^] ÂåπÈÖçÈô§‰∫ÜÊã¨Âè∑‰∏≠ÁöÑÂ≠óÁ¨¶ È¢ÑÂÆöÂ≠óÁ¨¶Á±ª \\d ÂåπÈÖçÊï∞Â≠ó \\D ÂåπÈÖçÈùûÊï∞Â≠ó \\w ÂåπÈÖçÂ≠óÊØç„ÄÅÊï∞Â≠óÊàñ‰∏ãÂàíÁ∫ø \\W ÂåπÈÖçÈùûÂ≠óÊØç„ÄÅÊï∞Â≠óÊàñ‰∏ãÂàíÁ∫ø \\s ÂåπÈÖçÁ©∫ÁôΩÂ≠óÁ¨¶ÊàñËÄÖtab \\S ÂåπÈÖçÈùûÁ©∫ÁôΩÂ≠óÁ¨¶ ËæπÁïåÂåπÈÖç ^ ÂåπÈÖçÂ≠óÁ¨¶‰∏≤ÁöÑÂºÄÂ§¥ $ ÂåπÈÖçÂ≠óÁ¨¶‰∏≤ÁöÑÁªìÂ∞æ \\b ÂåπÈÖçÂçïËØçÁöÑËæπÁïå, Â¶Ç \\bthe\\b ÂåπÈÖçthe \\B ÂåπÈÖçÈùûÂçïËØçËæπÁïå Êï∞ÈáèËØç * ÂåπÈÖçÂâçÈù¢ÁöÑÂ≠óÁ¨¶0Ê¨°ÊàñÂ§öÊ¨° + ÂåπÈÖçÂâçÈù¢ÁöÑÂ≠óÁ¨¶1Ê¨°ÊàñÂ§öÊ¨° ? ÂåπÈÖçÂâçÈù¢ÁöÑÂ≠óÁ¨¶0Ê¨°Êàñ1Ê¨° {n} ÂåπÈÖçÂâçÈù¢ÁöÑÂ≠óÁ¨¶nÊ¨° {n,} ÂåπÈÖçÂâçÈù¢ÁöÑÂ≠óÁ¨¶Ëá≥Â∞ënÊ¨° {n,m} ÂåπÈÖçÂâçÈù¢ÁöÑÂ≠óÁ¨¶Ëá≥Â∞ënÊ¨°, Ëá≥Â§ömÊ¨° ÈùûË¥™Â©™ÂåπÈÖç ÈáèËØçÈªòËÆ§ÊòØË¥™Â©™ÂåπÈÖç, Âç≥Â∞ΩÂèØËÉΩÂ§öÁöÑÂåπÈÖçÂ≠óÁ¨¶, Â¶Ç a.*b ‰ºöÂåπÈÖçÂà∞ÊúÄÈïøÁöÑ‰ª•aÂºÄÂ§¥ÁöÑb ÂêéÈù¢ÁöÑÈáèËØçÂä†‰∏ä? Âàô‰∏∫ÈùûË¥™Â©™ÂåπÈÖç, Âç≥Â∞ΩÂèØËÉΩÂ∞ëÁöÑÂåπÈÖçÂ≠óÁ¨¶, Â¶Ç a.*?b ‰ºöÂåπÈÖçÂà∞ÊúÄÁü≠ÁöÑ‰ª•aÂºÄÂ§¥ÁöÑb ÂàÜÁªÑ‰∏éÊçïËé∑ () Áî®Êù•ÂàõÂª∫ÂàÜÁªÑ, ÊçïËé∑Êã¨Âè∑‰∏≠ÁöÑÂ≠óÁ¨¶, Âπ∂Âú®ÂåπÈÖçÊó∂ËøîÂõûÂåπÈÖçÂà∞ÁöÑÂÜÖÂÆπ [] Áî®Êù•ÂàõÂª∫Â≠óÁ¨¶Á±ª, Â¶Ç [Pp] ÂåπÈÖçPÊàñp | Áî®Êù•ÂàõÂª∫ÊàñÂÖ≥Á≥ª, Â¶Ç a(bc|de) ÂåπÈÖçaÂêéÈù¢ÊòØbcÊàñde \\n ÂºïÁî®ÂàÜÁªÑ, Â¶Ç \\1 ÂºïÁî®Á¨¨‰∏Ä‰∏™ÂàÜÁªÑ $n ÂºïÁî®Á¨¨n‰∏™ÂàÜÁªÑ ?: ÈùûÊçïËé∑ÂàÜÁªÑ, Â¶Ç (?:abc) ÂåπÈÖçabc, ‰∏çÊçïËé∑ÂåπÈÖçÂà∞ÁöÑÂÜÖÂÆπ ÂâçÁûªÂíåÂêéÈ°æ Ê≠£ÂêëÂâçÁûª (?=abc) ÂåπÈÖçabcÂâçÈù¢ÁöÑÂ≠óÁ¨¶ ÂèçÂêëÂâçÁûª (?!abc) ÂåπÈÖçabcÂâçÈù¢ÁöÑÂ≠óÁ¨¶ Ê≠£ÂêëÂêéÈ°æ (?\u003c=abc) ÂåπÈÖçabcÂêéÈù¢ÁöÑÂ≠óÁ¨¶ ÂèçÂêëÂêéÈ°æ (?\u003c!abc) ÂåπÈÖçabcÂêéÈù¢ÁöÑÂ≠óÁ¨¶ ","date":"2024-06-29","objectID":"/tools/reg/:2:0","tags":null,"title":"Ê≠£ÂàôË°®ËææÂºèÁ¨îËÆ∞","uri":"/tools/reg/"},{"categories":["PRP"],"content":"PRPÁ≥ªÂàóÂ∞ÜÁî®‰∫éËÆ∞ÂΩïÁ†îÁ©∂È°πÁõÆÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éËØæÈ¢ò„ÄÅÁ†îÁ©∂ÊñπÊ≥ï„ÄÅÁ†îÁ©∂ÊàêÊûú„ÄÅÁ†îÁ©∂ÂøÉÂæó„ÄÅÁ†îÁ©∂ÁªèÈ™å„ÄÅÁ†îÁ©∂ÂøÉË∑ØÂéÜÁ®ãÁ≠â„ÄÇ ","date":"2024-05-21","objectID":"/experiences/prp/prp0/:0:0","tags":null,"title":"PRP0","uri":"/experiences/prp/prp0/"},{"categories":["C++"],"content":"learning pointer(advanced version) ‰∏∫‰∫ÜÈò≤Ê≠¢ÊêûÊ∑∑ËÄåÂÜô ","date":"2024-05-05","objectID":"/crash/cpp1/:0:0","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["C++"],"content":"‰∏ãÊ†á‰∏∫0?È¶ñÂú∞ÂùÄ? void test0(){ int arr[] = {1, 2, 3}; cout \u003c\u003c \u0026arr[0] \u003c\u003c endl; cout \u003c\u003c \u0026arr \u003c\u003c endl; cout \u003c\u003c arr \u003c\u003c endl; } arr \u0026arr \u0026arr[0] Â≠òÂÇ®ÁöÑÈÉΩÊòØÁõ∏ÂêåÁöÑÂú∞ÂùÄ arr Â∏∏ÈáèÊåáÈíà‰∏çËÉΩË¢´ÊîπÂèò ","date":"2024-05-05","objectID":"/crash/cpp1/:0:1","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["C++"],"content":"ÊåáÂêëÊï∞ÁªÑÂÖÉÁ¥†ÁöÑÊåáÈíà(‰∏ç‰∏ÄÂÆöÊòØÈ¶ñÂÖÉÁ¥†)‰ª•Áî®[]Êù•ËÆøÈóÆÊï∞ÁªÑÂÖÉÁ¥† void test2() { int a[3] = {1,2,3}; int *p = a; p++; cout \u003c\u003c p[0] \u003c\u003c endl; // 2 } ","date":"2024-05-05","objectID":"/crash/cpp1/:0:2","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["C++"],"content":"Êï∞ÁªÑÁ±ªÂûãÁöÑÊåáÈíà void test2(){ int arr[] = {1, 2, 3}; int (*p)[] = \u0026arr; // ÂÆö‰πâ‰∏Ä‰∏™ÊåáÂêëÊï∞ÁªÑÁöÑÊåáÈíà cout \u003c\u003c (*p)[0] \u003c\u003c endl; // ËæìÂá∫Êï∞ÁªÑÈ¶ñÂú∞ÂùÄ cout \u003c\u003c p[0] \u003c\u003c endl; // ËæìÂá∫Êï∞ÁªÑÈ¶ñÂú∞ÂùÄ cout \u003c\u003c p[0][0] \u003c\u003c endl; // ËæìÂá∫Êï∞ÁªÑÈ¶ñÂÖÉÁ¥† } int *p[] = \u0026arr vs int (*p)[] = \u0026arr???? [ ]‰ºòÂÖàÁ∫ßÈ´ò‰∫é* int (*p)[] = \u0026arr; *p --\u003e ‰∏Ä‰∏™ÊåáÈíà Ôºà*pÔºâ[] --\u003e ÊåáÂêëÊï∞ÁªÑÁöÑÊåáÈíà int (*p)[] --\u003e ÊåáÂêëÁöÑÊï∞ÁªÑÁöÑÂÖÉÁ¥†ÊòØintÁ±ªÂûã p = \u0026arr ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÊåáÂêëÊï∞ÁªÑÁöÑÊåáÈíàÔºå(*p) = arr Ëß£ÂºïÁî®ÊåáÈíàÂæóÂà∞Êï∞ÁªÑÈ¶ñÂú∞ÂùÄÔºå(*p)[0] = arr[0] ËÆøÈóÆÊï∞ÁªÑÈ¶ñÂÖÉÁ¥† p[0] = arr ËÆøÈóÆÊï∞ÁªÑÈ¶ñÂú∞ÂùÄÔºåp[0][0] = arr[0] ËÆøÈóÆÊï∞ÁªÑÈ¶ñÂÖÉÁ¥† ","date":"2024-05-05","objectID":"/crash/cpp1/:0:3","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["C++"],"content":"ÈÇ£‰πàint *(*p)[] = { };Ê∞¥Âà∞Ê∏†Êàê‰∫Ü ","date":"2024-05-05","objectID":"/crash/cpp1/:0:4","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["C++"],"content":"ÂÜÖÂ≠òÊò†ÂÉèÂõæ ÂÜÖÂ≠òÊò†ÂÉèÂõæ 1 2 ‚Ä¶ ÂÜÖÂ≠òÂú∞ÂùÄ‰ªé‰∏äÂæÄ‰∏ãÈÄíÂ¢û ÂíåCSAPPÈáåÈù¢ÁöÑÊ†àÁîªÊ≥ïÊúâÁÇπ‰∏ç‰∏ÄÊ†∑ ","date":"2024-05-05","objectID":"/crash/cpp1/:0:5","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["C++"],"content":"delete Áî≥ËØ∑‰∏Ä‰∏™ËøûÁª≠ÁöÑÂÜÖÂ≠òÂùóÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ËßÜ‰∏∫‰∫åÁª¥Êï∞ÁªÑÔºö int** arr = new int*[rows]; for (int i = 0; i \u003c rows; ++i) { arr[i] = new int[cols]; } ÈáäÊîæÊó∂Ôºå‰Ω†ÈúÄË¶ÅÂÖàÈáäÊîæÊØè‰∏ÄË°åÁöÑÂÜÖÂ≠òÔºåÁÑ∂ÂêéÈáäÊîæË°åÊåáÈíàÊï∞ÁªÑÔºö for (int i = 0; i \u003c rows; ++i) { delete[] arr[i]; } delete[] arr; Áî≥ËØ∑‰∏Ä‰∏™Ë∂≥Â§üÂ§ßÁöÑËøûÁª≠ÂÜÖÂ≠òÂùóÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂ËßÜ‰∏∫‰∫åÁª¥Êï∞ÁªÑÔºö int* arr = new int[rows * cols]; Âú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºå‰Ω†Âè™ÈúÄË¶ÅÈáäÊîæ‰∏ÄÊ¨°Ôºö delete[] arr; Ê≥®ÊÑèÔºåËøôÁßçÊñπÂºè‰∏ãÔºåarrÂÆûÈôÖ‰∏äÊòØ‰∏Ä‰∏™‰∏ÄÁª¥Êï∞ÁªÑÔºå‰ΩÜÊòØ‰Ω†ÂèØ‰ª•ÂÉèËÆøÈóÆ‰∫åÁª¥Êï∞ÁªÑ‰∏ÄÊ†∑‰ΩøÁî®ÂÆÉÔºà‰æãÂ¶ÇÔºåarr[i][j]ÂÆûÈôÖ‰∏äÊòØarr[i * cols + j]Ôºâ„ÄÇ Á°Æ‰øùÂú®ÈáäÊîæÂÜÖÂ≠òÂêéÂ∞ÜÊåáÈíàËÆæÁΩÆ‰∏∫nullptrÔºå‰ª•ÈÅøÂÖçÊÇ¨ÂûÇÊåáÈíàÈóÆÈ¢òÔºö delete[] arr; arr = nullptr; // ÊàñËÄÖ‰ΩøÁî®Êô∫ËÉΩÊåáÈíàËá™Âä®ÁÆ°ÁêÜÂÜÖÂ≠ò ","date":"2024-05-05","objectID":"/crash/cpp1/:0:6","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["C++"],"content":"char? int main() { char **p, *city[] = {\"aaa\",\"bbb\"}; for (p = city; p \u003c city + 2; ++p) { cout \u003c\u003c *p \u003c\u003c endl; } return 0; } ÁªìÊûú‰∏∫Ôºö aaa bbb ","date":"2024-05-05","objectID":"/crash/cpp1/:0:7","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["csapp"],"content":"ÂÆûÈ™å‰∏ÄÔºöÊ†àÊ∫¢Âá∫ÊîªÂáªÂÆûÈ™å ","date":"2024-04-22","objectID":"/csapp_attacklab/:1:0","tags":null,"title":"CSAPP_attacklab","uri":"/csapp_attacklab/"},{"categories":["csapp"],"content":"Ê†àÁöÑÂü∫Êú¨ÁªìÊûÑ ","date":"2024-04-22","objectID":"/csapp_attacklab/:1:1","tags":null,"title":"CSAPP_attacklab","uri":"/csapp_attacklab/"},{"categories":["csapp"],"content":"ÂÆûÈ™å‰∫åÔºöROPÊîªÂáªÂÆûÈ™å ","date":"2024-04-22","objectID":"/csapp_attacklab/:2:0","tags":null,"title":"CSAPP_attacklab","uri":"/csapp_attacklab/"},{"categories":["csapp"],"content":"csapp_bomblab ÈÉΩÊòØÊ±áÁºñËØ≠Ë®ÄÔºåÊ≤°Êúâ‰ªÄ‰πàÂ•ΩËØ¥ÁöÑ Ê≥®ÊÑèGDBË∞ÉËØï ","date":"2024-04-22","objectID":"/csapp_bomblab/:0:0","tags":null,"title":"CSAPP_bomblab","uri":"/csapp_bomblab/"},{"categories":["csapp"],"content":"Ê†∏ÂøÉÊ¶ÇÂøµ‰πã‰∏ÄÔºöÂØªÂùÄ Â¶Ç‰ΩïÂØªÂùÄÔºü $Imm(r_1,r_2,factor)$ Ê≥®ÊÑèÂÄºËøòÊòØÂú∞ÂùÄÔºü (%rdx)ÂèñmemoryÊó∂Ôºå$M[R_i]$ ‰∏≠M‰∏ÄÁõ¥Âú®ÊúÄÂ§ñÂ±Ç ","date":"2024-04-22","objectID":"/csapp_bomblab/:1:0","tags":null,"title":"CSAPP_bomblab","uri":"/csapp_bomblab/"},{"categories":["csapp"],"content":"Ê†∏ÂøÉÊ¶ÇÂøµ‰πã‰∫åÔºöGDBË∞ÉËØï ","date":"2024-04-22","objectID":"/csapp_bomblab/:2:0","tags":null,"title":"CSAPP_bomblab","uri":"/csapp_bomblab/"},{"categories":["csapp"],"content":"Â∏∏Áî®ÂëΩ‰ª§ run ËøêË°åÁ®ãÂ∫èÔºàÊ≥®ÊÑèÁªìÂêàÊï∞ÊçÆÊµÅpipelineÔºâ b +$Addr$ ËÆæÁΩÆÊñ≠ÁÇπ delete Âà†Èô§Êñ≠ÁÇπ next ÂçïÊ≠•ÊâßË°å step stepi``finishËøõÂÖ•ÂáΩÊï∞ p $eax ÊâìÂç∞ÂèòÈáè x /$nxb $Addr$ ÊâìÂç∞ÂÜÖÂ≠ò layout asm ÂàáÊç¢Âà∞Ê±áÁºñÊ®°ÂºèÊúâÂ•ΩÁúãÁöÑÁ™óÂè£ info registers ÊâìÂç∞ÂØÑÂ≠òÂô® info frame ÊâìÂç∞Ê†àÂ∏ß info args ÊâìÂç∞ÂáΩÊï∞ÂèÇÊï∞ info locals ÊâìÂç∞Â±ÄÈÉ®ÂèòÈáè info breakpoints ÊâìÂç∞Êñ≠ÁÇπ‰ø°ÊÅØ continue ÁªßÁª≠ËøêË°å quit stopÈÄÄÂá∫Ë∞ÉËØï ","date":"2024-04-22","objectID":"/csapp_bomblab/:2:1","tags":null,"title":"CSAPP_bomblab","uri":"/csapp_bomblab/"},{"categories":["csapp"],"content":"‰∏Ä‰∫õ‰∫õÊäÄÂ∑ß mov‰∏Ä‰∫õÂ•áÂ•áÊÄ™ÊÄ™ÁöÑÂú∞ÂùÄÊó∂ÔºåÂæàÂèØËÉΩÊòØÁ∫øÁ¥¢ÔºåÂèØ‰ª•Áî®x /$nxb $Addr$Êü•ÁúãÂÜÖÂ≠ò jne‰πãÁ±ªÁöÑËÉΩ‰∏çËÉΩÁõ¥Êé•ÂèñÁ≠âÊì¶ËæπÈÄöËøá Â∏∏ËßÅÁöÑÂü∫Á°ÄËØ≠Âè•ÔºàÊù°‰ª∂/Âæ™ÁéØÔºâÊúâ‰∏Ä‰∫õÂõ∫ÂÆöÁöÑËåÉÂºèÔºåÂèØ‰ª•Áî®x /6i $PCÁ≠âÊü•ÁúãÊåá‰ª§ ","date":"2024-04-22","objectID":"/csapp_bomblab/:3:0","tags":null,"title":"CSAPP_bomblab","uri":"/csapp_bomblab/"},{"categories":["CSAPP"],"content":"int bit-level operations ","date":"2024-04-21","objectID":"/csapp_datalab/:1:0","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"Âæ∑Êë©Ê†πÂæãÔºà‰ΩçËøêÁÆóÂíåÈõÜÂêàËÆ∫Ôºâ ‰∏éÔºö\u0026 ÈùûÔºö~ ‰∏§ËÄÖÁªÑÂêàÂ∑≤ÁªèÂèØ‰ª•Ë°®Á§∫Âõõ‰∏™Âü∫Êú¨ËøêÁÆóÔºö‰∏é„ÄÅÈùû„ÄÅÊàñ„ÄÅÂºÇÊàñ„ÄÇ ","date":"2024-04-21","objectID":"/csapp_datalab/:1:1","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"ÁßªÂä®‰ΩçËøêÁÆó Ê≥®ÊÑèÊòØÂê¶‰∏∫Êó†Á¨¶Âè∑Êï∞ÔºåÊúâÁ¨¶Âè∑Êï∞ÁöÑÁßª‰ΩçËøêÁÆóËßÑÂàô‰∏éÊó†Á¨¶Âè∑Êï∞‰∏çÂêå„ÄÇ ÊúâÁ¨¶Âè∑Êï∞ÁöÑÁßª‰ΩçËøêÁÆóËßÑÂàôÔºö Â∑¶ÁßªÔºöÁ¨¶Âè∑‰Ωç‰∏çÂèòÔºåÂè≥ËæπË°•0„ÄÇ Âè≥ÁßªÔºöÁ¨¶Âè∑‰Ωç‰∏çÂèòÔºåÂ∑¶ËæπË°•Á¨¶Âè∑‰Ωç„ÄÇ Êó†Á¨¶Âè∑Êï∞ÁöÑÁßª‰ΩçËøêÁÆóËßÑÂàôÔºö Â∑¶ÁßªÔºöÂ∑¶ËæπË°•0„ÄÇ Âè≥ÁßªÔºöÂè≥ËæπË°•0„ÄÇ ","date":"2024-04-21","objectID":"/csapp_datalab/:1:2","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"‰∏éËøêÁÆóÔºà\u0026ÔºâÂèñÁâπÂÆöÁöÑ‰ΩçÊï∞ÔºåÁî®‰∫é‰ΩçÂ±ÇÈù¢Êù°‰ª∂Âà§Êñ≠ ","date":"2024-04-21","objectID":"/csapp_datalab/:1:3","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"ÂáèÊ≥ïÁöÑÂÆûÁé∞ A + ~A = -1 ‚Äì\u003e A + (~A+1) = 0 ","date":"2024-04-21","objectID":"/csapp_datalab/:1:4","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"ÂáèÊ≥ïÁöÑÊèèËø∞ËåÉÂõ¥ÈóÆÈ¢ò ÂÅöÂ∑ÆÂèñÁ¨¶Âè∑‰Ωç ","date":"2024-04-21","objectID":"/csapp_datalab/:1:5","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"Êé©Á†ÅÊìç‰Ωú int conditional(int x, int y, int z) { int exp1 = ~(!!x) + 1; int exp2 = ~(!x) + 1; return (exp1\u0026y) + (exp2\u0026z); } ","date":"2024-04-21","objectID":"/csapp_datalab/:1:6","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"‰ΩçÂ±ÇÈù¢ÂàÜÁ±ªËÆ®ËÆ∫ /* howManyBits - return the minimum number of bits required to represent x in * two's complement(Ë°•Á†ÅÁ≥ªÁªü) * Examples: howManyBits(12) = 5 * howManyBits(298) = 10 * howManyBits(-5) = 4 Ë¥üÊï∞ÁöÑËØù ÂèñÂèç ÂêåÁêÜ * howManyBits(0) = 1 * howManyBits(-1) = 1 ÁâπÊÆäÁÇπ? * howManyBits(0x80000000) = 32 * Legal ops: ! ~ \u0026 ^ | + \u003c\u003c \u003e\u003e * Max ops: 90 * Rating: 4 */ int howManyBits(int x) { // Âèñ‰∏∫Ê≠£Êï∞Êìç‰Ωú int flag = x \u003e\u003e 31; x = (~flag \u0026 x) + (flag \u0026 (~x)); // Êé©Á†ÅÂàÜÁ±ªÊÄùÊÉ≥ // 0000 0100 0000 0000 0000 0000 0000 0000 // Flag_i = !!(x \u003e\u003e i) int bit16 = !!(x \u003e\u003e 16) \u003c\u003c 4; // log2 N x \u003e\u003e= bit16; // \u003e\u003e= Âè≥ÁßªËµãÂÄºËøêÁÆóÁ¨¶ int bit8 = !!(x \u003e\u003e 8) \u003c\u003c 3; x \u003e\u003e= bit8; int bit4 = !!(x \u003e\u003e 4) \u003c\u003c 2; x \u003e\u003e= bit4; int bit2 = !!(x \u003e\u003e 2) \u003c\u003c 1; x \u003e\u003e= bit2; int bit1 = !!(x \u003e\u003e 1) \u003c\u003c 0; x \u003e\u003e= bit1; int bit0 = x; // x = 1 return (bit0+bit1+bit2+bit4+bit8+bit16) + 1; } ","date":"2024-04-21","objectID":"/csapp_datalab/:1:7","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"float bit-level operations $$ float = (-1)^s * M * 2^E $$ ","date":"2024-04-21","objectID":"/csapp_datalab/:2:0","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"},{"categories":["CSAPP"],"content":"float to binary sign bit: $s$ exponent bits: $E$ ‚Äî\u003e $E = e - 127$ mantissa bits: $M$‚Äî\u003efrac add $1$ and $0$ to the left until $M$ has 23 bits ","date":"2024-04-21","objectID":"/csapp_datalab/:2:1","tags":null,"title":"CSAPP_datalab","uri":"/csapp_datalab/"}]