[{"categories":["UMich-EECS-498"],"content":"Attention Mechanisms in Neural Networks ","date":"2025-04-19","objectID":"/l13-attention/:0:0","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"introduction What if Seq to Seq models processed long long sequences? ","date":"2025-04-19","objectID":"/l13-attention/:1:0","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"Attention Mechanisms the core idea is that using weighted sum, and the coefficient can be learned from the model itself In math, we do not actually care that wether input is a sequence or not. given hidden states $h_i$ and the context vector $c$, we can calculate the attention weights as follows: $$ e_{t, i, j} = f_{att}(s_{t-1}, h_{i,j}) \\ a_{t, :, :} = softmax(e_{t, :, :}) \\ c_{t} = \\sum_{i,j} a_{t, i, j} h_{i,j} $$ â€œShow, attend and tellâ€ ICML 2015, which setoff â€œX, Attend, and Yâ€ ğŸ˜† ","date":"2025-04-19","objectID":"/l13-attention/:2:0","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"Attention Layers then we wanna abstract this attention mechanism into a general attention layer, as so many work proves that attention mechanism is a crucial component in neural networks Inputs: Query vector $q$, Shape: ($D_Q$,) Input vector $x$, Shape: ($N_X$, $D_Q$) Similarity function $f_{att}$, at first is (scaled) dot production Computation: Similarity: $e$, Shape: ($N_X$, ), usually $e_i = q*X_i/\\sqrt{D_Q}$ Attention weights: $a$, Shape: ($N_X$, ) Output vector: $y = \\sum_{i=1}a_i x_i$, Shape: ($D_Q$, ) now then we turn into matrix form: Inputs: Query matrix $Q$, Shape: ($N_Q$, $D_Q$) Input matrix $X$, Shape: ($N_X$, $D_Q$) Computation: Similarity: $E = Q*X^T/\\sqrt{D_Q}$, Shape: ($N_Q$, $N_X$) Attention weights: $A = softmax(E,dim=1)$, Shape: ($N_Q$, $N_X$) Output vector: $Y = A*X$, Shape: ($N_Q$, $D_Q$) $X$ was used twice (for similarity and output), and we wanna separate them into two matrices to make it more clear and flexibleâ€¦ KQV! ğŸ˜‰ Inputs: Query matrix $Q$, Shape: ($N_Q$, $D_Q$) Input matrix $X$, Shape: ($N_X$, $D_X$) Key matrix $W_K$, Shape: ($D_X$, $D_Q$) Value matrix $W_V$, Shape: ($D_X$, $D_V$) Computation: Key vectors: $K = XW_K$, Shape: ($N_X$, $D_Q$) Value vectors: $V = XW_V$, Shape: ($N_X$, $D_V$) Similarity: $E = Q*K^T$, Shape: ($N_Q$, $N_X$) Attention weights: $A = softmax(E,dim=1)$, Shape: ($N_Q$, $N_X$) Output vector: $Y = AV$, Shape: ($N_Q$, $D_V$), maybe product and sum ","date":"2025-04-19","objectID":"/l13-attention/:3:0","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"Self-Attention Layers one query for per input vector Inputs: Input matrix $X$, Shape: ($N_X$, $D_X$) Key matrix $W_K$, Shape: ($D_X$, $D_Q$) Value matrix $W_V$, Shape: ($D_X$, $D_V$) Query matrix $W_Q$, Shape: ($D_X$, $D_Q$) Computation: Query vectors: $Q = XW_Q$, Shape: ($N_X$, $D_Q$) Key vectors: $K = XW_K$, Shape: ($N_X$, $D_Q$) Value vectors: $V = XW_V$, Shape: ($N_X$, $D_V$) Similarity: $E = QK^T$, Shape: ($N_Q$, $N_X$), need to be scaled by $\\sqrt{D_Q}$ Attention weights: $A = softmax(E,dim=1)$, Shape: ($N_Q$, $N_X$) Output vector: $Y = AV$, Shape: ($N_Q$, $D_V$), maybe product and sum what happens if we change the order of input vectors? all the value will be the same, but permuted so we perform the attention on a SET of vectors to solve this problem, we can use positional encoding to add information about the position of each vector in the sequence, maybe using torch.cat ","date":"2025-04-19","objectID":"/l13-attention/:4:0","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"Masked Self-Attention Layers force the model only use the past information, and ignore the future information, predicting the next word, at hidden level or similarity level ","date":"2025-04-19","objectID":"/l13-attention/:4:1","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"Multi-Head Self-Attention Layers spilt the input vectors into equal parts $h$, Query dimension $D_Q$ ","date":"2025-04-19","objectID":"/l13-attention/:4:2","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"Summary of Ways of Processing Sequences RNNs: good at long seq bad at parallel 1D CNNs: bad at long seq good at parallel Self-Attention: good at long seq good at parallel bad at memory ","date":"2025-04-19","objectID":"/l13-attention/:5:0","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"Attention is all you need ğŸ˜† Transformer! then the General Pretrained Transformer (GPT) modelâ€™s story beginsâ€¦ ","date":"2025-04-19","objectID":"/l13-attention/:6:0","tags":null,"title":"L13-Attention","uri":"/l13-attention/"},{"categories":["UMich-EECS-498"],"content":"3D Vision back to the start ğŸ˜† ","date":"2025-04-19","objectID":"/l17-3d-vision/:0:0","tags":null,"title":"L17-3D Vision","uri":"/l17-3d-vision/"},{"categories":["UMich-EECS-498"],"content":"shape prediction and ingest 3D information 5 data representation depth map RGB + Depth image = RGB-D image 2.5D raw 3D sensor can easily capture depth information another type of depth map is surface normal map, which is a 2D image that represents the surface normal (using a normal vector) at each pixel location all the mentioned maps can be learned with Fully Convolutional Networks (FCN) ğŸ¤” Voxel grid Occupancies! like a binary matrix/grid, can be learned by a 3D CNN ECCV2016 paper: 3D-R2N2, using a MLP to bridge the gap between 2D and 3D can use 2D CNN to predict 3D occupancy grid, but sacrifices spatial invariant information need high resolution to capture fine details scaling is nontrivial :(, extremely computationally expensive! Oct tree Nested Shape Layers point cloud can represent 3D objects as a set of less points requires new architectures, losses need post-processing for other tasks Chamfer distance: $d_{CD}(S_1,S_2)=\\sum_{x \\in S_1}\\min_{y \\in S_2}\\lVert x-y\\rVert_2^2 + \\sum_{y \\in S_2} \\min_{x \\in S_1} \\lVert x - y \\rVert _2^2$ LiDAR ğŸ˜† mesh standard 3D object representation Explicit representation: vertices, faces, edges adaptive: flat? efficient! wanna more details? more faces! can attach data on verts and interpolate over the whole surface, like color, normal, textureâ€¦ need to come up with new arch to process meshes with NN ECCV2018 paper: Pixel2Mesh Iterative refinement of a 3D mesh Graph Convolutional Network (GCN) to process mesh ğŸ˜† Aligned vertex features between 2D and 3D Loss: convert mesh to point cloud, then use Chamfer distance as before Implicit surface / Signed Distance Function (SDF) trying to learn a function to separate the 3D points into different regions so that we can learn a implicit representation for the 3D shape ","date":"2025-04-19","objectID":"/l17-3d-vision/:1:0","tags":null,"title":"L17-3D Vision","uri":"/l17-3d-vision/"},{"categories":["UMich-EECS-498"],"content":"metric learning for 3D data F1 score: $F_1 = 2 \\frac{precision \\cdot recall}{precision + recall}$ IoU: $IoU = \\frac{TP}{TP + FP + FN}$ ","date":"2025-04-19","objectID":"/l17-3d-vision/:2:0","tags":null,"title":"L17-3D Vision","uri":"/l17-3d-vision/"},{"categories":["UMich-EECS-498"],"content":"camera system view cooridnate system (VCS) canonical view volume (CVV) ","date":"2025-04-19","objectID":"/l17-3d-vision/:3:0","tags":null,"title":"L17-3D Vision","uri":"/l17-3d-vision/"},{"categories":["UMich-EECS-498"],"content":"dataset ShapeNet Pix3D Mesh R-CNN, using Voxel and meshes ","date":"2025-04-19","objectID":"/l17-3d-vision/:4:0","tags":null,"title":"L17-3D Vision","uri":"/l17-3d-vision/"},{"categories":["UMich-EECS-498"],"content":"Reinforcement Learning So far, we have discussed supervised learning, and a little bit unsupervised learning (in UCB-data100 ğŸ˜‰) ","date":"2025-04-18","objectID":"/l21-reinforcement-learning/:0:0","tags":null,"title":"L21-Reinforcement Learning","uri":"/l21-reinforcement-learning/"},{"categories":["UMich-EECS-498"],"content":"What is Reinforcement Learning at time $t$, env $\\rightarrow^{state}$ agent $\\rightarrow^{action}$ env $\\rightarrow^{reward}$ agent, then env changed, agent learned, then repeated. state can be partial -\u003e noisy reward can be delayed, implicit and sparse -\u003e noisy AND Nondifferentiable ğŸ˜² Nonstationary environment, change over time ğŸ˜ Generative Adversarial Networks (GANs) somehow is a part of Reinforcement Learning. ","date":"2025-04-18","objectID":"/l21-reinforcement-learning/:1:0","tags":null,"title":"L21-Reinforcement Learning","uri":"/l21-reinforcement-learning/"},{"categories":["UMich-EECS-498"],"content":"MDP (Markov Decision Process) formalization of reinforcement learning problem: tuple $(S, A, P, R, \\gamma)$ Agent wanna find a policy $\\pi$ that maximizes the expected reward over time $S$ : set of states $A$ : set of actions $P(sâ€™|s,a)$ : state transition probability $R(s,a,sâ€™)$ : reward function $\\gamma$ : discount factor(tradeoff between immediate and future reward) How good is a policy $\\pi$? Value function $V_{\\pi}(s)$ : expected reward starting from state $s$ under policy $\\pi$ $$ V^{\\pi}(s) = \\mathbb{E}[\\sum_{t \\geq 0}\\gamma^tr_t | s_0=s, \\pi] $$ Q function $Q_{\\pi}(s,a)$ : expected reward starting from state $s$ and taking action $a$ under policy $\\pi$ $$ Q^{\\pi}(s,a) = \\mathbb{E}[\\sum_{t \\geq 0}\\gamma^tr_t | s_0=s, a_0=a, \\pi] $$ $$ Q^*(s,a) = \\max_{\\pi} \\mathbb{E}[\\sum_{t \\geq 0}\\gamma^tr_t | s_0=s, a_0=a, \\pi] $$ æ˜“æ¨å‡º $$ \\pi^*(s) = argmax_{aâ€™}Q(s,aâ€™) $$ ","date":"2025-04-18","objectID":"/l21-reinforcement-learning/:2:0","tags":null,"title":"L21-Reinforcement Learning","uri":"/l21-reinforcement-learning/"},{"categories":["UMich-EECS-498"],"content":"bellman equation Q-star satisfies the Bellman equation: $$ Q^*(s,a)=\\mathbb{E}[R(s,a)+\\gamma\\max_{aâ€™}Q^*(sâ€™,aâ€™)] $$ $R(s,a)$ : immediate reward $\\gamma \\max_{aâ€™}Q^*(sâ€™,aâ€™)$ : expected future reward if a function $V$ satisfies the Bellman equation, then it must be a Q-star if using Bellman equation to update from random to infinite, we will converge to $Q^*$ We can use DL to learn to approximate Q-star: DQL(Deep Q-learning) ","date":"2025-04-18","objectID":"/l21-reinforcement-learning/:2:1","tags":null,"title":"L21-Reinforcement Learning","uri":"/l21-reinforcement-learning/"},{"categories":["UMich-EECS-498"],"content":"Q-learning Q-learning is a simple and effective algorithm for learning Q-functions. Train a NN $Q_\\theta(s,a)$ to estimate future rewards for every (state, action) pair. move forward to Bellman equation (which is the loss function for training) Some problems are easier to learn a mapping from states to actions, others are harder. ","date":"2025-04-18","objectID":"/l21-reinforcement-learning/:3:0","tags":null,"title":"L21-Reinforcement Learning","uri":"/l21-reinforcement-learning/"},{"categories":["UMich-EECS-498"],"content":"Policy Gradient train a NN $\\pi_\\theta(a|s)$ that takes in a state and outputs a probability distribution over actionsâ€¦â€¦ Obj fn, expected reward under policy $\\pi_\\theta$: $J(\\theta)=\\mathbb{E}_{r \\sim p_\\theta}[\\sum_{t\\geq 0}\\gamma^tr_t ]$ then we can find the optimal policy by maximizing $\\theta^* = argmax_{\\theta} J(\\theta)$, using gradient ascent. can not find $\\partial J / \\partial \\theta$ directly General formulation: $J(\\theta) = \\mathbb{E}_{x \\sim p_\\theta}[f(x)]$, where $f(x)$ is a rewarding fn, and $x$ is trajectory of states, rewards, actions under policy $p_\\theta$. $$ \\begin{align} \\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{x \\sim p_\\theta} [f(x)] \u0026= \\frac{\\partial}{\\partial \\theta} \\int_{X} p_\\theta(x) f(x) dx = \\int_{X} f(x) \\frac{\\partial}{\\partial \\theta} p_\\theta(x) dx \\\\ \\because \\frac{\\partial}{\\partial \\theta} logp_\\theta(x) \u0026= \\frac{1}{p_\\theta(x)} \\frac{\\partial}{\\partial \\theta} p_\\theta(x) \\\\ \\therefore \\frac{\\partial J}{\\partial \\theta} \u0026= \\int_{X} f(x) p_\\theta(x) \\frac{\\partial}{\\partial \\theta} logp_\\theta(x) dx\\\\ \u0026= \\mathbb{E}_{x \\sim p_\\theta}[f(x) \\frac{\\partial}{\\partial \\theta} logp_\\theta(x)]\\\\ \\end{align} $$ then we can sampling to estimate the expectation (sth like Monte Carlo ğŸ¤”) let $x = (s_0, a_0, s_1, a_1, â€¦, s_t, a_t, r_t)$, random: $x \\sim p_\\theta(x)$ then write out the probability of $x$ under policy $\\pi_\\theta$ (with Markov property): $$ p_\\theta(x) = \\Pi_{t\\geq 0} P(s_{t+1}|s_t)\\pi_\\theta(a_t|s_t) \\Rightarrow logp_\\theta(x) = \\sum_{t\\geq 0} logP(s_{t+1}|s_t) + log\\pi_\\theta(a_t|s_t) $$ where $P(s_{t+1}|s_t)$ is the ENVâ€™s transition probability, can not compute! $\\pi_\\theta(a_t|s_t)$ is the NNâ€™s output (which is action probability), can compute! note that $P$ is not a function of $\\theta$, so we can write the gradient as: $$ \\frac{\\partial}{\\partial \\theta} logp_\\theta(x) = \\sum_{t\\geq 0} \\frac{\\partial}{\\partial \\theta} log\\pi_\\theta(a_t|s_t) $$ put it back into (4) $$ \\begin{align} \\frac{\\partial J}{\\partial \\theta} \u0026= \\mathbb{E}_{x \\sim p_\\theta}[f(x) \\frac{\\partial}{\\partial \\theta} logp_\\theta(x)]\\\\ \u0026= \\mathbb{E}_{x \\sim p_\\theta}[f(x) \\sum_{t\\geq 0} \\frac{\\partial}{\\partial \\theta} log\\pi_\\theta(a_t|s_t)]\\\\ \\end{align} $$ $x\\sim p_\\theta$: can be sampled from the policy, because it is a sequence of states, actions, rewards $f(x)$: rewarding function, can be observed and computed $\\frac{\\partial}{\\partial \\theta}log\\pi_\\theta(a_t|s_t)$: NNâ€™s output gradient w.r.t. model weights $\\theta$ so we come up to a algorithm: Initialize $\\theta$ Collect trajectories $x_t$ and get rewards $f(x)$ by running policy $\\pi_\\theta$ in the environment Compute the gradient of $J(\\theta)$ w.r.t. $\\theta$ Update $\\theta$ using gradient ascent Repeat 2-4 until convergence Intuition: When rewards $f(x)$ high, increase the probability of actions we took, vice versa. è¶‹åˆ©é¿å®³ need SO MANY data to trial and error what is the future from 2019? Actor-Critic Model-based RL Imitation Learning Inverse RL Adversarial RL, GANs ","date":"2025-04-18","objectID":"/l21-reinforcement-learning/:4:0","tags":null,"title":"L21-Reinforcement Learning","uri":"/l21-reinforcement-learning/"},{"categories":null,"content":"lessons learned and taught é¸¡æ±¤\u0026è¥é”€å·çŸ­æ–‡ ","date":"2025-03-06","objectID":"/beyondcode/thinking5/:0:0","tags":null,"title":"Thinking5","uri":"/beyondcode/thinking5/"},{"categories":null,"content":"è®­è¯« ä¸è¦åœ¨èƒ½é‡ä½è½çš„æ—¶å€™æ€è€ƒæœªæ¥ / åšå‡ºé‡è¦å†³ç­– / sth important â€¦â€¦ âš ï¸ è§„åˆ’ï¼Œè§„åˆ’ï¼Œè§„åˆ’ï¼æˆ˜ç•¥ï¼Œæˆ˜ç•¥ï¼Œæˆ˜ç•¥ï¼ äº«å—æ— æ„ä¹‰æ„Ÿï¼Œä¼‘æ¯ä¸è¦æ„§ç–šï¼Œå°¤å…¶æ˜¯å‰¥å‰Šè€…å’Œè¢«å‰¥å‰Šè€…æ˜¯åŒä¸€ä¸ªä¸ªä½“çš„æ—¶å€™ å¯ä»¥çš„è¯åˆ«å¼€å¤§ç»„ä¼š æ¯å¤©åšä¸€ä»¶æ²¡é‚£ä¹ˆèˆ’é€‚çš„äº‹æƒ… ä¸å¬æƒ…ç»ªåŒ–è¡¨è¾¾ï¼Œä¸é—»æ²¡æœ‰è¯æ®/ä¾æ®çš„è§‚ç‚¹ ","date":"2025-03-06","objectID":"/beyondcode/thinking5/:1:0","tags":null,"title":"Thinking5","uri":"/beyondcode/thinking5/"},{"categories":null,"content":"ä»–å±±ä¹‹çŸ³ ","date":"2025-03-06","objectID":"/beyondcode/thinking5/:2:0","tags":null,"title":"Thinking5","uri":"/beyondcode/thinking5/"},{"categories":null,"content":"tips for potential phd TL;DR: https://www.ruder.io/10-tips-for-research-and-a-phd/amp/ Read broadly. need a pool and a flow Work on two things. build a pipeline Be ambitious. again, work on two things Collaborate. Be proactive. This is probably the most important piece of advice. Donâ€™t restrict yourself to the people in your immediate circle. Reach out to people. The main value of conferences is in bringing people together. Before a conference, look up who is going (by checking authors of accepted papers) and email them. Try to be respectful, briefly introduce yourself, and state why youâ€™d like to meet them (a useful mnemonic is Inigo Montoyaâ€™s Guide to Networking Success). Most senior people make time for such meetings. Try to talk to many people and particularly seek out those who are not already well-known. Outside of conferences, it is often useful to ask people who have worked in your area for research advice via email. Itâ€™s amazing to see how many people in our field are genuinely helpful. Proper email etiquette is important, however and makes it more likely that a busy researcher will respond. In particular, you should make it clear that youâ€™ve done your research and explored alternative solutions before contacting them. Beyond advice, such connections may lead to other opportunities further down the line: job offers, collaborations, mentorship, and even friendship. Many of my collaborations started through such connectionsâ€”meetings at a conference, a cold email, a Twitter message. The important thing is that they are based on mutual interests and respect. So be conscious of otherâ€™s people time. In addition, early career researchers with shared interests will often be much more open to collaborations than senior researchers who already have many commitments. Being proactive also relates to how you view and talk about your research: Make it easy for other people to discover your work by highlighting it on your website, talking about it online, and writing a blog. Write a blog. Keep a source of positive energy. æ¯å¤©åšæŒåŠå°æ—¶ä»¥ä¸Šçš„å†¥æƒ³ï¼Œå¤©æ°”å¥½çš„è¯30min 5kmæ³•ç‰¹è±å…‹ Play to your strengths. Intern or visit a university. Play the long game. çæƒœï¼Œæ„Ÿæ©ï¼Œç²¾è¿› ","date":"2025-03-06","objectID":"/beyondcode/thinking5/:2:1","tags":null,"title":"Thinking5","uri":"/beyondcode/thinking5/"},{"categories":null,"content":"jbhuang-awesome-tips https://github.com/jbhuang0604/awesome-tips?tab=readme-ov-file#doing-research How to do research with my mentors effectively? Key idea: Help them help you! win-win, win, win, win. ","date":"2025-03-06","objectID":"/beyondcode/thinking5/:2:2","tags":null,"title":"Thinking5","uri":"/beyondcode/thinking5/"},{"categories":null,"content":"random tips learn to be more likeable count your blessings, I am grateful for â€¦â€¦ æ„Ÿæ©æ„Ÿæ¿€æ„ŸåŒ–æ„Ÿè°¢æ„Ÿå¿µ gather heroic moments, gonna be hero for sb, huh? // random acts of kindness million dollar ideas sometimes strike at 3 am, or when youâ€™re deep in thought showering take charge of your own attitude â€“ that ultimately determines your quality of life Sleeping is the most important thing in the world, should always come first. é›¶æ¶ˆè€—ï¼šä¸è½»æ˜“è·Ÿéšå¯¹æ–¹çš„æ€ç»´ä¸æƒ…ç»ªï¼Œä¸è½»æ˜“æœŸå¾…ï¼Œä¸è½»æ˜“å›åº”ï¼Œæˆ‘ä¸åŒæ„ä½ è¯´çš„æ¯ä¸€å¥è¯ï¼Œä½†æ˜¯æˆ‘å°Šé‡ä½ è¯´è¯çš„æƒåˆ©ã€‚ è¿™æ˜¯ç»¿è‰²ã€18åƒç´ å¤§å°çš„ Courier New å­—ä½“ è¿™æ˜¯ Times New Roman å­—ä½“ è¿™æ˜¯ Times New Roman å­—ä½“ ","date":"2025-03-06","objectID":"/beyondcode/thinking5/:2:3","tags":null,"title":"Thinking5","uri":"/beyondcode/thinking5/"},{"categories":["TOOLS"],"content":"uncond train, and eval ","date":"2025-03-05","objectID":"/tools/linux/cli/:1:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"train # vae OMP_NUM_THREADS=3 python main.py -b configs/autoencoder/kitti/autoencoder_c2_p4.yaml -t --gpus 0,1, -r logs/kitti/2025-02-27T11-19-07_autoencoder_c2_p4 OMP_NUM_THREADS=3 python main.py -b configs/autoencoder/kitti/autoencoder_c2_p4.yaml -t --gpus 3, # lidm both for cond and uncond OMP_NUM_THREADS=3 python main.py -b configs/lidar_diffusion/kitti/uncond_vig_dec0.yaml -t --gpus 0, OMP_NUM_THREADS=3 python main.py -b configs/lidar_diffusion/kitti/cond_t2l_vig_dec0.yaml -t --gpus 1,3 -r logs/kitti/2025-02-25T12-14-50_cond_t2l_vig_dec0 ","date":"2025-03-05","objectID":"/tools/linux/cli/:1:1","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"eval # sample and eval CUDA_VISIBLE_DEVICES=2 python scripts/sample.py -d kitti -r models/lidm/kitti/vig_dec0/epoch=000027.ckpt -n 2000 -s 250 --eval # for current best model, and specify the seed CUDA_VISIBLE_DEVICES=1 python scripts/sample.py -d kitti -r models/lidm/kitti/vig_dec0/epoch=000027.ckpt -n 2000 -c 25 --eval # for inference steps CUDA_VISIBLE_DEVICES=1 python scripts/sample.py -d kitti -r models/lidm/kitti/vig_dec0/last.ckpt -n 2000 --eval # eval only CUDA_VISIBLE_DEVICES=1 python scripts/sample.py -d kitti -f models/lidm/kitti/gcn_tr1d/samples/00452624/2025-02-03-15-44-42/numpy/samples.pcd --eval conditional sample # text2lidar OMP_NUM_THREADS=3 CUDA_VISIBLE_DEVICES=1 python scripts/text2lidar.py -r models/lidm/kitti/t2l/epoch=000020.ckpt -d kitti -p \"there is a crosswalk in front of the ego vehicle\" # æœ‰ä¸ªåå­—è·¯å£çš„åœºæ™¯ ","date":"2025-03-05","objectID":"/tools/linux/cli/:1:2","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"docker setup ","date":"2025-03-05","objectID":"/tools/linux/cli/:2:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"unix helpful commands ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"find and delete folders containing sth åœ¨Unixç³»ç»Ÿä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨findå‘½ä»¤ç»“åˆrmå‘½ä»¤æ¥åˆ é™¤æ‰€æœ‰åŒ…å«breakçš„æ–‡ä»¶å¤¹ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªå‘½ä»¤ç¤ºä¾‹ï¼Œå®ƒä¼šæŸ¥æ‰¾å½“å‰è·¯å¾„ä¸‹æ‰€æœ‰åŒ…å«breakçš„æ–‡ä»¶å¤¹ï¼Œå¹¶åˆ é™¤å®ƒä»¬ï¼š find . -type d -name '*2025-03-02-10-04-43*' -exec rm -rf {} + find . -type f -name '*00002*' -exec rm {} + ä¸‹é¢æ˜¯å‘½ä»¤çš„è¯¦ç»†è§£é‡Šï¼š find .ï¼šä»å½“å‰ç›®å½•å¼€å§‹æŸ¥æ‰¾ã€‚ -type dï¼šæŸ¥æ‰¾çš„ç±»å‹æ˜¯ç›®å½•ã€‚ -name â€˜*break*â€™ï¼šæŸ¥æ‰¾åç§°ä¸­åŒ…å«breakçš„ç›®å½•ã€‚ -execï¼šå¯¹æ‰¾åˆ°çš„æ¯ä¸ªé¡¹ç›®æ‰§è¡Œåé¢çš„å‘½ä»¤ã€‚ rm -rf {} +ï¼šåˆ é™¤æ‰¾åˆ°çš„æ¯ä¸ªç›®å½•åŠå…¶å†…å®¹ã€‚{}æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œä»£è¡¨findå‘½ä»¤æ‰¾åˆ°çš„æ¯ä¸ªé¡¹ç›®ï¼Œ+è¡¨ç¤ºå°†æ‰€æœ‰æ‰¾åˆ°çš„é¡¹ç›®ä¼ é€’ç»™rmå‘½ä»¤ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡ä¼ é€’ä¸€ä¸ªã€‚ ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:1","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"zip zip -r topo_1000.zip *.txt ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:2","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"peek the gpu usage watch -n 1 nvidia-smi ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:3","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"peek the cpu usage top OMP_NUM_THREADS=3 ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:4","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"kill a process kill -9 \u003cpid\u003e åœ¨ä½ æä¾›çš„æˆªå›¾ä¸­ï¼Œé€šè¿‡ ps aux | grep python å‘½ä»¤åˆ—å‡ºäº†åŒ…å« â€œpythonâ€ å…³é”®è¯çš„è¿›ç¨‹ã€‚ å¦‚æœä½ æƒ³è¦ç»ˆæ­¢æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ Python è¿›ç¨‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ pkill å‘½ä»¤ï¼š pkill -f python ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:5","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"peek the disk usage df -h ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:6","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"ssh to a remote server ssh -P \u003cport\u003e \u003cusername\u003e@\u003cremote_server_ip\u003e ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:7","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"nohup nohup python GLiDR_kitti.py --data /data/data_odometry_static_dynamic/scan/ --exp_name find_shapes_kitti_mlpdiff --beam 16 --dim 8 --batch_size 32 --mode kitti \u003e training_mlp_diff_100.log 2\u003e\u00261 \u0026 ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:8","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"cd cd - # å›åˆ°ä¸Šä¸€çº§ç›®å½• cd \"/path/to/directory with spaces\" # è¿›å…¥å¸¦æœ‰ç©ºæ ¼çš„ç›®å½• ","date":"2025-03-05","objectID":"/tools/linux/cli/:3:9","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"tmuxåå°ä½¿ç”¨é€»è¾‘ # install tmux sudo apt-get update sudo apt-get install tmux ","date":"2025-03-05","objectID":"/tools/linux/cli/:4:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"Session æ–°å»ºä¼šè¯ tmux new -s lidm ç¦»å¼€ä¼šè¯ Ctrl+b d é‡è¿ä¼šè¯ tmux attach -t my_session åˆ—å‡ºä¼šè¯ tmux ls æ€æ­»ä¼šè¯ Ctrl+b :kill-session ä¸Šä¸‹æ»šåŠ¨ Ctrl+b [ q ","date":"2025-03-05","objectID":"/tools/linux/cli/:4:1","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"Window åˆ›å»ºæ–°çª—å£ Ctrl+b c åˆ‡æ¢çª—å£ Ctrl+b p ï¼ˆä¸Šä¸€ä¸ªçª—å£ï¼‰ Ctrl+b n ï¼ˆä¸‹ä¸€ä¸ªçª—å£ï¼‰Ctrl+b \u003cnumber\u003e ï¼ˆæŒ‡å®šçª—å£ï¼‰Ctrl+b w ï¼ˆçª—å£åˆ—è¡¨ï¼‰ ä¸ºçª—å£å‘½å Ctrl+b , ","date":"2025-03-05","objectID":"/tools/linux/cli/:4:2","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"Panes åˆ‡åˆ†çª—æ ¼ Ctrl+b % å‚ç›´åˆ†å‰² Ctrl+b \" æ°´å¹³åˆ†å‰² è½¬æ¢çª—å£ Ctrl+b \u003carrow key\u003e å…³é—­å½“å‰ pane Ctrl+b x ","date":"2025-03-05","objectID":"/tools/linux/cli/:4:3","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"build essentials sudo apt-get update sudo apt-get install build-essential ","date":"2025-03-05","objectID":"/tools/linux/cli/:4:4","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"ssh docker image transfer ","date":"2025-03-05","objectID":"/tools/linux/cli/:5:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"docker run nvidia-docker run -it --name 'hz_lidargen' --shm-size 16g -v /data0/dataset/:/data -v /home/liujiuming/hz:/home/hz fzhiheng/deep_envs:py38-torch110-cu113-pointnet2 /bin/bash docker start hz_lidargen # lidardiff / lidargen / ultralidar docker exec -it hz_lidargen /bin/bash ","date":"2025-03-05","objectID":"/tools/linux/cli/:5:1","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"docker save and load docker save -o my-image.tar my-image:tag scp 'epoch=000037.ckpt' liujiuming@10.129.22.67:~/GLIDR/GLiDR-main/LiDAR-Diffusion-main/models/lidm/kitti/uncond_vig_dec1 # folder scp -r my-folder user@remote-server:/path/to/destination docker load -i /path/to/destination/my-image.tar ","date":"2025-03-05","objectID":"/tools/linux/cli/:5:2","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"ifconfig ifconfig 6012 10.129.22.57 6017 10.129.22.67 6016 10.129.22.66 ","date":"2025-03-05","objectID":"/tools/linux/cli/:5:3","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"Python Lib ","date":"2025-03-05","objectID":"/tools/linux/cli/:6:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"knn_cuda pip install --upgrade https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl -i http://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com ","date":"2025-03-05","objectID":"/tools/linux/cli/:6:1","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"pointnet2_ops_lib pip install git+https://github.com/erikwijmans/Pointnet2_PyTorch.git#subdirectory=pointnet2_ops_lib ","date":"2025-03-05","objectID":"/tools/linux/cli/:6:2","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"topologylayer pip install git+https://github.com/Topologic/topologylayer.git # topologylayer np.int --\u003e np.int64 ","date":"2025-03-05","objectID":"/tools/linux/cli/:6:3","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"ninja https://kimi.moonshot.cn/chat/ctjnjk8pe77ot9u80lvg sudo apt-get install ninja-build ","date":"2025-03-05","objectID":"/tools/linux/cli/:6:4","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"pip configs / æ¢æº / conda æ¢æº pip install open3d -i https://pypi.tuna.tsinghua.edu.cn/simple # ä¸´æ—¶ pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # æ°¸ä¹… conda config --show channels conda config --set show_channel_urls yes # åŠ è¿›å» conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ # conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ # æ³¨æ„windows prefers http instead of https ","date":"2025-03-05","objectID":"/tools/linux/cli/:7:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":["TOOLS"],"content":"lidargen / ultralidar commands export PYTHONPATH=/home/hz/mmdetection3d:$PYTHONPATH # in docker CUDA_VISIBLE_DEVICES=3 ./tools/dist_test.sh configs/ultralidar_kitti360_static_blank_code.py ./epoch_200.pth 1 --eval \"mIoU\" CUDA_VISIBLE_DEVICES=3 ./tools/dist_test.sh configs/ultralidar_kitti360_gene.py ./epoch_200.pth 1 --eval \"mIoU\" for viz draft export KITTI360_DATASET=/data/KITTI-360/ OMP_NUM_THREADS=3 CUDA_VISIBLE_DEVICES=3 python lidargen.py --sample --exp kitti_pretrained --config kitti.yml OMP_NUM_THREADS=3 CUDA_VISIBLE_DEVICES=3 python try.py --loop_count 200 --mid 4 OMP_NUM_THREADS=3 CUDA_VISIBLE_DEVICES=3 python try.py --loop_count 5000 --mid 4 ","date":"2025-03-05","objectID":"/tools/linux/cli/:8:0","tags":null,"title":"Useful Linux CLI Commands","uri":"/tools/linux/cli/"},{"categories":null,"content":"Excellent Sheep ä¼˜ç§€çš„ç»µç¾Š ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:0:0","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"å‰è¨€ æœ¬ä¹¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼š ç¬¬ä¸€éƒ¨åˆ†ï¼šè®¨è®ºç¾å›½ç²¾è‹±æ•™è‚²ç³»ç»Ÿçš„ç°çŠ¶åŠå…¶å¯¹å­¦ç”Ÿçš„å½±å“ã€‚ ç¬¬äºŒéƒ¨åˆ†ï¼šæ¢è®¨å­¦ç”Ÿå¦‚ä½•æ‘†è„±ç²¾è‹±æ•™è‚²ç³»ç»Ÿçš„æŸç¼šï¼Œæ‰¾åˆ°è‡ªå·±çš„äººç”Ÿæ–¹å‘ã€‚ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¼ºè°ƒåšé›…æ•™è‚²çš„é‡è¦æ€§ï¼Œä»¥åŠæ•™å¸ˆåœ¨åŸ¹å…»å­¦ç”Ÿç‹¬ç«‹æ€è€ƒèƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚ ç¬¬å››éƒ¨åˆ†ï¼šåˆ†æç²¾è‹±æ•™è‚²ç³»ç»Ÿå¯¹ç¤¾ä¼šçš„å½±å“ï¼Œæ¢è®¨å¦‚ä½•é€šè¿‡æ•™è‚²æ”¹é©å®ç°ç¤¾ä¼šçš„å…¬å¹³å’Œè¿›æ­¥ã€‚ â€”â€” è‡³äºæ•™è‚²åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä½ ä¸ºä»€ä¹ˆè¦è¯»å¤§å­¦ï¼Œå¤§å­¦å¦‚ä½•å¸®åŠ©ä½ æ‰¾åˆ°è‡ªæˆ‘ï¼Œæˆ–è€…è¯´å¤§å­¦å¦‚ä½•å¸®åŠ©ä½ ç‹¬ç«‹æ€è€ƒï¼Œæ‰¾åˆ°è‡ªå·±åœ¨è¿™ä¸ªä¸–ç•Œä¸Šçš„ä½ç½®ï¼Œè¿™äº›é—®é¢˜ä½ æ ¹æœ¬æƒ³éƒ½æ²¡æƒ³è¿‡ã€‚ ğŸ˜ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:1:0","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬ä¸€éƒ¨åˆ† ä¼˜ç§€çš„ç»µç¾Š â€”â€” ç»å¸¸æœ‰äººä¼šè¯´ï¼Œä»–ä»¬æ²¡æœ‰å»ä½“ä¼šè‡ªå·±çš„é’æ˜¥ï¼Œä»–ä»¬ä»æ²¡æœ‰ç”Ÿæ´»åœ¨å½“ä¸‹ï¼Œä»–ä»¬æ€»æ˜¯åœ¨è¿½é€ä¸€äº›æœªç»æ·±æ€ç†Ÿè™‘çš„ç›®æ ‡ã€‚ä»–ä»¬æ€»ä¼šæ€ç´¢ï¼Œæ›¾ç»çš„åŠªåŠ›æ˜¯å¦éƒ½å€¼å¾—ï¼Ÿ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:2:0","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬ä¸€ç«  é‚£äº›å¤´é¡¶å…‰ç¯çš„å¹´è½»äºº ç°çŠ¶ ğŸª å…‰é²œäº®ä¸½ vs å†…åœ¨çª’æ¯çš„ææƒ§ã€ç„¦è™‘ã€å¤±è½ã€æ— åŠ©ã€ç©ºè™šå­¤ç‹¬ å¹¶ä¸”ä¸ä¼šè‡ªæ„ˆ ç¼ºå°‘å¼ºå¤§çš„è‡ªç† è‡ªç«‹ è‡ªæ§èƒ½åŠ› å¹¶éä¸ªæ¡ˆ å¯¹å¤–æ²¡æœ‰æ—¶é—´å»ºç«‹çœŸæ­£çš„æ„Ÿæƒ… å¯¹æˆåŠŸå‹è¿«å¼çš„è¿½æ±‚ å†…å¿ƒçš„ææƒ§ å¯¹å†…æ²¡æœ‰å’Œè‡ªå·±å»ºç«‹æ·±å±‚è”ç³» æˆå¹´äººå¯¹è¿™äº›ç°è±¡æ²¡æœ‰æ„è¯†ï¼Œå®¡è§†çš„è§’åº¦é”™äº† å¹´è½»äººæ“…é•¿éšè—è‡ªå·±çš„é—®é¢˜ ä¸æ‡‚å¾—å¦‚ä½•æ€è€ƒ å¤§å±€çš„è®¤çŸ¥ æ— æ­¢å¢ƒçš„è¯¾å†…å¤–æ´»åŠ¨ï¼Œç‰ºç‰²äº†ç†æ™ºæ¢ç´¢çš„æœºä¼šï¼Œæ²¡æœ‰æ—¶é—´æ‰¾åˆ°æŒšçˆ± â€œæˆ‘å’Œæˆ‘çš„æœ‹å‹å¹¶éå»å°è¯•è¿‡ä¸Šåƒç§èŒä¸šé“è·¯ï¼Œæ¸¸éäº†ä¸–ç•Œå„åœ°ï¼Œæ‰å†³å®šè‡ªå·±è¦åšä»€ä¹ˆã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬éƒ½æ˜¯æŠ±ç€ä»ä¼—å¿ƒç†ï¼Œåœ¨ä¸€æ¡ä¹…ç»æµ‹è¯•è¿‡çš„èŒä¸šé“è·¯ä¸Šï¼Œæ­¥æ­¥è°¨æ…ï¼Œæ­¥æ­¥ä¸ºè¥ï¼Œç¡®ä¿è‡ªå·±èƒ½å¤Ÿåœ¨å‡ å¹´ä¹‹åæœ‰æ‰€æ”¶è·ã€‚åŒæ—¶è¿˜æ˜¯èƒ½åšå›å¹²ç»†èƒï¼Œä¸å¤§å®¶ä¸€æ ·ï¼Œå……æ»¡ç€æ— é™çš„å¯èƒ½ã€‚â€â€”â€”æŸç§å¿ƒæ€ â€œä¸‰è§’æ¬²â€ï¼šå½“ä½ è§‚å¯Ÿåˆ°ä¼—äººéƒ½åœ¨è¿½é€åŒä¸€æ ·ä¸œè¥¿çš„æ—¶å€™ï¼Œä½ åˆ¤æ–­å®ƒè‚¯å®šæ˜¯æœ‰ä»·å€¼çš„ã€‚ æ‰€æœ‰å†³å®šåŠ¨æœºçš„å…³é”®åœ¨äºå®‰å…¨æ„Ÿï¼Œè¢«ææƒ§é©±åŠ¨ æ›´å¤šçš„å­¦ç”Ÿåˆ™ä¼šé•¿æœŸæ·±é™·äºé‚£ç§é€‰æ‹©çš„è¿·èŒ«å’Œå‹åŠ›ä¹‹ä¸­ ä½†æ˜¯è¿›å…¥ç²¾è‹±æ•™è‚²ä¹‹å‰ï¼Œä¸æ˜¯ä¼˜ç§€çš„ç»µç¾Šã€‚ä¹‹åæ€€ç€â€œèµ„è´¨è‡³ä¸Šâ€çš„æƒ³æ³•ï¼Œè§†é‡å—é™åˆ¶ â€”â€” åå°”è¡—å¯¹å¤§å­¦ç”Ÿçš„å¿ƒæ€äº†å¦‚æŒ‡æŒã€‚é¡¶å°–å¤§å­¦æ¯•ä¸šçš„å­¦ç”Ÿæ˜¯ä¸€ç¾¤æå…¶èªæ˜ï¼Œä½†æ˜¯åˆå®Œå…¨ç¼ºä¹æ–¹å‘æ„Ÿçš„å¹´è½»äººã€‚è¿™äº›å¹´è½»äººæ‹¥æœ‰æœ€å¼ºçš„å¤§è„‘å’Œæ— å¯æŒ‘å‰”çš„æ•¬ä¸šç²¾ç¥ï¼Œä½†æ˜¯ä¸¥é‡ç¼ºä¹å¯¹è‡ªå·±çš„æ´å¯ŸåŠ›ã€‚ä»–ä»¬æŒ‰éƒ¨å°±ç­åœ°ç”Ÿæ´»ï¼Œç¼ºä¹æ–°ç”Ÿæ´»çš„æƒ³è±¡åŠ›ï¼Œåœ¨å†…å¿ƒæ·±å¤„ï¼Œä»–ä»¬ä¹Ÿç¼ºä¹å‹‡æ°”å’Œè‡ªç”±æ¥åˆ›é€ è‡ªå·±çš„é“è·¯ã€‚ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:2:1","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬äºŒç«  â€œå“ˆè€¶æ™®â€ä¸Šä½å² è®²äº†è®²ç¾å›½ç²¾è‹±æ•™è‚²ç³»ç»Ÿçš„èµ·æºã€æ¼”å˜åŠå…¶å¯¹ç¤¾ä¼šçš„å½±å“ ä¸‰é©¾é©¬è½¦ï¼šè´¢å¯Œã€èµ„å†å’Œå£°èª‰ ç«äº‰è¶‹å‘ç™½çƒ­åŒ–â€”â€”å”¯ä¸€çš„ç­”æ¡ˆæ”¶æ•›äºè¶…è¶Šä»–äºº ç³»ç»Ÿæ€§é—®é¢˜ â€”â€” å°±ç®—ä½ è‡ªå·±çš„å®¶åº­æˆ–çˆ¶æ¯æ˜¯ç†æ€§çš„ï¼Œä»–ä»¬ä¹Ÿä¼šæ˜¾å¾—åŠ›ä¸ä»å¿ƒï¼Œå› ä¸ºå…¶ä»–å®¶é•¿å¹¶ä¸ç†æ€§ï¼Œå› æ­¤å­¦æ ¡ä¹Ÿå°±ä¸ç†æ€§ï¼Œç”šè‡³æ•´ä¸ªç¤¾ä¼šéƒ½ä¸ç†æ€§ã€‚ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:2:2","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬ä¸‰ç«  â€œå¤©æ‰â€èƒŒåçš„â€œé­”é¬¼â€å¼è®­ç»ƒ è®²äº†è®²ç²¾è‹±è‚²å„¿æ¨¡å¼å’Œè¿‡åº¦ç«äº‰çš„æ•™è‚²ç¯å¢ƒï¼Œç›²ç›®çš„é‡å¿ƒï¼Œå¯¹é£é™©ä¹ æƒ¯æ€§çš„å›é¿ï¼Œéœå¸ƒæ–¯å¼çš„ç‰©è´¨ä¸Šçš„é‡è›®æ€§ç«äº‰ éœå¸ƒæ–¯è®¤ä¸ºï¼Œåœ¨è‡ªç„¶çŠ¶æ€ä¸‹ï¼Œèµ„æºæ˜¯ç¨€ç¼ºçš„ï¼Œäººä»¬ä¸ºäº†äº‰å¤ºæœ‰é™çš„èµ„æºï¼Œå¾€å¾€ä¼šé™·å…¥ â€œæ‰€æœ‰äººå¯¹æ‰€æœ‰äººçš„æˆ˜äº‰â€ çŠ¶æ€ã€‚è¿™ç§çŠ¶æ€æ˜¯ç”±äºäººç±»çš„æ¬²æœ›å’Œèµ„æºçš„ç¨€ç¼ºæ€§å¯¼è‡´çš„ï¼Œäººä»¬ä¸ºäº†æ»¡è¶³è‡ªå·±çš„éœ€æ±‚ï¼Œä¸å¾—ä¸ä¸ä»–äººç«äº‰ã€‚åœ¨è¿™ç§çŠ¶æ€ä¸‹ï¼Œäººä»¬çš„è¡Œä¸ºå¾€å¾€è¢«ææƒ§å’Œç«äº‰å¿ƒç†æ‰€é©±åŠ¨ï¼Œç¼ºä¹åˆä½œå’Œä¿¡ä»»ã€‚ ç¾å›½å¤§å­¦ç‹‚é£æš´é›¨çš„å½•å–ï¼šå®è´¨ä¸Šï¼Œè¿™æ˜¯èµ„æœ¬é˜¶å±‚å®¶åº­ç¡®è®¤è‡ªå·±åœ¨è¿™ä¸ªé˜¶å±‚é‡Œå…·ä½“æ’ä½çš„æ¸¸æˆã€‚ ä¸¤ç§è‚²å„¿ç»éªŒï¼šç›´å‡æœº / æººçˆ±å¼ï¼Œéƒ½æ˜¯æ¥è‡ªå¯¹å­©å­â€œè¿‡åº¦ä¿æŠ¤â€çš„å†²åŠ¨ å¹¼é¾„åŒ–ï¼šå­¦ç”Ÿä¼¼ä¹å¹¶ä¸æ’æ–¥è¢«å½“ä½œå°å­©å­å¯¹å¾… åŠ¨æœºï¼šä¸å°‘çˆ¶æ¯åˆ©ç”¨å­©å­çš„æˆå°±æ¥å¡«è¡¥è‡ªå·±è„†å¼±çš„å†…å¿ƒ æœ¬è´¨æ˜¯æ§åˆ¶ ä¼˜ç­‰ç”Ÿçš„çˆ¶æ¯å¾€å¾€æ„è¯†ä¸åˆ°ï¼Œç”šè‡³æœ‰æ„è¯†ä¸å»äº†è§£ä»–ä»¬çš„å­©å­çš„ç—›è‹¦æŒ£æ‰ï¼Œæ— æƒ…çš„å‹åŠ›\u0026å¯¹æˆç»©çš„æ— é™è¿½æ±‚ï¼Œæ˜¯å¤§é”™ç‰¹é”™çš„ å¦‚ä»Šæ•™è‚²ä¼¼ä¹å›´ç»•ç€å•ä¸€çš„ç›®æ ‡è¿›è¡Œå»ºè®¾ï¼Œä¼¼ä¹åœ¨å­©å­çš„æ½œæ„è¯†é‡Œé¢ç§ä¸‹â€œåªæœ‰è‡ªå·±æˆåŠŸæ‰å€¼å¾—è¢«çˆ±â€çš„ç§å­ï¼Œç»“æœæ˜¯æåº¦è‡ªå‘è¿˜æœ‰è‡ªè´Ÿäº¤é”™æŠ˜ç£¨ ä¸ºäº†è¿åˆçˆ¶æ¯çš„éœ€æ±‚è€Œå»ºç«‹èµ·æ¥çš„â€œè‡ªæˆ‘â€æ˜¯â€œä¼ªè‡ªæˆ‘â€ å­¦ç”Ÿå¾ˆéš¾å¯Ÿè§‰åˆ°è‡ªå·±çš„ç”Ÿæ´»å…¶å®æ˜¯å—åˆ¶äºä»–äººçš„ï¼ŒåŒæ ·çš„ï¼Œçˆ¶æ¯ä¹Ÿå¾ˆéš¾å¯Ÿè§‰è‡ªå·±æ˜¯â€œé‚£ç§â€çˆ¶æ¯ â€”â€” åœ¨æˆ‘ä¸å†å› ä»–äººçš„æˆç»©è€Œæ„Ÿåˆ°å¨èƒæ—¶ï¼Œæˆ‘çš„ç”Ÿå‘½å¾—åˆ°äº†è§£æ”¾ï¼Œé‚£ä»½è½»æ¾è®©æˆ‘å­¦ä¼šäº†æ¬£èµä»–äººçš„æˆå°±ç»™è¿™ä¸ªä¸–ç•Œæ‰€å¸¦æ¥çš„ç¾å¦™ã€‚ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:2:3","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬å››ç«  ä¸€æµåæ ¡æ˜¯å¦‚ä½•è¿è¡Œçš„ï¼Ÿ å†å²é—ç•™é—®é¢˜ï¼šä¼ ç»Ÿè‹±å›½ï¼ˆäººæ–‡ é“å¾·å“è´¨ï¼‰ vs å¾·å›½ï¼ˆtechï¼‰é«˜ç­‰æ•™è‚² æ•™å­¦è¿œè¿œå°äºç§‘ç ”ï¼Œç¢ç‰‡åŒ–å’Œä¸“ä¸šåŒ–çš„è¯¾é¢˜ä¸»å¯¼äº†æœ¬ç§‘æ•™è‚² æ•™è‚²å•†ä¸šåŒ–è¿ä½œï¼Œå­¦ç”Ÿ\u0026å®¶é•¿é€æ­¥å˜ä¸ºæ¶ˆè´¹è€…ï¼Œå¤§å­¦ä¹‹é—´çš„ç«äº‰æ›´åŠ æ¿€çƒˆ ä½†æ˜¯ç‰ºç‰²äº†é•¿æœŸåˆ©ç›Šï¼šå­¦æ ¡è¦åšçš„æ˜¯ï¼Œä¸æ–­åœ°å‘å­¦ç”Ÿæé—®ï¼Œè€Œä¸”é—®å¾—æœ€å¤šçš„é—®é¢˜åº”è¯¥æ˜¯ä»–ä»¬åˆ°åº•è¿½æ±‚ä»€ä¹ˆã€‚ é€šè¯†æ•™è‚²å‡å¼± æˆç»©è†¨èƒ€ï¼ŒçœŸå®çš„åé¦ˆæ¶ˆå¤±ï¼Œè¦æ±‚é€æ­¥å˜é«˜ï¼Œé—®é¢˜åœ¨äºï¼Œä½ å‚ä¸çš„äº‹æƒ…è¶Šå¤šï¼Œä½ èƒ½åšå¥½çš„äº‹æƒ…å°±è¶Šå°‘ï¼Œå¹¶ä¸”æœ€åä»€ä¹ˆäº‹æƒ…éƒ½åšå¾—ä¸ç†æƒ³ æ•™è‚²ç†æƒ³çš„ç¼ºå¤±ï¼Œéœ€è¦å­¦ç”Ÿå’Œå­¦æ ¡è¿›è¡Œåšå¼ˆï¼Œä»¥æ›´å¥½åœ°è·å¾—æ•™è‚² â€”â€” é¦–å…ˆï¼Œå¤§å­¦å‡ ä¹è®¤è¯†ä¸åˆ°å­¦ç”Ÿçš„é—®é¢˜æ‰€åœ¨ï¼Œå› æ­¤ä¹Ÿå°±ä¸ä¼šå¸®åŠ©å­¦ç”Ÿè®¤æ¸…ä»–ä»¬é«˜ä¸­æœŸé—´æ‰€å½¢æˆçš„ä¸è‰¯ä¹ æƒ¯å’Œæ‰­æ›²çš„ä»·å€¼è§‚ã€‚æ­£å› å¦‚æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå°±ä¸éš¾ç†è§£ï¼Œä¸ºä»€ä¹ˆä¼—äººçš„å¤§å­¦æ±‚å­¦ç»å†å§‹ç»ˆæ˜¯ä»–ä»¬é«˜ä¸­çš„ç¿»ç‰ˆã€‚ ğŸª ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:2:4","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬äºŒéƒ¨åˆ† è‡ªæˆ‘ å¤§å­¦çš„4å¹´ï¼Œä¹Ÿå°±æ˜¯é’å°‘å¹´å‘æˆå¹´è½¬å˜æœ€é»„é‡‘çš„4å¹´ï¼Œå€˜è‹¥ä»…ä»…æ˜¯ä¸ºäº†èŒä¸šåšå‡†å¤‡ï¼Œè€Œå¿½è§†äº†å…¶ä»–æ–¹é¢çš„åŸ¹å…»ï¼Œé‚£ç®€ç›´æ˜¯è’è°¬è‡³æã€‚å¦‚æœä½ åœ¨å¤§å­¦æ¯•ä¸šä¹‹é™…ä¸ä½ å…¥å­¦åˆæœŸå¹¶æ— åŒºåˆ«ï¼Œä½ çš„ä¿¡å¿µã€ä»·å€¼è§‚ã€æ„¿æœ›ä»¥åŠäººç”Ÿç›®æ ‡ä¾æ—§å¦‚æ•…ï¼Œé‚£ä¹ˆä½ å…¨ç›˜çš†è¾“ï¼Œå¿…é¡»é‡æ–°å¼€å§‹ã€‚ ğŸ˜­ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:3:0","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬äº”ç«  å¤§å­¦çš„ä½¿å‘½ ä¸»è¦å›´ç»•å¤§å­¦æ•™è‚²çš„çœŸæ­£ç›®çš„å±•å¼€è®¨è®ºï¼Œæ‰¹åˆ¤äº†å½“å‰ç²¾è‹±æ•™è‚²ä½“ç³»ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†å¤§å­¦æ•™è‚²åº”æœ‰çš„ä½¿å‘½å’Œç›®æ ‡ ğŸ¤” å¤§å­¦é¦–å…ˆçš„èŒè´£æ˜¯æ•™ä¼šå­¦ç”Ÿå¦‚ä½•æ€è€ƒï¼šç®€å•æ¥è®²ï¼Œå­¦ä¼šæ€è€ƒå°±æ˜¯ä»¥æ‰¹åˆ¤çš„çœ¼å…‰å®¡è§†èº«è¾¹ä»»ä½•äº‹ç‰©ï¼Œä¸è‡ªä»¥ä¸ºæ˜¯ï¼Œä¸å¦„ä¸‹ç»“è®ºã€‚æ¯å½“å­¦ä¹ ä¹‹å‰ï¼Œè®°å¾—â€œæ”¾ç©ºâ€ã€‚è®¤æ¸…ï¼Œè´¨ç–‘ï¼Œæ€è€ƒâ€”â€”çœŸæ­£çš„æ•™è‚²ï¼ˆåšé›…æ•™è‚²çš„é¦–è¦è´£ä»»ï¼‰ å¤§å­¦ç»™æ¯ä½å­¦å­æä¾›äº†ä¸€æ¬¡çœŸæ­£æ€è€ƒçš„æœºä¼šï¼šè¿œç¦»ç¤¾ä¼šï¼Œæš‚æ—¶æ‘†è„±æ¥è‡ªå®¶åº­å½±å“å’ŒèŒä¸šè§„åˆ’çš„æŸç¼šï¼Œç«™åœ¨è¿œå¤„ä»¥çº¯å‡€å¹¶æ‰¹åˆ¤çš„çœ¼å…‰å®¡è§†æ•´ä¸ªä¸–ç•Œã€‚ å¤§å­¦æ‰€èµ‹äºˆçš„è‡ªç”±ç®€ç›´æ˜¯ä¸€ç§ç‰¹æƒï¼Œä¸æ˜¯å—ï¼Ÿç»å¯¹æ˜¯ã€‚ä½ æ€ä¹ˆå¯ä»¥è½»æ˜“åœ°æŠ›å¼ƒå‘¢ï¼Ÿ å¤§å­¦çš„æ„ä¹‰æ˜¯å¸®åŠ©æˆ‘ä»¬ç”Ÿæ´»å¾—æ›´è­¦è§‰ï¼Œæ›´æœ‰è´£ä»»æ„Ÿï¼Œæ›´æœ‰è‡ªç”±åº¦å¹¶æ›´åŠ å®Œæ•´ çœŸæ­£çš„æ•™è‚²æ˜¯è®©å­¦ç”Ÿå¸¦ç€é—®é¢˜è¿æ¥ç¤¾ä¼šï¼Œè€Œä¸æ˜¯ç»™å­¦ç”Ÿä¸€ä»½ä¸ªäººç®€å†ã€‚ ä¸€ç§è‹æ ¼æ‹‰åº•å¼çš„è€å¸ˆbelike: è¿™ä½è€å¸ˆçš„æ•™å­¦æ–¹å¼ä¸è‹æ ¼æ‹‰åº•ï¼ˆå³æŸæ‹‰å›¾çš„è€å¸ˆï¼‰çš„æ–¹å¼å¦‚å‡ºä¸€è¾™ï¼šä»–å€¾å¬å­¦ç”Ÿçš„æ„è§ï¼Œå¯å‘å¼åœ°æŠŠé—®é¢˜æ˜ å°„å›å»ï¼Œæˆ–è€…è¿«ä½¿å­¦ç”Ÿæ¸…æ™°åœ°è¡¨è¾¾è‡ªå·±çš„è§‚ç‚¹ã€‚å­¦ç”Ÿå¿…é¡»å…ˆæ‰¿è®¤è‡ªå·±çš„è§‚ç‚¹æœ¬è´¨å¹¶ä¸ºå…¶è¾©æŠ¤ï¼Œåœ¨èšå…‰ç¯ä¹‹ä¸‹ä»¥ä¸¥è°¨çš„è§’åº¦å®¡è§†è‡ªå·±çš„è§‚ç‚¹ï¼Œä»è€Œæ¯ä½å­¦ç”Ÿéƒ½èƒ½å¤Ÿå‰–æè‡ªå·±çš„æƒ³æ³•ï¼Œä»¥æ‰¹åˆ¤å¼é€»è¾‘æ¥æ£€æŸ¥è‡ªå·±çš„æ€æƒ³ï¼Œè¿™æ ·å­¦ç”Ÿæ‰å¼€å§‹åŸ¹å…»å‡ºè‡ªå·±çš„ç†æ™ºã€‚æ•´ä¸ªè¿‡ç¨‹å°±æ˜¯è®©å­¦ç”Ÿåœ¨ä¸ç†Ÿæ‚‰ã€ä¸èˆ’é€‚çš„ç¯å¢ƒä¸­å¾—åˆ°é”»ç‚¼ï¼Œé€šè¿‡ä¸æ–­æå‡ºç–‘é—®ï¼Œè€Œä¸æ˜¯å¸æ”¶è€å¸ˆçš„è§‚ç‚¹ï¼Œå­¦ä¼šç‹¬ç«‹æ€è€ƒã€‚ æˆ‘ä»¬ç”Ÿæ´»ä¸­éœ€è¦çš„æ˜¯ï¼Œæœ‰ä¸“ä¸šäººå£«æé†’æˆ‘ä»¬åˆ°åº•å“ªé‡Œå‡ºé”™äº†ã€‚ å¤§å­¦æ‰€æä¾›çš„å¦ä¸€ä¸ªé‡è¦èµ„æºå°±æ˜¯æœå¤•ç›¸å¤„çš„åŒå­¦ è¯¾å ‚/å®¿èˆé—²èŠï¼šå‰è€…æ˜¯ä¸ºäº†å»ºç«‹ä¸€ç§å…±è¯†ï¼›åè€…æ˜¯ä¸ºäº†æ¨ç¿»å…±è¯† æœ€ç‰¹æ®Šçš„æ€è€ƒå¯¹è±¡â€”â€”ä½ è‡ªå·± å»ºç«‹è‡ªæˆ‘ï¼ˆè‡ªæˆ‘çš„å†…æ¶µï¼‰æ˜¯æ¯ä¸ªäººå¿…é¡»è¦ä¸ºè‡ªå·±åšçš„ä¸€ä»¶äº‹ï¼Œè¿‡ç¨‹å°†å……æ»¡æŒ‘æˆ˜å’Œå›°éš¾ ğŸ˜© ä»€ä¹ˆæ˜¯ä¼˜è´¨çš„ç”Ÿæ´»ï¼Ÿå¦‚ä½•å®ç°ï¼Ÿ å¤§å­¦é‡Œå›°éš¾ã€æƒ…æ„Ÿï¼Œæ‰€å¬æ‰€é—»æ‰€æ„Ÿæ‰€è§æ‰€æ€æ‰€æƒ³æ‰€è§¦ï¼Œä¿ƒä½¿ä½ è´¨ç–‘ä½ ä¹‹å‰æ‰€æœ‰çš„è‡ªæˆ‘è®¤çŸ¥ï¼Œå…¨èº«å¿ƒæŠ•å…¥ï¼Œé¿å…æ‰å…¥ä»–äººå“ºå–‚ç»™ä½ çš„æ€æƒ³ä»¥åŠä¸ºä½ è®¾è®¡æ¢¦æƒ³çš„â€œäºŒæ‰‹ç”Ÿæ´»â€ â€œåšé›…æ•™è‚²å°†è´¨ç–‘æ‰€æœ‰äº‹ç‰©ï¼Œå› æ­¤å®ƒè¦æ±‚å­¦ç”Ÿæ„¿æ„æ¥å—æ¨ç¿»å…ˆå‰æ‰€æœ‰è®¤çŸ¥çš„é£é™©ã€‚åªæœ‰è¿™æ ·ï¼ŒçœŸæ­£çš„åšé›…æ•™è‚²æ‰èƒ½å½»åº•æ”¹å˜ä¸€ä¸ªå­¦ç”Ÿçš„ç”Ÿå‘½â€ å¤§å­¦æ˜¯æˆ‘ä»¬åº”è¯¥å¼€å§‹ä¸ºè‡ªå·±åšå†³å®šçš„åˆå§‹é˜¶æ®µã€‚ æ¯ä¸ªäººåœ¨å¤§å­¦æœŸé—´çœŸæ­£éœ€è¦åŸ¹å…»çš„æ˜¯åæ€çš„ä¹ æƒ¯ï¼Œå³æ‹¥æœ‰ä»å˜åŒ–ä¸­æˆé•¿çš„èƒ½åŠ›ã€‚ è‡ªæˆ‘æ„è¯†æ˜¯ä¸€ä¸ªæå…¶ç§å¯†çš„ç©ºé—´ï¼Œåœ¨è¿™ä¸ªç©ºé—´é‡Œï¼Œä½ èƒ½æ‰¾åˆ°è‡ªå·±çš„åŠ›é‡ã€å®‰å…¨æ„Ÿã€è‡ªä¸»ã€åˆ›é€ åŠ›å’Œå¿«ä¹ã€‚äººå¯ä»¥ä¸€ç›´å¿™å¿™ç¢Œç¢Œï¼Œä¸éœ€è¦çµé­‚ï¼Œåªéœ€è¦æ”¾å¤§çš„è‡ªæˆ‘å’Œæ„å¿—åŠ›ï¼Œä½†æ˜¯ä¸ä¹å†…å¿ƒç©ºè™šè€…ã€‚ å¤§å­¦çš„ä½¿å‘½æ˜¯æˆå°±ä¸€ä¸ªæ›´æœ‰æ„æ€çš„ä½ ï¼Œä½ è®¤è¯†åˆ°ä½ å°†æ˜¯é™ªä¼´è‡ªå·±ç»ˆå…¶ä¸€ç”Ÿçš„å”¯ä¸€äººé€‰ï¼šä¸€ä¸ªäººä¹‹æ‰€ä»¥æœ‰æ„æ€ï¼Œæ˜¯å› ä¸ºä»–å¤§é‡é˜…è¯»ï¼Œä¹ æƒ¯æ€è€ƒï¼Œæ”¾ç¼“è„šæ­¥ï¼ŒæŠ•å…¥æ·±åº¦å¯¹è¯ï¼Œå¹¶ä¸ºè‡ªå·±åˆ›å»ºäº†ä¸€ä¸ªä¸°æ»¡çš„å†…å¿ƒä¸–ç•Œã€‚ â€”â€” è¿™ç§äººç¾¤æ ¹æœ¬ä¸èƒ½è¡¨è¾¾ä¸â€œæˆ‘â€ç›¸å…³çš„æ¦‚å¿µï¼Œæ¯”å¦‚â€œæˆ‘æƒ³è¦â€ï¼Œå› ä¸ºâ€œæˆ‘æƒ³è¦â€çš„å‰ææ˜¯è¦æ˜ç™½â€œæˆ‘æ˜¯è°â€ï¼Œå› æ­¤è¿™äº›äººåªä¼šè¯´â€œè¦é’±â€â€œè¦è±ªå®…â€æˆ–â€œè¦å“ˆä½›â€ï¼ˆæœ€ç»ˆè·Ÿâ€œæˆ‘â€æ²¡æœ‰å…³ç³»ï¼‰ã€‚ğŸ˜ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:3:1","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬å…­ç«  åˆ›å»ºè‡ªå·±çš„ç”Ÿæ´» æ–¹å‘ â€œæˆ‘æ“…é•¿åšä»€ä¹ˆï¼Ÿâ€â€œæˆ‘å…³å¿ƒä»€ä¹ˆï¼Ÿâ€â€œæˆ‘åšä¿¡ä»€ä¹ˆï¼Ÿâ€ï¼Œè‡ªæˆ‘äº†è§£æ˜¯æœ‰æ•ˆçš„å·¥å…·ã€‚æ‰¾åˆ°è‡ªæˆ‘çš„å‰ææ˜¯è§£æ”¾è‡ªæˆ‘ï¼Œä¸è¢«å¤–ç•Œæ‰€å¹²æ‰°ï¼Œå€¾å¬å‘¼å”¤ å¯¹åˆ›æ–°éœ€è¦æ–°çš„æ€è€ƒï¼šå¦ä¸€ç§åˆ›æ–°â€”â€”ç”Ÿæ´»åˆ›æ–°ï¼Œâ€œç²¾ç¥æƒ³è±¡åŠ›â€æ„æŒ‡ä¸€ä¸ªäººæœ‰èƒ½åŠ›æ‘†è„±å›ºæœ‰çš„ç”Ÿæ´»æ–¹å¼ï¼Œä»¥æœ€å¤§å°ºåº¦å»æƒ³è±¡æ‰€æœ‰çš„å¯èƒ½æ€§ï¼Œåˆ›é€ å±äºè‡ªå·±çš„ç”Ÿæ´»æ–¹å¼ã€‚ å¯¹å‹‡æ°”çš„æ€è€ƒï¼šè‚‰ä½“å’Œç²¾ç¥ã€‚ç²¾ç¥å‹‡æ°”çš„ä¸æ˜“ä¹‹å¤„å°±æ˜¯ï¼Œä¸ªä½“å¿…é¡»åªèº«ä½œæˆ˜ã€‚è¦æœ‰å£®å£«æ–­è…•çš„å‹‡æ°”ä¸ä¼ ç»Ÿå’Œå¹³åº¸å†³è£‚ï¼Œä¸å†…å¿ƒé‚£ä¸ªå£°éŸ³å¹¶è‚©ä½œæˆ˜ï¼ç²¾ç¥å‹‡æ°”è€…ï¼Œæ›´æ„¿æ„æ‰¿æ‹…é£é™© ä¼ ç»Ÿå¤§ä¼—çš„å¤–ç•Œå£°éŸ³ï¼Œæˆ–è®¸æˆ‘ä»¬å°±æ˜¯å…¶åˆ›é€ è€…è€Œä¸è‡ªçŸ¥ å¦‚ä½•æ‰¾åˆ°çƒ­çˆ±ï¼Ÿ é€‰æ‹©åšä¸€äº›è‡ªå‘çº¯ç²¹çš„äº‹æƒ…ï¼Œå°±å¦‚åŒä½ å°æ—¶å€™é‚£æ ·ï¼› é€‰æ‹©åšä¸€äº›å³ä½¿æ²¡æœ‰å¤–åœ¨å¥–åŠ±ä½ ä¹Ÿä¼šé€‰æ‹©åšçš„äº‹æƒ…ï¼› é€‰æ‹©åšä¸€äº›ä½ å¯ä»¥åºŸå¯å¿˜é£Ÿåœ°ä¸“æ³¨å»åšçš„äº‹æƒ…ï¼› åšä½ æœ€å–œæ¬¢åšçš„äº‹ï¼Œä¸æ˜¯ä½ è®¤ä¸ºè‡ªå·±å–œæ¬¢æˆ–è€…åº”è¯¥å–œæ¬¢çš„ï¼Œè€Œæ˜¯ä½ çš„çœŸçˆ±ã€‚ åœ¨æ»¡è¶³åŸºç¡€ç‰©è´¨æ¡ä»¶ä¹‹ä¸Šï¼Œä¸€ä¸ªäººçš„å¹¸ç¦æ„Ÿæ¥è‡ªå¥åº·çš„ç¤¾äº¤åœˆä»¥åŠä»äº‹æœ‰æ„ä¹‰çš„å·¥ä½œï¼Œé¸¡æ±¤æ–‡å­¦ã€‚ç„¶é¹…æ‰¾ä¸åˆ°çƒ­çˆ±ï¼Œä¼šæœ‰ææƒ§æ„Ÿï¼Œè®°ä½æ„ä¹‰åœ¨äºä½ åœ¨åšä»€ä¹ˆï¼Œè€Œä¸æ˜¯ä½ æ˜¯ä»€ä¹ˆã€‚ç”Ÿæ´»åº”è¯¥æ˜¯å¯æŒç»­æ€§çš„ç”Ÿæ´» ä¸€ä¸ªäººå¯ä»¥ä¸ºäº†ç­‰å¾…æœªæ¥çš„æ”¶è·ï¼Œæ— é™æœŸå¿å—å·¥ä½œçš„ç…ç†¬ï¼Œå»¶è¿Ÿè‡ªå·±çš„æ»¡è¶³æ„Ÿæˆ–è€…æ”¾å¼ƒäº«å—å·¥ä½œæœ¬èº«ï¼Œä½†æ˜¯é—æ†¾ä¼¼ä¹ä¸å¯é¿å…ä¼šåœ¨è¿‡ç¨‹ä¸­äº§ç”Ÿã€‚ å§‹ç»ˆæœ‰å¯»æ‰¾çƒ­çˆ±çš„è‡ªç”±ï¼šç°ä»£ç¤¾ä¼šç»™äºˆäº†ä¸ªäººæ€è€ƒå’Œé€‰æ‹©çš„è‡ªç”±ï¼Œè¿™æ—¢æ˜¯ä¸€ä»¶ç¤¼ç‰©ä¹Ÿæ˜¯ä¸€ä¸ªè´Ÿæ‹…ã€‚è‡ªç”±å¯èƒ½å¸¦æ¥ææ…Œå’Œè¿·å¤±ï¼Œå› æ­¤æˆ‘ä»¬æ›´å®¹æ˜“é€‰æ‹©æ”¾å¼ƒï¼Œè®©ä»–äººå‘Šè¯‰æˆ‘ä»¬åº”è¯¥æ€ä¹ˆå»åšï¼Œä½†æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬è‡³å°‘ä¸èƒ½æ— è§†è¿™ç§è‡ªç”±çš„å­˜åœ¨ã€‚åŒæ—¶ï¼Œé€Ÿæˆ˜é€Ÿå†³å¦‚æ­¤ä»¤äººå…´å¥‹ï¼ŒåŒæ—¶ä¹Ÿä»¤äººå®‰å¿ƒã€‚é¢å¯¹å¤§å­¦çš„å„ç§é™Œç”Ÿå’Œä¸ç¡®å®šï¼Œæˆ‘ä¸çŸ¥æ‰€æªï¼Œå› æ­¤æˆ‘çš„ç¬¬ä¸€ååº”ä¸æ˜¯å¼€å¯å¤§é—¨ï¼Œè€Œæ˜¯è¦è¿…é€ŸæŠŠå®ƒå…³é—­ã€‚ â€œé‡åˆ°-è§£å†³é—®é¢˜+æ‰¾åˆ°è‡ªæˆ‘â€æœ‰å±€é™ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œç†æƒ³æ‹¥æœ‰æ›´å¼ºå¤§çš„åŠ›é‡ï¼Œæ¯”ä¸–ç•Œä¸Šä»»ä½•ä¸œè¥¿éƒ½çè´µï¼Œå®ƒç»™äºˆæˆ‘ä»¬åŠ›é‡æŠµæŒ¡åœ°ä½ã€è´¢å¯Œå’ŒæˆåŠŸçš„è¯±æƒ‘ã€‚ æ³¨æ„æ˜¯å¦æ›¾ç»é‡Šæ”¾è¿‡ä¿¡å· æœ‰äº†æ–¹å‘ï¼Œå¹¶ä¸æ˜¯ä¸€å¸†é£é¡ºï¼Œåè€Œå¯èƒ½å›°éš¾é‡é‡ï¼ é£é™© æœ‰ä¸€ä¸ªå‰æï¼šé“²é™¤å¿ƒé‡Œæ½œæ„è¯†çš„ç§å­â€”â€”å®³æ€•å¤±è´¥ã€‚ä¸€ç›´æˆåŠŸï¼ˆå³ä»æœªå¤±è´¥ï¼‰å¹¶ä¸æ˜¯èƒ½åŠ›çš„ä½“ç°ï¼Œè€Œæ˜¯è„†å¼±çš„è¡¨ç°ï¼Œå› ä¸ºå‡ºäºå®³æ€•å¤±è´¥ï¼Œä¸ªä½“å¾€å¾€æ”¾å¼ƒä¸€äº›æœ¬æ¥èƒ½å¤Ÿé€ å°±ä»–æˆå°±è‡ªå·±çš„æœºä¼šã€‚ ä¸è®ºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼Ÿå¤§çš„è¿˜æ˜¯å¹³å‡¡çš„ï¼Œæˆ‘ä»¬å°è¯•èµ°è‡ªå·±çš„é“è·¯çš„ç†ç”±æ˜¯ï¼šè¿™å°†æ˜¯è‡ªå·±çš„ç”Ÿæ´»ï¼Œè‡ªå·±çš„é€‰æ‹©ï¼Œè‡ªå·±çš„é”™è¯¯ã€‚ç”Ÿå‘½æœ¬æ¥å°±æ˜¯ä¸€åœºé•¿é€”æ—…è¡Œï¼Œé€æ¸æ‰èƒ½å‘ç°è‡ªå·±æœ€çœŸå®çš„ç”Ÿæ´»æ–¹å¼ï¼Œå‰ææ˜¯ï¼Œä½ çš„æ–¹å‘æ˜¯å¯¹çš„ã€‚ é¢å¯¹ææƒ§ã€‚å¿…é¡»é™ä¼å®ƒï¼Œç›¸å½“ä¸€éƒ¨åˆ†ææƒ§ç¼ºä¹ç«‹è¶³çš„é€»è¾‘ ä½†æ˜¯ä¸ªäººæ¢ç´¢æ˜¯æœ‰æˆªæ­¢æ—¶é—´çš„ï¼Œå¹´è½»äººä¸åº”è¯¥æƒ§æ€•è‡ªå·±ç”Ÿæ´»ä¸­æ‰€ç»å†çš„å†²åŠ¨å’Œç–‘è™‘â€”â€”å³ä½¿å®ƒä»¬å¾ˆæœ‰å¯èƒ½è¿«ä½¿è‡ªå·±åç¦»ç”šè‡³æ”¹å˜è‡ªå·±åŸæ¥çš„äººç”Ÿè½¨é“ è™½ç„¶æˆ‘å¼ºè°ƒåˆ›å»ºè‡ªå·±çš„ç”Ÿæ´»æ–¹å¼ï¼Œä½†æ˜¯æˆ‘åŒæ ·è¦å¼ºè°ƒä¸ºä¹‹ä»˜å‡ºçš„ä»£ä»·ã€‚â€œå¯»æ‰¾ä½ çš„çƒ­çˆ±â€çš„åŒæ—¶ï¼Œä½ å¿…é¡»æ¸…æ¥šâ€œä½ è¦ä¸ºæ­¤åšå‡ºç‰ºç‰²â€ï¼ˆè€Œè¿™ç§ç‰ºç‰²å¯èƒ½ä¸ä»…ä»…æ˜¯æ”¾å¼ƒä½ å¯èƒ½è·å¾—çš„ç¤¾ä¼šåœ°ä½ï¼‰ ä¸ºäº†å­¦ä¹ è€Œå­¦ä¹ vsä¸ºäº†åæ¬¡è€Œå­¦ä¹ ï¼Œæœ¬è´¨ä¸Šä¸å¯è°ƒå’Œ ä¸å…¶ä¸ºæˆåŠŸè€Œå·¥ä½œï¼Œä¸å¦‚ä¸ºå·¥ä½œæœ¬èº«è€Œå…¨èº«å¿ƒæŠ•å…¥ï¼Œè¯„åˆ¤æ ‡å‡†åªæœ‰ä¸€ä¸ªï¼šæ˜¯å¦è¿‡ä¸Šæ»¡æ„çš„ç”Ÿæ´»ï¼Ÿ â€œåšè‡ªå·±â€ä¼¼ä¹è¢«å£å·åŒ–äº†ï¼Œæˆä¸ºç‹¬ç«‹ä¸ªä½“ï¼Œæ‹¥æœ‰ç²¾ç¥å‹‡æ°”æ˜¯ä¸å¯èƒ½é€šè¿‡é…å¤‡è£…é¥°çš„æ–¹å¼è€Œå®ç°çš„ã€‚æ€»è€Œè¨€ä¹‹ï¼Œå¦‚æœä½ æœªæ›¾æ”¾å¼ƒä»€ä¹ˆï¼Œé‚£æ ¹æœ¬å°±è°ˆä¸ä¸Šç²¾ç¥å‹‡æ°”ã€‚æŒ«æŠ˜ã€ç‰ºç‰²ã€å†…å¿ƒæŒ£æ‰ã€å‡ºå¸ˆä¸åˆ©ã€èµ°å¼¯è·¯ã€ä¸å®¶äººå’Œæœ‹å‹ä¹‹é—´çš„çŸ›ç›¾ç­‰ï¼Œè¿™äº›æ‰ç®—æ˜¯çœŸæ­£æ„ä¹‰ä¸Šèµ°å‘ç‹¬ç«‹çš„ç‰¹å¾ã€‚ è‡ªæ¬ºæ¬ºäººæ˜¯å¦ä¸€ç§é£é™©ï¼Œæˆ‘ä»¬éƒ½æ— æ³•å¿½ç•¥ä¸€ä¸ªå¤§å‰æâ€”â€”ç°å®ï¼Œé‡‘é’±æ˜¯åŸºç¡€ï¼Œä½†æ˜¯ä¸æ˜¯å†³å®šå› ç´ ã€‚è¿™é‡Œçš„æ ¸å¿ƒé—®é¢˜æ˜¯ä¸€ä¸ªäººçš„å†…å¿ƒè‡ªç”±åº¦ã€‚ä½ åœ¨20å¤šå²æ‰€èƒ½å¤Ÿæ‰¿å—çš„ä¸ç¡®å®šæ€§çš„ç¨‹åº¦ä»¥åŠèƒ½å¤Ÿç®¡ç†çš„è´¢å¯Œå–å†³äºä½ æ˜¯è°ã€‚æ€æƒ³å¯ä»¥æ‰©å±•ï¼Œä»·å€¼è§‚å¯ä»¥æ”¹å˜ï¼Œä½†æ˜¯æˆ‘ä»¬çš„ä¸ªæ€§ä¸€æ—¦å½¢æˆå°±å¾ˆéš¾å˜åŒ– å¹³æ—¶ç”Ÿæ´»æœ´ç´ æœ¬èº«å°±æ˜¯ä¸€ç§è‡ªç”±ï¼Œå› ä¸ºä½ èƒ½æ›´é€‚åº”ç®€å•çš„ç”Ÿæ´»ã€‚ ç»æµåŸºç¡€è¾ƒå·®ï¼Œæœ¬èº«è¯•é”™æˆæœ¬è¾ƒä½ é‡å»ºè‡ªå·±çš„ç”Ÿæ´»æ˜¯ä¸€ç§ç‰¹æƒï¼Œæœ‰æœºä¼šè¿½éšè‡ªå·±çš„çƒ­æƒ…æ˜¯æ¯ä¸ªäººæœ€ç»ˆåº”å¾—çš„æƒåˆ©ï¼Œå³ä½¿å…¬å¼€åå¯¹è¿™äº›ä¸äº‰çš„äº‹å®ï¼Œæˆ‘ä»¬ä¹Ÿå¹¶ä¸èƒ½ä½¿å®ƒä»¬å®Œå…¨æ¶ˆå¤± å¾—åˆ°çˆ¶æ¯çš„æ”¯æŒå›ºç„¶é‡è¦ï¼Œä½†ç›¸å¯¹è€Œè¨€ï¼Œå­¦ä¼šä¸å—çˆ¶æ¯å·¦å³æ›´åŠ é‡è¦ï¼Œè¿™ä¹Ÿæ˜¯æˆç†Ÿçš„è¡¨ç° å½“ç„¶åå›å¹¶éä¸€è¹´è€Œå°±ï¼Œè€Œæ˜¯ä¸ªç¼“æ…¢çš„è¿‡ç¨‹ï¼Œæ¯ä¸ªäººéƒ½éœ€è¦ç©ºé—´æµ‹è¯•æé™ï¼Œä½†æ˜¯å¦‚æœä¸€ä¸ªäººä¸è·¨å‡ºç¬¬ä¸€æ­¥ï¼Œè®¤ä¸ºæ¯ä¸€æ­¥éƒ½æå…·é£é™©ï¼Œé‚£ä¹ˆæˆé•¿å°†æ°¸è¿œä¸ä¼šå¼€å§‹ã€‚ ä¸çˆ¶æ¯çš„äº¤æµé¢‘ç‡ä¸è¶…è¿‡ä¸€å‘¨ä¸€æ¬¡ï¼Œæœ€å¥½æ˜¯ä¸€ä¸ªæœˆä¸€æ¬¡ åŒæ—¶ï¼Œä¹Ÿä¸è¦æœŸæœ›æ±‚åŠ©äºçˆ¶æ¯ èŠ±æ—¶é—´ä¼‘æ•´æˆ–è€…æ”¾æ…¢è„šæ­¥ï¼Œè·³å‡ºæ°¸æ— æ­¢å¢ƒçš„ååˆ©è¿½é€çš„é™·é˜±ï¼Œè„±ç¦»æ— æ—¶æ— åˆ»çš„è¢«ç®¡ç†çš„çŠ¶æ€ï¼Œå»æ¢ç´¢å­¦æ ¡ä¹‹å¤–çš„ä¸–ç•Œï¼Œå»å‘å±•ä½ ä¸€ç›´ä»¥æ¥æ²¡æœ‰æœºä¼šè·å¾—çš„æŠ€èƒ½ã€‚è„±ç¦»æ¡†æ¶å¼çš„ç”Ÿæ´»ã€‚å¯»æ‰¾ç­”æ¡ˆçš„å”¯ä¸€é€”å¾„å°±æ˜¯ä»˜å‡ºè¡ŒåŠ¨ é¢å¯¹æ— ç©·çš„é€‰æ‹©æ— ä»ä¸‹æ‰‹ï¼Œä½ é€‰æ‹©äº†ä¸€æ¡è·¯ï¼Œåœ¨è¿™æ¡é“è·¯ä¸Šå…‹æœåƒè¾›ä¸‡è‹¦å¾’æ­¥å‰è¿›ï¼Œä½ ä¼šé‡è§ä¸åŒçš„äººï¼Œå‘ç°æ–°é²œæœªçŸ¥çš„ä¸–ç•Œï¼Œä»è€Œä½ ä¼šæ€è€ƒè¿™ä¸ªä¸–ç•Œèƒ½å¤Ÿç»™ä½ å¸¦æ¥ä»€ä¹ˆï¼ˆå†µä¸”è¿™ä¸ªä¸–ç•Œæ— æ—¶æ— åˆ»åœ¨å˜ï¼‰ï¼Œä»¥åŠä½ èƒ½å¤Ÿç»™è¿™ä¸ªä¸–ç•Œå¸¦æ¥ä»€ä¹ˆã€‚é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œé‡è§é‚£ä¸ªæ›´å¥½çš„è‡ªå·±ã€‚ åˆ›å»ºè‡ªå·±çš„ç”Ÿæ´»å¹¶ä¸æ„å‘³ç€ä½ å¯ä»¥æˆå°±ä»»ä½•äº‹æƒ…ã€‚å¿«ä¹æ¥è‡ªæˆ‘ä»¬èƒ½å¤Ÿå‘æŒ¥è‡ªå·±æ“…é•¿çš„èƒ½åŠ›ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å„è‡ªæ‹¥æœ‰æŸäº›ç‰¹é•¿ï¼Œä½†åªæ˜¯æŸäº›ã€‚ åˆ›å»ºè‡ªå·±çš„ç”Ÿæ´»ä¹Ÿå¹¶ä¸æ„å‘³ç€ä¸–ç•Œå›´ç»•ç€è‡ªå·±è½¬åŠ¨ã€‚åœ¨å·¥ä½œçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šç»å†ç„¦è™‘ã€æŒ«è´¥ã€ç¾è€»ç­‰ç­‰ï¼Œç”šè‡³æœ‰äº›æ—¶å€™æƒ³æ”¾å¼ƒï¼Œé‡æ–°åšé€‰æ‹©ï¼Œç¨ç¨åšæŒ æœ€åè¡¥å……ä¸€å¥ï¼Œåˆ›å»ºè‡ªå·±çš„ç”Ÿæ´»å¹¶éä¸æ€è¿›å–ã€‚ä½ è¿˜æ˜¯è¦åŠªåŠ›å·¥ä½œï¼Œå°¤å…¶æ˜¯åœ¨èµ·æ­¥é˜¶æ®µã€‚ä¸åŒçš„æ˜¯ï¼Œå½“ä½ æ‰€åšçš„äº‹æƒ…å…·æœ‰å¼ºå¤§çš„ä½¿å‘½æ„Ÿæ—¶ï¼Œä½ ä¼šæ„Ÿå—åˆ°æ— æ¯”çš„æˆå°±æ„Ÿã€‚ â€”â€” æ¬²åº¦å…³å±±ï¼Œä½•æƒ§ç‹‚æ¾œï¼Œé£ç”Ÿæ°´èµ·ï¼Œæ­£å¥½æ‰¬å¸†ï¼Œæé†’è‡ªå·±è¦æ”¾æ…¢è„šæ­¥ç»™è‡ªå·±æ€è€ƒçš„ç©ºé—´ï¼Œç»™è‡ªå·±æœºä¼šã€‚å½“ä½ è¯•å›¾æ‹’ç»ä¸ç¡®å®šæ€§ï¼Œé‚£ä¹ˆä½ å°±æ‹’ç»äº†ç”Ÿå‘½çš„æ„ä¹‰ã€‚å¯èƒ½ä¼šè½åï¼Œä¹Ÿè®¸å§ã€‚ä½†æ˜¯å¦‚æœæ–¹å‘é”™äº†ï¼Œè·‘å¾—å¿«åˆæœ‰ä½•æ„ä¹‰å‘¢ï¼Ÿè¿™æ—¶åœæ­¢å°±æ˜¯å‰è¿›ï¼ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:3:2","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬ä¸ƒç«  èªæ˜äººçš„é“å¾·å’Œè´£ä»» å¤§å­¦æ•™è‚²çš„ç›®çš„ä¸æ­¢åœåœ¨ä¸ªä½“å±‚é¢ ä»€ä¹ˆæ˜¯é¢†å¯¼åŠ›ï¼Ÿä»€ä¹ˆæ˜¯æœªæ¥çš„ç¤¾ä¼šé¢†å¯¼è€…ï¼Ÿ äº‹å®ä¸Šï¼Œé¡¶çº§å¤§å­¦æ‰€å€¡å¯¼çš„é¢†å¯¼åŠ›ä¸ç¤¾ä¼šè¿›æ­¥çš„å…³ç³»ç›¸å½“è–„å¼±ï¼Œä¸è¿‡å»å¤§å­¦æ‰€æå€¡çš„ç¤¾ä¼šè¿›æ­¥æ›´ç›¸å»ç”šè¿œï¼Œæ›´ä¸ç”¨è¯´ä¸é¢†å¯¼åŠ›çš„æœ¬æ„æ‰€å­˜åœ¨çš„è·ç¦»äº†ã€‚ å…¶æ ¸å¿ƒå°±æ˜¯ï¼Œåªè¦æŒæƒï¼Œä¸åœ¨ä¹ä½ æ‰€é€‰æ‹©çš„é¢†åŸŸ å®˜åƒšç³»ç»Ÿé‡Œï¼Œddå½’å…¶åŸå› æ˜¯å› ä¸ºï¼Œåœ¨è¿™ç§ä½“åˆ¶ä¸‹ï¼Œä¸Šå‡çš„é€Ÿåº¦å–å†³äºä¸ªäººåœ¨ä½“åˆ¶å†…å‘¨æ—‹çš„å¨´ç†Ÿç¨‹åº¦ï¼Œè€Œéä¸“ä¸šèƒŒæ™¯çš„å“è¶Šã€‚ ä¸å…¶åŸ¹å…»é¢†å¯¼è€…ï¼Œä¸å¦‚è‡´åŠ›äºåŸ¹å…»å…¬æ°‘ and æ€æƒ³å®¶ æœ€å¥½çš„é¢†å¯¼è€…å°±æ˜¯æ€æƒ³å®¶ã€‚æ€æƒ³å®¶å…·æœ‰å›é€†æ€ç»´ï¼šä¸åªæ˜¯è§£ç­”é—®é¢˜ï¼Œè¿˜æå‡ºæ–°çš„é—®é¢˜ï¼›ä¸åªæ˜¯å®Œæˆä»»åŠ¡ï¼Œè¿˜æ€è€ƒä»»ä½•ä¸€ä»¶äº‹æƒ…çš„ä»·å€¼æ‰€åœ¨ï¼›ä¸åªæ˜¯å‹‡äºå†²é”‹é™·é˜µï¼Œæ›´é‡è¦çš„æ˜¯ä¸ºå…¬å¸ã€è¡Œä¸šä¹ƒè‡³æ•´ä¸ªå›½å®¶åˆ¶è®¢æ–°çš„æˆ˜ç•¥æ–¹å‘ã€‚ æ‹¥æœ‰é¢†å¯¼åŠ›ä¸äºšäºé‡å»ºè‡ªå·±ï¼Œæœ€æ ¸å¿ƒçš„å…ƒç´ æ˜¯å‹‡æ°”å’Œæƒ³è±¡åŠ›ã€‚æœ€æ ¸å¿ƒçš„ä»»åŠ¡æ˜¯ï¼Œç­¹å»ºä¸€ä¸ªæœ‰èƒ½åŠ›ä¸è¿™ä¸ªç¤¾ä¼šç°çŠ¶å¯¹æŠ—çš„ä¸ªä½“ã€‚éœ€è¦ æ„å¿—åŠ›ï¼Œ æ‰¾å‡ºå¹¶æ­ç¤ºæ£˜æ‰‹çš„é—®é¢˜ åªæœ‰å½“ä½ çš„å†…å¿ƒæˆ–è€…çµé­‚æ„Ÿå—åˆ°ç°åœ¨çš„è‡ªæˆ‘å’Œè¿½æ±‚çš„è‡ªæˆ‘ä¹‹é—´çš„å·®è·æ—¶ï¼Œå˜åŒ–æ‰ä¼šéšä¹‹äº§ç”Ÿã€‚ ä¸ºä»€ä¹ˆåœ¨é¡¶å°–é«˜æ ¡å°±è¯»çš„å­¦ç”Ÿä¸è‡ªå·±æ‰€å¤„çš„ç³»ç»Ÿå¦‚æ­¤äº²å¯†ï¼Ÿé‚£æ˜¯å› ä¸ºä»–ä»¬åœ¨è¿™ä¸ªç³»ç»Ÿé‡Œå¦‚é±¼å¾—æ°´ã€‚ä»¥è‡³äºç¼ºä¹é­„åŠ›andæƒ³è±¡åŠ›ã€‚è¯•é—®æ–°ä¸€ä»£ä¸­ï¼Œåˆ°åº•æœ‰å¤šå°‘äººè€ƒè™‘è¿‡æŠ•èº«æ”¹å˜ç¤¾ä¼šçš„æ¶æ„ï¼›å°±ç®—æ€è€ƒè¿‡ï¼Œåˆæœ‰å¤šå°‘äººæ„¿æ„å»è¡ŒåŠ¨å‘¢ï¼Ÿä»»ä½•ç†æƒ³ã€ç†å¿µã€è¿œå¤§æŠ±è´Ÿä¸è¿‡æ˜¯20ä¸–çºªçš„æŒ£æ‰ã€‚è¿™ç§ç§‘æŠ€ä¸»ä¹‰è‡³ä¸Šçš„æ€è·¯æ­£åœ¨ç»Ÿé¢†ç€ç°ä»£é«˜ç­‰æ•™è‚²ï¼Œå…¶æ‰€ç¼ºå¤±çš„æ­£æ˜¯æ•´ä½“å¤§å±€è§‚çš„æ•™è‚²ï¼Œç¼ºå¤±çš„æ˜¯å¯¹ç¤¾ä¼šæœ€æ ¹æœ¬å­˜åœ¨çš„å®¡è§†ã€‚ é—®é¢˜å…³é”®æ˜¯ï¼Œä½ æ˜¯å¦çŸ¥é“è‡ªå·±åšäº‹çš„ç†å¿µå’ŒåŸåˆ™æ˜¯ä»€ä¹ˆï¼Ÿå¦‚æœä¸æ¸…æ¥šï¼Œé‚£ä¹ˆä½ å¾ˆæœ‰å¯èƒ½éšæ³¢é€æµï¼Œæ¥å—æœ€æ—¶é«¦çš„äº‹ç‰©ï¼Œå¹¶ä¸”ä¸çŸ¥é“ä½ çš„è¨€è¡Œæ­£åœ¨å—å…¶å½±å“ã€è¢«å…¶æµ¸æŸ“ã€‚ äººä»¬æ™®é€šæŠ±ç€ä¸€ç§è¿œç¦»çº·æ‰°çš„å¤§ç¯å¢ƒçš„å¿ƒæ€ï¼Œæ¯”å¦‚è¿œç¦»æ”¿æ²»èˆå°ï¼Œåƒç¦§å¹´ä¸€ä»£å°¤ä¸ºå¦‚æ­¤ã€‚ å€˜è‹¥æŒæƒè€…æ˜¯ä¸ä¸Šå¿ƒçš„æ”¿æ²»å®¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ‰€åšçš„å°äº‹å°†æ°¸è¿œå¾®ä¸è¶³é“ã€‚è¿œç¦»æ”¿æ²»å¹¶ä¸èƒ½æ¶ˆé™¤æ”¿æ²»æ— æ‰€ä¸åœ¨çš„å½±å“åŠ›ã€‚æ”¿æ²»æ˜¯ä¸€åœºä¸‘é™‹çš„ã€æ¼«é•¿çš„æˆ˜äº‰ã€‚å‚ä¸è¿™åœºæˆ˜äº‰çš„ç»å¤§å¤šæ•°äººéƒ½å¤„äºç¬¬ä¸€çº¿ã€‚ â€œæœåŠ¡ä»–äººâ€åŒæ ·è¢«ä¸¥é‡æ›²è§£ æ”¹å˜ä¸–ç•Œéœ€è¦æ˜¯æˆ‘ä»¬çš„å…±åŒç›®æ ‡ï¼Œä½†æ˜¯æ¯ä¸ªäººéƒ½éœ€è¦å¯»æ‰¾ç¬¦åˆè‡ªå·±çš„é“è·¯ã€‚å“²å­¦å®¶çˆ±é»˜ç”Ÿå‘¼åï¼Œæˆ‘ä»¬æ¯ä¸ªäººéƒ½å¿…é¡»å‘åŠ¨ä¸ªäººé©å‘½ï¼ŒæŠŠè‡ªå·±ä»ç°ä¸–çš„æ„è¯†å½¢æ€ä¸­è§£æ•‘å‡ºæ¥ å¤§å­¦æ•™è‚²çš„æ„ä¹‰çš„ç¡®è¿œè¶…ä¸ªäººæ•™è‚²ã€‚å¦‚æœä½ å°±æ˜¯å¤§å­¦æœ¬æ„è¦åŸ¹å…»çš„é‚£ç§é¢†å¯¼è€…ï¼Œé‚£ä¹ˆè‡³å°‘ä½ å°±åº”è¯¥å…·å¤‡è´¨ç–‘çš„èƒ½åŠ›ï¼Œå¹¶ä¸”é¦–å…ˆåº”è¯¥è´¨ç–‘ä½ è‡ªå·±æ‰€æ¥å—çš„å¤§å­¦æ•™è‚² ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:3:3","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬ä¸‰éƒ¨åˆ† æˆ‘ä»¬åˆ°åº•åº”è¯¥åœ¨å¤§å­¦é‡Œå¾—åˆ°ä»€ä¹ˆï¼Ÿ æˆ‘ä»¬æ‰€å­¦çš„åœ¨10å¹´å†…å°±ä¼šè¢«æ·˜æ±°ã€‚æœ€é‡è¦çš„æ˜¯å­¦ä¼šå¦‚ä½•å­¦ä¹  ğŸ¤“â˜ï¸ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:4:0","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬å…«ç«  åšé›…æ•™è‚²å’Œäººæ–‡ç»å…¸ ç»ä¹…ä¸è¡°çš„æœ€ä½³æ–¹æ¡ˆå°±æ˜¯ä»¥äººæ–‡ä¸ºä¸­å¿ƒã€ç”±æ•¬ä¸šçš„æ•™æˆä¸»å¯¼å°ç­æ•™å­¦çš„åšé›…æ•™è‚²ã€‚ åšé›…æ•™è‚²æ˜¯ä»€ä¹ˆ åšé›…æ•™è‚²å®šä¹‰æ¶µç›–äº†è‡ªç„¶ç§‘å­¦å’Œç¤¾ä¼šç§‘å­¦ã€‚ç®€å•æ¥è®²ï¼Œåšé›…æ•™è‚²è¿½æ±‚å­¦è¯†çš„ç›®çš„æ˜¯å­¦è¯†æœ¬èº«ï¼Œå³ä¸€ç§çº¯å‡€çš„æ±‚å­¦ç†å¿µã€‚ åšé›…æ•™è‚²æ˜¯æ¢ç©¶å’Œè¿½æ±‚çœŸç†çš„æ•™è‚²ï¼Œè€Œéä¸ºäº†ä»»ä½•å½¢å¼çš„å®ç”¨æ€§å›æŠ¥ã€‚ åšé›…æ•™è‚²æ‰€æ¢ç©¶çš„æ˜¯çŸ¥è¯†çš„äº§ç”Ÿè¿‡ç¨‹ï¼Œæ˜¯å¯¹çŸ¥è¯†çš„æº¯æºï¼Œè€Œä¸æ˜¯å»æ¥å—ç°æœ‰çš„çŸ¥è¯†ï¼›å­¦ç”Ÿä¸æ˜¯å¸æ”¶çŸ¥è¯†ï¼Œè€Œæ˜¯å¯¹æ–°æ—§çŸ¥è¯†è¿›è¡Œæ€è¾¨ã€‚å­¦ä¼šè®ºè¯ï¼ æ€ä¹ˆåšï¼Ÿ æ¥å—çŸ¥è¯†çš„æ·±åº¦ï¼Œå‰æ²¿ä¸Šfeel your way æ¥å—çŸ¥è¯†å¹¿åº¦ï¼Œä¸ä»…è¦å­¦ä¼šæ€è€ƒï¼Œè¿˜è¦å­¦ä¼šä¸åŒçš„æ€è€ƒæ–¹å¼ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä½ å­¦ä¼šäº†è‡ªæˆ‘å¼•å¯¼ ä¸ºä»€ä¹ˆï¼Ÿ åŸ¹å…»åˆ›é€ æ€§äººæ‰ ç°åœ¨çš„ç¤¾ä¼šï¼Œä¿¡æ¯å”¾æ‰‹å¯å¾—ï¼Œå…³é”®åœ¨äºæ˜¯å¦æ‡‚å¾—å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨ä¿¡æ¯ã€‚ åšé›…æ•™è‚²çš„ç»ˆæç›®æ ‡å¹¶éå®ç”¨ä¸»ä¹‰ï¼Œè€Œæ˜¯åŸ¹å…»ä½ è¶…è¶Šç©ºé—´å’Œæ—¶é—´æ¥æ€è€ƒé—®é¢˜ã€ä¸å—å·¥ä½œæ€§è´¨æ‰€é™åˆ¶çš„èƒ½åŠ› æ„å»ºè‡ªå·±å¹¶éç©ºç©´æ¥é£ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„åŠæ³•æ˜¯å‘å‰äººå€Ÿé‰´æ™ºæ…§ã€‚ æˆ‘ä»¬åœ¨äº«ç”¨ä»–ä»¬ç¡•æœçš„æ—¶å€™ï¼Œé’ˆå¯¹çš„å¹¶ä¸æ˜¯æŸä¸ªå›ºå®šé¢†åŸŸæˆ–æŸç§èŒä¸šï¼Œè€Œæ˜¯äººæ€§ æ”¹å˜å…¶ä»–äººçš„ç”Ÿæ´» ã€ä¸“æ‰\u0026é€šæ‰ã€‘åšé›…æ•™è‚²çš„æœ€ç»ˆç›®çš„æ˜¯åˆäºŒä¸ºä¸€çš„ï¼Œå­¦ç”Ÿåœ¨åšé›…æ•™è‚²ä¸­æ‰€è·å¾—çš„æ™ºæ…§å°†è´¯ç©¿æœªæ¥çš„ä»»ä½•ä¸“ä¸šæˆ–é¢†åŸŸï¼Œæˆ‘ä»¬å°†ä¸å†åˆ»æ„åŒºåˆ†å·¦è¾¹æ˜¯å·¥ä½œï¼Œå³è¾¹æ˜¯ç”Ÿæ´»ï¼›ä¸Šé¢æ˜¯é€šè¯†ï¼Œä¸‹é¢æ˜¯ä¸“ä¸šã€‚å®ƒä»¬æ˜¯èåˆçš„ç»Ÿä¸€ä½“ï¼Œæ˜¯ç›¸äº’æ¸—é€çš„ã€‚ å¦‚ä»Šå¯»æ±‚çœŸç†çš„é€”å¾„å˜å¾—å¤šå…ƒåŒ–ï¼Œç»å†ä¹Ÿæ›´åŠ ä¸ªæ€§åŒ–ï¼Œäººä»¬æ‘’å¼ƒäº†è¿‡å»çš„æ•™æ¡å¼æ¨¡å¼ï¼Œé€æ¸æ¥åˆ°äººæ–‡ä¸»ä¹‰/å”¯ç¾ä¸»ä¹‰/è‰ºæœ¯ æˆ‘ä»¬åœ¨é˜…è¯»æ–‡ç« æˆ–è€…æ¬£èµè‰ºæœ¯å“æ—¶ï¼Œæœ€å…³é”®çš„é—®é¢˜ä¸æ˜¯æˆ‘æ˜¯å¦çœ‹æ˜ç™½äº†ï¼Œè€Œæ˜¯è¯¥ç¯‡æ–‡ç« æˆ–è¯¥ä»¶è‰ºæœ¯å“æ˜¯å¦èƒ½å¼•èµ·æˆ‘çš„å…±é¸£ï¼Œä»è€Œå¸®åŠ©æˆ‘æ›´æ‡‚å¾—è‡ªå·±ã€‚è¿™ä¹Ÿåº”è¯¥æ˜¯å¤§å­¦æ•™è‚²çš„ä½œç”¨â€œè¿™å°±æ˜¯æˆ‘â€çš„æ¬¢å‘¼å°±æ˜¯è‰ºæœ¯ä½œå“çš„æœ€é«˜å¢ƒç•Œã€‚æˆ‘ä»¬ä»ä»–äººèº«ä¸Šçœ‹åˆ°äº†è‡ªå·±ï¼Œä»–äººä¹Ÿåœ¨æˆ‘ä»¬èº«ä¸Šçœ‹åˆ°ä»–ä»¬è‡ªå·± å¯ä»¥æ‰¾åˆ°ç”Ÿæ´»çš„èŒƒæœ¬/ç¼©æœ¬ï¼Œäº‹å®ä¸Šï¼Œç”Ÿæ´»ä¸­çš„ä¸€åˆ‡éƒ½å¯ä»¥åœ¨è‰ºæœ¯ä¸­æ‰¾åˆ° è‰ºæœ¯ä¸å…¶ä»–å½¢å¼çš„åˆºæ¿€çš„æœ¬è´¨åŒºåˆ«åœ¨äºï¼šå®ƒä¸ä»…ä»…ä¸ºè¯»è€…æä¾›äº†ç”Ÿæ´»æ¦œæ ·ï¼Œè€Œä¸”ä¹Ÿæä¾›äº†è´¨ç–‘è‰ºæœ¯æœ¬èº«çš„æ–¹æ³•ã€‚é˜…è¯»æ–‡å­¦ä½œå“è¦æ±‚çš„æ˜¯ä¸€ä»½å®¡æ…çš„æ€åº¦ï¼Œè€Œéå†²åŠ¨ã€‚ çœ‹é€ä»–ï¼Œçœ‹å‡ºâ€œä¸ªäººæ¬²æœ›â€ æˆ‘ä»¬éœ€è¦ç ”ç©¶äººç±»æ˜¯å¦‚ä½•æ€è€ƒçš„ï¼Œäººç±»çš„è¿½æ±‚æ˜¯ä»€ä¹ˆï¼Œè¡Œä¸ºä¹ æƒ¯æ˜¯ä»€ä¹ˆç­‰ç­‰ï¼ŒåŒæ—¶æˆ‘ä»¬è¦äº†è§£æœªæ¥çš„è§„åˆ’ä¸­å¯èƒ½å­˜åœ¨çš„ç¼ºé™·ã€‚äººæ–‡è‰ºæœ¯ä»¥ç®€å•çš„æ–¹å¼å¡«è¡¥äº†ç¤¾ä¼šç§‘å­¦çš„ç¼ºå¤±ã€‚ åŒæ—¶ä¹Ÿèƒ½äº†è§£è¿‡å»ï¼šåœ¨ä¸ªäººå±‚é¢ï¼Œäº†è§£è‡ªå·±çš„è¿‡å»ï¼Œä½ æ‰ä¼šæ˜ç™½è‡ªå·±è¨€è¯­èƒŒåçš„åŠ¨æœº äººæ–‡ç»å…¸æœ‰å…¶ä»·å€¼ï¼Œæ¯ä¸ªäººéƒ½åº”è¯¥å­¦ç‚¹ åŸ¹å…»å…¬æ°‘æ„è¯†æ˜¯å­¦ä¹ å¤å…¸å­¦çš„é¦–è¦ç›®æ ‡ï¼Œä½†æ˜¯æ›´å…³é”®çš„äº‹æƒ…æ˜¯å­¦ä¹ åè‘—ï¼Œè€Œä¸ä»…é™äºè¥¿æ–¹èŒƒç•´çš„åè‘— â€”â€” æ—¢ç„¶ç°åœ¨å¯ä»¥ä¸åŒäºè¿‡å»ï¼Œé‚£ä¹ˆæœªæ¥ä¹Ÿå¯ä»¥ä¸åŒäºç°åœ¨ã€‚è¦æˆä¸ºä¸€ä½é¢†å¯¼è€…ï¼Œè¿‡å»å°±æ˜¯æœ€åˆé€‚çš„å¼€ç«¯ã€‚ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:4:1","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬ä¹ç«  æ•™å¸ˆçš„æœ¬èŒå’Œå­¦ç”Ÿæœ€æ ¸å¿ƒçš„èƒ½åŠ› æ•™å¸ˆå·¥ä½œçš„æœ¬è´¨æ˜¯æ¿€å‘å¹¶å”¤é†’æ½œä¼åœ¨æ¯ä¸ªå­¦ç”Ÿä½“å†…å¤„äºç¡çœ çŠ¶æ€çš„èƒ½é‡ã€‚ å®¶åº­ä¹‹å¤–çš„æˆå¹´äººå¯¹å­©å­çš„æˆé•¿èµ·å…³é”®ä½œç”¨ã€‚ å¯¼å¸ˆä¸å±€é™äºæŠ€æœ¯çŸ¥è¯†ï¼Œæ›´å¤šçš„æ˜¯å€¾å¬ï¼Œå¸®åŠ©å­¦ç”Ÿå¬åˆ°è‡ªå·±çš„å£°éŸ³ è®²è¯¾çš„æ—¶å€™åˆ†äº«ä¸€å®šçš„äººç”Ÿ æ—å¾åšå¼•ï¼Œå…¨èƒ½çš„ æ”¹å˜äº†ç”Ÿæ´» æ€è€ƒçš„åŸ¹å…»ä¸èº«ä½“æ–¹é¢çš„æŠ€èƒ½åŸ¹å…»æœ¬è´¨ä¸Šæ²¡æœ‰å·®å¼‚ï¼Œæˆ‘ä»¬ä¸è¦å¿˜å´ï¼Œå¤§å­¦è‡´åŠ›äºåŸ¹å…»å­¦ç”Ÿçš„æœ€æ ¸å¿ƒçš„èƒ½åŠ›æ˜¯å­¦ä¼šåˆ†æä»–äººçš„è§‚ç‚¹å¹¶é˜è¿°è‡ªå·±çš„è§‚ç‚¹ã€‚â€œè‡ªæˆ‘å®šä½â€ä¹Ÿæ˜¯ä¸€ç§éœ€è¦è·å–çš„èƒ½åŠ› å­¦ä¹ æ–°äº‹ç‰©éœ€è¦åœ¨å°ç­ç¯å¢ƒä¸­å¾—åˆ°è¶³å¤Ÿçš„æœºä¼šå°è¯•å’Œé”»ç‚¼ï¼Œå¹¶æ ¹æ®è‡ªå·±çš„å¤©èµ„å’Œéœ€æ±‚æ¥å—ä¸€å¯¹ä¸€çš„è¾…å¯¼ã€‚å¤§å­¦è‡´åŠ›äºåŸ¹å…»å­¦ç”Ÿçš„æœ€æ ¸å¿ƒçš„èƒ½åŠ›æ˜¯å­¦ä¼šåˆ†æä»–äººçš„è§‚ç‚¹å¹¶é˜è¿°è‡ªå·±çš„è§‚ç‚¹ã€‚ è®²è¯¾æ˜¯ä¸€ç§è½åçš„æ•™å­¦æ–¹å¼ï¼Œå¯¹æ­¤æˆ‘è¡¨ç¤ºåŒæ„ã€‚å› æ­¤æˆ‘ä»¬çš„è¯¾å ‚åº”è¯¥å°è€Œç²¾ï¼Œæ–¹ä¾¿ç ”è®¨.è¯¾å ‚æ˜¯ä¸€ç§ç¼“æ…¢è‰°è‹¦çš„è¿‡ç¨‹ã€‚ â€”â€” æœ¬ç§‘æ•™è‚²\u0026æ•™å¸ˆé¢ä¸´çš„æŒ‘æˆ˜çš„è¯„ä»·å‚è§ã€Šç”Ÿå­˜æ‰‹å†Œã€‹ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:4:2","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬åç«  éšè—çš„å¸¸æ˜¥è—¤ åæ ¡ä¹‹å¤–çš„ï¼Œèƒ½å¤Ÿå¼¥è¡¥åæ ¡ä¸è¶³çš„é€‰æ‹©ï¼ŸğŸ¤” å­¦ç”Ÿä¹‹é—´çš„å½±å“æ˜¯æ—¶æ—¶åˆ»åˆ»ï¼Œå®å®åœ¨åœ¨çš„ å­¦ç”ŸèƒŒæ™¯å¤šå…ƒåŒ–ï¼Œæœ‰åˆ©ã€‚åæ ¡åŒè´¨åŒ–èƒŒæ™¯ï¼Œä¸åˆ©ã€‚â€œå­¦æ ¡ä¹‹é—´æœ€å…³é”®çš„å·®å¼‚æ°¸è¿œæ˜¯å­¦ç”Ÿç¾¤ä½“â€ æœ‰åšé›…æ•™è‚²çš„é¡¹ç›®ï¼Œäººæ–‡å­¦ç§‘æ•™å­¦è´¨é‡ä¸å·® å¤§å­¦æ•™è‚²çš„å‡ºå£ä¸å…¥é—¨åŒç­‰é‡è¦ï¼Œå› æ­¤æ‹©æ ¡çš„å¦ä¸€ä¸ªè€ƒè™‘æ˜¯ï¼Œå¤§å­¦å¦‚ä½•å¸®åŠ©å­¦ç”Ÿä»æœ¬ç§‘è¿‡æ¸¡åˆ°å¤§å­¦ä¹‹åçš„ç”Ÿæ´» ä½ ä¸ºä»€ä¹ˆè¦ä¸Šå¤§å­¦ä»¥åŠå¦‚ä½•åˆ©ç”¨å¤§å­¦æ•™è‚²ï¼Ÿâ€”â€” ç›¸æ¯”äºå»å“ªæ‰€å¤§å­¦ï¼Ÿ åˆåˆ°å¤§å­¦ï¼Œæˆ‘ä»¬è¦ä»¥æ•é”çš„å—…è§‰è¿…é€Ÿå¯»æ‰¾è‰¯å¸ˆï¼Œå¹¶è¦å‹‡äºåœ¨è¯¾å ‚ä¹‹å¤–åŒä»–ä»¬å»ºç«‹å…³ç³»ï¼Œé“ä¹‹æ‰€å­˜ï¼Œå¸ˆä¹‹æ‰€å­˜ä¹Ÿ è‡³äºå…·ä½“çš„ä¸“ä¸šé€‰æ‹©ï¼Œé‚£è¦å¬ä»ä½ çš„ç›´è§‰ï¼Œä¸€å®šè¦é€‰æ‹©ä¸€ä¸ªè®©ä½ èƒ½å¤Ÿå…´å¥‹å››å¹´çš„ä¸“ä¸š â€”â€” å“ˆä½›ã€è€¶é²ã€æ–¯å¦ç¦è¯¥ä½•å»ä½•ä»ï¼Ÿæ·±çº¢è‰²ã€å“è“è‰²å’Œé²œçº¢è‰²ä¸‰ç§æ ¡å›­é¢œè‰²ï¼Œæœ€å–œæ¬¢å“ªä¸€ç§ï¼Ÿ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:4:3","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬å››éƒ¨åˆ† ç¤¾ä¼š æ·±å…¥æ¢è®¨äº†ç¾å›½ç²¾è‹±æ•™è‚²ç³»ç»Ÿå¯¹ç¤¾ä¼šé˜¶å±‚å›ºåŒ–å’Œç¤¾ä¼šä¸å¹³ç­‰çš„åŠ å‰§ä½œç”¨ ğŸ‘€ ç¤¾ä¼šæ­£ä¹‰æ„å‘³ç€ä½ éœ€è¦æ”¾å¼ƒä¸€äº›ä½ æ‹¥æœ‰çš„ä¸œè¥¿ï¼Œä»è€Œè®©åˆ«äººèƒ½å¤Ÿæ‹¥æœ‰æ›´å¤šã€‚ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:5:0","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬åä¸€ç«  çœ‹ä¸è§çš„â€œç‰¹æƒå ¡å’â€ ç®€è¨€ä¹‹ï¼Œç²¾è‹±æ•™è‚²ç³»ç»Ÿåœ¨ä¸æ–­å¤åˆ¶ç¹è¡ç¾å›½çš„é˜¶çº§ç³»ç»Ÿï¼Œå…¶å½±å“ä¸ä¸€ä¸ªä¸–çºªä¹‹å‰çš„â€œä¸‰é©¾é©¬è½¦â€æ‰€ä¸ºå¹¶æ— äºŒè‡´ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ˜¯åœ¨æ‰©å¤§ç¤¾ä¼šä¸å¹³ç­‰ï¼Œå¦¨ç¢ç¤¾ä¼šæµåŠ¨æ€§ï¼Œå›ºåŒ–é˜¶çº§ç‰¹æƒï¼Œå¹¶åˆ›é€ äº†ä¸€ä¸ªä¸ç›æ ¼é²-æ’’å…‹é€Šè´µæ—ç±»ä¼¼çš„ä¸ç¤¾ä¼šéš”ç¦»çš„ç²¾è‹±é˜¶å±‚ åŸ¹å…»æˆæœ¬è¶Šæ¥è¶Šé«˜ ç²¾è‹±æ²»ç† å±€é™åœ¨ä¸€å®šçš„èŒƒå›´ä¹‹å†… è‡ªæˆ‘æ­ç»´ï¼Œè‡ªæˆ‘å¤¸å¤§ è¡¨é¢å¤šæ ·æ€§æ©ç›–äº†ç»æµèƒŒæ™¯çš„å·®å¼‚ ç²¾è‹±å¿ƒæ€ï¼Œä¸ç¤¾ä¼šçš„è„±ç¦»ï¼šæˆ‘ä¹Ÿä¸çŸ¥é“ï¼Œæˆ‘æœ€å¤šåªèƒ½å»ºè®®ä»–ä»¬å»å…¬ç«‹å¤§å­¦çœ‹çœ‹ã€‚è¦æƒ³çœŸæ­£æ‡‚å¾—ä¸è‡ªå·±èƒŒæ™¯ä¸åŒçš„äººï¼Œä½ åªèƒ½å®é™…è¿›å…¥ä»–ä»¬çš„ä¸–ç•Œ â€œå¥‰çŒ®æ„è¯†â€ï¼Œä¸ºäº†å¼ºåŒ–å‚²æ…¢ä¸ä¼˜è¶Šæ„Ÿ ä¸å®‰å…¨æ„Ÿï¼šæƒåŠ›æ„Ÿæ€»æ˜¯ä¼´éšç€ä¸€ç§ç„¦èºå’Œè‡ªç§ï¼Œæ‹¥æœ‰æƒåŠ›æ„Ÿçš„äººæ€»æ˜¯è¢«ç¬¼ç½©åœ¨å¯¹å¤±è´¥çš„ææƒ§ä¹‹ä¸­ã€‚ ç†æ‰€åº”å½“ï¼Œåªæ˜¯æ›´åŠ åŠªåŠ›ç½¢äº† çœŸæ­£çš„é—®é¢˜æ˜¯ï¼Œå½“å‰çš„ç¯å¢ƒè®©æˆ‘ä»¬å¾ˆéš¾åšå‡ºå…¶ä»–é€‰æ‹©ã€‚äº‹å®ä¸Šï¼ŒçœŸæ­£çš„é—®é¢˜åœ¨äºç²¾è‹±æ²»ç†æœ¬èº«ã€‚ â€”â€” å­¦ç”Ÿä»¬åœ¨æ ¡çš„å­¦ä¹ æ–¹å¼ï¼Œç”šè‡³åŒ…æ‹¬ä»–ä»¬åœ¨æ ¡æ‰€æ¥è§¦çš„å­¦ä¹ ä¹‹å¤–çš„ä¸œè¥¿ï¼Œéƒ½ä¼šå½±å“ä»–ä»¬æœªæ¥æ‰€å¤„çš„é˜¶å±‚ä½ç½®ã€‚å·¥è–ªé˜¶å±‚çš„å­©å­ä»¬ä¼šå­¦ä¹ å¦‚ä½•ä¸¥æ ¼éµå®ˆçºªå¾‹ï¼Œæ‰€å—çš„æ•™è‚²ä¹Ÿéƒ½æ¯”è¾ƒæœºæ¢°ï¼Œå¤§éƒ½æ˜¯æ­»è®°ç¡¬èƒŒçš„ï¼›èŒä¸šäººå£«çš„å­å¥³ä»¬åˆ™ä¼šå­¦ä¹ å¦‚ä½•åˆ›é€ ï¼Œå¦‚ä½•è¡¨è¾¾è‡ªæˆ‘ï¼›è€Œå•†ä¸šé˜¶å±‚çš„å­©å­ä»¬åˆ™ä¼šå­¦ä¹ æŒæ¡æƒåŠ›ã€ç»Ÿæ²»å’Œè‡ªæˆ‘æŒæ§ç­‰å†…å®¹ã€‚ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:5:1","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"ç¬¬åäºŒç«  ç²¾è‹±æ•™è‚²çš„è‡ªæˆ‘æ•‘èµ é›†ä¸­æ¢è®¨äº†ç¾å›½ç²¾è‹±æ•™è‚²ç³»ç»Ÿçš„é—®é¢˜æ ¹æºï¼Œå¹¶æå‡ºäº†å¯¹è¿™ä¸€ç³»ç»Ÿè¿›è¡Œæ”¹é©å’Œè‡ªæˆ‘æ•‘èµçš„å¿…è¦æ€§ä¸å¯èƒ½è·¯å¾„ ç²¾è‹±æ²»ç†çš„ç°çŠ¶ ä¸æ‰€æœ‰ç»Ÿæ²»é˜¶çº§ä¸€æ ·ï¼Œç²¾è‹±é˜¶å±‚ä¹Ÿä¸»å¼ â€œä¸€åˆ‡ä»å¤§ä¼—åˆ©ç›Šå‡º å‘â€ã€‚æ ¹æ®å®šä¹‰ï¼Œè¿™ä¸€æ€æƒ³æœ¬èº«å°±æ˜¯ä¸€ç§æå«è‡ªèº«åˆ©ç›Šçš„æ–¹å¼ï¼Œå®ƒç» å£ä¸æè´£ä»»å’Œä¹‰åŠ¡ï¼Œä¸ææ€§æ ¼æˆ–é¢†å¯¼åŠ›ï¼Œè€Œåªæ˜¯å¼ºè°ƒä¸ªäººæˆåŠŸå’Œä¸ªäºº åˆ©ç›Šã€‚ æ‰€æœ‰é¢†å¯¼é˜¶å±‚éƒ½ä¼šå‘å±•ä¸€å¥—è¯æ˜è‡ªå·±æƒåŠ›åˆæ³•æ€§çš„æ„è¯†å½¢æ€ å…¶å®ç²¾è‹±ä»¬çŸ¥é“é‚£äº›äººæ˜¯å­˜åœ¨çš„ï¼Œä½†ä»–ä»¬å°±æ˜¯æ— æ³•æƒ³è±¡åè€…çš„ç”Ÿ æ´»æ˜¯æ€æ ·çš„ã€‚è™½ç„¶ä»–ä»¬åšå‡ºçš„å¾ˆå¤šå†³å®šéƒ½ä¼šå½±å“æ™®ç½—å¤§ä¼—çš„ç”Ÿæ´»ï¼Œä½† ä»–ä»¬å´æ ¹æœ¬æ²¡å…´è¶£å»ä½“éªŒå¤§ä¼—çš„ç”Ÿæ´»ã€‚ æ‰€æœ‰çš„æ€»ç»Ÿå€™é€‰äººï¼Œä¸€æ—¦è¾¾åˆ°è¿™ä¸ªå±‚æ¬¡ï¼Œéƒ½ä¼šå±•ç°å‡ºæƒŠäºº çš„é‡å¿ƒã€‚æ¯”å¦‚å…‹æ—é¡¿ï¼Œæ‰€æœ‰äººéƒ½çœ‹å¾—å‡ºæ¥ï¼Œä»–å°±æ˜¯åœ¨é‡å¿ƒä¸­æ³¡å¤§çš„ã€‚ ä»–å¾ˆæ¸…æ¥šè‡ªå·±æƒ³è¦ä»€ä¹ˆï¼Œä½†å´ä¸çŸ¥é“è‡ªå·±ä¸ºä»€ä¹ˆè¦é‚£ä¸ªã€‚ ä½†æ˜¯ç¬¨è›‹ã€‚ä»–ä»¬çš„ç¡®èªæ˜ã€æœ‰å¤©åˆ†ã€æ´»åŠ›å››å°„ï¼Œä½†åŒæ—¶åˆå……æ»¡ç„¦è™‘ã€è´ªå©ªã€å†·æ¼ ã€æ€¯æ‡¦ï¼Œæ²¡æœ‰å‹‡æ°”ï¼Œæ²¡æœ‰è¿œè§â€”â€”è¿™å°±æ˜¯ä»Šå¤©çš„ç²¾è‹±é˜¶å±‚ã€‚åˆ»æ„å›é¿å›°éš¾çš„äº‹æƒ… éšç€ä¼˜ç§€å¤§å­¦çš„å¢å¤šä»¥åŠè¶Šæ¥è¶Šå¤šçš„ä¼˜ç§€å­¦ç”Ÿè¢«å“ˆä½›ã€è€¶é²ã€æ™® æ—æ–¯é¡¿å’Œæ–¯å¦ç¦æ‹’ä¹‹é—¨å¤–ï¼Œä¸ºä»€ä¹ˆè¿™ä¸ªç¤¾ä¼šè¿˜é‚£ä¹ˆçœ‹é‡åæ ¡å‘¢ï¼Ÿæ ¹æº è¿˜æ˜¯åœ¨äºæ¨åŠ¨æ•´ä¸ªç³»ç»Ÿçš„é‚£ç§ç²¾è‹±ä¸»ä¹‰å¿ƒæ€ã€‚ ç²¾è‹±æ²»ç†ä¸ä»…è‡ªæˆ‘å°é—­ï¼Œè‡ªæˆ‘å¼ºåŒ–ï¼Œå®ƒè¿˜ä¼šå‡å…¬æµç§ã€‚ ä¼¼ä¹æ²¡æœ‰äººè®¤ä¸ºé¢†å¯¼æœ¬èº«å°±æ„å‘³ç€æœºé‡å’Œè´£ä»»ã€‚ç²¾è‹±ä»¬åªæ•ˆå¿ è‡ªå·±çš„é‡å¿ƒï¼Œå…¶ä»–ä¸€åˆ‡éƒ½ä¸é‡è¦ã€‚ äº‹å®ä¸Šï¼Œç²¾è‹±é˜¶å±‚æ˜¯åœ¨ä»¥ç‰ºç‰²å­å­™åä»£çš„å¹¸ç¦ä¸ºä»£ä»·æ¥æ¢å–è‡ªå·± ä»Šå¤©çš„åœ°ä½ã€‚ æ”¹å˜æ–¹å¼ æ–°çš„ç¤¾ä¼šå½¢æ€å¿…é¡»ä¿è¯ç‰¹æƒä¸ä¼šä»£ä»£ç›¸ä¼ ï¼Œè¿™ä¸ªéå¸¸é‡è¦ã€‚ ä»–ä»¬éœ€è¦é‡æ–°æ€è€ƒâ€œç¾å¾·â€ä¸€è¯çš„å«ä¹‰ æˆ‘ä»¬å¸Œæœ›å­©å­ä»¬æ›´èƒ½æ‰¿å—æŒ«æŠ˜ï¼Œæ›´è‡ªç«‹ï¼Œæ›´æœ‰ç²¾ç¥ç‹¬ç«‹æ€§ï¼Œå¯¹ä¸–ç•Œå……æ»¡å¥½å¥‡ï¼Œæ›´æœ‰åˆ›é€ åŠ›ï¼Œæ›´æ„¿æ„å»å†’é™©ï¼Œæ›´æ„¿æ„å»çŠ¯é”™è¯¯ã€‚è°è¿˜ä¼šè®¤ä¸ºä»Šå¤©çš„å¸¸æ˜¥è—¤ç›Ÿæ ¡å­¦ç”Ÿåœ¨æ™ºåŠ›æ´»åŠ¨ä¸Šä¾ç„¶ç”Ÿæœºå‹ƒå‹ƒå‘¢ï¼Ÿè°è¿˜ä¼šè®¤ä¸ºä»Šå¤©çš„ä»–ä»¬ï¼ˆæå°‘æ•°é™¤å¤–ï¼‰ä¾ç„¶æ‰åæ¨ªæº¢å‘¢ï¼Ÿï¼ˆåˆ«å¿˜äº†ï¼Œæ‰åæ¨ªæº¢å’Œæå…¶èªæ˜å¹¶ä¸æ˜¯ä¸€å›äº‹ï¼‰ æ”¹å˜å½•å–æµç¨‹æˆ–è®¸èƒ½å¤Ÿè§£å†³åº¸æ‰çš„é—®é¢˜ï¼Œä½†å´æ— æ³•æ”¹å˜è¿™ä¸ªç¤¾ä¼šçš„ä¸å¹³ç­‰ã€‚ç§ç«‹å­¦æ ¡å’Œå¤§å­¦å¯ä»¥å‘ç©·äººæˆ–ä¸­äº§é˜¶çº§æ•å¼€å¤§é—¨ï¼Œä½†ä»–ä»¬èƒ½åšçš„ï¼Œä¹Ÿåªæœ‰è¿™äº›ã€‚æˆ‘ä»¬éœ€è¦çš„æ˜¯å¯¹æ•´ä¸ªæ•™è‚²ç³»ç»Ÿæ¥ä¸€æ¬¡å¤§ä¿®ã€‚ğŸ¤· ç°åœ¨æˆ‘çªç„¶æ„è¯†åˆ°ï¼Œæˆ‘ä»¬çœŸæ­£è¦åšçš„ï¼Œä¸æ˜¯è®©æ‰€æœ‰å­©å­éƒ½èƒ½ä¸Šå¸¸æ˜¥è—¤ç›Ÿæ ¡ï¼Œè€Œæ˜¯è¦è®©é‚£äº›æ²¡ä¸Šå¸¸æ˜¥è—¤ç›Ÿæ ¡ï¼ˆæˆ–ä»»ä½•ç§ç«‹å­¦æ ¡ï¼‰çš„å­©å­ï¼Œä¹Ÿèƒ½äº«å—åˆ°ä¸€æµçš„æ•™è‚² ä½†å¦‚æœæˆ‘ä»¬æƒ³åˆ›é€ ä¸€ä¸ªçœŸæ­£å…¬å¹³çš„ç¤¾ä¼šï¼Œä»…ä»…æä¾›ä¸€æµçš„å…è´¹é«˜ç­‰æ•™è‚²æ˜¯ä¸å¤Ÿçš„ã€‚è¦æƒ³è®©å­©å­æœ‰å…¬å¹³çš„æœºä¼šè€ƒä¸Šå¤§å­¦ï¼Œå°±è¦è®©ä»–ä»¬åœ¨å‡†å¤‡ç”³è¯·å¤§å­¦ä¹‹å‰å°±äº«å—å…¬å¹³ é¦–å…ˆï¼Œæˆ‘ä»¬è¦æ¶ˆé™¤K-12é˜¶æ®µçš„ä¸å¹³ç­‰ ç¨æ”¶ï¼Œä¸è¦å›é¿é˜¶çº§é—®é¢˜ è‡ªæˆ‘æ•‘èµæ˜¯ä¸€ä»¶éå¸¸ä¸¥è‚ƒçš„äº‹æƒ…ã€‚ è¡€ç»ŸçœŸçš„æ˜¯ä¸€ä¸ªå¾ˆæ„šè ¢çš„é—®é¢˜ã€‚ä¸ºä»€ä¹ˆè¦è€ƒè™‘è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿå½“ä½ æƒ³è¦åŸ¹å…»ä¸€ä¸ªå­©å­æ—¶ï¼Œä»–åˆ°åº•æ˜¯ä¸æ˜¯ä½ çš„äº²ç”Ÿéª¨è‚‰åˆæœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿæˆ‘ä»¬è¿™ä¸ªæ—¶ä»£çš„æ‰€æœ‰å­©å­ï¼Œéƒ½æ˜¯è¿™ä¸ªæ—¶ä»£æˆå¹´äººçš„å­©å­ï¼Œéƒ½åº”è¯¥å¾—åˆ°æ‰€æœ‰æˆå¹´äººçš„ç…§é¡¾ã€‚ â€”â€” ä¸€æ—¦ç²¾è‹±ä¸»ä¹‰å¼€å§‹å†…éƒ¨ç¹æ®–ï¼Œå®ƒå°±ä¼šç»µç»µä¸ç»ã€‚æ‰€ä»¥é‚£äº›ä½é«˜æƒé‡çš„äººï¼Œè™½ç„¶ä»–ä»¬ä¸ªä¸ªå‡ºèº«æ˜¾èµ«ï¼Œå´æ€»æ˜¯ä¼šä¸€æ¬¡åˆä¸€æ¬¡åœ°é‡å¤ç›¸åŒçš„é”™è¯¯ã€‚ç»“æœå°±æ˜¯ï¼Œç¾å›½æ•´ä¸ªå›½å®¶çš„å‘½è¿éƒ½å¯ä»¥è¿½æº¯åˆ°å°å­¦ï¼Œæˆ–è€…æ›´è´´åˆ‡åœ°è¯´ï¼Œè¿½æº¯åˆ°å­å®«ã€‚ ","date":"2025-02-13","objectID":"/beyondcode/thinking4/:5:2","tags":null,"title":"Thinking4","uri":"/beyondcode/thinking4/"},{"categories":null,"content":"è¶…é€Ÿå­¦ä¹ ç¬”è®° ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:0:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬ä¸€ç«  å¼•å…¥ï¼Œè®²äº†è®²mitæŒ‘æˆ˜ã€æµåˆ©å¤–è¯­ã€å­¦ä¹ ä»»ä½•é—®é¢˜ã€ä»¥åŠä¸€ç¾¤è¶…é€Ÿå­¦ä¹ çš„äºº ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:1:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬äºŒç«  ä»€ä¹ˆæ˜¯è¶…é€Ÿå­¦ä¹ ï¼Ÿ ä¸€ç§ç­–ç•¥ï¼Œè‡ªä¸»çš„ï¼ˆå­¦ä»€ä¹ˆã€ä¸ºä»€ä¹ˆå­¦ï¼‰ã€é«˜å¼ºåº¦çš„ï¼ˆé‡‡å–ç‰¹æ®Šæ­¥éª¤è¾¾åˆ°æœ€é«˜å­¦ä¹ æ•ˆç‡ï¼‰ è¶…é€Ÿå­¦ä¹ çš„å¥½å¤„ï¼Ÿ survive å¸¦ç»™ä½ ä¿¡å¿ƒ è¶…é€Ÿå­¦ä¹ çš„å› ç´  å¤©èµ‹ã€æ—¶é—´éƒ½ä¸æ˜¯ä¸»è¦å½±å“å› ç´  â€”â€”æ¯ä¸ªäººéƒ½èƒ½è¶…é€Ÿå­¦ä¹  ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:2:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬ä¸‰ç«  å¦‚ä½•è¶…é€Ÿå­¦ä¹ ï¼Ÿ ä¸€ä¸ªé’¢ç´å®¶è®­ç»ƒå…¬å¼€æ¼”è¯´çš„æ•…äº‹ å› ä¸ºè¿˜æ²¡æœ‰ç³»ç»Ÿæå‡ºæ³•åˆ™ï¼Œæ‰€ä»¥æˆ‘æ¥ç®€å•åˆ—ä¸¾ä¸€ä¸‹å…³é”®è¯ é€‰å®šå­¦ä¹ ä¸»é¢˜ ç»„ç»‡ ç»ƒä¹  åé¦ˆ æ¯…åŠ› å†³å¿ƒ æ¨è®ºï¼šæœ‰ä¹ä¸ªæ³•åˆ™ + ä¸€ç§å­¦ä¹ ç²¾ç¥ â€”â€”ä½ æ˜¯æŒç®¡å…¨å±€çš„äººï¼Œä¹Ÿæ˜¯æœ€ç»ˆå¯¹ä½ åšå‡ºçš„æˆæœè´Ÿè´£çš„äººã€‚è‹¥ä½ ç”¨é‚£æ ·çš„ç²¾ç¥å»ä»äº‹è¶…é€Ÿå­¦ä¹ ï¼Œåº”è¯¥æŠŠè¿™äº›åŸåˆ™å½“ä½œæœ‰å¼¹æ€§çš„æŒ‡å¯¼æ–¹æ¡ˆï¼Œè€Œéæ­»æ¿çš„è§„åˆ™ã€‚ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:3:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬å››ç«  1 åè®¾å­¦ä¹  ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:4:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"æ˜¯ä»€ä¹ˆ å­¦ä¹ å¦‚ä½•å­¦ä¹ ï¼Œç”»ä¸€å¼ å­¦ä¹ åœ°å›¾ï¼Œå…·æœ‰çŸ­æœŸï¼ˆæœ‰å˜åŒ–ï¼‰å’Œé•¿æœŸï¼ˆæˆ˜ç•¥ä¸Šæ¥çœ‹ï¼‰æ•ˆåº” ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:4:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"å¦‚ä½•ç”»å‡ºåœ°å›¾ï¼Ÿ å›ç­”ä¸‰ä¸ªé—®é¢˜ ä¸ºä»€ä¹ˆï¼ˆåŠŸåˆ©æ€§ æœ¬è´¨æ€§ åäº”åˆ†é’Ÿä»¥å†…ä¸“å®¶è®¿è°ˆæ³•ï¼‰ åšä»€ä¹ˆï¼ˆæ¦‚å¿µ äº‹å® ç¨‹åºï¼‰æ ‡å‡ºæœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ çŸ¥é“ç“¶é¢ˆæ˜¯ä»€ä¹ˆ æ€ä¹ˆåšï¼šæ ‡æ†å­¦ä¹ æ³•ï¼ˆäººä»¬çš„å…±åŒæ–¹å¼ï¼‰ + å¼ºè°ƒæ’é™¤æ³•ï¼ˆæ‰¾å‡ºä¸ç›®æ ‡ç›¸ä¸€è‡´çš„ + å»¶è¿Ÿä¸ç›®æ ‡ä¸ä¸€è‡´çš„ï¼‰ ä½†æ˜¯æ€ä¹ˆæŠŠæ¡è°ƒç ” vs å­¦ä¹ ï¼Ÿ ç™¾åˆ†ä¹‹åæ³•åˆ™ï¼šé¢„æœŸæŠ•å…¥çš„10%å·¦å³ è®¡ç®—æŠ¥é…¬é€’å‡ä¸è¾¹é™…æ•ˆåº”ï¼šé—®è‡ªå·±ç ”ç©¶æ˜¯å¦æœ‰ç”¨ï¼Ÿåšç ”ç©¶ï¼šä¸åšç ”ç©¶ äº‹å®ä¸Šï¼Œé•¿æœŸæ•ˆåº”æ›´å…·æœ‰ä»·å€¼ï¼Œä¸€æ¬¡æˆåŠŸçš„ç»éªŒå¸¦æ¥ä¿¡å¿ƒï¼Œæ¥ç€æŠ•å…¥ä¸‹ä¸€åœºè®¡åˆ’ä¹‹ä¸­ â€”â€”ä½ çŸ¥é“è‡ªå·±åœ¨æ‹–å»¶æ—¶é—´ï¼Œå¼€å§‹å»åšå°±å¯¹äº† ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:4:2","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬äº”ç«  2 ä¸“å¿ƒè‡´å¿— æ‹–å»¶ï¼Ÿ æœ‰äº›æ—¶å€™æ˜¯åœ¨ç”¨å·¥ä½œé€ƒé¿å¦ä¸€ä¸ªå·¥ä½œ ğŸ˜ æ‹–å»¶æ›´å¤šæ˜¯æ— æ„è¯†çš„ ç»ƒä¹ å¼€å§‹ä¸“æ³¨çš„ä¸‰ä¸ªæ”¯æ¶ï¼š1. æ”¯æ’‘å¼€å¤´éš¾ 2. ç•ªèŒ„é’Ÿå‡çŸ­ä¼‘æ¯ 3. æ—¥ç¨‹è¡¨ åˆ†å¿ƒï¼Ÿ å¿ƒæµå¯èƒ½å­˜åœ¨ é•¿æ—¶é—´å­¦ä¹ è®°å¾—åˆ‡æ¢ä¸»é¢˜ ä¸‰ä¸ªæ¥æºå–æ¶ˆï¼š1. ç¯å¢ƒï¼Ÿ 2. å­¦ä¹ ä»»åŠ¡ï¼Ÿï¼ˆéš¾åº¦ï¼Ÿæœ‰æ²¡æœ‰åé¦ˆï¼Ÿï¼‰ 3. ä½ çš„å¿ƒï¼Ÿï¼ˆé¿å…è´Ÿé¢æƒ…ç»ªï¼‰ çœŸæ­£çš„ä¸“æ³¨ï¼Ÿ å…´å¥‹æ„Ÿè¿‡å¼ºï¼Œä¸åˆ©äºä¸“æ³¨ æ”¾æ¾çš„ä¸“æ³¨ or é•¿æ—¶é—´çš„ä¸“æ³¨åå®Œå…¨æ”¾æ¾ ä¸“æ³¨åŠ›å¯ä»¥è¢«åˆ»æ„é”»ç‚¼å‡ºæ¥çš„ â€”â€”æ”¾ä¸‹ä¸€ä¸ªä¸»é¢˜ï¼Œä¹‹åèƒ½å¤Ÿç«‹åˆ»å›åˆ°é‚£ä¸ªä¸»é¢˜ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:5:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬å…­ç«  3 ç›´æˆ³äº†å½“ è‡ªå­¦çš„æ—¶å€™ä¸ä¸€å®šç›´æ¥ï¼å› ä¸ºç›´æ¥å­¦ä¹ å¯èƒ½ä¼šä¸å¤ªèˆ’æœã€æ— èŠæˆ–è€…æŒ«æŠ˜ï¼Œä½†æ˜¯çœŸæ­£çš„å­¦ä¹ å…¶å®æ˜¯åœ¨ç›´æ¥åšä½ æƒ³å˜å¾—æ“…é•¿çš„äº‹æƒ…ã€‚ç®€å•è‡ªå­¦å®¹æ˜“ä¸çŸ¥ä¸è§‰é™·å…¥ç³Ÿç³•ä¸”æ— æ•ˆçš„å­¦ä¹ ç­–ç•¥é‡Œé¢ æ­£å¼æ•™è‚²å¹¶ä¸èƒ½ä¿è¯å­¦ä¹ è¿ç§» å­¦ä¹ æ–°äº‹ç‰©çš„æ—¶å€™ï¼Œæ°¸è¿œè‡´åŠ›äºå’Œæƒ³ä½¿ç”¨è¿™äº›èƒ½åŠ›çš„ç¯å¢ƒæƒ³è”ç»“â€”â€”ç›´æ¥ä»æºå¤´å­¦ä¹  å¦‚ä½•ç›´æ¥å­¦ä¹ ï¼Ÿ minds and hands æˆ–è®¸å¹¶ä¸æ˜¯å…·ä½“çš„æŠ€èƒ½ï¼Œè€Œæ˜¯æƒ³æ·±å…¥äº†è§£æŸä¸€é¢†åŸŸï¼Ÿæ­å»ºçŸ¥è¯†ä½“ç³» å¹¶ä¸” è¿ç§» å…·ä½“æ–¹æ³•ï¼Ÿ ä»¥è®¡åˆ’ä¸ºåŸºç¡€ï¼Œåˆ›é€ ä¸ºå¯¼å‘ æ²‰æµ¸å¼å­¦ä¹  æ¨¡æ‹Ÿè®¤çŸ¥å­¦ä¹ ç¯å¢ƒ çªç„¶æŠŠè‡ªå·±æ”¾åˆ°ä¸€ä¸ªè¦æ±‚æé«˜çš„ç¯å¢ƒé‡Œï¼Œä¸æ–­è·å¾—æ•™è®­orå›é¦ˆ â€”â€”å¿…é¡»ç»™ä»–ä»¬çœ‹ï¼Œä½ ä»ç¬¬ä¸€å¤©å°±èƒ½ç›´æ¥å¼€å§‹å·¥ä½œï¼Œå¹¶ä¸”æ˜¯æœ‰ç”¨çš„å›¢é˜Ÿæˆå‘˜ï¼Œè€Œéè´Ÿæ‹… ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:6:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬ä¸ƒç«  4 ç›´å‡»å¼±ç‚¹ ä»”ç»†åˆ†æï¼ˆæ‰¾åˆ°ç“¶é¢ˆï¼‰ï¼Œåˆ»æ„ç»ƒä¹ ï¼ˆèµ„æºé›†ä¸­åœ¨è¿™ä¸€éƒ¨åˆ†ï¼‰ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:7:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"å…ˆç›´æ¥å­¦ä¹ ï¼Œååå¤æ“ç»ƒ ç›´æ¥ç»ƒä¹ æƒ³è¦å­¦ä¼šçš„æŠ€èƒ½ åˆ†æè¯¥æŠ€èƒ½ç‹¬ç«‹çš„è¦ç´  å›åˆ°ç›´æ¥ç»ƒä¹ ï¼Œæ•´åˆæ‰€å­¦ æ³¨æ„ä»¥ä¸Šå¾ªç¯ä¼šè¶Šæ¥è¶Šå¿«ï¼Œä¸”æ—¶é—´ä¼šè¶Šæ¥è¶Šé›†ä¸­åˆ°åå¤æ“ç»ƒä¸Šé¢ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:7:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"åå¤æ“ç»ƒçš„ä¸‰ä¸ªæ­¥éª¤ æ—¶é—´andé¡¹ç›® è®¾è®¡æ€ä¹ˆæ“ç»ƒ å¦‚ä½•å…‹æœæŒ«è´¥æ„Ÿï¼Ÿ åˆ‡å‰²ä¸€å°æ®µæ—¶é—´ / è®¤çŸ¥ç»ƒä¹ ï¼ˆåªæ˜¯é’ˆå¯¹æ¦‚å¿µï¼‰ / æ¨¡ä»¿åˆ«äººï¼ˆç›´æ¥å¤åˆ¶ä¸æƒ³æ“ç»ƒçš„éƒ¨åˆ†ï¼‰ / æ”¾å¤§é•œæ³•ï¼ˆä¾§é‡èŠ±æ—¶é—´ï¼‰ / è¿åŠ¨æ³•ï¼ˆä»¥ç»ˆä¸ºå§‹ï¼Œé€’å½’å­¦ä¹ ï¼‰ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:7:2","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"æœ‰æ„è¯†çš„æ“ç»ƒ è¦æœ‰ å¼ºçƒˆçš„åŠ¨æœº ï¼ŒçŸ¥é“èƒŒåçš„ç¼˜ç”±æ¥é¾™å»è„‰ â€”â€”åˆ«å®³æ€•ç›´é¢é‡æ‹³ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:7:3","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬å…«ç«  5 æå–è®°å¿† ç›´æ¥å¤ä¹ ï¼Œä¹¦è¶Šè¯»è¶Šå·®ï¼Œä¸»åŠ¨å»æ„å»ºçŸ¥è¯† æé«˜éš¾åº¦å¹¶ä¸”é€‰æ‹©åœ¨â€œå‡†å¤‡å¥½â€ä¹‹å‰å°±è‡ªæˆ‘æµ‹éªŒï¼ˆæ³¨æ„éš¾åº¦ä¸è¦å¤ªé«˜ï¼‰ â€”â€” åœ¨è¯¾ç¨‹å¼€å§‹å‰ï¼Œå°±å‚åŠ æœŸæœ«è€ƒã€å‰å‘æµ‹éªŒæ•ˆåº”ã€‘ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:8:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"æå–ä»€ä¹ˆè®°å¿†ï¼Ÿ ç›´æ¥ç»ƒä¹  ç§¯ç´¯æ®Šé€”åŒå½’çš„çŸ¥è¯†ç‚¹ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:8:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"å¦‚ä½•æå–è®°å¿†ï¼Ÿ å¿«é—ªå¡ è‡ªç”±å›æƒ³ åˆä¸Šä¹¦æœ¬ï¼Œquestion yourself â€”â€”æ— è®ºä½ æ˜¯å¦å‡†å¤‡å¥½ï¼Œæå–è®°å¿†ç»ƒä¹ çš„æ•ˆæœéƒ½æ¯”è¾ƒå¥½ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:8:2","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬ä¹ç«  6 æ„è§å›é¦ˆ åŠ¡å¿…å–å¾—å›é¦ˆï¼Œå›é¦ˆæ˜¯ä¸èˆ’æœçš„ï¼Œå¯¹äºå›é¦ˆçš„ææƒ§æœ¬èº«æ›´åŠ ä¸èˆ’æœï¼Œæ‰€ä»¥è¦æœ‰ æå¤§çš„è‡ªä¿¡ã€å†³å¿ƒã€åšæŒ å›é¦ˆçš„ç±»å‹ï¼š ç»“æœï¼ˆå°ç”¨ï¼‰ / ä¿¡æ¯ï¼ˆå“ªé‡Œåšé”™äº†ï¼‰ / æ”¹æ­£ï¼ˆæ‰¾åˆ°ä¸“å®¶ï¼‰ ã€ä»è¾ƒå¼±åˆ°è¾ƒå¼ºçš„å½¢å¼ï¼Œå¦‚æœå®é™…ä¸Šåšä¸åˆ°ï¼Œé‚£ä¹ˆä¸è¦å¼ºæ±‚ã€‘ å¤šå¿«å–å¾—å›é¦ˆï¼Ÿç«‹åˆ» ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:9:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"å¦‚ä½•æ”¹å–„å›é¦ˆ æ¶ˆé™¤å™ªéŸ³ï¼Œæœ‰ä¸€äº›å¯ä»¥è¢«é‡åŒ–çš„ä¿¡å·èƒ½å¸®ä½ å‚è€ƒ å°½é‡é¿å…æ€»æ˜¯è®©ä½ å¯¹è‡ªæˆ‘è¡¨ç°æ„Ÿè§‰è‰¯å¥½orä¸å¥½çš„æƒ…å¢ƒ å¯ä»¥è¿½è¸ªå­¦ä¹ é€Ÿç‡çš„å›é¦ˆï¼Œå­¦ä¹ é€Ÿåº¦æœ‰å¤šå¿«ï¼Ÿ é«˜å¼ºåº¦ä¸”å¿«é€Ÿçš„å›é¦ˆ / æš´éœ²åœ¨å¤§é‡çš„å›é¦ˆé‡Œé¢ â€”â€”åè€Œæœ€å¥½æ˜¯ææ—©ä¸Šåœºï¼Œæ¥å—é‡å‡»ï¼Œæ‰ä¸ä¼šè®©å›é¦ˆæŠŠä½ å‡»å€’ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:9:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬åç«  7 ä¿ç•™è®°å¿† æ‰§ç€çš„é«˜å¼ºåº¦è®­ç»ƒï¼Œè¶…è¶Šäº†è¢«è§†ä¸ºæ­£å¸¸çš„åŠªåŠ›çš„ä»˜å‡º ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:10:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"å¦‚ä½•é¢„é˜²é—å¿˜ï¼Ÿ ä¼šè¾¾æˆä½ çš„ç›®æ ‡ã€ä¸”ç®€å•åˆ°è¶³ä»¥æŒç»­çš„è®°å¿†æ–¹æ³• é—´éš”æ³•ï¼šé‡å¤è®°å¿† ç¨‹åºåŒ–ï¼šä½ æ˜¯æ€ä¹ˆéª‘è½¦çš„ è¿‡åº¦å­¦ä¹ ï¼šé¢‘ç¹çš„ä½¿ç”¨æŸç‚¹æŠ€èƒ½ï¼š ç²¾ç»ƒ / é¢„å…ˆç»ƒä¹ æ›´é«˜é˜¶çš„æŠ€èƒ½ è®°å¿†æ³•ï¼šuse graph â€”â€”ä¸ºä»€ä¹ˆä¸ç”¨diskå‘¢ï¼Ÿ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:10:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬åä¸€ç«  8 åŸ¹å…»ç›´è§‰ ä¸¤ç§å¤©æ‰ï¼šä¸€æ—¦äº†è§£åˆ«äººåšåˆ°äº†ä»€ä¹ˆäº‹ï¼Œæˆ‘ä»¬å°±å¾ˆç¡®å®šè‡ªå·±ä¹Ÿèƒ½åšåˆ° vs å³ä½¿æˆ‘ä»¬äº†è§£ä»–åšäº†ä»€ä¹ˆäº‹æƒ…ï¼Œå¯¹äºä»–åšé‚£äº›äº‹æƒ…çš„è¿‡ç¨‹ï¼Œå´å®Œå…¨ä¸å¯çŸ¥ è´¹æ›¼çš„ç›´è§‰ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:11:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"å¦‚ä½•å»ºç«‹ç›´è§‰ï¼Ÿ åˆ«è½»æ˜“æ”¾å¼ƒå›°éš¾çš„é—®é¢˜ ã€5minæŒ£æ‰è®¡æ—¶å™¨ã€‘ ç”¨è¯æ˜çš„è¿‡ç¨‹æ¥ç†è§£ï¼Œå°å¿ƒç†è§£çš„é”™è§‰ æ°¸è¿œä»å…·ä½“çš„ä¾‹å­å¼€å§‹ åˆ«æ¬ºéª—è‡ªå·±ï¼Œè€Œä½ æ˜¯æœ€å®¹æ˜“æ¬ºéª—çš„äººï¼Œå¤šé—®ç¬¨é—®é¢˜ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:11:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"è´¹æ›¼å­¦ä¹ æ³• æˆ‘å–œæ¬¢æ©¡çš®é¸­ï¼ŸğŸ¦† ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:11:2","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬åäºŒç«  9 å‹‡äºå®éªŒ å‡è®¾/å®éªŒ/æˆæœ/é‡å¤ ä¸æ–­å¾ªç¯ è¿ç”¨çš„æ–¹æ³•ã€è§‚å¿µã€èµ„æºçš„å¤šæ ·æ€§ + é«˜å¼ºåº¦ å®éªŒçš„ç±»å‹ï¼š å­¦ä¹ èµ„æº / æŠ€å·§ / é£æ ¼ å®éªŒçš„å¿ƒæ€ï¼š growth mindset ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:12:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"å¦‚ä½•è¿›è¡Œå®éªŒï¼Ÿ å…ˆæ¨¡ä»¿ï¼Œå†åˆ›é€  æŠŠå„ç§æ–¹æ³•è¿›è¡Œæ¯”è¾ƒ æ·»åŠ æ–°çš„é™åˆ¶ï¼Œé€¼å‡ºæ–°çš„èƒ½åŠ› æ··åˆä¸åŒæŠ€èƒ½ 0.8*0.8 æ¢ç´¢æé™ â€”â€”ä½ æŒæ¡è¿™äº›åŸºç¡€åï¼Œèƒ½åŠ›å¯èƒ½åœæ»ä¸å‰ï¼Œå¼€å§‹å®éªŒå§ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:12:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬åä¸‰ç«  ç¬¬ä¸€ä¸ªè¶…é€Ÿå­¦ä¹ è®¡åˆ’ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:13:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"1.æ·±å…¥ç ”ç©¶ å­¦ä¹ çš„ä¸»é¢˜ä¸å¤§è‡´èŒƒå›´ï¼šç‹­çª„å¼€å§‹ï¼Œå­¦ä½ å¿…é¡»å­¦çš„ ä¸»è¦èµ„æºï¼šäººã€ç‰© ä»–äººå¦‚ä½•å­¦ä¼šè¿™é¡¹æŠ€èƒ½çš„åŸºå‡†ç‚¹ï¼šé¿å…å®Œå…¨é”™è¿‡æŸä»¶äº‹æƒ… ç›´æ¥ç»ƒä¹ æ´»åŠ¨ï¼šä½ å¯èƒ½ä¼šå¦‚ä½•ä½¿ç”¨è¿™é¡¹æŠ€èƒ½ï¼Ÿ å¤‡ç”¨æ•™æä¸æ“ç»ƒï¼šless is more ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:13:1","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"2.å®‰æ’æ—¶é—´ whyï¼š æ§åˆ¶ä½ çš„æ½œæ„è¯†\u0026é¢„ç•™æ—¶é—´ ä¸‰ä¸ªå†³å®šï¼šæŠ•å…¥æ—¶é—´ã€ä»€ä¹ˆæ—¶å€™å¼€å§‹å»åšã€èŠ±çš„æ—¶é—´é•¿åº¦ï¼ˆçŸ­çš„commitä¼˜äºé•¿çš„ã€è¾ƒçŸ­çš„é—´éš”å¼çš„æ—¶é—´åŒºå—æ›´ä¼˜è®°å¿†ï¼‰ï¼Ÿ æ”¾å…¥æ—¥å†è¡¨ï¼šå…­ä¸ªæœˆå·¦å³çš„è®¡åˆ’ä»¥å‘¨ä¸ºå•ä½ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:13:2","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"3.æ‰§è¡Œè®¡åˆ’ ä¸æ–­å›é¡¾9ä¸ªåŸåˆ™ï¼ŒDP ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:13:3","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"4.æ£€è§†æˆæœ æ¯å¤©åˆ†æï¼Œæ¯å‘¨åˆ†æï¼Œæ¯æœˆåˆ†æ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:13:4","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"5.é€‰æ‹©ç»´æŒorä¸“ç²¾ ç»´æŒ æŠ•å…¥ç»ƒä¹  èå…¥ç”Ÿæ´» é‡æ–°å­¦ä¹  ä¸“ç²¾ ğŸ¤” å¯èƒ½çš„æ›¿ä»£é€‰æ‹© ä½å¼ºåº¦å­¦ä¹ ï¼šç§¯ç´¯-\u003eä¹ æƒ¯ï¼Œæƒ³è¦å¿˜è®°æ— æ•ˆçš„è¡Œä¸ºæˆ–æŠ€èƒ½orå­˜åœ¨éšœç¢ä½¿å¾—æ— æ³•è½»æ¾å»ºç«‹ä¹ æƒ¯ï¼Ÿ-\u003eè¶…é€Ÿå­¦ä¹  å¤§å­¦æ•™è‚²ï¼šåŸ¹å…»æ­£ç¡®çš„å¿ƒæ€ï¼Œä¸æ˜¯æ’æ–¥ä»»ä½•è¾ƒæ…¢çš„æˆ–è¾ƒæ ‡å‡†åŒ–çš„å­¦ä¹ æ–¹å¼ï¼Œè€Œæ˜¯æ‰¿è®¤å­¦ä¹ ä»»ä½•äº‹ç‰©çš„å¯èƒ½æ€§æ¯”ä¸€å¼€å§‹çœ‹æ¥è¦å®½å¹¿å¾—å¤š â€”â€”æœ€å¤§çš„éšœç¢æ˜¯ä½ ä¸åœ¨ä¹è‡ªå·±çš„è‡ªå­¦è®¡åˆ’ï¼Œä»¥è‡³äºæ— æ³•å±•å¼€è¡ŒåŠ¨ï¼›ç»ˆèº«å­¦ä¹ ï¼Œæ˜¯è¦æ‰©å±•è€Œéç¼©å°ä½ èƒ½å¾—åˆ°çš„æœºä¼š ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:13:5","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":null,"content":"ç¬¬åå››ç«  ä¸€ç§éä¼ ç»Ÿçš„æ•™è‚² tldr: ç†è®ºä¸Šå¯ä»¥æ•™ä¼šåˆ«äººè¶…é€Ÿå­¦ä¹  ğŸ˜ ","date":"2025-02-06","objectID":"/beyondcode/thinking3/:14:0","tags":null,"title":"Thinking3","uri":"/beyondcode/thinking3/"},{"categories":["Stanford-CS224W"],"content":"Scaling up GNNs ç›´æ¥loadå…¨éƒ¨nodesåˆä¸å¤ªå¯èƒ½ã€naive approachã€‘4090 / A100å¸¦ä¸åŠ¨ ","date":"2025-01-29","objectID":"/l17-huge-gnn/:0:0","tags":null,"title":"L17-huge GNN","uri":"/l17-huge-gnn/"},{"categories":["Stanford-CS224W"],"content":"neighbor sampling å¯¹hub nodeçš„æ€è€ƒ see the paper ","date":"2025-01-29","objectID":"/l17-huge-gnn/:1:0","tags":null,"title":"L17-huge GNN","uri":"/l17-huge-gnn/"},{"categories":["Stanford-CS224W"],"content":"cluster-GCN advanced ","date":"2025-01-29","objectID":"/l17-huge-gnn/:2:0","tags":null,"title":"L17-huge GNN","uri":"/l17-huge-gnn/"},{"categories":["Stanford-CS224W"],"content":"Simplified GCN èˆå¼ƒäº†GCNçš„non-linearityï¼Œç›´æ¥ç”¨linear layer åŒè´¨æ€§ï¼Ÿä½†æ˜¯æˆ‘æƒ³çŸ¥é“å’Œglidarçš„åŒºåˆ«ï¼Ÿ ","date":"2025-01-29","objectID":"/l17-huge-gnn/:3:0","tags":null,"title":"L17-huge GNN","uri":"/l17-huge-gnn/"},{"categories":["Stanford-CS224W"],"content":"Improved GNN ","date":"2025-01-29","objectID":"/l16-improvedgnn/:0:0","tags":null,"title":"L16-improvedGNN","uri":"/l16-improvedgnn/"},{"categories":["Stanford-CS224W"],"content":"positionally-aware gnn anchoræ•°é‡ å¤§å° ï¼ˆæŒ‡æ•°å®ˆæ’ï¼‰ refer to the paper of Position-aware Graph Neural Networks (P-GNN) ","date":"2025-01-29","objectID":"/l16-improvedgnn/:0:1","tags":null,"title":"L16-improvedGNN","uri":"/l16-improvedgnn/"},{"categories":["Stanford-CS224W"],"content":"L7-GNN theory 1 æ‰€æœ‰gnnçš„åŸå‹åœ¨è¿™é‡Œ ä¸€ä¸ªlayer layersä¹‹é—´çš„äº¤äº’ input graph -\u003e computational graphçš„æ„å»º GO!!!! ","date":"2025-01-28","objectID":"/l7-gnn-t1/:0:0","tags":null,"title":"L7-GNN t1","uri":"/l7-gnn-t1/"},{"categories":["Stanford-CS224W"],"content":"a simple layer of GNN msg computations åŸç¥å¤„ç†dddd aggregation æ ¸å¿ƒï¼šorder invariance ç„¯è¿™å°±æ˜¯pointnetçš„æ ¸å¿ƒä¹‹ä¸€å•Šï¼ï¼ï¼ issueï¼š å®¹æ˜“å¿½ç•¥è‡ªå·±èŠ‚ç‚¹ ","date":"2025-01-28","objectID":"/l7-gnn-t1/:1:0","tags":null,"title":"L7-GNN t1","uri":"/l7-gnn-t1/"},{"categories":["Stanford-CS224W"],"content":"some examples of GNNs GCN GraphSAGE è¿™é‡Œçš„ç»†èŠ‚åœ¨äºå¦‚ä½•èšåˆé‚»å±…çš„ä¿¡æ¯ åŒæ—¶æ¯å±‚æ¥ä¸ªnorm GAT é¦–å…ˆè§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯attention mechanism å•ä¸ªattentionçš„è®¡ç®— å¤šå¤´attentionçš„è®¡ç®—ï¼ˆæ›´å®¹æ˜“æ”¶æ•›ï¼‰ ","date":"2025-01-28","objectID":"/l7-gnn-t1/:1:1","tags":null,"title":"L7-GNN t1","uri":"/l7-gnn-t1/"},{"categories":["Stanford-CS224W"],"content":"GNN layers in practice glidarå‘¼ä¹‹æ¬²å‡ºäº† ğŸ˜± æœ‰æ„æ€çš„ä¸œè¥¿ğŸ˜‹ ","date":"2025-01-28","objectID":"/l7-gnn-t1/:1:2","tags":null,"title":"L7-GNN t1","uri":"/l7-gnn-t1/"},{"categories":["Stanford-CS224W"],"content":"stacking GNN layers ","date":"2025-01-28","objectID":"/l7-gnn-t1/:2:0","tags":null,"title":"L7-GNN t1","uri":"/l7-gnn-t1/"},{"categories":["Stanford-CS224W"],"content":"GNN Intro ","date":"2025-01-27","objectID":"/l6-gnn-intro/:0:0","tags":null,"title":"L6-GNN intro","uri":"/l6-gnn-intro/"},{"categories":["Stanford-CS224W"],"content":"shallow ","date":"2025-01-27","objectID":"/l6-gnn-intro/:1:0","tags":null,"title":"L6-GNN intro","uri":"/l6-gnn-intro/"},{"categories":["Stanford-CS224W"],"content":"deep graph encoder tmd ç»ˆäºæ¥äº† !!! å€Ÿé‰´cnnï¼Œä½†æ˜¯ æ²¡æœ‰fixed notion or sliding window permutation invarianceï¼ ","date":"2025-01-27","objectID":"/l6-gnn-intro/:2:0","tags":null,"title":"L6-GNN intro","uri":"/l6-gnn-intro/"},{"categories":["Stanford-CS224W"],"content":"GCNN ç›´è§‰ä¸Šçš„æ€è€ƒ å…¶å®å°±æ˜¯3 + 1æ­¥èµ° æ‰¾åˆ°aggregate function æ‰¾åˆ°loss function train on a set of nodes apply to new nodes 7 8.1 9 16.2 17 ","date":"2025-01-27","objectID":"/l6-gnn-intro/:2:1","tags":null,"title":"L6-GNN intro","uri":"/l6-gnn-intro/"},{"categories":["Stanford-CS224W"],"content":"Node Embeddings https://web.stanford.edu/class/cs224w/slides/02-nodeemb.pdf ","date":"2025-01-27","objectID":"/l3-node-embeddings/:0:0","tags":null,"title":"L3-Node Embeddings","uri":"/l3-node-embeddings/"},{"categories":["Stanford-CS224W"],"content":"encoder and decoder ","date":"2025-01-27","objectID":"/l3-node-embeddings/:1:0","tags":null,"title":"L3-Node Embeddings","uri":"/l3-node-embeddings/"},{"categories":["Stanford-CS224W"],"content":"encoder: simple example ï¼Ÿï¼Ÿæ³¨æ„è¿™é‡ŒçŸ©é˜µæ˜¯one column per nodeï¼Œ è¿™é‡Œä¼¼ä¹è§£é‡Šé€šäº†ä¸ºä»€ä¹ˆglidaré‡Œé¢nodeåœ¨encodeçš„è¿‡ç¨‹ä¸­æ•°é‡ä¸å˜ï¼Œæ¢å¥è¯è¯´å°±æ˜¯ not scalable å‘¼ä¹‹æ¬²å‡ºå•Šå•Šå•Šå•Šå•Š ğŸ˜± ä»¥ä¸‹å†…å®¹éå¸¸å…·æœ‰å¯å‘æ€§ ","date":"2025-01-27","objectID":"/l3-node-embeddings/:1:1","tags":null,"title":"L3-Node Embeddings","uri":"/l3-node-embeddings/"},{"categories":["Stanford-CS224W"],"content":"Random walks æ€ä¹ˆç†è§£é«˜æ•ˆç‡ï¼Ÿ å¯¹ç‰¹å¾å­¦ä¹ çš„è€ƒé‡ æå‡ºæŸå¤±å‡½æ•° ç”¨äº†ä¸€ä¸ªè¿‘ä¼¼æ¥åŒ–ç®€ ï¼ˆä¸çº¦è€ŒåŒèµ°åˆ°äº†noise-denoiseï¼‰ kåœ¨5~20ä¹‹é—´ï¼åˆæ˜¯glidarçš„è®ºæ–‡ï¼ ","date":"2025-01-27","objectID":"/l3-node-embeddings/:2:0","tags":null,"title":"L3-Node Embeddings","uri":"/l3-node-embeddings/"},{"categories":["Stanford-CS224W"],"content":"summary ","date":"2025-01-27","objectID":"/l3-node-embeddings/:2:1","tags":null,"title":"L3-Node Embeddings","uri":"/l3-node-embeddings/"},{"categories":["Stanford-CS224W"],"content":"node2vec ","date":"2025-01-27","objectID":"/l3-node-embeddings/:2:2","tags":null,"title":"L3-Node Embeddings","uri":"/l3-node-embeddings/"},{"categories":["Stanford-CS224W"],"content":"embedding the entire graph SKIP ","date":"2025-01-27","objectID":"/l3-node-embeddings/:3:0","tags":null,"title":"L3-Node Embeddings","uri":"/l3-node-embeddings/"},{"categories":["Stanford-CS224W"],"content":"Traditional Methods in Graph Learning ","date":"2025-01-27","objectID":"/l2-traditional-methods/:0:0","tags":null,"title":"L2-traditional methods","uri":"/l2-traditional-methods/"},{"categories":["Stanford-CS224W"],"content":"node-level features degree Centrality measures: degree centrality, betweenness centrality, eigenvector centrality, closeness centrality, clustering coefficent è¿›ä¸€æ­¥å¼•ç”³GDV ","date":"2025-01-27","objectID":"/l2-traditional-methods/:0:1","tags":null,"title":"L2-traditional methods","uri":"/l2-traditional-methods/"},{"categories":["Stanford-CS224W"],"content":"edge-level features æˆ‘è§‰å¾—è¿™ä¸ªå¾ˆåƒæˆ‘ä»¬çš„vae encoderæƒ³æ³• è·ç¦»è¡¨å¾å®¹æ˜“ç†è§£â€¦â€¦ é‚»æ¥çŸ©é˜µçš„å¹‚ï¼ˆç±»ä¼¼äºå½’çº³æ³• ç•¥ï¼‰ ","date":"2025-01-27","objectID":"/l2-traditional-methods/:0:2","tags":null,"title":"L2-traditional methods","uri":"/l2-traditional-methods/"},{"categories":["Stanford-CS224W"],"content":"graph-level features æ ¸æ–¹æ³•ï¼ˆä½†æ˜¯skipï¼‰ bag of sthâ€¦ recall when you need ","date":"2025-01-27","objectID":"/l2-traditional-methods/:0:3","tags":null,"title":"L2-traditional methods","uri":"/l2-traditional-methods/"},{"categories":["Stanford-CS224W"],"content":"CS224W: Intro to Graph Deep Learning ","date":"2025-01-26","objectID":"/l1-intro/:0:0","tags":null,"title":"L1-Intro","uri":"/l1-intro/"},{"categories":["Stanford-CS224W"],"content":"GNN for img ä¹Ÿå°±æ˜¯è¯´gnnä¼šæ¯”ä¼ ç»Ÿçš„cnnæ›´å¥½ ğŸ¤” gnn as encoder and diffusion process! ","date":"2025-01-26","objectID":"/l1-intro/:0:1","tags":null,"title":"L1-Intro","uri":"/l1-intro/"},{"categories":["Stanford-CS224W"],"content":"how to build a graph? åŸºæœ¬å›¾è®ºçŸ¥è¯†è§CS61B nodes??? edges??? æ–°çš„å›¾çš„è¡¨ç¤º åŒå‘å›¾ä¸æŠ•å½±å›¾ï¼ˆä»”ç»†çœ‹çº¿æ¡ï¼‰ è‡ªç¯çš„å¢åŠ çš„degreeä¸º1 æ¥é‚»çŸ©é˜µè¡¨ç¤ºè¿æ¥æ€§ ","date":"2025-01-26","objectID":"/l1-intro/:0:2","tags":null,"title":"L1-Intro","uri":"/l1-intro/"},{"categories":["UCB-CS61C"],"content":"Lecture 16: Combinational Logic ","date":"2025-01-23","objectID":"/lec16-cl/:0:0","tags":null,"title":"Lec16-CL","uri":"/lec16-cl/"},{"categories":["UCB-CS61C"],"content":"Lecture 15: State and State Machines https://www.learncs.site/resource/cs61c/lectures/lec15.pdf ","date":"2025-01-22","objectID":"/lec15-state-and-state-machines/:0:0","tags":null,"title":"Lec15-State and State Machines","uri":"/lec15-state-and-state-machines/"},{"categories":["UCB-CS61C"],"content":"Flip-flops? details of registers! nä½å¯„å­˜å™¨ï¼Œnä¸ªå¹¶è¡Œçš„1ä½è§¦å‘å™¨ ","date":"2025-01-22","objectID":"/lec15-state-and-state-machines/:1:0","tags":null,"title":"Lec15-State and State Machines","uri":"/lec15-state-and-state-machines/"},{"categories":["UCB-CS61C"],"content":"timing of flip-flops clk to q è¶Šå°è¶Šå¥½ ","date":"2025-01-22","objectID":"/lec15-state-and-state-machines/:1:1","tags":null,"title":"Lec15-State and State Machines","uri":"/lec15-state-and-state-machines/"},{"categories":["UCB-CS61C"],"content":"Accumulators revisited å¦‚æœè¶…é¢‘â€¦ ","date":"2025-01-22","objectID":"/lec15-state-and-state-machines/:2:0","tags":null,"title":"Lec15-State and State Machines","uri":"/lec15-state-and-state-machines/"},{"categories":["UCB-CS61C"],"content":"pipelines and pipelining ","date":"2025-01-22","objectID":"/lec15-state-and-state-machines/:3:0","tags":null,"title":"Lec15-State and State Machines","uri":"/lec15-state-and-state-machines/"},{"categories":["UCB-CS61C"],"content":"max clock frequency ç®€å•æ€»ç»“ æ„Ÿè§‰æ•°ç”µçŸ¥è¯†æ›´å¤šä¸€ç‚¹ï¼Ÿ å‡å°‘å»¶æ—¶ ","date":"2025-01-22","objectID":"/lec15-state-and-state-machines/:3:1","tags":null,"title":"Lec15-State and State Machines","uri":"/lec15-state-and-state-machines/"},{"categories":["UCB-CS61C"],"content":"Finite State Machines ä¸€ä¸ªä¾‹å­ ç­‰ä»·ç¿»è¯‘ ","date":"2025-01-22","objectID":"/lec15-state-and-state-machines/:4:0","tags":null,"title":"Lec15-State and State Machines","uri":"/lec15-state-and-state-machines/"},{"categories":["UCB-CS61C"],"content":"Lecture 14: Intro to Synchronous Digital Systems ","date":"2025-01-22","objectID":"/lec14-intro-to-synchronous-digital-systems/:0:0","tags":null,"title":"Lec14-Intro to Synchronous Digital Systems","uri":"/lec14-intro-to-synchronous-digital-systems/"},{"categories":["UCB-CS61C"],"content":"MOS Transistors normal n-channel gate G is low, open. ","date":"2025-01-22","objectID":"/lec14-intro-to-synchronous-digital-systems/:1:0","tags":null,"title":"Lec14-Intro to Synchronous Digital Systems","uri":"/lec14-intro-to-synchronous-digital-systems/"},{"categories":["UCB-CS61C"],"content":"NAND Gates ä¸éé—¨ ","date":"2025-01-22","objectID":"/lec14-intro-to-synchronous-digital-systems/:2:0","tags":null,"title":"Lec14-Intro to Synchronous Digital Systems","uri":"/lec14-intro-to-synchronous-digital-systems/"},{"categories":["UCB-CS61C"],"content":"ç”µè·¯å­˜å‚¨ä¸æ„æˆ ç»„åˆé€»è¾‘ç”µè·¯ æœ‰çŠ¶æ€å…ƒä»¶ like å¯„å­˜å™¨ ","date":"2025-01-22","objectID":"/lec14-intro-to-synchronous-digital-systems/:3:0","tags":null,"title":"Lec14-Intro to Synchronous Digital Systems","uri":"/lec14-intro-to-synchronous-digital-systems/"},{"categories":["UMich-EECS-498"],"content":"Visualizing and Understanding ","date":"2024-11-06","objectID":"/l14-visualizing-and-understanding/:0:0","tags":null,"title":"L14-Visualizing and Understanding","uri":"/l14-visualizing-and-understanding/"},{"categories":["UMich-EECS-498"],"content":"Visualizing å¯¹ç¬¬ä¸€å±‚ã€ç¬¬äºŒå±‚ä»¥åŠæœ€åè¿›å…¥FCå±‚çš„ç‰¹å¾å›¾è¿›è¡Œå¯è§†åŒ– PCAâˆš t-SNEâˆš éçº¿æ€§é™ç»´ æœ€å¤§æ¿€æ´» ","date":"2024-11-06","objectID":"/l14-visualizing-and-understanding/:1:0","tags":null,"title":"L14-Visualizing and Understanding","uri":"/l14-visualizing-and-understanding/"},{"categories":["UMich-EECS-498"],"content":"Understanding ç»†èŠ‚ ","date":"2024-11-06","objectID":"/l14-visualizing-and-understanding/:2:0","tags":null,"title":"L14-Visualizing and Understanding","uri":"/l14-visualizing-and-understanding/"},{"categories":["UMich-EECS-498"],"content":"Training I ","date":"2024-11-04","objectID":"/l10-training-i/:0:0","tags":null,"title":"L10-Training I","uri":"/l10-training-i/"},{"categories":["UMich-EECS-498"],"content":"Activation Functions Sigmoid function: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ ä¸æ˜¯0ä¸­å¿ƒ ä¸¤ç«¯é¥±å’Œ always all positive or negative :( exp() è®¡ç®—å¤æ‚ï¼Œä½†æ˜¯å¯¹äºGPUä¸æ˜¯é—®é¢˜ tanh function: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ sigmoidå˜ä½“ ReLU function: $f(x) = max(0, x)$ ä¸ä¼šé¥±å’Œ è®¡ç®—å¿« é0ä¸­å¿ƒ dead relu ==\u003e leaky relu Leaky ReLU function: $f(x) = max(0.01x, x)$ è§£å†³äº†dead relué—®é¢˜ ==\u003e PRelu functionï¼šæŠŠ0.01æ”¹æˆå¯å­¦ä¹ çš„å‚æ•° ELU function: $f(x) = \\begin{cases} x \u0026 x \\geq 0 \\ \\alpha(e^x - 1) \u0026 x \u003c 0 \\end{cases}$ ","date":"2024-11-04","objectID":"/l10-training-i/:1:0","tags":null,"title":"L10-Training I","uri":"/l10-training-i/"},{"categories":["UMich-EECS-498"],"content":"Data Preprocessing å‚è§DATA-100ç›¸å…³è¯¾ç¨‹ æ•æ„Ÿæ€§é—®é¢˜ ","date":"2024-11-04","objectID":"/l10-training-i/:2:0","tags":null,"title":"L10-Training I","uri":"/l10-training-i/"},{"categories":["UMich-EECS-498"],"content":"Weight Initialization å‚è€ƒcmu 10414è¯¾ç¨‹notes HW3å·¦å³éƒ¨åˆ† or MSRA ï¼ˆMicroSoft Research Asiaï¼‰ ","date":"2024-11-04","objectID":"/l10-training-i/:3:0","tags":null,"title":"L10-Training I","uri":"/l10-training-i/"},{"categories":["UMich-EECS-498"],"content":"Regularization L1/L2 regularizationç•¥ï¼ŒDATA-100ç›¸å…³è¯¾ç¨‹ ","date":"2024-11-04","objectID":"/l10-training-i/:4:0","tags":null,"title":"L10-Training I","uri":"/l10-training-i/"},{"categories":["UMich-EECS-498"],"content":"Dropout NNDLçš„è§‚ç‚¹ä¹Ÿæœ‰è®²åˆ° éšæœºæ€§ å­æ¨¡å‹å åŠ  Test Timeæ¿€æ´»æ‰€æœ‰çš„ç¥ç»å…ƒï¼Œç”¨ä¸€ä¸ªç¼©æ”¾å› å­ $p$ å¤„ç†ï¼Œæˆ–è€…inverting dropout mask BNå’Œdropoutçœ‹èµ·æ¥ä¼¼ä¹ä¸€æ · ","date":"2024-11-04","objectID":"/l10-training-i/:4:1","tags":null,"title":"L10-Training I","uri":"/l10-training-i/"},{"categories":["UMich-EECS-498"],"content":"Augmentation import torchvision.transforms as transforms transform_train = transforms.Compose([ transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ]) ä»¥åŠå…¶ä»–çš„ä¸€äº›æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæ¥è‡ªECCV ICLRç­‰ ","date":"2024-11-04","objectID":"/l10-training-i/:5:0","tags":null,"title":"L10-Training I","uri":"/l10-training-i/"},{"categories":["UMich-EECS-498"],"content":"Hard and Soft ware ","date":"2024-11-02","objectID":"/l9-hard-and-software/:0:0","tags":null,"title":"L9-Hard and Software","uri":"/l9-hard-and-software/"},{"categories":["UMich-EECS-498"],"content":"Hardware eecs 598.009 GPU programming! å…¶å®æˆ‘å¾ˆæƒ³äº†è§£ä¸€ä¸‹cudaç¼–ç¨‹ tensorflowæ”¯æŒTPUï¼Œpytorchå‘¢ï¼Ÿ è®¡ç®—å›¾å­˜å‚¨åœ¨GPUå†…å­˜é‡Œé¢ ","date":"2024-11-02","objectID":"/l9-hard-and-software/:1:0","tags":null,"title":"L9-Hard and Software","uri":"/l9-hard-and-software/"},{"categories":["UMich-EECS-498"],"content":"Software the point of deep learning frameworks allow rapid prototyping automatically compute gradients run it all efficiently on GPUs or else ","date":"2024-11-02","objectID":"/l9-hard-and-software/:2:0","tags":null,"title":"L9-Hard and Software","uri":"/l9-hard-and-software/"},{"categories":["UMich-EECS-498"],"content":"PyTorch sigmoid0å‡å°‘è®¡ç®—å›¾èŠ‚ç‚¹çš„è®¾è®¡ï¼Œå› ä¸ºåå‘ä¼ æ’­é‡å†™äº† class Sigmoid(torch.autograd.Function): @staticmethod def forward(ctx, input): y = 1 / (1 + torch.exp(-input)) ctx.save_for_backward(input) return y @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (1 - y) * y return grad_input def sigmoid(x): return Sigmoid.apply(x) åŠ¨æ€è®¡ç®—å›¾ loss.backward() ä¹‹åè®¡ç®—å›¾è¢«ä¸¢å¼ƒï¼Ÿ ä¸‹ä¸€æ¬¡è®¡ç®—çš„æ—¶å€™é‡æ–°æ„å»ºï¼Ÿ éåŠ¨æ€è®¡ç®—å›¾JITï¼Œä¼˜åŒ–ï¼Œåºåˆ—åŒ– def model(x): return x * x graph = torch.jit.script(model, torch.rand(1, 1)) ","date":"2024-11-02","objectID":"/l9-hard-and-software/:2:1","tags":null,"title":"L9-Hard and Software","uri":"/l9-hard-and-software/"},{"categories":["UMich-EECS-498"],"content":"TensorFlow 1.0é™æ€ 2.0åŠ¨æ€ TensorBroad ","date":"2024-11-02","objectID":"/l9-hard-and-software/:2:2","tags":null,"title":"L9-Hard and Software","uri":"/l9-hard-and-software/"},{"categories":["UMich-EECS-498"],"content":"CNN Architectures ä»Šæ—¥é¦–ç»· ","date":"2024-11-02","objectID":"/l8-cnn-arch/:0:0","tags":null,"title":"L8-CNN Arch","uri":"/l8-cnn-arch/"},{"categories":["UMich-EECS-498"],"content":"è®¡ç®—é¢˜ 4 bytes per elem å³è¾¹ä¸‰åˆ—ä½“ç°äº†ä¸€ä¸ªè§„å¾‹ 2013çš„ImageNet winnerä»ç„¶æ˜¯AlexNetå˜ä½“(ZFNet, ECCV)ï¼Œåªæ˜¯trial and errorçš„ç»“æœ 2014çš„ImageNet winneræ˜¯VGGNet ICLRï¼Œæå‡ºäº†è§„åˆ™åŒ– 3x3å·ç§¯æ ¸? ä¸¤ä¸ª3x3å·ç§¯æ ¸ æ¯” ä¸€ä¸ª5x5å·ç§¯æ ¸ Paramså’ŒFLOPsæ›´å°‘ï¼Œä½†æ˜¯æ„Ÿå—é‡ä¸€æ ·ï¼Œå¹¶ä¸”å¯ä»¥æ’å…¥æ›´å¤šçš„relu channelç¿»å€ï¼Œæ¯æ¬¡å·ç§¯è®¡ç®—cost same amount of floating points computation 2014çš„ImageNetæœ‰GoogLeNet CVPR: åˆæœŸå¿«é€Ÿä¸‹é‡‡æ · Inceptionæ¨¡å—: 1x1, 3x3, 5x5å·ç§¯æ ¸(ä½¿å¾—kernel sizeä¸å†æ˜¯ä¸€ä¸ªè¶…å‚æ•°) 1x1é€‚é…å™¨çš„å¼•å…¥ resneté›å½¢ Global Average Pooling: æ›¿æ¢æ‰ä¸€å±‚fcnn å…¶æ¬¡è¿˜æœ‰auxiliary classifierå–ä¸­é—´å±‚è¾“å‡ºï¼Œä½œä¸ºlossåŠ å…¥åˆ°loss functionä¸­ 2015å¹´é¦–å…ˆæ˜¯BNè¢«å‘ç°äº†ï¼Œauxiliary classifierè¢«å¼ƒç”¨ æ¥ç€ResNet CVPR: å¼•å…¥æ®‹å·®ç»“æ„ï¼Œæå‡å‡†ç¡®ç‡ å¼•å…¥bottleneckç»“æ„ï¼Œå±‚æ•°å¢åŠ ï¼Œä½†æ˜¯flopså‡å°‘ ECCVæœ‰ä¸€ç¯‡è¿›ä¸€æ­¥è®¨è®ºäº†æ®‹å·®å—çš„ç»“æ„ CVPR2017æœ‰ä¸€ç¯‡æ–‡ç« æå‡ºäº†ResNeXt torch.nn.Conv2d(groups=) # groupså‚æ•°æ§åˆ¶äº†åˆ†ç»„å·ç§¯çš„æ•°é‡ 2017å¹´çš„ImageNetç»“æŸ DenseNet: fancier è¶‹åŠ¿ MobileNet: è½»é‡åŒ–è¶‹åŠ¿ ICLR 2017è‡ªåŠ¨åŒ–è®¾è®¡ç¥ç»ç½‘ç»œç»“æ„ Neural Architecture Search ","date":"2024-11-02","objectID":"/l8-cnn-arch/:1:0","tags":null,"title":"L8-CNN Arch","uri":"/l8-cnn-arch/"},{"categories":["UMich-EECS-498"],"content":"Convolutional Neural Networks ","date":"2024-10-29","objectID":"/l7-cnn/:0:0","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"Components of a CNN Convolutional layers Pooling layers Normalization layers ","date":"2024-10-29","objectID":"/l7-cnn/:1:0","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"Convolutional Layers æ³¨æ„åˆ°ä¸€ä¸ªé€šé“çš„å·ç§¯æ ¸ä¹Ÿæ˜¯å…¨é€šé“æ•° 3 x5x5 åç½®æ˜¯ä¸€ä¸ªå‘é‡ (b, c, h, w)è¡¨ç¤ºbatch size, channel, height, width! æ³¨æ„å››ä¸ªç»´åº¦çš„æ„ä¹‰ å·ç§¯æœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ç§linear layerï¼Œæ‰€ä»¥è¦reluç­‰ é«˜ç»´å…¨å±€ï¼Œä½ç»´å±€éƒ¨ ","date":"2024-10-29","objectID":"/l7-cnn/:2:0","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"1x1 Convolutions ä¸€ç§é€‚é…å™¨ï¼Œè°ƒæ•´é€šé“æ•° ","date":"2024-10-29","objectID":"/l7-cnn/:2:1","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"other types of convolutions ","date":"2024-10-29","objectID":"/l7-cnn/:2:2","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"PyTorch Implementation ","date":"2024-10-29","objectID":"/l7-cnn/:2:3","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"Pooling Layers another way to downsample data, no learnable parameters å±€éƒ¨æœ€å¤§å€¼å¾®å°ç§»åŠ¨ä¸å˜æ€§ ","date":"2024-10-29","objectID":"/l7-cnn/:3:0","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"Normalization Layers ä¸»è¦è®¨è®ºçš„æ˜¯batch normalization å±‚ä¸å±‚ä¹‹é—´æ•°æ®åˆ†å¸ƒæ›´åŠ ç¨³å®š æ­¤æ—¶ model.eval() æ­¤æ—¶bnå¯ä»¥ä½œä¸ºçº¿å½¢å±‚è¢«fuseè¿›å…¥fcnn or conv layer normä¹Ÿæœ‰ï¼Œä¸»è¦æ˜¯rnnå’Œtransformerç”¨åˆ°äº† ","date":"2024-10-29","objectID":"/l7-cnn/:4:0","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"Example: LeNet-5 ","date":"2024-10-29","objectID":"/l7-cnn/:5:0","tags":null,"title":"L7-CNN","uri":"/l7-cnn/"},{"categories":["UMich-EECS-498"],"content":"Backpropagation å‚è§cmu 10-414 ğŸ˜€ ","date":"2024-10-24","objectID":"/l6-bp/:0:0","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"RNN åˆè§ ","date":"2024-10-24","objectID":"/l6-bp/:1:0","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"Computation Graph ğŸ˜ A2è¦hardcodeç›´æ¥çš„åå‘ä¼ æ’­äº† 555 çœŸæ­£çš„ä»£ç  ğŸ˜‹ class Multiply(torch.autograd.Function): @staticmethod def forward(ctx, x, y): ctx.save_for_backward(x, y) return x * y @staticmethod def backward(ctx, grad_output): x, y = ctx.saved_tensors return grad_output * y, grad_output * x # è§£æè®¡ç®— PyTorch operators in deep engine ğŸ¤” ","date":"2024-10-24","objectID":"/l6-bp/:2:0","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"BP rules ","date":"2024-10-24","objectID":"/l6-bp/:3:0","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"BP with vector-valued functions å‡è£…æ ‡é‡æ±‚å¯¼ï¼Œç„¶ååŒ¹é…çŸ©é˜µå½¢çŠ¶å³å¯ï¼ˆå…¸ä¸­å…¸ï¼‰ ","date":"2024-10-24","objectID":"/l6-bp/:3:1","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"element-wise functions in BP ä¸ç”¨ä½¿ç”¨çŸ©é˜µæ±‚å¯¼ï¼Œç›´æ¥ä¸€ä¸€å¯¹åº”å»æƒ³æ¢¯åº¦çš„ä¼ é€’å³å¯ æ­¤æ—¶é›…å„æ¯”çŸ©é˜µååˆ†ç¨€ç–ï¼ˆä½æ•ˆï¼‰ï¼Œä¸ç›´æ¥æ˜¾å¼è®¡ç®—ï¼Œè€Œæ˜¯é€šè¿‡shape / maskéšå¼æ¥åš ","date":"2024-10-24","objectID":"/l6-bp/:3:2","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"matrix(or tensor)-valued functions in BP a possible strategy å…ƒç´ å±‚é¢æ¥çœ‹ï¼ˆè¿™ä¸ªæ€è·¯æ¯”è¾ƒç®€å•ï¼Œä½†æ•ˆç‡ä¸é«˜ï¼‰ ç§‘å­¦è®¡ç®—AD ","date":"2024-10-24","objectID":"/l6-bp/:3:3","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"BP for Higher Order Derivatives PyTorchçœŸæ­£çš„å®ç°ï¼Œå¹¶éç›´æ¥åå‘ ","date":"2024-10-24","objectID":"/l6-bp/:4:0","tags":null,"title":"L6-BP","uri":"/l6-bp/"},{"categories":["UMich-EECS-498"],"content":"Neural Networks ","date":"2024-10-24","objectID":"/l5-nn/:0:0","tags":null,"title":"L5-NN","uri":"/l5-nn/"},{"categories":["UMich-EECS-498"],"content":"çº¿æ€§ä¸å¯åˆ†æ€ä¹ˆå¤„ç†ï¼Ÿ æ€ä¹ˆencodeä¸åŒçš„ä¿¡æ¯ï¼Ÿ ç‹æœè½å¹• ","date":"2024-10-24","objectID":"/l5-nn/:1:0","tags":null,"title":"L5-NN","uri":"/l5-nn/"},{"categories":["UMich-EECS-498"],"content":"Neural Networks Architecture data -\u003e input layer -\u003e hidden layer -\u003e output layer data driven, non-linear ğŸ˜‹ ğŸ˜„ ? ","date":"2024-10-24","objectID":"/l5-nn/:2:0","tags":null,"title":"L5-NN","uri":"/l5-nn/"},{"categories":["UMich-EECS-498"],"content":"ReLU, é€šç”¨è¿‘ä¼¼å®šç† è¯¦ç»†è¯æ˜çœ‹ä¸€çœ‹å®˜æ–¹ç»™å‡ºçš„è¯¾æœ¬ ","date":"2024-10-24","objectID":"/l5-nn/:3:0","tags":null,"title":"L5-NN","uri":"/l5-nn/"},{"categories":["UMich-EECS-498"],"content":"å‡¸ä¼˜åŒ– ç‰¢NNå½“ç„¶æ˜¯è¿›è¡Œéå‡¸ä¼˜åŒ– ğŸ˜€ ","date":"2024-10-24","objectID":"/l5-nn/:4:0","tags":null,"title":"L5-NN","uri":"/l5-nn/"},{"categories":["UMich-EECS-498"],"content":"Optimization ","date":"2024-10-24","objectID":"/l4-optimization/:0:0","tags":null,"title":"L4-Optimization","uri":"/l4-optimization/"},{"categories":["UMich-EECS-498"],"content":"grad check è§£æè®¡ç®—åæ•°å€¼è®¡ç®—æ¢¯åº¦éªŒè¯ä¹‹ import torch torch.autograd.gradcheck(func, inputs, eps=1e-6, atol=1e-4, raise_exception=True) torch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-6, atol=1e-4, raise_exception=True) # numpy allclose np.allclose(a, b, rtol=1e-5, atol=1e-8) ","date":"2024-10-24","objectID":"/l4-optimization/:1:0","tags":null,"title":"L4-Optimization","uri":"/l4-optimization/"},{"categories":["UMich-EECS-498"],"content":"æ¢¯åº¦ä¸‹é™æ³• ç•¥ ","date":"2024-10-24","objectID":"/l4-optimization/:2:0","tags":null,"title":"L4-Optimization","uri":"/l4-optimization/"},{"categories":["UMich-EECS-498"],"content":"è’™ç‰¹å¡æ´› æŠŠæœŸæœ›è½¬åŒ–ä¸ºæ•°å€¼æ±‚è§£ï¼Œä¸Šå›¾çš„ç­‰å¼ä¸¤è¾¹æ±‚å¯¼ä¹Ÿå¯ï¼Œä»è€Œå¾—åˆ°æ¢¯åº¦ ","date":"2024-10-24","objectID":"/l4-optimization/:3:0","tags":null,"title":"L4-Optimization","uri":"/l4-optimization/"},{"categories":["UMich-EECS-498"],"content":"æ··åˆä¼˜åŒ–æ–¹æ³• Adam = RMSprop + Momentum ğŸ˜® ä¸ºä»€ä¹ˆåªæ˜¯ä¸€é˜¶å‡½æ•°ä¼˜åŒ–ï¼Ÿ ","date":"2024-10-24","objectID":"/l4-optimization/:4:0","tags":null,"title":"L4-Optimization","uri":"/l4-optimization/"},{"categories":["UCB-CS61A"],"content":"Final Project: The Game of Life ? about the title: well, umm, this title just pop up in the cursor and I donâ€™t mean to write thatâ€¦ it just feels like a good idea to write something about the game of life ==\u003e 61B or 61C must have a project on this topic ğŸ¤” ","date":"2024-10-23","objectID":"/lec37-final/:0:0","tags":null,"title":"Lec37-Final","uri":"/lec37-final/"},{"categories":["UCB-CS61A"],"content":"Trees ","date":"2024-10-23","objectID":"/lec37-final/:1:0","tags":null,"title":"Lec37-Final","uri":"/lec37-final/"},{"categories":["UCB-CS61A"],"content":"Processing defé‡Œé¢defçš„é£æ ¼æ„Ÿè§‰æ˜¯CS106é‡Œé¢æåˆ°çš„é€’å½’helper functionçš„é£æ ¼ ğŸ˜® ","date":"2024-10-23","objectID":"/lec37-final/:1:1","tags":null,"title":"Lec37-Final","uri":"/lec37-final/"},{"categories":["UCB-CS61A"],"content":"Recursive Accumulation using static variables to accumulate the results of recursive calls ğŸ˜® ","date":"2024-10-23","objectID":"/lec37-final/:2:0","tags":null,"title":"Lec37-Final","uri":"/lec37-final/"},{"categories":["UCB-CS61A"],"content":"how to design a Function? example driven ğŸ‰ summer went away, stillness stayed ğŸ‰ ","date":"2024-10-23","objectID":"/lec37-final/:3:0","tags":null,"title":"Lec37-Final","uri":"/lec37-final/"},{"categories":["UCB-CS61A"],"content":"Databases ","date":"2024-10-23","objectID":"/lec36-databases/:0:0","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"method of tables ","date":"2024-10-23","objectID":"/lec36-databases/:1:0","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"create and drop tables ğŸ¤“ create table numbers (n, note); create table numbers (n UNIQUE, note DEFAULT 'unknown'); drop table if exists t; ","date":"2024-10-23","objectID":"/lec36-databases/:1:1","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"insert data into tables insert into t values (1, 'one'); insert into t(col1) values (2); ","date":"2024-10-23","objectID":"/lec36-databases/:1:2","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"update update t set col1 = 3 where col2 = 'two'; ","date":"2024-10-23","objectID":"/lec36-databases/:1:3","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"delete delete from t where col1 = 1; ","date":"2024-10-23","objectID":"/lec36-databases/:1:4","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"Python and SQL import sqlite3 # connect to the database conn = sqlite3.connect('mydatabase.db') conn.execute('''CREATE TABLE IF NOT EXISTS mytable (id INTEGER PRIMARY KEY, name TEXT, age INTEGER)''') conn.execute(\"INSERT INTO mytable (name, age) VALUES ('Alice', 25)\") # a cursor object is used conn.commit() # save changes into db file ","date":"2024-10-23","objectID":"/lec36-databases/:2:0","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"database connection ä¸€ä¸ªdbå¯ä»¥è¢«å¤šä¸ªç¨‹åºè¿æ¥æ“ä½œ ğŸ˜‹ ","date":"2024-10-23","objectID":"/lec36-databases/:3:0","tags":null,"title":"Lec36-Databases","uri":"/lec36-databases/"},{"categories":["UCB-CS61A"],"content":"Aggregation see here ","date":"2024-10-23","objectID":"/lec35-agg/:0:0","tags":null,"title":"Lec35-Agg","uri":"/lec35-agg/"},{"categories":["UCB-CS61A"],"content":"Tables ","date":"2024-10-23","objectID":"/lec34-tables/:0:0","tags":null,"title":"Lec34-Tables","uri":"/lec34-tables/"},{"categories":["UCB-CS61A"],"content":"Joining Tables è§CS186 é‡ç‚¹ä»eval / SICP è§’åº¦çœ‹è¯­å¥æ˜¯å¦‚ä½•è¢«evalçš„ ","date":"2024-10-23","objectID":"/lec34-tables/:1:0","tags":null,"title":"Lec34-Tables","uri":"/lec34-tables/"},{"categories":["UCB-CS61A"],"content":"Strings methods ","date":"2024-10-23","objectID":"/lec34-tables/:2:0","tags":null,"title":"Lec34-Tables","uri":"/lec34-tables/"},{"categories":["UCB-CS61A"],"content":"SQL DATA100 / CS186å…ˆå¯¼è¯¾åå‰¯å…¶å® ğŸ˜ ","date":"2024-10-23","objectID":"/lec33-sql/:0:0","tags":null,"title":"Lec33-SQL","uri":"/lec33-sql/"},{"categories":["UCB-CS61A"],"content":"create ä¸è¯´äº†è§186ç¬¬ä¸€èŠ‚è¯¾ç¬¬äºŒèŠ‚è¯¾ ","date":"2024-10-23","objectID":"/lec33-sql/:1:0","tags":null,"title":"Lec33-SQL","uri":"/lec33-sql/"},{"categories":["UCB-CS61A"],"content":"overview ","date":"2024-10-23","objectID":"/lec33-sql/:2:0","tags":null,"title":"Lec33-SQL","uri":"/lec33-sql/"},{"categories":["UCB-CS61A"],"content":"select select ... union select ... union select ... union ","date":"2024-10-23","objectID":"/lec33-sql/:3:0","tags":null,"title":"Lec33-SQL","uri":"/lec33-sql/"},{"categories":["UCB-CS61A"],"content":"Macros ","date":"2024-10-23","objectID":"/lec32-macros/:0:0","tags":null,"title":"Lec32-Macros","uri":"/lec32-macros/"},{"categories":["UCB-CS61A"],"content":"quote or unquote åœ¨äºæ€ä¹ˆeval ğŸ¤” , åé¢çš„ (some_expression) ä¸ä¼šè¢«quote ","date":"2024-10-23","objectID":"/lec32-macros/:1:0","tags":null,"title":"Lec32-Macros","uri":"/lec32-macros/"},{"categories":["UCB-CS61A"],"content":"Macros in Scheme define new special form in schemeâ€¦ define-macro å®šä¹‰ä¸€ä¸ªæ–°çš„å®ï¼Œæ¥æ”¶ä¸€ä¸ª \u003cexpression\u003e ä½œä¸ºå‚æ•°ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„\u003cexpression\u003e å¹¶ä¸”evalä¹‹, å…ˆä¸å¯¹å‚æ•°eval æ³¨æ„å’Œdefineçš„åŒºåˆ«ï¼Œdefineåœ¨æ„çš„æ˜¯ä¼ è¿›æ¥çš„ value another example, to show info of your expression using macro (define-macro (check expr) (list 'if expr ''pass (list 'quote (list 'failed: expr)))) ","date":"2024-10-23","objectID":"/lec32-macros/:2:0","tags":null,"title":"Lec32-Macros","uri":"/lec32-macros/"},{"categories":["UCB-CS61A"],"content":"For Macro in Scheme (define (map fn vals) (if (null? vals) () (cons (fn (car vals)) (map fn (cdr vals))))) \u003e (map (lambda (x) (* x x)) '(2 3 4 5)) ; without using macro case (4 9 16 25) (define-macro (for sym vals expr) (list 'map lambda (sym) (expr) (vals)) ;? wrong! (list 'map (list 'lambda (list sym) expr) vals)) ","date":"2024-10-23","objectID":"/lec32-macros/:3:0","tags":null,"title":"Lec32-Macros","uri":"/lec32-macros/"},{"categories":["UCB-CS61A"],"content":"Trace def trace(fn): def traced(n): print(f\"Calling {fn.__name__} with {n}\") return fn(n) # n is fn's argument! return traced @trace def fact(n): if n == 0: return 1 else: return n * fact(n-1) print(fact(5)) scheme example, without using macro with using macro ","date":"2024-10-23","objectID":"/lec32-macros/:4:0","tags":null,"title":"Lec32-Macros","uri":"/lec32-macros/"},{"categories":["UCB-CS61A"],"content":"Interpreters ","date":"2024-10-22","objectID":"/lec29-interpreters/:0:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Special Forms ","date":"2024-10-22","objectID":"/lec29-interpreters/:1:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Quotation ","date":"2024-10-22","objectID":"/lec29-interpreters/:2:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Logical Forms ","date":"2024-10-22","objectID":"/lec29-interpreters/:3:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Lambda Expressions (lambda (x) (+ x 1)) ","date":"2024-10-22","objectID":"/lec29-interpreters/:4:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Frame and Environment ","date":"2024-10-22","objectID":"/lec29-interpreters/:5:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Dynamic Scope who calls you can provide the value you need rather than static scopeâ€¦ ","date":"2024-10-22","objectID":"/lec29-interpreters/:6:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Interpreting Scheme Code æ ¸å¿ƒåœ¨æ­¤ ","date":"2024-10-22","objectID":"/lec29-interpreters/:7:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Define Expressions ","date":"2024-10-22","objectID":"/lec29-interpreters/:8:0","tags":null,"title":"Lec29-Interpreters","uri":"/lec29-interpreters/"},{"categories":["UCB-CS61A"],"content":"Tail Calls ","date":"2024-10-22","objectID":"/lec30-tail-calls/:0:0","tags":null,"title":"Lec30-Tail Calls","uri":"/lec30-tail-calls/"},{"categories":["UCB-CS61A"],"content":"Functional Programming ","date":"2024-10-22","objectID":"/lec30-tail-calls/:1:0","tags":null,"title":"Lec30-Tail Calls","uri":"/lec30-tail-calls/"},{"categories":["UCB-CS61A"],"content":"Tail Call ç›¸åŒæ—¶é—´å¤æ‚åº¦ï¼Œå¯¹ç©ºé—´å¤æ‚åº¦çš„é™åˆ¶ï¼Œå¸¸é‡ç©ºé—´è¿è¡Œ get the len of a list ","date":"2024-10-22","objectID":"/lec30-tail-calls/:2:0","tags":null,"title":"Lec30-Tail Calls","uri":"/lec30-tail-calls/"},{"categories":["UCB-CS61A"],"content":"Map and Reduce ","date":"2024-10-22","objectID":"/lec30-tail-calls/:3:0","tags":null,"title":"Lec30-Tail Calls","uri":"/lec30-tail-calls/"},{"categories":["UCB-CS61A"],"content":"Reduce eg (reduce * '(3 4 5) 2) ; 120 ","date":"2024-10-22","objectID":"/lec30-tail-calls/:3:1","tags":null,"title":"Lec30-Tail Calls","uri":"/lec30-tail-calls/"},{"categories":["UCB-CS61A"],"content":"Map Map with only a constant numbers of Frames ","date":"2024-10-22","objectID":"/lec30-tail-calls/:3:2","tags":null,"title":"Lec30-Tail Calls","uri":"/lec30-tail-calls/"},{"categories":["UCB-CS61A"],"content":"General Computing Machine ğŸ˜® ","date":"2024-10-22","objectID":"/lec30-tail-calls/:4:0","tags":null,"title":"Lec30-Tail Calls","uri":"/lec30-tail-calls/"},{"categories":["UCB-CS61A"],"content":"Programs as Data ","date":"2024-10-22","objectID":"/lec31-programs-as-data/:0:0","tags":null,"title":"Lec31-Programs as Data","uri":"/lec31-programs-as-data/"},{"categories":["UCB-CS61A"],"content":"eval åœ¨å­—ç¬¦ä¸²ä¸Šé¢eval ","date":"2024-10-22","objectID":"/lec31-programs-as-data/:1:0","tags":null,"title":"Lec31-Programs as Data","uri":"/lec31-programs-as-data/"},{"categories":["UCB-CS61A"],"content":"Generating Code Quasiquotes while statements in Schemeâ€¦how to generate generic code ","date":"2024-10-22","objectID":"/lec31-programs-as-data/:2:0","tags":null,"title":"Lec31-Programs as Data","uri":"/lec31-programs-as-data/"},{"categories":["UCB-CS61A"],"content":"Calculator ","date":"2024-10-21","objectID":"/lec28-calculator/:0:0","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"Exception raise Exception(\"Invalid input\") try: # code that may raise an exception except Exception as e: print(e) è§java try-catch ğŸ˜ float('inf') # positive infinity float('-inf') # negative infinity ","date":"2024-10-21","objectID":"/lec28-calculator/:1:0","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"Programming Languages Programs are treesâ€¦ and the way interpreters work is through a tree recursion. ","date":"2024-10-21","objectID":"/lec28-calculator/:2:0","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"Parsing æŠŠæ–‡æœ¬è½¬åŒ–ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆAbstract Syntax Treeï¼ŒASTï¼‰ base case: only symbols and numbers recursive case: expressions and statements ","date":"2024-10-21","objectID":"/lec28-calculator/:3:0","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"Scheme-Syntax Calculator using Python Pair to describe pairs of expressions and statements ","date":"2024-10-21","objectID":"/lec28-calculator/:4:0","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"the eval function def calc_apply(op, args): \"\"\" args: Iterable \"\"\" if op == '+': ... elif op == '-': ... elif op == '*': ... elif op == '/': ... else: raise Exception(\"Invalid operator\") ","date":"2024-10-21","objectID":"/lec28-calculator/:4:1","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"interactive cli Read-Eval-Print-Loop (REPL) ğŸ˜® ","date":"2024-10-21","objectID":"/lec28-calculator/:4:2","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"raise exception ","date":"2024-10-21","objectID":"/lec28-calculator/:4:3","tags":null,"title":"Lec28-Calculator","uri":"/lec28-calculator/"},{"categories":["UCB-CS61A"],"content":"Scheme Lists ","date":"2024-10-21","objectID":"/lec27-scheme-lists/:0:0","tags":null,"title":"Lec27-Scheme Lists","uri":"/lec27-scheme-lists/"},{"categories":["UCB-CS61A"],"content":"cons / car / cdr / nil (null? nil) ; #t (null? (cons 1 nil)) ; #f (car (cons 1 2)) ; 1 (list 1 2 3) ; (1 2 3) ","date":"2024-10-21","objectID":"/lec27-scheme-lists/:1:0","tags":null,"title":"Lec27-Scheme Lists","uri":"/lec27-scheme-lists/"},{"categories":["UCB-CS61A"],"content":"Symbolic Programming Lisp is a symbolic programming language, which uses in AI for a long timeâ€¦? æ³¨æ„å•å¼•å· (car (cdr (car (cdr '(1 (2 3) 4))))) ; 3 ","date":"2024-10-21","objectID":"/lec27-scheme-lists/:2:0","tags":null,"title":"Lec27-Scheme Lists","uri":"/lec27-scheme-lists/"},{"categories":["UCB-CS61A"],"content":"List Processing çº¯çœ‹schemeå±å®æœ‰ç‚¹æŠ½è±¡äº† helper function åŒ–ç®€ä¹‹ è¿›ä¸€æ­¥åŒ–ç®€ è¯­æ³•ç¨å¾®äº†è§£ä¸€äº›ï¼Œä¼¼ä¹interpreteræ‰æ˜¯é‡ç‚¹ ğŸ¤” ","date":"2024-10-21","objectID":"/lec27-scheme-lists/:3:0","tags":null,"title":"Lec27-Scheme Lists","uri":"/lec27-scheme-lists/"},{"categories":["UCB-CS61A"],"content":"Efficiency ","date":"2024-10-21","objectID":"/lec23-eifficiency/:0:0","tags":null,"title":"Lec23-Eifficiency","uri":"/lec23-eifficiency/"},{"categories":["UCB-CS61A"],"content":"Memorization Idea: Memorize a large set of information and use it to quickly recall it. ","date":"2024-10-21","objectID":"/lec23-eifficiency/:1:0","tags":null,"title":"Lec23-Eifficiency","uri":"/lec23-eifficiency/"},{"categories":["UCB-CS61A"],"content":"fib def memo(f): cache = {} def memoized(n): if n not in cache: cache[n] = f(n) return cache[n] return memoized ","date":"2024-10-21","objectID":"/lec23-eifficiency/:2:0","tags":null,"title":"Lec23-Eifficiency","uri":"/lec23-eifficiency/"},{"categories":["UCB-CS61A"],"content":"Exponentiation å‰è€… O(N) ï¼Œåè€… O(logN) ","date":"2024-10-21","objectID":"/lec23-eifficiency/:3:0","tags":null,"title":"Lec23-Eifficiency","uri":"/lec23-eifficiency/"},{"categories":["UCB-CS61A"],"content":"çœæµ CS61Bå‰ç»ï¼Œå¯¹æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦åˆæ­¥æ¢ç´¢ ","date":"2024-10-21","objectID":"/lec23-eifficiency/:4:0","tags":null,"title":"Lec23-Eifficiency","uri":"/lec23-eifficiency/"},{"categories":["UCB-CS61A"],"content":"Decomposition ","date":"2024-10-21","objectID":"/lec24-decomposition/:0:0","tags":null,"title":"Lec24-Decomposition","uri":"/lec24-decomposition/"},{"categories":["UCB-CS61A"],"content":"Modular Design dddd ","date":"2024-10-21","objectID":"/lec24-decomposition/:1:0","tags":null,"title":"Lec24-Decomposition","uri":"/lec24-decomposition/"},{"categories":["UCB-CS61A"],"content":"æœ‰è¶£çš„è¯­æ³• ... # ç”¨äºçœç•¥ä»£ç  import json for i in open('data.json'): data = json.loads(i) # å¤„ç†æ•°æ®... ","date":"2024-10-21","objectID":"/lec24-decomposition/:2:0","tags":null,"title":"Lec24-Decomposition","uri":"/lec24-decomposition/"},{"categories":["UCB-CS61A"],"content":"Data Examples ","date":"2024-10-21","objectID":"/lec25-data-examples/:0:0","tags":null,"title":"Lec25-Data Examples","uri":"/lec25-data-examples/"},{"categories":["UCB-CS61A"],"content":"Lists in Environment Diagrams æ³¨æ„ï¼šåˆ‡ç‰‡ or addition æ˜¯å¼•ç”¨ï¼Œè€Œä¸æ˜¯å¤åˆ¶ï¼Œæœ‰æ—¶å€™å¯èƒ½ä¸ç»æ„æ”¹å˜ \u003e\u003e\u003e t = [1,2,3] \u003e\u003e\u003e t[1:3] = [t] \u003e\u003e\u003e t.extend(t) \u003e\u003e\u003e print(t) [1, [...], 1, [...]] åœ¨Pythonä¸­ï¼Œå½“ä½ å°è¯•æ‰“å°ä¸€ä¸ªåŒ…å«è‡ªèº«çš„åˆ—è¡¨æ—¶ï¼Œåˆ—è¡¨æ— æ³•é€’å½’åœ°æ‰“å°å‡ºæ‰€æœ‰çš„å…ƒç´ ï¼Œå› ä¸ºå®ƒä¼šæ— é™å¾ªç¯åœ°å¼•ç”¨è‡ªå·±ã€‚ä¸ºäº†é¿å…è¿™ç§æ— é™å¾ªç¯ï¼ŒPythonä¼šç”¨[â€¦]æ¥è¡¨ç¤ºåˆ—è¡¨ä¸­è¢«çœç•¥çš„éƒ¨åˆ†ã€‚ ","date":"2024-10-21","objectID":"/lec25-data-examples/:1:0","tags":null,"title":"Lec25-Data Examples","uri":"/lec25-data-examples/"},{"categories":["UCB-CS61A"],"content":"Obj Systems in Python return a string -\u003e 'str', print a string -\u003e str .some_attr: can create a new attribute for the instance at once, or get the value of an existing attribute ","date":"2024-10-21","objectID":"/lec25-data-examples/:2:0","tags":null,"title":"Lec25-Data Examples","uri":"/lec25-data-examples/"},{"categories":["UCB-CS61A"],"content":"Iterators and Iterables in Python training test def digit_dict(s): \"\"\" \u003e\u003e\u003e digit_dict([5, 2, 13]) {2: [2], 3: [13], 5: [5]} \"\"\" last_digit = [x % 10 for x in s] return {d: [x for x in s if x % 10 == d] for d in range(10) if d in last_digit} .count() method: \u003cbuilt-in method count of list object at 0x000001\u003e [1 for x in range(10) if ...] è®¡æ•° ","date":"2024-10-21","objectID":"/lec25-data-examples/:3:0","tags":null,"title":"Lec25-Data Examples","uri":"/lec25-data-examples/"},{"categories":["UCB-CS61A"],"content":"Scheme ","date":"2024-10-21","objectID":"/lec26-scheme/:0:0","tags":null,"title":"Lec26-Scheme","uri":"/lec26-scheme/"},{"categories":["UCB-CS61A"],"content":"Scheme Fundamentals (number? 123) ; #t (number? \"123\") ; #f (string? \"hello\") ; #t ","date":"2024-10-21","objectID":"/lec26-scheme/:1:0","tags":null,"title":"Lec26-Scheme","uri":"/lec26-scheme/"},{"categories":["UCB-CS61A"],"content":"Special Forms (define x 10) ; define a variable x with value 10 (if #t 10 20) ; if #t is true, return 10, otherwise return 20 (cond ((= x 10) \"x is 10\") ((= x 20) \"x is 20\") (else \"x is not 10 or 20\")) ; conditional statement (and #t #f) ; #f (or #t #f) ; #t (let ((x 10) (y 20)) (+ x y)) ; let statement to create local variables (let* ((x 10) (y (+ x 10))) (+ x y)) ; let* statement to create local variables and use the value of previous variables (lambda (x) (+ x 10)) ; lambda expression to create a function that adds 10 to a given number (map (lambda (x) (+ x 10)) (list 1 2 3)) ; map function to apply a function to each element of a list (define (sum-of-squares x y) (+ (* x x) (* y y))) ; define a function that takes two numbers and returns their sum of squares (sum-of-squares 3 4) ; 25 ","date":"2024-10-21","objectID":"/lec26-scheme/:2:0","tags":null,"title":"Lec26-Scheme","uri":"/lec26-scheme/"},{"categories":["UCB-CS61A"],"content":"lambda expression (define (add-ten x) (+ x 10)) ; define a function that adds 10 to a given number (add-ten 5) ; 15 ","date":"2024-10-21","objectID":"/lec26-scheme/:2:1","tags":null,"title":"Lec26-Scheme","uri":"/lec26-scheme/"},{"categories":["UCB-CS61A"],"content":"Cond and Begin ","date":"2024-10-21","objectID":"/lec26-scheme/:2:2","tags":null,"title":"Lec26-Scheme","uri":"/lec26-scheme/"},{"categories":["UCB-CS61A"],"content":"let ä¸´æ—¶ç»‘å®š defineåˆ™æ˜¯æ°¸ä¹…ç»‘å®š ","date":"2024-10-21","objectID":"/lec26-scheme/:2:3","tags":null,"title":"Lec26-Scheme","uri":"/lec26-scheme/"},{"categories":["UCB-CS61A"],"content":"Composition ","date":"2024-10-20","objectID":"/lec22-composition/:0:0","tags":null,"title":"Lec22-Composition","uri":"/lec22-composition/"},{"categories":["UCB-CS61A"],"content":"Linked Lists ğŸ˜ é€’å½’ å¢ã€åˆ ã€æ”¹ã€æŸ¥ ","date":"2024-10-20","objectID":"/lec22-composition/:1:0","tags":null,"title":"Lec22-Composition","uri":"/lec22-composition/"},{"categories":["UCB-CS61A"],"content":"Trees ç”¨å¥½é€’å½’ ","date":"2024-10-20","objectID":"/lec22-composition/:2:0","tags":null,"title":"Lec22-Composition","uri":"/lec22-composition/"},{"categories":["UCB-CS61A"],"content":"Representation ","date":"2024-10-20","objectID":"/lec21-representation/:0:0","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"Strings ä¸€ä¸ªæ˜¯Pythonè¡¨è¾¾å¼ï¼ˆè§£é‡Šå™¨å¯ä»¥ç›´æ¥è¿è¡Œçš„å½¢å¼ï¼‰ï¼Œä¸€ä¸ªæ˜¯å­—ç¬¦ä¸²ï¼ˆäººç±»å¯è¯»çš„å½¢å¼ï¼‰ ","date":"2024-10-20","objectID":"/lec21-representation/:1:0","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"repr \u003e\u003e\u003e s = 'hello' \u003e\u003e\u003e print(repr(s)) 'hello' \u003e\u003e\u003e repr(min) '\u003cbuilt-in function min\u003e' ","date":"2024-10-20","objectID":"/lec21-representation/:1:1","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"str \u003e\u003e\u003e from fractions import Fraction \u003e\u003e\u003e half = Fraction(1, 2) \u003e\u003e\u003e print(half) # calls __str__ 1/2 \u003e\u003e\u003e repr(half) # calls __repr__ 'Fraction(1, 2)' \u003e\u003e\u003e eval(repr(half)) # creates a new Fraction object Fraction(1, 2) ","date":"2024-10-20","objectID":"/lec21-representation/:1:2","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"F-Strings { }é‡Œé¢çš„æŒ‰ç…§è¡¨è¾¾å¼æ¥è®¡ç®— \u003e\u003e\u003e x = 10 \u003e\u003e\u003e y = 20 \u003e\u003e\u003e f\"x + y = {x + y}\" 'x + y = 30' ","date":"2024-10-20","objectID":"/lec21-representation/:2:0","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"å¤šæ€å‡½æ•° Functions that apply to many different forms of dataâ€¦ \u003e\u003e\u003e half.__repr__() 'Fraction(1, 2)' \u003e\u003e\u003e half.__str__() '1/2' å®ç° __repr__ å’Œ __str__ æ–¹æ³• __repr__: Only class attributes are found! Ignore instance attributes. __str__: Ignore instance attributes. If no __str__ method is defined, it will use __repr__ method. def repr(obj): return type(obj).__repr__(obj) ","date":"2024-10-20","objectID":"/lec21-representation/:3:0","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"Interfaces ç±»ä¼¼61Bçš„è¯¾ç¨‹ï¼Œå®ç°æ¥å£ï¼ˆé•¿å¾—åƒæ˜¯__xx__ï¼‰ å–œæ¬¢Java ğŸ˜‹ ","date":"2024-10-20","objectID":"/lec21-representation/:4:0","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"special methods ç¾¤ è´¤ æ¯• è‡³ ç»ˆäºå’Œcmu 10-414å¯¹ä¸Šäº† å·¦å³å€¼ ","date":"2024-10-20","objectID":"/lec21-representation/:4:1","tags":null,"title":"Lec21-Representation","uri":"/lec21-representation/"},{"categories":["UCB-CS61A"],"content":"Generators a kind of special iterator! ","date":"2024-10-20","objectID":"/lec17-generators/:0:0","tags":null,"title":"Lec17-Generators","uri":"/lec17-generators/"},{"categories":["UCB-CS61A"],"content":"yield def count_to(n): for i in range(n): yield i for i in count_to(5): print(i) t = count_to(5) # t is like a generator object at \u003cgenerator object count_to at 0x000001\u003e print(next(t)) ","date":"2024-10-20","objectID":"/lec17-generators/:1:0","tags":null,"title":"Lec17-Generators","uri":"/lec17-generators/"},{"categories":["UCB-CS61A"],"content":"Generator vs Iterator yield from is a new syntax in Python 3.3 that allows you to delegate iteration to another generator. def countdown(k): if k \u003e 0: yield k yield from countdown(k-1) # element-wise iteration of the sub-generator for i in countdown(5): print(i) list(countdown(5)) # [5, 4, 3, 2, 1] ","date":"2024-10-20","objectID":"/lec17-generators/:2:0","tags":null,"title":"Lec17-Generators","uri":"/lec17-generators/"},{"categories":["UCB-CS61A"],"content":"Example: ","date":"2024-10-20","objectID":"/lec17-generators/:3:0","tags":null,"title":"Lec17-Generators","uri":"/lec17-generators/"},{"categories":["UCB-CS61A"],"content":"Object-Oriented Programming ","date":"2024-10-20","objectID":"/lec18-oop/:0:0","tags":null,"title":"Lec18-OOP","uri":"/lec18-oop/"},{"categories":["UCB-CS61A"],"content":"class class MyClass: def __init__(self, x, y): # constructor # instance variables self.x = x self.y = y def my_method(self): # non-static method print(self.x, self.y) @staticmethod def my_static_method(x, y): # static method print(x, y) Python uses dynamic typingâ€¦â€¦ ğŸ˜‹ Binding an object to a new name does not create a new object. It creates a new reference to the same object. ","date":"2024-10-20","objectID":"/lec18-oop/:1:0","tags":null,"title":"Lec18-OOP","uri":"/lec18-oop/"},{"categories":["UCB-CS61A"],"content":"Attributes ","date":"2024-10-20","objectID":"/lec19-attributes/:0:0","tags":null,"title":"Lec19-Attributes","uri":"/lec19-attributes/"},{"categories":["UCB-CS61A"],"content":"class attribute class MyClass: x = 10 def __init__(self): self.y = 20 print(MyClass.x) # 10 like static member variables ","date":"2024-10-20","objectID":"/lec19-attributes/:1:0","tags":null,"title":"Lec19-Attributes","uri":"/lec19-attributes/"},{"categories":["UCB-CS61A"],"content":"getattr and hasattr class MyClass: x = 10 def __init__(self): self.y = 20 print(getattr(MyClass, 'x')) # 10 print(hasattr(MyClass, 'z')) # False ","date":"2024-10-20","objectID":"/lec19-attributes/:1:1","tags":null,"title":"Lec19-Attributes","uri":"/lec19-attributes/"},{"categories":["UCB-CS61A"],"content":"assignment to attributes å¦‚æœé‡åï¼Œå…ˆæŸ¥çœ‹å®ä¾‹çš„å±æ€§ ","date":"2024-10-20","objectID":"/lec19-attributes/:2:0","tags":null,"title":"Lec19-Attributes","uri":"/lec19-attributes/"},{"categories":["UCB-CS61A"],"content":"Function calls in class bound method class MyClass: def __init__(self, x): self.x = x def my_method(self): print(self.x) obj = MyClass(10) obj.my_method # \u003cbound method MyClass.my_method of \u003c__main__.MyClass object at 0x000001E8D7D7D708\u003e\u003e why bound? because the method is bound to the instance of the class, so it is filled with the self parameter ","date":"2024-10-20","objectID":"/lec19-attributes/:3:0","tags":null,"title":"Lec19-Attributes","uri":"/lec19-attributes/"},{"categories":["UCB-CS61A"],"content":"Inheritance ","date":"2024-10-20","objectID":"/lec20-inheritance/:0:0","tags":null,"title":"Lec20-Inheritance","uri":"/lec20-inheritance/"},{"categories":["UCB-CS61A"],"content":"Object-Oriented Design ä»£ç å¤ç°ï¼ æ³¨æ„instanceå±æ€§å¯ä»¥éšæ—¶æ›´æ”¹,look at the instance, then the subclass before looking at the superclass ","date":"2024-10-20","objectID":"/lec20-inheritance/:1:0","tags":null,"title":"Lec20-Inheritance","uri":"/lec20-inheritance/"},{"categories":["UCB-CS61A"],"content":"Multiple Inheritance class A: def __init__(self, x): self.x = x class B: def __init__(self, y): self.y = y def __str__(self): return f\"B(y={self.y})\" class C(A, B): def __init__(self, x, y): super().__init__(x) self.y = y def __str__(self): return f\"C(x={self.x}, y={self.y})\" def f(self): return self.x + self.y c = C(1, 2) print(c) print(c.f()) ","date":"2024-10-20","objectID":"/lec20-inheritance/:2:0","tags":null,"title":"Lec20-Inheritance","uri":"/lec20-inheritance/"},{"categories":["UCB-CS61A"],"content":"Iterator iter(iterable) next(iterator) ","date":"2024-10-20","objectID":"/lec16-iterator/:0:0","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"List list(iterator) åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—è¡¨ï¼ŒåŒ…å«è¿­ä»£å™¨ä¸­çš„æ‰€æœ‰å…ƒç´  ","date":"2024-10-20","objectID":"/lec16-iterator/:1:0","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"Dictionary values, keys, items can be iterated using iter() and next() functions è¿­ä»£çš„æ—¶å€™ä¸è¦æ”¹å˜å­—å…¸çš„ç»“æ„ï¼ˆé•¿åº¦ï¼‰ï¼Œå¦åˆ™ä¼šå¯¼è‡´è¿­ä»£å‡ºé”™ ","date":"2024-10-20","objectID":"/lec16-iterator/:2:0","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"for r in rangeâ€¦ if use iterator in for statement, it will not be able to use again, because it will be exhausted after first iteration ","date":"2024-10-20","objectID":"/lec16-iterator/:3:0","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"built-in functions in Python LAZY MODE: map / filter / zip / reversed ","date":"2024-10-20","objectID":"/lec16-iterator/:4:0","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"map / filter filter see data100 ğŸ˜‹ ","date":"2024-10-20","objectID":"/lec16-iterator/:4:1","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"zip unpack the zip object into multiple variables(can be useful!) ","date":"2024-10-20","objectID":"/lec16-iterator/:4:2","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"why Iterator? ","date":"2024-10-20","objectID":"/lec16-iterator/:5:0","tags":null,"title":"Lec16-Iterator","uri":"/lec16-iterator/"},{"categories":["UCB-CS61A"],"content":"Mutability ","date":"2024-10-17","objectID":"/lec15-mutability/:0:0","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"date ","date":"2024-10-17","objectID":"/lec15-mutability/:1:0","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"Obj in Python ","date":"2024-10-17","objectID":"/lec15-mutability/:2:0","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"String s = \"Hello\" s.swapcase() # \"hELLO\" ","date":"2024-10-17","objectID":"/lec15-mutability/:3:0","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"ASCII å’Œè¡¨æ ¼å¯¹åº” 0x41 â€“\u003e row 4, col 1 ğŸ˜® from unicodedata import name, lookup name('A') # 'LATIN CAPITAL LETTER A' lookup('LATIN CAPITAL LETTER A') # 'A' lookup('SNOWMAN') # 'â˜ƒ' lookup('FACE WITH TEARS OF JOY').encode('utf-8') # 'ğŸ˜‚'.encode('utf-8') ","date":"2024-10-17","objectID":"/lec15-mutability/:3:1","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"Mutation operations ","date":"2024-10-17","objectID":"/lec15-mutability/:4:0","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"Mutable objects List Dictionary Set? # List, pop, remove, append, extend lst = [1, 2, 3] lst.pop() # 3 lst.remove(2) # [1] lst.append(4) # [1, 4] lst.extend([5, 6]) # [1, 4, 5, 6] èµ‹å€¼çš„æ—¶å€™ï¼Œå¦‚æœæ˜¯å¯å˜å¯¹è±¡ï¼Œåˆ™ä¼šå½±å“åˆ°åŸå¯¹è±¡ï¼Œå¦‚æœæ˜¯ä¸å¯å˜å¯¹è±¡ï¼Œåˆ™ä¼šåˆ›å»ºæ–°çš„å¯¹è±¡ã€‚ ğŸ˜® ","date":"2024-10-17","objectID":"/lec15-mutability/:4:1","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"Immutable objects Tuple String Number Immutable objects are hashable, which means that they can be used as keys in dictionaries and as elements in sets. An immutable object can be changed if it contains a mutable object. s = ([1, 2], 4) s[0][0] = 8 # correct s[0] = 5 # incorrect ","date":"2024-10-17","objectID":"/lec15-mutability/:4:2","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"Mutation ","date":"2024-10-17","objectID":"/lec15-mutability/:5:0","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"same or change? Identity: a is b is in Python Equality: a == b == in Python def f(s=[]): s.append(1) return s f() # [1] f() # [1, 1] f() # [1, 1, 1] frameé‡Œé¢å¼•ç”¨ä¼ é€’æ›´åŠ å¸¸è§ ğŸ¤” ","date":"2024-10-17","objectID":"/lec15-mutability/:5:1","tags":null,"title":"Lec15-Mutability","uri":"/lec15-mutability/"},{"categories":["UCB-CS61A"],"content":"Trees A tree has a root label and a list of branches and each branch is a tree itself def tree(label, branches=[]): for branch in branches: assert is_tree(branch), 'branches must be trees' return [label] + list(branches) # make sure branches is a list def label(tree): return tree[0] def branches(tree): return tree[1:] def is_tree(tree): if type(tree)!= list or len(tree) \u003c 1: return False for branch in branches(tree): if not is_tree(branch): return False return True methods: def is_leaf(tree): return not branches(tree) def fib_tree(n): if n \u003c= 1: return tree(n) else: left, right = fib_tree(n-2), fib_tree(n-1) return tree(label(left) + label(right), [left, right]) def count_leaves(tree): if is_leaf(tree): return 1 else: return sum(count_leaves(branch) for branch in branches(tree)) def leaves(tree): \"\"\" \u003e\u003e\u003e leaves(fib_tree(5)) [1, 0, 1, 0, 1, 1, 0, 1] \"\"\" if is_leaf(tree): return [label(tree)] else: return sum([leaves(branch) for branch in branches(tree)], []) def increment_leaves(tree): if is_leaf(tree): return tree(label(tree) + 1) else: return tree(label(tree), [increment_leaves(branch) for branch in branches(tree)]) def increment(tree): return tree(label(tree) + 1, [increment(branch) for branch in branches(tree)]) def print_tree(tree, indent=0): print(' ' * indent + str(label(tree))) for branch in branches(tree): print_tree(branch, indent + 1) def print_sum(tree, so_far=0): so_far += label(tree) if is_leaf(tree): print(so_far) else: for branch in branches(tree): print_sum(branch, so_far) def count_paths(tree, total): if label(tree) == total: found = 1 else: found = 0 return found + sum(count_paths(branch, total - label(tree)) for branch in branches(tree)) ","date":"2024-10-17","objectID":"/lec14-trees/:0:0","tags":null,"title":"Lec14-Trees","uri":"/lec14-trees/"},{"categories":null,"content":"Thinking2 æ–°çš„å¯èƒ½ï¼š æœºå™¨å­¦ä¹ ç¼–è¯‘ MLC æœºå™¨å­¦ä¹ ç³»ç»Ÿ MLSys æœºå™¨å­¦ä¹ è¿ç»´ MLOps é‚£ä¼šæ˜¯æ€ä¹ˆæ ·çš„å‘¢ï¼ŸğŸ¤” :( â€“å†™äº2024-10-16 61a/c \u0026 eecs498 \u0026 10414çºª ","date":"2024-10-16","objectID":"/beyondcode/thinking2/:0:0","tags":null,"title":"Thinking2","uri":"/beyondcode/thinking2/"},{"categories":["UCB-CS61A"],"content":"Data Representation what is data??? ä»å¯¹è±¡çš„è¡Œä¸ºï¼ˆå‡½æ•°ä¸æ–¹æ³•ï¼‰æ¥å®šä¹‰ ","date":"2024-10-15","objectID":"/lec13-data-representation/:0:0","tags":null,"title":"Lec13-Data Representation","uri":"/lec13-data-representation/"},{"categories":["UCB-CS61A"],"content":"Example: Rational Numbers çº¯ç²¹ä½¿ç”¨å‡½æ•°æ¥åšæ¥å£è€Œä¸æ˜¯å†…åœ¨è®¾ç½®åˆ—è¡¨å­˜å‚¨ ğŸ¤” ","date":"2024-10-15","objectID":"/lec13-data-representation/:1:0","tags":null,"title":"Lec13-Data Representation","uri":"/lec13-data-representation/"},{"categories":["UCB-CS61A"],"content":"data representation â€“ a methodology constructor \u0026 selector thoughts æ„Ÿè§‰è¿™é‡Œæ›´åƒæ˜¯apiè®¾è®¡çš„æ€è·¯ï¼Œå“²å­¦æ€æƒ³ ä»”ç»†æ€è€ƒç”¨å¥½ constructor å’Œ selector çš„ä½œç”¨ ï¼ æœ‰æ²¡æœ‰ä¸€ç§å¯èƒ½ï¼Œjavaæ²¡è¿™ä¹ˆå¤šäº‹ ","date":"2024-10-15","objectID":"/lec13-data-representation/:2:0","tags":null,"title":"Lec13-Data Representation","uri":"/lec13-data-representation/"},{"categories":["UCB-CS61A"],"content":"Pairs å’Œ __getitem__ ? from operator import getitem getitem(pair, 0) # first element getitem(pair, 1) # second element from fractions import gcd ### gcd is used to find the greatest common divisor of two numbers gcd(pair[0], pair[1]) # greatest common divisor ","date":"2024-10-15","objectID":"/lec13-data-representation/:3:0","tags":null,"title":"Lec13-Data Representation","uri":"/lec13-data-representation/"},{"categories":["UCB-CS61C"],"content":"Compiling, Assembling, Linking and Loading CALL ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:0:0","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Interpretation and Translation ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:1:0","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Interpretation æœ‰ä¸€ä¸ªè§£é‡Šå™¨ï¼ˆæ˜¯ä¸€ä¸ªç¨‹åºï¼‰ ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:1:1","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Translation ç¿»è¯‘ä¸ºä½çº§çš„è¯­è¨€é’ˆå¯¹hardwareæ›´å¿«æ“ä½œ ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:1:2","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Compiler CS164 ğŸ¤” è¿™ä¹ˆçœ‹æ¥pseudo codeç¡®å®å­˜åœ¨ï¼Ÿ ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:2:0","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Assembler ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:3:0","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Directives ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:3:1","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Replacements æŠŠpseudo codeç¿»è¯‘æˆçœŸå®çš„RISC-VæŒ‡ä»¤ ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:3:2","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Producing real machine code è®©.oæ–‡ä»¶ç¡®å®šç»ˆå€¼ ==\u003e object file ç®€å•caseï¼šç›´æ¥ç”¨.oæ–‡ä»¶ Forward reference problemï¼šç¡®å®šæ ‡ç­¾ä½ç½®ï¼Œç„¶åå†ç”¨.oæ–‡ä»¶ PC ç›¸å¯¹å¯»å€ ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:3:3","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Symbol Table and Relocation Table symbol Table label Table æ±‡ç¼–å™¨å±‚é¢ä¸çŸ¥é“static ä¹‹ç±»çš„ä¸œè¥¿ï¼Œæ‰€ä»¥éœ€è¦æš‚æ—¶åšä¸ªè®°å·ç­‰å¾…linkå¤„ç† ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:3:4","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Object File Format ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:3:5","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Linker what happen? 4 types of addressing which instructions must be linked? J-format: j / jal L-, S-format: there is a gp ! ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:4:0","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Resolving reference ç„¶ååœ¨\"user\" symbol tableä¸­æ‰¾åˆ°å¯¹åº”çš„åœ°å€ï¼Œç„¶åæ›¿æ¢æ‰åŸæ¥çš„ç¬¦å· æ¥ç€åœ¨library filesåŒæ ·æ“ä½œ æœ€åè¾“å‡ºï¼šexecutable fileï¼Œcontaining text and data (plus header)==\u003e å­˜å‚¨åœ¨ ç£ç›˜ ä¸Šé¢ ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:4:1","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"static and dynamic linking ç°åœ¨æˆ‘çŸ¥é“.dll æ–‡ä»¶æ˜¯ä»€ä¹ˆäº†ğŸ˜‹ åŠ¨æ€linké€šå¸¸åœ¨æœºå™¨ç çº§åˆ«è¿›è¡Œï¼Œè€Œä¸æ˜¯æ±‡ç¼–å™¨çº§åˆ« ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:4:2","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"Loader ä»€ä¹ˆæ˜¯loaderï¼Ÿ â€“ CS162 OSå…ˆå¯¼è¯¾ç¨‹ ğŸ˜¤ Loaderçš„ä½œç”¨ï¼š æ³¨æ„æœ€åä¸€è¡Œstart-up routineçš„programâ€™s arguments æ­£æ˜¯å’Œ argc \u0026 argv ç›¸å…³çš„ ğŸ˜® ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:5:0","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61C"],"content":"EXAMPLE hello world ! #include \u003cstdio.h\u003e int main() { printf(\"Hello, %s\\n\", \"world\"); return 0; } .s file: .o file: åªæœ‰å­—ç¬¦å­˜å‚¨åœ¨.oæ–‡ä»¶ä¸­ï¼ .out file: çº¢è‰²çš„å­—ç¬¦è¢«è¡¥å……äº† ","date":"2024-10-15","objectID":"/lec13-compiling-assembling-linking-and-loading/:6:0","tags":null,"title":"Lec13-Compiling, Assembling, Linking and Loading","uri":"/lec13-compiling-assembling-linking-and-loading/"},{"categories":["UCB-CS61A"],"content":"Container ","date":"2024-10-15","objectID":"/lec12-container/:0:0","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"box and pointers è§61B ğŸ˜‹ ","date":"2024-10-15","objectID":"/lec12-container/:1:0","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"slice ä¼ ç»Ÿpythonä¸­ï¼Œsliceæ“ä½œç¬¦çš„è¯­æ³•ä¸º[start:stop:step]ï¼Œå…¶ä¸­startå’Œstopæ˜¯åˆ‡ç‰‡çš„èµ·æ­¢ä½ç½®ï¼Œstepæ˜¯åˆ‡ç‰‡çš„æ­¥é•¿ï¼ˆå·¦é—­å³å¼€ï¼‰ åˆ‡ç‰‡ç”Ÿæˆæ–°çš„åºåˆ—ï¼Œå¯¹åŸåºåˆ—çš„ä¿®æ”¹ä¸ä¼šå½±å“åˆ‡ç‰‡ï¼ˆç‰©åŒ–viewçš„æ„Ÿè§‰ï¼‰ ","date":"2024-10-15","objectID":"/lec12-container/:2:0","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"Process value in container ","date":"2024-10-15","objectID":"/lec12-container/:3:0","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"aggregate functions sum(container)ï¼šè¿”å›å®¹å™¨ä¸­æ‰€æœ‰å…ƒç´ çš„å’Œ sum([1, 2, 3, 4, 5]) # 15 sum([1, 2], 9) # 12 sum([[2, 3], [4]], []) # [2, 3, 4] max(container)ï¼šè¿”å›å®¹å™¨ä¸­æœ€å¤§çš„å…ƒç´  max(range(10), key=lambda x: 7-(x-4)*(x-2)) # 3 all(container)ï¼šå¦‚æœå®¹å™¨ä¸­æ‰€æœ‰å…ƒç´ éƒ½ä¸ºçœŸï¼Œåˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False all([True, True, True]) # True all([True, False, True]) # False all([x \u003c 5 for x in range(5)]) ","date":"2024-10-15","objectID":"/lec12-container/:3:1","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"Strings æ³¨æ„äº‹å®ä¸Šç¨‹åºåªæ˜¯å­—ç¬¦ä¸² ğŸ˜‹ å’Œ61Bçš„textå†™javaä¸€ä¸ªé“ç† exec('some_code') ","date":"2024-10-15","objectID":"/lec12-container/:4:0","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"Dict ğŸ’¢ æˆ‘ä¸€ç›´è§‰å¾—è¿™ä¸ªå’Œ Map\u003cK, V\u003e ä»¥åŠjsonçš„å…³ç³»ä¸æ¸…ä¸æ¥š num = {'I': 1, 'V': 5, 'X': 10} list(num) # ['I', 'V', 'X'] num.values() # dict_values([1, 5, 10]) empty = {} ","date":"2024-10-15","objectID":"/lec12-container/:5:0","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"dict comprehension {x: x**2 for x in range(5)} # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} ","date":"2024-10-15","objectID":"/lec12-container/:5:1","tags":null,"title":"Lec12-Container","uri":"/lec12-container/"},{"categories":["UCB-CS61A"],"content":"Sequences ","date":"2024-10-15","objectID":"/lec11-sequences/:0:0","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"List my_list = [1, 2, 3, 4, 5] getitem(my_list, 2) # Output: 3 åŠ æ³•ä¹˜æ³•æ˜¯ æ‹¼æ¥ ","date":"2024-10-15","objectID":"/lec11-sequences/:1:0","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"Container ","date":"2024-10-15","objectID":"/lec11-sequences/:2:0","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"in my_list = [1, 2, 3, 4, 5] 5 in my_list # Output: True for i in my_list: print(i) ","date":"2024-10-15","objectID":"/lec11-sequences/:2:1","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"unpacking ","date":"2024-10-15","objectID":"/lec11-sequences/:2:2","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"range range(5) # Output: range(0, 5) range(1, 5) # Output: range(1, 5) range(1, 10, 2) # Output: range(1, 10, 2) å·¦é—­å³å¼€ ","date":"2024-10-15","objectID":"/lec11-sequences/:2:3","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"ä¸‹åˆ’çº¿å˜é‡å‘½å _ = 1 # å•ä¸ªä¸‹åˆ’çº¿ __ = 2 # åŒä¸‹åˆ’çº¿ __my_var = 3 # åŒä¸‹åˆ’çº¿å¼€å¤´çš„å˜é‡å ","date":"2024-10-15","objectID":"/lec11-sequences/:2:4","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"List comprehension [x for x in range(5) if x % 2 == 0] ","date":"2024-10-15","objectID":"/lec11-sequences/:2:5","tags":null,"title":"Lec11-Sequences","uri":"/lec11-sequences/"},{"categories":["UCB-CS61A"],"content":"Tree Recursion ","date":"2024-10-14","objectID":"/lec10-tree-recursion/:0:0","tags":null,"title":"Lec10-Tree Recursion","uri":"/lec10-tree-recursion/"},{"categories":["UCB-CS61A"],"content":"Inverse Cascade ","date":"2024-10-14","objectID":"/lec10-tree-recursion/:1:0","tags":null,"title":"Lec10-Tree Recursion","uri":"/lec10-tree-recursion/"},{"categories":["UCB-CS61A"],"content":"Tree Recursion ","date":"2024-10-14","objectID":"/lec10-tree-recursion/:2:0","tags":null,"title":"Lec10-Tree Recursion","uri":"/lec10-tree-recursion/"},{"categories":["UCB-CS61A"],"content":"è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ— def fib(n): if n == 0: return 0 elif n == 1: return 1 else: return fib(n-1) + fib(n-2) å¹¶ä¸æ˜¯æœ€å¥½çš„å®ç° -\u003e åœ¨éå†çš„æ—¶å€™ä¼šé‡å¤è®¡ç®—ç›¸åŒçš„èŠ‚ç‚¹ï¼Œå¾—æƒ³åŠæ³•å­˜å‚¨ä¸­é—´ç»“æœ ğŸ¤” ","date":"2024-10-14","objectID":"/lec10-tree-recursion/:2:1","tags":null,"title":"Lec10-Tree Recursion","uri":"/lec10-tree-recursion/"},{"categories":["UCB-CS61A"],"content":"Counting Partitions def count_partitions(n, m): \"\"\" \u003e\u003e\u003e count_partitions(6, 4) 9 \"\"\" if n == 0: return 1 elif n \u003c 0 or m \u003c 0: return 0 elif m == 0: return 0 else: with_m = count_partitions(n-m, m) without_m = count_partitions(n, m-1) return with_m + without_m ","date":"2024-10-14","objectID":"/lec10-tree-recursion/:2:2","tags":null,"title":"Lec10-Tree Recursion","uri":"/lec10-tree-recursion/"},{"categories":["UCB-CS61A"],"content":"Recursion ","date":"2024-10-14","objectID":"/lec9-recursion/:0:0","tags":null,"title":"Lec9-Recursion","uri":"/lec9-recursion/"},{"categories":["UCB-CS61A"],"content":"Self-reference éƒ½æ˜¯è¿”å›å‡½æ•°å¯¼è‡´çš„ ğŸ˜‹ ç„¶åå‡½æ•°å†æ¬¡è¢«callï¼Œå°±å½¢æˆäº†è‡ªæˆ‘é€’å½’ ","date":"2024-10-14","objectID":"/lec9-recursion/:1:0","tags":null,"title":"Lec9-Recursion","uri":"/lec9-recursion/"},{"categories":["UCB-CS61A"],"content":"Recursive Functions æœ‰ä¸€ä¸ªç»†èŠ‚ï¼Œé€’å½’è°ƒç”¨çš„frameçš„parent frameéƒ½æ˜¯globalè€Œä¸æ˜¯nested ğŸ¤” ","date":"2024-10-14","objectID":"/lec9-recursion/:2:0","tags":null,"title":"Lec9-Recursion","uri":"/lec9-recursion/"},{"categories":["UCB-CS61A"],"content":"Mutual Recursion äº’ç›¸è°ƒç”¨ ","date":"2024-10-14","objectID":"/lec9-recursion/:3:0","tags":null,"title":"Lec9-Recursion","uri":"/lec9-recursion/"},{"categories":["UCB-CS61A"],"content":"Recursion to Iteration key: figure out what state must be maintained during the while loop ","date":"2024-10-14","objectID":"/lec9-recursion/:4:0","tags":null,"title":"Lec9-Recursion","uri":"/lec9-recursion/"},{"categories":["UCB-CS61A"],"content":"Iteration to Recursion Iteration is a special case of recursion ğŸ˜® ","date":"2024-10-14","objectID":"/lec9-recursion/:5:0","tags":null,"title":"Lec9-Recursion","uri":"/lec9-recursion/"},{"categories":["UCB-CS61A"],"content":"Function Example ","date":"2024-10-14","objectID":"/lec8-function-example/:0:0","tags":null,"title":"Lec8-Function Example","uri":"/lec8-function-example/"},{"categories":["UCB-CS61A"],"content":"1 def delay(arg): print('delayed') def g(): return arg return g ","date":"2024-10-14","objectID":"/lec8-function-example/:1:0","tags":null,"title":"Lec8-Function Example","uri":"/lec8-function-example/"},{"categories":["UCB-CS61A"],"content":"2 def pirate(arggg): print('matey') def plunder(): return arggg return plunder ","date":"2024-10-14","objectID":"/lec8-function-example/:2:0","tags":null,"title":"Lec8-Function Example","uri":"/lec8-function-example/"},{"categories":["UCB-CS61A"],"content":"3 horse mask å…³é”®æ˜¯ä¸¤æ¡çº¢çº¿ ","date":"2024-10-14","objectID":"/lec8-function-example/:3:0","tags":null,"title":"Lec8-Function Example","uri":"/lec8-function-example/"},{"categories":["UCB-CS61A"],"content":"è£…é¥°å™¨ æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªHoF ","date":"2024-10-14","objectID":"/lec8-function-example/:4:0","tags":null,"title":"Lec8-Function Example","uri":"/lec8-function-example/"},{"categories":["UCB-CS61A"],"content":"Functional Abstraction ","date":"2024-10-13","objectID":"/lec7-functional-abstraction/:0:0","tags":null,"title":"Lec7-Functional Abstraction","uri":"/lec7-functional-abstraction/"},{"categories":["UCB-CS61A"],"content":"which a ? a = 1 def f(g): a = 2 return lambda y: a * g(y) # a is 2 f(lambda y: a + y)(a) # a is 1 ","date":"2024-10-13","objectID":"/lec7-functional-abstraction/:1:0","tags":null,"title":"Lec7-Functional Abstraction","uri":"/lec7-functional-abstraction/"},{"categories":["UCB-CS61A"],"content":"Return æ±‚åå‡½æ•°æ•´æ•°ç‚¹çš„è§£æ³•ï¼Œæœ‰æ„æ€è®°å½•ä¸€ä¸‹ ","date":"2024-10-13","objectID":"/lec7-functional-abstraction/:2:0","tags":null,"title":"Lec7-Functional Abstraction","uri":"/lec7-functional-abstraction/"},{"categories":["UCB-CS61A"],"content":"Abstraction åœ¨pythoné‡Œé¢å‡½æ•°çš„å†…åœ¨åç§°(intrinsic name)ç”¨äºç”¨æˆ·è¯†åˆ«è€Œå·² ","date":"2024-10-13","objectID":"/lec7-functional-abstraction/:3:0","tags":null,"title":"Lec7-Functional Abstraction","uri":"/lec7-functional-abstraction/"},{"categories":["UCB-CS61A"],"content":"Sound what the HoF usage? ğŸ¤” make a song! çœæµï¼šå†™äº†ä¸€ä¸ªé©¬é‡Œå¥¥çš„songï¼Œæ„Ÿè§‰å’Œ61BåæœŸçš„ä¸€ä¸ªlabå¾ˆç›¸ä¼¼ï¼ˆ61 Bçš„ç”µéŸ³ä¸€ç‚¹hhhï¼‰ ğŸ˜ ","date":"2024-10-12","objectID":"/lec6-sound/:0:0","tags":null,"title":"Lec6-Sound","uri":"/lec6-sound/"},{"categories":["UCB-CS61A"],"content":"Environments ","date":"2024-10-12","objectID":"/lec5-environments/:0:0","tags":null,"title":"Lec5-Environments","uri":"/lec5-environments/"},{"categories":["UCB-CS61A"],"content":"Nested Environments in Python æ³¨æ„ç¯å¢ƒè¿½æº¯çš„é¡ºåºï¼Œä»å†…åˆ°å¤– ","date":"2024-10-12","objectID":"/lec5-environments/:1:0","tags":null,"title":"Lec5-Environments","uri":"/lec5-environments/"},{"categories":["UCB-CS61A"],"content":"Lambda Functions å‡½æ•°å¼ç¼–ç¨‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯æŠ½è±¡å’Œå‡½æ•°å¼ç¼–ç¨‹ï¼Œlambdaå‡½æ•°æ˜¯ä¸€ç§åŒ¿åå‡½æ•°ï¼Œå¯ä»¥æŠŠå‡½æ•°ä½œä¸ºå‚æ•°ä¼ é€’ç»™å…¶ä»–å‡½æ•°ï¼Œæˆ–è€…ä½œä¸ºè¿”å›å€¼è¿”å› f = lambda x: x + 1 print(f(1)) # Output: 2 print((lambda x: x + 8)(1)) ","date":"2024-10-12","objectID":"/lec5-environments/:2:0","tags":null,"title":"Lec5-Environments","uri":"/lec5-environments/"},{"categories":["UCB-CS61A"],"content":"å’Œdefå‡½æ•°çš„åŒºåˆ« ","date":"2024-10-12","objectID":"/lec5-environments/:2:1","tags":null,"title":"Lec5-Environments","uri":"/lec5-environments/"},{"categories":["UCB-CS61A"],"content":"Function Currying Currying: transforming a multi-argument function into a single-argument, higher-order function. def curry2(f): def g(x): def h(y): return f(x, y) return h return g # Another example curry2b = lambda f: (lambda x: (lambda y: f(x, y))) ","date":"2024-10-12","objectID":"/lec5-environments/:3:0","tags":null,"title":"Lec5-Environments","uri":"/lec5-environments/"},{"categories":["UCB-CS61A"],"content":"Control ","date":"2024-10-12","objectID":"/lec3-control/:0:0","tags":null,"title":"Lec3-Control","uri":"/lec3-control/"},{"categories":["UCB-CS61A"],"content":"div in Python from opertor import truediv, floordiv a = 10 b = 3 c = a / b d = a // b print(c) # 3.3333333333333335 print(d) # 3 ","date":"2024-10-12","objectID":"/lec3-control/:1:0","tags":null,"title":"Lec3-Control","uri":"/lec3-control/"},{"categories":["UCB-CS61A"],"content":"doctest in Python ","date":"2024-10-12","objectID":"/lec3-control/:2:0","tags":null,"title":"Lec3-Control","uri":"/lec3-control/"},{"categories":["UCB-CS61A"],"content":"Higher Order Functions ","date":"2024-10-12","objectID":"/lec4-higher-order-functions/:0:0","tags":null,"title":"Lec4-Higher-Order Functions","uri":"/lec4-higher-order-functions/"},{"categories":["UCB-CS61A"],"content":"Control Statements vs Functions all params will be evaluated before the function is called ! ä¸å­˜åœ¨è·³è¿‡å‚æ•°çš„æƒ…å†µ ","date":"2024-10-12","objectID":"/lec4-higher-order-functions/:1:0","tags":null,"title":"Lec4-Higher-Order Functions","uri":"/lec4-higher-order-functions/"},{"categories":["UCB-CS61A"],"content":"Assertions in Python assert condition, message ","date":"2024-10-12","objectID":"/lec4-higher-order-functions/:2:0","tags":null,"title":"Lec4-Higher-Order Functions","uri":"/lec4-higher-order-functions/"},{"categories":["UCB-CS61A"],"content":"Returning a Function from a Function å¸¦å‚æ•°çš„ä¿®é¥°å™¨ ğŸ˜® def make_adder(n): def adder(x): return x + n return adder # Example usage add_three = make_adder(3) print(add_three(4)) # Output: 7, WHY? functions in Python are first-class values! ","date":"2024-10-12","objectID":"/lec4-higher-order-functions/:3:0","tags":null,"title":"Lec4-Higher-Order Functions","uri":"/lec4-higher-order-functions/"},{"categories":["UCB-CS61C"],"content":"RISC-V Instruction Formats II ","date":"2024-10-09","objectID":"/lec12-risc-v-instruction-formats-ii/:0:0","tags":null,"title":"Lec12-RISC-V Instruction Formats II","uri":"/lec12-risc-v-instruction-formats-ii/"},{"categories":["UCB-CS61C"],"content":"B-Format Layout branch/åˆ†æ”¯ ==\u003e if-else, while, for encode Label: PCå¯»å€, ç”¨imm fieldæ¥è¡¨ç¤ºåç§»é‡ å®é™…ä¸Š RV compressed instruction format! 16bit å‹ç¼©æŒ‡ä»¤æ ¼å¼ï¼Œåç§»é‡ä¸å†æ˜¯4çš„å€æ•°ï¼Œè€Œæ˜¯2çš„å€æ•°ï¼ˆæ‰€ä»¥imm äºŒè¿›åˆ¶ç»“å°¾ä¸€å®šæ˜¯0ï¼‰ ç†è®ºlayout è§£é‡Šä¸€ä¸‹å¦‚ä½•ä»æŒ‡ä»¤è§£æå‡ºç«‹å³æ•°çš„æ•°å€¼ B-type â€œ|â€ æ„æ€æ˜¯ä¸“é—¨åˆ†å‡ºä¸€å—åŒºåŸŸæ¥å­˜ä¸€ä½æ•°å­— ","date":"2024-10-09","objectID":"/lec12-risc-v-instruction-formats-ii/:1:0","tags":null,"title":"Lec12-RISC-V Instruction Formats II","uri":"/lec12-risc-v-instruction-formats-ii/"},{"categories":["UCB-CS61C"],"content":"Long Immediateï¼Œ U-Format Layout I, B, S immçš„12ä½æ‰©å±•åˆ°longï¼Œæ‰¾ä¸ªåœ°æ–¹æ”¾ä¸‹å‰©ä¸‹çš„20ä½ æ³¨æ„è¿™é‡Œä¸ç›´æ¥ä½¿ç”¨branchæŒ‡ä»¤è·³è½¬ï¼Œè€Œæ˜¯é‡‡ç”¨jumpç›´æ¥æ¥åš åœ°æ–¹æ¥äº†ï¼šåŒæ—¶æ¥äº†ä¸¤ä¸ªæ–°çš„æŒ‡ä»¤lui \u0026 auipc ","date":"2024-10-09","objectID":"/lec12-risc-v-instruction-formats-ii/:2:0","tags":null,"title":"Lec12-RISC-V Instruction Formats II","uri":"/lec12-risc-v-instruction-formats-ii/"},{"categories":["UCB-CS61C"],"content":"Corner case æœ‰ç¬¦å·æ‰©å±•å¸¦è¿‡æ¥çš„ï¼Œ1å¼€å¤´çš„ç¬¦å·æ‰©å±• ç”¨+1æ¥é¿å…è¿™ä¸ªé—®é¢˜ è¡¥å…… auipc æŒ‡ä»¤ ","date":"2024-10-09","objectID":"/lec12-risc-v-instruction-formats-ii/:2:1","tags":null,"title":"Lec12-RISC-V Instruction Formats II","uri":"/lec12-risc-v-instruction-formats-ii/"},{"categories":["UCB-CS61C"],"content":"J-Format Layout åªæœ‰jalï¼Œå› ä¸ºjalræ˜¯I-typeçš„ ä½¿ç”¨ç¤ºä¾‹ jalr ä½¿ç”¨ç¤ºä¾‹ ç•™ä¸€ä¸ªreference ","date":"2024-10-09","objectID":"/lec12-risc-v-instruction-formats-ii/:3:0","tags":null,"title":"Lec12-RISC-V Instruction Formats II","uri":"/lec12-risc-v-instruction-formats-ii/"},{"categories":["UCB-CS61C"],"content":"C Memory Management ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:0:0","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"malloc // with the help of a typecast and sizeof ptr = (int *) malloc(n * sizeof(int)); /* check if malloc was successful */ if (ptr == NULL) { /* handle error */ } ptr = (int *) realloc(ptr, (n+1) * sizeof(int)); free(ptr); // realloc(ptr, 0) ä¸è¦è¸©å‘ï¼ An array name is not a variable. â€“ K\u0026R when call \u0026arr , just get the address of the first element of the array ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:1:0","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"Linked List Example struct node { char *data; struct node *next; }; typedef struct node *List; // å®šä¹‰ List ä¸º struct node çš„æŒ‡é’ˆç±»å‹ List create_empty_list() { return NULL; } ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:2:0","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"Memory Locations åˆ†é…å†…å­˜çš„æ–¹å¼ ä¸‰ä¸ªå­˜å‚¨æ±  æ ˆçš„æ˜ åƒå›¾ LIFO ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:3:0","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"Memory Management stack, static memory are easy to handle, but heap is a bit more complicatedâ€¦ ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:4:0","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"Implementing malloc and free ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:4:1","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"When Memory Goes Bad ä¸è¦è½»æ˜“çš„è®¿é—®æ ˆåŒºæŒ‡é’ˆ / åœ°å€ å¿˜è®°reallocä¼šç§»åŠ¨æ•°æ® ç§»åŠ¨æŒ‡é’ˆä½†æ˜¯èƒ¡ä¹±free or double free ğŸ¤¯ ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:5:0","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"Valgrind? Valgrind is a tool for detecting memory errors in C and C++ programs. ","date":"2024-10-09","objectID":"/lec5-c-memory-management/:6:0","tags":null,"title":"Lec5-C Memory Management","uri":"/lec5-c-memory-management/"},{"categories":["UCB-CS61C"],"content":"Floating Point ","date":"2024-10-09","objectID":"/lec6-floating-point/:0:0","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"Introduction note that binary can directly calculate we can use normal format to represent floating point numbers eg: $1.xxxxx * 2_{two}^{yyyyy}$ â€œ1.â€œé»˜è®¤ï¼Œä¸éœ€bitçº§åˆ«ä¸Šè€ƒè™‘ ","date":"2024-10-09","objectID":"/lec6-floating-point/:1:0","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"underflow \u0026 IEEE 754 å¸Œæœ›å…¨æ˜¯0çš„bitè¡¨ç¤ºçš„æ˜¯æœ€å°çš„æ•°å­—ï¼Œè€Œä¸æ˜¯+0æˆ–-0ï¼Œå¼•å…¥bias $bias = 2^{n-1} - 1$ è¯¦è§number representationçš„biaséƒ¨åˆ† çœŸæ­£çš„è¡¨ç¤ºæ–¹æ³•ï¼š IEEE 754 ğŸ‰ ","date":"2024-10-09","objectID":"/lec6-floating-point/:2:0","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"Special Values infinity, NaN, zero NaN (Not a Number) : æ— æ•ˆæ•°å€¼ï¼Œå¦‚0/0, sqrt(-1) infinity : æ— ç©·å¤§ï¼Œå¦‚1/0, 10^1000 zero : é›¶ï¼Œå¦‚0/1, 1.0-1.0 gapæ•°é‡çº§åœ¨800ä¸‡å·¦å³ï¼Œå› ä¸ºimplicit oneçš„å‡ºç° denormalized number : è§„æ ¼åŒ–æ•°å€¼ï¼ŒæŒ‡æ•°éƒ¨åˆ†ä¸º0ï¼ˆimplicit 2^-126ï¼‰ï¼Œå°æ•°éƒ¨åˆ†ä¸ä¸º0==\u003e ä»æ­¥é•¿2^-149å¼€å§‹ï¼ŒexpåŠ 1ï¼Œæ­¥é•¿ç¿»å€ï¼ŒåŒæ—¶ä»denormåˆ°normçš„æ—¶å€™æ­¥é•¿ä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼ æ€»ç»“ ä»0 11111110 111â€¦11(23ä¸ª) åŠ ä¸€ï¼Œå¾—åˆ° 0 11111111 000â€¦00(23ä¸ª)è¿™å°±æ˜¯æ— ç©· one more step, 0 11111111 000â€¦00(23ä¸ª)åŠ ä¸€ï¼Œå¾—åˆ° 0 11111111 000â€¦01(23ä¸ª)è¿™å°±æ˜¯NaNï¼ˆä¸€å¤§ç‰‡éƒ½æ˜¯NaNï¼‰ 1åº”è¯¥æ˜¯ 0 01111111 000â€¦00(23ä¸ª) ğŸ˜‹ 1, 2, 4, 8, 16, 32â€¦ä¹‹é—´éƒ½æ˜¯800ä¸‡ä¸ªå·¦å³æ•°å­—ï¼ŒæŸä¸€åˆ»å¼€å§‹é—´éš”1è®¡æ•° ğŸ˜ ","date":"2024-10-09","objectID":"/lec6-floating-point/:3:0","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"example and discussion ","date":"2024-10-09","objectID":"/lec6-floating-point/:4:0","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"example $\\frac{1}{3} = $ 0 01111101 010101â€¦0101(å…±23ä¸ª) ","date":"2024-10-09","objectID":"/lec6-floating-point/:4:1","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"discussion FP add associative law: $a+b+c = (a+b)+c$ ? precision and accuracy rounding å‘ä¸Š / å‘ä¸‹ / å››èˆäº”å…¥ / æˆªæ–­ add casting // most large integers do not have exact representation in float if (i == (int)((float)i)) { prtinf(\"true\"); // not always true } if (f == (float) ((int)f)) { printf(\"true\"); // not always true, eg: 1.5 } ","date":"2024-10-09","objectID":"/lec6-floating-point/:4:2","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"other representation double precision float (64 bits) åŠ é€Ÿå™¨æ‰€æ”¯æŒçš„æ ¼å¼ ","date":"2024-10-09","objectID":"/lec6-floating-point/:5:0","tags":null,"title":"Lec6-Floating Point","uri":"/lec6-floating-point/"},{"categories":["UCB-CS61C"],"content":"RISC-V Introduction ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:0:0","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"Instruction Set Architecture (ISA) ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:1:0","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"Assembly Variables each statement is called an instruction ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:2:0","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"Registers where are registers ? 32 general purpose registers (GPRs) are available in RISC-V architecture.(x0 - x31) word: 32 bits (can be 64 bits in RV64) x0: always 0 # is the comment character ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:2:1","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"no type casting in RISC-V assembly language the registers have no type ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:2:2","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"add/sub instructions ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:3:0","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"syntax of instructions add rd, rs1, rs2 sub rd, rs1, rs2 # d(rd) = e(rs1) - f(rs2), æ³¨æ„é¡ºåº ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:3:1","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"Immediate valuesï¼ˆç«‹å³æ•°ï¼‰ addi rd, rs1, 10 æ²¡æœ‰subi ï¼ŒåŠ ä¸Šç›¸åæ•°å³å¯ ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:4:0","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"Register 0 ","date":"2024-10-09","objectID":"/lec7-risc-v-introduction/:4:1","tags":null,"title":"Lec7-RISC-V Introduction","uri":"/lec7-risc-v-introduction/"},{"categories":["UCB-CS61C"],"content":"RISC-V Instruction Formats I äº‹å®ä¸Šå·²ç»æ¥åˆ°ä¸‹ä¸€ä¸ªå±‚çº§(äºŒè¿›åˆ¶)äº†ï¼Œä½†æ˜¯çœ‹æ ‡é¢˜ä¼¼ä¹è¿˜æ˜¯RISC-V ğŸ¤” ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:0:0","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"Background and Consequences ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:1:0","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"1. addressing modes everything has a memory address, so branches and jumps can use them PC (program counter, again ğŸ˜„) is a register that holds the address of the next instruction to be executed ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:1:1","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"2. äºŒè¿›åˆ¶å…¼å®¹å¦ï¼Ÿå¦‚ä½•è¡¨ç¤ºæŒ‡ä»¤ï¼Ÿ ä¸€ä¸ªæŒ‡ä»¤ç”¨1ä¸ªword(32bits)æ¥è£…ç»°ç»°æœ‰ä½™ divide instruction into â€œfieldsâ€ ğŸ˜‹ ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:1:2","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"R-Format Layout ç®—æ•°é€»è¾‘æŒ‡ä»¤ funct3: åŠŸèƒ½ç ï¼Œå†³å®šæŒ‡ä»¤çš„æ“ä½œç±»å‹ funct7: æ‰©å±•åŠŸèƒ½ç ï¼Œç”¨äºä¸€äº›å¤æ‚çš„æŒ‡ä»¤ opcode: æ“ä½œç ï¼Œå†³å®šæŒ‡ä»¤çš„ç±»åˆ« å…·ä½“æŸ¥è¡¨ new one: slt and sltu â€“ set less than, when rs1 \u003c rs2, then set the destination register to 1, otherwise 0. sltu is similar but for unsigned numbers. sub \u0026 sra 0 1 00000 æŒ‡ç¤ºç¬¦å·æ‹“å±• ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:2:0","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"I-Format Layout funct3: åŠŸèƒ½ç ï¼Œå†³å®šæŒ‡ä»¤çš„æ“ä½œç±»å‹ï¼Œåœ¨è¿™é‡Œåªèƒ½æŒ‡ç¤º8ä¸ªï¼Œç„¶è€Œæœ‰9æ¡æŒ‡ä»¤ å…·ä½“æŸ¥è¡¨ new one: slti \u0026 sltiu â€“ set less than immediate, when rs1 \u003c imm, then set the destination register to 1, otherwise 0. sltiu is similar but for unsigned numbers. shamt ? â€“ shift amount , 5 bits, è¿‡å¤šæº¢å‡ºå…¨ä¸º0 ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:3:0","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"RV Loads æœ¬è´¨ä¸Šæ˜¯I-Format å…·ä½“æŸ¥è¡¨ place in the lower part, and ç¬¦å·æ‹“å±• ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:3:1","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"S-Format Layout ååˆ†ä¸‘é™‹åˆ’åˆ†32ä½ å…·ä½“æŸ¥è¡¨ ","date":"2024-10-09","objectID":"/lec11-risc-v-instruction-formats-i/:4:0","tags":null,"title":"Lec11-RISC-V Instruction Formats I","uri":"/lec11-risc-v-instruction-formats-i/"},{"categories":["UCB-CS61C"],"content":"RISC-V Procedures è°ƒç”¨å‡½æ•°çš„æ—¶å€™æœ‰ä¸€äº›æ— å…³çš„ä¸»è¿›ç¨‹å˜é‡çš„valueéœ€è¦å­˜å‚¨ï¼Œbut where? ","date":"2024-10-08","objectID":"/lec10-risc-v-procedures/:0:0","tags":null,"title":"Lec10-RISC-V Procedures","uri":"/lec10-risc-v-procedures/"},{"categories":["UCB-CS61C"],"content":"æ ˆå¸§ / Stack Frame å­˜æ”¾äº†ä»€ä¹ˆï¼Ÿ æ³¨æ„stack ä»ä¸Šå¾€ä¸‹å¢é•¿ï¼Œpush spâ€“, pop sp++ ","date":"2024-10-08","objectID":"/lec10-risc-v-procedures/:1:0","tags":null,"title":"Lec10-RISC-V Procedures","uri":"/lec10-risc-v-procedures/"},{"categories":["UCB-CS61C"],"content":"åºè¨€prologue \u0026 ç»“å°¾epilogue int Leaf (int g, int h, int i, int j) { int f; f = (g + h) - (i + j); return f; } Leaf: # åºè¨€prologue addi sp, sp, -8 # ä¿å­˜ä¹‹å‰çš„æ ˆæŒ‡é’ˆ sw s1, 4(sp) # ä¿å­˜å‚æ•° sw s2, 0(sp) # è®¡ç®— ... # ç»“å°¾epilogue lw s0, 0(sp) # æ¢å¤å‚æ•° lw s1, 4(sp) # æ¢å¤ä¹‹å‰çš„æ ˆæŒ‡é’ˆ addi sp, sp, 8 # é‡Šæ”¾æ ˆç©ºé—´ ret # jr ra ","date":"2024-10-08","objectID":"/lec10-risc-v-procedures/:1:1","tags":null,"title":"Lec10-RISC-V Procedures","uri":"/lec10-risc-v-procedures/"},{"categories":["UCB-CS61C"],"content":"Nested Function Calls and Registers Conventions æ­¤æ—¶raæ˜¾ç„¶ä¸å¤Ÿç”¨ï¼Œæ€ä¹ˆè®°å½•å¥½è¿”å›çš„åœ°å€å‘¢ï¼Ÿ é¦–å…ˆ31ä¸ªregisterè¦å¥½å¥½åˆ©ç”¨ï¼Œæ€ä¹ˆç”¨,æ€ä¹ˆå­˜å‚¨éœ€è¦çš„ï¼Œæ”¾å¼ƒä¸éœ€è¦çš„ï¼Ÿ å°½å¯èƒ½é¿å…ä½¿ç”¨å†…å­˜memoryï¼Œå°½é‡ä½¿ç”¨å¯„å­˜å™¨register ä¸¤ç§registerï¼Œpreserved and not-preserved(into stack) 0-31å…¨å®¶ç¦ ğŸ˜‹ æ³¨æ„å¦‚æœcalleeæƒ³è¦ä½¿ç”¨caller-saved registerï¼Œéœ€è¦å…ˆä¿å­˜callerçš„registerï¼Œç„¶åå†æ¢å¤calleeçš„register ","date":"2024-10-08","objectID":"/lec10-risc-v-procedures/:2:0","tags":null,"title":"Lec10-RISC-V Procedures","uri":"/lec10-risc-v-procedures/"},{"categories":["UCB-CS61C"],"content":"Memory Allocation å†…å­˜çº§åˆ«çš„åˆ’åˆ†ï¼ˆä¹‹å‰çš„è®²åº§æåˆ°è¿‡ï¼‰ å…·ä½“åœ°å€åˆ’åˆ†å¦‚ä¸‹ï¼Œæœ‰å‡ ä¸ªç‰¹æ®Šçš„å¯„å­˜å™¨ç®¡ç† ","date":"2024-10-08","objectID":"/lec10-risc-v-procedures/:3:0","tags":null,"title":"Lec10-RISC-V Procedures","uri":"/lec10-risc-v-procedures/"},{"categories":["UCB-CS61C"],"content":"Conclusion so far instructions ","date":"2024-10-08","objectID":"/lec10-risc-v-procedures/:4:0","tags":null,"title":"Lec10-RISC-V Procedures","uri":"/lec10-risc-v-procedures/"},{"categories":["UCB-CS61C"],"content":"RISC-V Decisions II ","date":"2024-10-08","objectID":"/lec9-risc-v-decisions-ii/:0:0","tags":null,"title":"Lec9-RISC-V Decisions II","uri":"/lec9-risc-v-decisions-ii/"},{"categories":["UCB-CS61C"],"content":"Logical Instruction ","date":"2024-10-08","objectID":"/lec9-risc-v-decisions-ii/:1:0","tags":null,"title":"Lec9-RISC-V Decisions II","uri":"/lec9-risc-v-decisions-ii/"},{"categories":["UCB-CS61C"],"content":"and, andi, not 2ç§å¯¹åº”å˜ä½“ and: and x5, x6, x7 # x5 = x6 \u0026 x7 andi: andi x5, x6, 0x7 # x5 = x6 \u0026 0x7 # for masking ğŸ˜‹ or: or x5, x6, x7 # x5 = x6 | x7 xor: xor x5, x6, x7 # x5 = x6 ^ x7 Not not, xor with 111111â€¦1111å³å¯ ","date":"2024-10-08","objectID":"/lec9-risc-v-decisions-ii/:1:1","tags":null,"title":"Lec9-RISC-V Decisions II","uri":"/lec9-risc-v-decisions-ii/"},{"categories":["UCB-CS61C"],"content":"shift instructions logical shift arithmetic shift ","date":"2024-10-08","objectID":"/lec9-risc-v-decisions-ii/:1:2","tags":null,"title":"Lec9-RISC-V Decisions II","uri":"/lec9-risc-v-decisions-ii/"},{"categories":["UCB-CS61C"],"content":"A bit about machine programming PC: program counter (special register), ç¨‹åºè®¡æ•°å™¨ æŒ‡å‘ä¸‹ä¸€æ¡æŒ‡ä»¤çš„åœ°å€ï¼ˆnext 4 byte / 1 word awayï¼‰ side note: symbolic name pseudo code ","date":"2024-10-08","objectID":"/lec9-risc-v-decisions-ii/:2:0","tags":null,"title":"Lec9-RISC-V Decisions II","uri":"/lec9-risc-v-decisions-ii/"},{"categories":["UCB-CS61C"],"content":"Function calls 6 steps to call a function: ","date":"2024-10-08","objectID":"/lec9-risc-v-decisions-ii/:3:0","tags":null,"title":"Lec9-RISC-V Decisions II","uri":"/lec9-risc-v-decisions-ii/"},{"categories":["UCB-CS61C"],"content":"conventions in RISC-V, all instructions are 32-bit long, and stored in memory just like any other data. so below we show the address of where the programs are stored in memory. 8ä¸ªar 1ä¸ªra 12ä¸ªsr æ–°çš„jump: jr: jr ra (return from subroutine) æ­£æ˜¯ ret çš„å…·ä½“å®ç° jalr : jalr rd, rs, imm jump and link register, è·³è½¬åˆ°æŒ‡å®šåœ°å€å¹¶ä¿å­˜è¿”å›åœ°å€åˆ°ra jal : jal FunctionLabel / jal rd, Label äº‹å®ä¸Šï¼Œæ²¡æœ‰jrï¼Œj j: jal x0, Label # è·³è½¬åˆ°Labelå¹¶ä¿å­˜è¿”å›åœ°å€åˆ°x0 ","date":"2024-10-08","objectID":"/lec9-risc-v-decisions-ii/:3:1","tags":null,"title":"Lec9-RISC-V Decisions II","uri":"/lec9-risc-v-decisions-ii/"},{"categories":["UCB-CS61C"],"content":"RISC-V lw, sw, Decision I ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:0:0","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"Intro ç¼–è¯‘å™¨ä¼šæœ€å°åŒ–å¯„å­˜å™¨ä½¿ç”¨ ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:1:0","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"layout in Memory side note: a register is a 32-bit register(hold 32-bit data) word: 4 bytes, 32 bits risc-v å°ç«¯æ³•ï¼ˆä¸»æµï¼‰ ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:2:0","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"Data Transfer Instructions ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:3:0","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"lw (load word) look note ğŸ¤” ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:3:1","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"sw (store word) ç»“åˆlec7çš„æ¨¡å¼å›¾æ¥è®°å¿† ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:3:2","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"lb/sb (load byte/store byte) same as lw/sw lb ç¬¦å·æ‰©å±• lbu: load byte unsigned ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:3:3","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"why still addi? æ›´å¿« ä½†æ˜¯immçš„èŒƒå›´æ›´å° (32bitä»¥å†…) ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:3:4","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"Decision I RV32 so far ğŸ˜‹ add rd, rs1, rs2 sub rd, rs1, rs2 addi rd, rs1, imm lw rd, imm(rs1) sw rs2, imm(rs1) lb rd, imm(rs1) sb rs2, imm(rs1) lbu rd, imm(rs1) ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:4:0","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"beq/bne (branch if equal/not equal) ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:4:1","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"Decision in RISC-V Assembly beq: branch if equal bne: branch if not equal blt: branch if less than blt reg1, reg2, label # if (reg1 \u003c reg2) goto label bge: branch if greater than or equal bltu: branch if unsigned less than bltu reg1, reg2, label # if (reg1 \u003c reg2) goto label bgeu: branch if unsigned greater than or equal j: jump/always branch j label # æ— æ¡ä»¶jumpçš„èŒƒå›´æ¯”æœ‰æ¡ä»¶jumpæ„é€ å‡ºæ¥çš„æ— æ¡ä»¶jumpæ›´å¤§ï¼ˆ32ä½é™åˆ¶ï¼‰ æ²¡æœ‰ï¼šbgt or ble, only have BLTï¼ˆåŸ¹æ ¹ç”Ÿèœç•ªèŒ„ï¼‰ sandwiches æ³¨æ„â€œæµâ€, å¤šæ•°æƒ…å†µä¸‹æ¡ä»¶ä¼¼ä¹æ˜¯ç¿»ç€æ¥ç¿»è¯‘çš„ æ³¨æ„â€œj Exitâ€ ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:5:0","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"loops in C/Assembly ","date":"2024-10-08","objectID":"/lec8-risc-v-lw-sw-decision-i/:5:1","tags":null,"title":"Lec8-RISC-V lw, sw, Decision I","uri":"/lec8-risc-v-lw-sw-decision-i/"},{"categories":["UCB-CS61C"],"content":"C intro: Pointers, Arrays, Strings ","date":"2024-10-05","objectID":"/lec4-c-intropointers-arrays-strings/:0:0","tags":null,"title":"Lec4-C introï¼šPointers, Arrays, Strings","uri":"/lec4-c-intropointers-arrays-strings/"},{"categories":["UCB-CS61C"],"content":"Pointers int (*fn) (void *, void *) = \u0026foo; // pointer to function arrow operator -\u003e is used to access members of a struct pointed to by a pointer. NULL pointer is used to indicate an invalid pointer. if (p == NULL) { // handle error } è°ˆåˆ°äº†åœ°å€å¯¹é½ -\u003e word alignment sizeof(arg) sizeof(structtype) ","date":"2024-10-05","objectID":"/lec4-c-intropointers-arrays-strings/:1:0","tags":null,"title":"Lec4-C introï¼šPointers, Arrays, Strings","uri":"/lec4-c-intropointers-arrays-strings/"},{"categories":["UCB-CS61C"],"content":"Arrays ç»å…¸é—®é¢˜ä¹‹æ•°ç»„åæ˜¯æŒ‡é’ˆ char *foo() { char arr[10]; return arr; // A BIG mistake! å‡½æ•°æ ˆå›æ”¶æ—¶ï¼ŒarræŒ‡é’ˆæŒ‡å‘çš„ç©ºé—´å¯èƒ½è¢«é‡Šæ”¾æ‰äº†ï¼Œå¯¼è‡´ç¨‹åºå´©æºƒ } ","date":"2024-10-05","objectID":"/lec4-c-intropointers-arrays-strings/:2:0","tags":null,"title":"Lec4-C introï¼šPointers, Arrays, Strings","uri":"/lec4-c-intropointers-arrays-strings/"},{"categories":["UCB-CS61C"],"content":"Strings always end with a null character \\0 ğŸ˜‰ Segmentation fault :s æˆ–è®¸rustå¯èƒ½æ›´å®‰å…¨ ","date":"2024-10-05","objectID":"/lec4-c-intropointers-arrays-strings/:3:0","tags":null,"title":"Lec4-C introï¼šPointers, Arrays, Strings","uri":"/lec4-c-intropointers-arrays-strings/"},{"categories":["UCB-CS61C"],"content":"Summary void incrementPtr(int *p) { p += 1; // wrong again! have to ... } void incrementPtr(int **p) { (*p) += 1; // correct } æ³¨æ„å¼•ç”¨ä¼ é€’å³å¯ ","date":"2024-10-05","objectID":"/lec4-c-intropointers-arrays-strings/:4:0","tags":null,"title":"Lec4-C introï¼šPointers, Arrays, Strings","uri":"/lec4-c-intropointers-arrays-strings/"},{"categories":["UCB-CS61C"],"content":"Great Ideas in Computer Architecture Intro ","date":"2024-10-05","objectID":"/lec1-great-ideas-in-computer-architecture-intro/:0:0","tags":null,"title":"Lec1-Great Ideas in Computer Architecture Intro","uri":"/lec1-great-ideas-in-computer-architecture-intro/"},{"categories":["UCB-CS61C"],"content":"six great ideas Mooreâ€™s Law Abstraction Principle of Locality / Memory Hierarchy Parallelism Performance Measurement \u0026 Improvement Dependability via Redundancy ğŸ˜‰ ","date":"2024-10-05","objectID":"/lec1-great-ideas-in-computer-architecture-intro/:1:0","tags":null,"title":"Lec1-Great Ideas in Computer Architecture Intro","uri":"/lec1-great-ideas-in-computer-architecture-intro/"},{"categories":["UCB-CS61C"],"content":"Number Representation ","date":"2024-10-05","objectID":"/lec2-number-representation/:0:0","tags":null,"title":"Lec2-Number Representation","uri":"/lec2-number-representation/"},{"categories":["UCB-CS61C"],"content":"Summary å‰ä¸¤ä¸ªç¼–ç æ–¹å¼ä¸å¸¸ç”¨ åä¸‰ä¸ªæ— ç¬¦å·æ•° / 2çš„è¡¥ç  / biased representation æ¯”è¾ƒå¸¸è§ ","date":"2024-10-05","objectID":"/lec2-number-representation/:1:0","tags":null,"title":"Lec2-Number Representation","uri":"/lec2-number-representation/"},{"categories":["UCB-CS61C"],"content":"C intro: Basic ","date":"2024-10-05","objectID":"/lec3-c-introbasic/:0:0","tags":null,"title":"Lec3-C introï¼šBasic","uri":"/lec3-c-introbasic/"},{"categories":["UCB-CS61C"],"content":"Compile vs Interpret Java æ˜¯å…ˆç¼–è¯‘åè§£é‡Šå™¨è§£é‡Š C æ˜¯ç¼–è¯‘å®Œæˆåç›´æ¥è¿è¡Œï¼Œä¸éœ€è¦è§£é‡Šå™¨ ","date":"2024-10-05","objectID":"/lec3-c-introbasic/:1:0","tags":null,"title":"Lec3-C introï¼šBasic","uri":"/lec3-c-introbasic/"},{"categories":["UCB-CS61C"],"content":"Syntax int8_t a = 10; int64_t b = 20; int16_t c = a + b; uint32_t d = a - b; typedef uint8_t Byte; typedef struct { Byte a; int b; } MyStruct; MyStruct myStruct = {10, 20}; ","date":"2024-10-05","objectID":"/lec3-c-introbasic/:2:0","tags":null,"title":"Lec3-C introï¼šBasic","uri":"/lec3-c-introbasic/"},{"categories":["CMU-10-414-714"],"content":"Hardware Acceleration Implementation ","date":"2024-10-01","objectID":"/lec13-hardware-acceleration-implementation/:0:0","tags":null,"title":"Lec13-Hardware Acceleration Implementation","uri":"/lec13-hardware-acceleration-implementation/"},{"categories":["CMU-10-414-714"],"content":"GPU Acceleration ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:0:0","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"GPU Programming gpu å…·æœ‰è‰¯å¥½çš„å¹¶è¡Œæ€§ ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:1:0","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"a single CUDA example æ³¨æ„åˆ°è®¡ç®—æ‰€éœ€å˜é‡äº’ä¸ç›¸å…³ï¼Œæ‰€ä»¥å¯ä»¥å¹¶è¡Œè®¡ç®— æ•°æ®IOæ“ä½œæ˜¯ç“¶é¢ˆ keep data in GPU memory as long as possible â€“\u003e call .numpy() less frequently ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:1:1","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"GPU memory hierarchy åˆ©ç”¨shared memory launch thread grid and blocks cooperative fetch common to shared memory to increase reuse ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:1:2","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"case study: matrix multiplication on GPU Compute C = dot(A.T, B) ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:2:0","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"thread level __global__ void mm(float A[N][N], float B[N][N], float C[N][N]) { int ybase = blockIdx.y * blockDim.y + threadIdx.y; int xbase = blockIdx.x * blockDim.x + threadIdx.x; float c[V][V] = {0}; float a[V], b[V]; for (int k = 0; k \u003c N; k++) { a[:] = A[k, ybase*V : ybase*V+V]; b[:] = B[k, xbase*V : xbase*V+V]; for (int y = 0; y \u003c V; y++) { for (int x = 0; x \u003c V; x++) { c[y][x] += a[y] * b[x]; } } } C[ybase*V : ybase*V+V, xbase*V : xbase*V+V] = c[:]; } ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:2:1","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"block level: shared memory tiling ğŸ¤¯ åƒäº†æ²¡æœ‰å®Œå…¨å­¦ä¹ å¥½æ¶æ„ä½“ç³»çš„äº! å¤šçº¿ç¨‹ä½¿å¾—è®¡ç®—å’ŒåŠ è½½æ•°æ®åŒæ—¶è¿›è¡Œ åˆä½œfetchingä¹Ÿæœ‰æ„æ€æ ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:2:2","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"Moreâ€¦ ","date":"2024-10-01","objectID":"/lec12-hardware-acceleration--gpus/:2:3","tags":null,"title":"Lec12-Hardware Acceleration + GPUs","uri":"/lec12-hardware-acceleration--gpus/"},{"categories":["CMU-10-414-714"],"content":"Hardware Acceleration!! ","date":"2024-10-01","objectID":"/lec11-hardware-acceleration-for-linear-algebra/:0:0","tags":null,"title":"Lec11-Hardware Acceleration for Linear Algebra","uri":"/lec11-hardware-acceleration-for-linear-algebra/"},{"categories":["CMU-10-414-714"],"content":"General acceleration techniques ","date":"2024-10-01","objectID":"/lec11-hardware-acceleration-for-linear-algebra/:1:0","tags":null,"title":"Lec11-Hardware Acceleration for Linear Algebra","uri":"/lec11-hardware-acceleration-for-linear-algebra/"},{"categories":["CMU-10-414-714"],"content":"Vectorization NumPyçš„å‘é‡åŒ–æ˜¯é€šè¿‡åº•å±‚çš„Cè¯­è¨€ä»¥åŠç¼–è¯‘è¿‡çš„å‡½æ•°å®ç°çš„ï¼Œå…¶æ ¸å¿ƒæœºåˆ¶ä¾èµ–äºå‡ ä¸ªå…³é”®æŠ€æœ¯ï¼š å†…å­˜è¿ç»­å­˜å‚¨ï¼šNumPyæ•°ç»„åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­å­˜å‚¨çš„ï¼Œè¿™æ„å‘³ç€æ•°æ®å­˜å‚¨åœ¨è¿ç»­çš„å†…å­˜å—ä¸­ï¼Œè¿™ä½¿å¾—CPUç¼“å­˜èƒ½æ›´æœ‰æ•ˆåœ°å·¥ä½œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒPythonåˆ—è¡¨ä¸­çš„å…ƒç´ å¯èƒ½åœ¨å†…å­˜ä¸­æ˜¯åˆ†æ•£å­˜å‚¨çš„ã€‚ Cè¯­è¨€å®ç°ï¼šNumPyçš„åº•å±‚æ“ä½œæ˜¯ç”¨Cè¯­è¨€ç¼–å†™çš„ï¼Œè¿™æ„å‘³ç€NumPyçš„æ•°ç»„æ“ä½œæ˜¯åœ¨ç¼–è¯‘åçš„ä»£ç ä¸­æ‰§è¡Œçš„ï¼Œè€Œä¸æ˜¯åœ¨Pythonè§£é‡Šå™¨ä¸­ã€‚Cè¯­è¨€çš„æ‰§è¡Œé€Ÿåº¦æ¯”Pythonå¿«å¾—å¤šã€‚ ç»Ÿä¸€å‡½æ•°æ¥å£ï¼šNumPyå®šä¹‰äº†ä¸€ç§ç‰¹æ®Šçš„å‡½æ•°æ¥å£ï¼Œç§°ä¸ºufuncï¼ˆUniversal Functionï¼‰ï¼Œè¿™ç§å‡½æ•°å¯ä»¥å¯¹æ•°ç»„çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œå‘é‡åŒ–æ“ä½œã€‚ å¹¶è¡Œè®¡ç®—ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒNumPyè¿˜å¯ä»¥ä½¿ç”¨å¹¶è¡Œè®¡ç®—æ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œå¦‚ä½¿ç”¨BLASï¼ˆBasic Linear Algebra Subprogramsï¼‰åº“è¿›è¡ŒçŸ©é˜µè®¡ç®—ã€‚ æ•°æ®ç±»å‹ä¸€è‡´æ€§ï¼šNumPyæ•°ç»„ä¸­çš„æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ç›¸åŒçš„æ•°æ®ç±»å‹ï¼Œè¿™ä½¿å¾—å¯ä»¥å¯¹æ•°ç»„è¿›è¡Œæ‰¹é‡æ“ä½œã€‚ å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€ï¼šåœ¨å‘é‡åŒ–æ“ä½œä¸­ï¼Œå‡½æ•°è°ƒç”¨æ˜¯æ‰¹é‡è¿›è¡Œçš„ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸ªå…ƒç´ ä¸Šå•ç‹¬è°ƒç”¨ï¼Œè¿™å‡å°‘äº†å‡½æ•°è°ƒç”¨çš„å¼€é”€ã€‚ ä¼˜åŒ–çš„æ•°å­¦è¿ç®—ï¼šNumPyä¸­å¾ˆå¤šæ“ä½œéƒ½ç»è¿‡äº†ä¼˜åŒ–ï¼Œæ¯”å¦‚ä½¿ç”¨SIMDï¼ˆå•æŒ‡ä»¤å¤šæ•°æ®ï¼‰æŒ‡ä»¤é›†ï¼Œè¿™äº›æŒ‡ä»¤å¯ä»¥åœ¨ä¸€ä¸ªCPUå‘¨æœŸå†…å¯¹å¤šä¸ªæ•°æ®æ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚ é€šè¿‡è¿™äº›æŠ€æœ¯ï¼ŒNumPyå®ç°äº†é«˜æ•ˆçš„å‘é‡åŒ–æ“ä½œã€‚å½“ä½ ä½¿ç”¨å‘é‡åŒ–è¡¨è¾¾å¼æ—¶ï¼ŒNumPyä¼šå°†è¿™äº›æ“ä½œè½¬æ¢ä¸ºåº•å±‚çš„Cè¯­è¨€è°ƒç”¨ï¼Œä»è€Œå®ç°å¿«é€Ÿçš„æ•°ç»„è®¡ç®—ã€‚ ","date":"2024-10-01","objectID":"/lec11-hardware-acceleration-for-linear-algebra/:1:1","tags":null,"title":"Lec11-Hardware Acceleration for Linear Algebra","uri":"/lec11-hardware-acceleration-for-linear-algebra/"},{"categories":["CMU-10-414-714"],"content":"Data layout and strides row major: default in C column major: Fortranâ€¦ strides format: common in linalg libraries strides formatä½¿å¾—æ•°ç»„å­˜å‚¨å¹¶ä¸ç´§å¯†ï¼Œéš¾ä»¥vectorizeï¼Œæ‰€ä»¥åœ¨torchç­‰åº“é‡Œé¢æœ‰ä¸€ä¸ªå‡½æ•°å«contiguous()æ¥å°†æ•°ç»„å˜æˆè¿ç»­å­˜å‚¨çš„ï¼Œæœ‰åˆ©äºè®¡ç®— ","date":"2024-10-01","objectID":"/lec11-hardware-acceleration-for-linear-algebra/:1:2","tags":null,"title":"Lec11-Hardware Acceleration for Linear Algebra","uri":"/lec11-hardware-acceleration-for-linear-algebra/"},{"categories":["CMU-10-414-714"],"content":"Parallelization OpenMP: multi-threading, loopsåˆ†é…ç»™ä¸åŒçš„cpuæ¥åš ","date":"2024-10-01","objectID":"/lec11-hardware-acceleration-for-linear-algebra/:1:3","tags":null,"title":"Lec11-Hardware Acceleration for Linear Algebra","uri":"/lec11-hardware-acceleration-for-linear-algebra/"},{"categories":["CMU-10-414-714"],"content":"case study: matrix multiplication // c = dot(a, b) float A[N][N], B[N][N], C[N][N]; for (int i = 0; i \u003c N; i++) { for (int j = 0; j \u003c N; j++) { C[i][j] = 0; for (int k = 0; k \u003c N; k++) { C[i][j] += A[i][k] * B[k][j]; } } } æ—¶é—´å¤æ‚åº¦ï¼š$O(N^3)$ ","date":"2024-10-01","objectID":"/lec11-hardware-acceleration-for-linear-algebra/:2:0","tags":null,"title":"Lec11-Hardware Acceleration for Linear Algebra","uri":"/lec11-hardware-acceleration-for-linear-algebra/"},{"categories":["CMU-10-414-714"],"content":"cpu architecture aware analysis naive implementation // c = dot(a, b) dram float A[N][N], B[N][N], C[N][N]; for (int i = 0; i \u003c N; i++) { for (int j = 0; j \u003c N; j++) { register float c = 0; for (int k = 0; k \u003c N; k++) { register float a = A[i][k]; register float b = B[k][j]; c += a * b; } C[i][j] = c; } } naiveåˆ†æ register tiled matrix multiplication side note: $$ \\frac{n}{v_1} \\times \\frac{n}{v_2} \\times \\frac{n}{v_3} \\times v_1 \\times v_3 = \\frac{n^3}{v_2} $$ let $v_3 = 1$ cache line aware tiling åœ¨ä¸€çº§ç¼“å­˜ä¸­å­˜æ•°æ®ï¼Œæ³¨æ„é™åˆ¶ all in one l1speed: l1 -\u003e register dram speed: dram -\u003e l1cache key insight: memory load reuse ","date":"2024-10-01","objectID":"/lec11-hardware-acceleration-for-linear-algebra/:2:1","tags":null,"title":"Lec11-Hardware Acceleration for Linear Algebra","uri":"/lec11-hardware-acceleration-for-linear-algebra/"},{"categories":["CMU-10-414-714"],"content":"Convolutional Neural Networks è€æœ‹å‹äº† â€œcapture the featuresâ€ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:0:0","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Convolutional Operator äº‹å®ä¸Šçš„è®¡ç®—ï¼Œæ˜¯ä¿¡å·å¤„ç†é‡Œé¢çš„äº’ç›¸å…³è¿ç®— ä¼ ç»Ÿå·ç§¯å¤„ç† å¤šé€šé“å·ç§¯æ–°è§†è§’ğŸ¤“ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:1:0","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Elements of practical convolution ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:2:0","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Padding ä¸ºäº†ç»´æŒå°ºå¯¸ä¸å˜ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:2:1","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Strides Convolution / Pooling é™ä½resolutionï¼Œâ€œdownsamplingâ€ ğŸ¤“ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:2:2","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Grouped Convolution! åˆ†ç»„å·ç§¯ï¼Œå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:2:3","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Dilations Convolution è´Ÿè´£å¤„ç†æ„Ÿå—é‡çš„é—®é¢˜ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:2:4","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Differentiating Convolutional Layers!! Naive way: just matrix and vector multiplication products ğŸ¤”, but can lead to too much waste memoryâ€¦ Be an op in needle, not a module! ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:3:0","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"wrt. Input é¦–å…ˆæœ‰ $v^TW \\iff W^Tv$ è‡ªåŠ¨å¾®åˆ†é“¾å¼æ³•åˆ™çš„æ—¶å€™ äº‹å®ä¸Šå·ç§¯å¯ä»¥æœ‰ä¸ªç­‰ä»·çš„çŸ©é˜µè¡¨ç¤º ç„¶åå†™å‡ºæ¥ï¼Œå‘ç°ç­‰ä»·äº$conv(v, flip(W))$ ğŸ¤¯ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:3:1","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"wrt. Weights â€œim2col\"æ“ä½œååˆ†æœ‰è¶£!åœ¨è¿™é‡Œæˆ‘ä»¬ç›´æ¥æ˜¾å¼æ„é€ é»‘è‰²çš„çŸ©é˜µï¼Œä¸€æ–¹é¢ç”¨æ¥è®¡ç®—å·ç§¯ï¼Œå¦ä¸€æ–¹é¢ç”¨æ¥è®¡ç®—æ¢¯åº¦ğŸ˜ ","date":"2024-10-01","objectID":"/lec10-convolutional-networks/:3:2","tags":null,"title":"Lec10-Convolutional Networks","uri":"/lec10-convolutional-networks/"},{"categories":["CMU-10-414-714"],"content":"Normalization and Regularization ","date":"2024-09-28","objectID":"/lec9-normalization-dropout--implementation/:0:0","tags":null,"title":"Lec9-Normalization, Dropout, + Implementation","uri":"/lec9-normalization-dropout--implementation/"},{"categories":["CMU-10-414-714"],"content":"Normalization and Initialization æ³¨æ„çœ‹weight varianceçš„æ›²çº¿ï¼Œå‡ ä¹ä¸å˜ normçš„æ€æƒ³æ¥æº layer normalization batch normalization è¿™ä¹ˆçœ‹æ¥batch_normç¡®å®å¾ˆå¥‡æ€ª, odd! ğŸ˜¢ ","date":"2024-09-28","objectID":"/lec9-normalization-dropout--implementation/:1:0","tags":null,"title":"Lec9-Normalization, Dropout, + Implementation","uri":"/lec9-normalization-dropout--implementation/"},{"categories":["CMU-10-414-714"],"content":"Regularization ","date":"2024-09-28","objectID":"/lec9-normalization-dropout--implementation/:2:0","tags":null,"title":"Lec9-Normalization, Dropout, + Implementation","uri":"/lec9-normalization-dropout--implementation/"},{"categories":["CMU-10-414-714"],"content":"L2 Regularization é’ˆå¯¹çš„æ˜¯è¿‡æ‹Ÿåˆ?ä½†æ˜¯åªè¦æ˜¯å‡å°‘function classçš„æ“ä½œéƒ½æ˜¯regularizationçš„ä¸€ç§ ç„¶åå‘ç°weight decayå’Œregularizationæœ‰è”ç³»ï¼ ","date":"2024-09-28","objectID":"/lec9-normalization-dropout--implementation/:2:1","tags":null,"title":"Lec9-Normalization, Dropout, + Implementation","uri":"/lec9-normalization-dropout--implementation/"},{"categories":["CMU-10-414-714"],"content":"dropout ","date":"2024-09-28","objectID":"/lec9-normalization-dropout--implementation/:2:2","tags":null,"title":"Lec9-Normalization, Dropout, + Implementation","uri":"/lec9-normalization-dropout--implementation/"},{"categories":["CMU-10-414-714"],"content":"Neural Networks lib implementation ","date":"2024-09-28","objectID":"/lec8-nn-library-implementation/:0:0","tags":null,"title":"Lec8-NN Library Implementation","uri":"/lec8-nn-library-implementation/"},{"categories":["CMU-10-414-714"],"content":"refreshment import needle as ndl def data(self): return self.detach() data ä¸è¦grad ","date":"2024-09-28","objectID":"/lec8-nn-library-implementation/:1:0","tags":null,"title":"Lec8-NN Library Implementation","uri":"/lec8-nn-library-implementation/"},{"categories":["CMU-10-414-714"],"content":"numerical stability è½¯å›å½’æ•°å€¼ä¸å˜æ€§ï¼Œä¸Šä¸‹åŒé™¤ def softmax(x): x = x - np.max(x) z = np.exp(x) return z / np.sum(z) ","date":"2024-09-28","objectID":"/lec8-nn-library-implementation/:2:0","tags":null,"title":"Lec8-NN Library Implementation","uri":"/lec8-nn-library-implementation/"},{"categories":["CMU-10-414-714"],"content":"nn.Module ","date":"2024-09-28","objectID":"/lec8-nn-library-implementation/:3:0","tags":null,"title":"Lec8-NN Library Implementation","uri":"/lec8-nn-library-implementation/"},{"categories":["CMU-10-414-714"],"content":"å‚æ•° class Parameter(ndl.Tensor): def __init__(self, data: np.ndarray, requires_grad=True, dtype=\"float32\"): super().__init__(data, requires_grad=requires_grad, dtype=dtype) w = Parameter([2, 1], dtype=\"float32\") isinstance(w, Parameter) # True # recursive function to get all parameters def _get_params(value: ndl.Tensor)-\u003eList[Parameter]: if isinstance(value, Parameter): return [value] elif isinstance(value, dict): result = [] for k, v in value.items(): result += _get_params(v) return result if isinstance(value, Module): return value.parameters() class Module: def parameters(self)-\u003eList[Parameter]: return _get_params(self.__dict__) # dict -\u003e self's attributes, will be recursively searched for Parameter instances def __call__(self, *args, **kwargs): return self.forward(*args, **kwargs) class ScalarAdd(Module): def __init__(self, init_s=1, init_b=0): self.s = Parameter([init_s], dtype=\"float32\") self.b = Parameter([init_b], dtype=\"float32\") def forward(self, x): return self.s * x + self.b è¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å«å‚æ•°çš„åˆ—è¡¨ ä»¥ä¸Šæ˜¯æ— å‚æ•°çš„å±‚ ","date":"2024-09-28","objectID":"/lec8-nn-library-implementation/:3:1","tags":null,"title":"Lec8-NN Library Implementation","uri":"/lec8-nn-library-implementation/"},{"categories":["CMU-10-414-714"],"content":"Optimizer ç”¨dataåŸåœ°æ›´æ–°å‚æ•° ","date":"2024-09-28","objectID":"/lec8-nn-library-implementation/:4:0","tags":null,"title":"Lec8-NN Library Implementation","uri":"/lec8-nn-library-implementation/"},{"categories":["CMU-10-414-714"],"content":"Neural Networks Abstraction ","date":"2024-09-28","objectID":"/lec7-neural-network-library-abstractions/:0:0","tags":null,"title":"Lec7-Neural Network Library Abstractions","uri":"/lec7-neural-network-library-abstractions/"},{"categories":["CMU-10-414-714"],"content":"Programming Abstraction æ ¸å¿ƒæ€æƒ³æ˜¯host languageæ˜¯ä¸€ä¸ªè¯­è¨€ï¼Œä½†æ˜¯æ‰§è¡Œè®¡ç®—å›¾çš„æ—¶å€™å¯ä»¥ç”¨å…¶ä»–è¯­è¨€æ¥ä¼˜åŒ– å’Œsql \u0026 RDBMSæœ‰ç‚¹ç›¸ä¼¼ ğŸ¤” ","date":"2024-09-28","objectID":"/lec7-neural-network-library-abstractions/:1:0","tags":null,"title":"Lec7-Neural Network Library Abstractions","uri":"/lec7-neural-network-library-abstractions/"},{"categories":["CMU-10-414-714"],"content":"declarative è¿™åº”è¯¥æ¯”è¾ƒè‡ªç„¶çš„æƒ³æ³•ï¼Œfrom google â€œscalable computational systemsâ€ æè¿°å›¾ ==\u003e æŒ‡å®šè¿è¡Œæœºå™¨ ==\u003e è¿è¡Œ ==\u003e ç»“æœ ","date":"2024-09-28","objectID":"/lec7-neural-network-library-abstractions/:1:1","tags":null,"title":"Lec7-Neural Network Library Abstractions","uri":"/lec7-neural-network-library-abstractions/"},{"categories":["CMU-10-414-714"],"content":"imperative define and run å¯¹èåˆç®—å­å‹å¥½ æŒ‡å®šç‰¹å®šå€¼æœ‰ä¸Šé¢declarativeçš„åŒæ ·æ•ˆæœ ","date":"2024-09-28","objectID":"/lec7-neural-network-library-abstractions/:1:2","tags":null,"title":"Lec7-Neural Network Library Abstractions","uri":"/lec7-neural-network-library-abstractions/"},{"categories":["CMU-10-414-714"],"content":"High level modular lib components ç»å…¸ä¸‰æ˜æ²» loss function is a special case of a â€œmoduleâ€ æ­£åˆ™åŒ–: è¦ä¹ˆæ˜¯æŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œè¦ä¹ˆæ˜¯ä¼˜åŒ–å™¨çš„ä¸€éƒ¨åˆ† åˆå§‹åŒ–: åŒ…å«åœ¨nn.Moduleä¸­ ","date":"2024-09-28","objectID":"/lec7-neural-network-library-abstractions/:2:0","tags":null,"title":"Lec7-Neural Network Library Abstractions","uri":"/lec7-neural-network-library-abstractions/"},{"categories":["CMU-10-414-714"],"content":"æ€»ç»“ ","date":"2024-09-28","objectID":"/lec7-neural-network-library-abstractions/:3:0","tags":null,"title":"Lec7-Neural Network Library Abstractions","uri":"/lec7-neural-network-library-abstractions/"},{"categories":["CMU-10-414-714"],"content":"fcnn, optimization, and initialization ","date":"2024-09-28","objectID":"/lec6-optimization/:0:0","tags":null,"title":"Lec6-Optimization","uri":"/lec6-optimization/"},{"categories":["CMU-10-414-714"],"content":"fcnn ","date":"2024-09-28","objectID":"/lec6-optimization/:1:0","tags":null,"title":"Lec6-Optimization","uri":"/lec6-optimization/"},{"categories":["CMU-10-414-714"],"content":"optimization ","date":"2024-09-28","objectID":"/lec6-optimization/:2:0","tags":null,"title":"Lec6-Optimization","uri":"/lec6-optimization/"},{"categories":["CMU-10-414-714"],"content":"initialization ","date":"2024-09-28","objectID":"/lec6-optimization/:3:0","tags":null,"title":"Lec6-Optimization","uri":"/lec6-optimization/"},{"categories":["CMU-10-414-714"],"content":"kaiming initialization æ„Ÿè§‰è¿™èŠ‚è¯¾æ¯”è¾ƒæœ‰ç”¨çš„æ˜¯è¿™å¼ pptï¼Œæˆ‘æ›´å–œæ¬¢ä»å®éªŒçš„è§’åº¦æ¥çœ‹kaiming initializationï¼ˆé¿å…æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ï¼‰ ","date":"2024-09-28","objectID":"/lec6-optimization/:3:1","tags":null,"title":"Lec6-Optimization","uri":"/lec6-optimization/"},{"categories":["UMich-EECS-498"],"content":"Linear Classifiers ","date":"2024-09-23","objectID":"/l3-linear-classifiers/:0:0","tags":null,"title":"L3-linear classifiers","uri":"/l3-linear-classifiers/"},{"categories":["UMich-EECS-498"],"content":"Viewpoints of Linear Classifiers ","date":"2024-09-23","objectID":"/l3-linear-classifiers/:1:0","tags":null,"title":"L3-linear classifiers","uri":"/l3-linear-classifiers/"},{"categories":["UMich-EECS-498"],"content":"hard cases of linear classifiers ","date":"2024-09-23","objectID":"/l3-linear-classifiers/:2:0","tags":null,"title":"L3-linear classifiers","uri":"/l3-linear-classifiers/"},{"categories":["UMich-EECS-498"],"content":"hinge loss hinge loss is a loss function, linear and zeros-in-margin æ„Ÿè§‰æ˜¯ä¸€ä¸ªæœ‰æ„æ€çš„çº¿æ€§æŸå¤±å‡½æ•°ï¼Œä½†æ˜¯ç›®å‰ä½œç”¨ä¸å¤§ SVM loss å¯¹æŸå¤±å‡½æ•°åˆå§‹åŒ–ä¼°å€¼å¯ä»¥åˆæ­¥éªŒè¯æ˜¯å¦ç”±bug ","date":"2024-09-23","objectID":"/l3-linear-classifiers/:3:0","tags":null,"title":"L3-linear classifiers","uri":"/l3-linear-classifiers/"},{"categories":["UMich-EECS-498"],"content":"Regularization ä»çº¿æ€§æ¨¡å‹çš„è§’åº¦çœ‹ï¼Œæ·»åŠ å¸¸æ•°å› å­æ•ˆæœä¸å˜ï¼Œä½†æ˜¯æƒé‡çŸ©é˜µçš„èŒƒæ•°ä¼šå˜åŒ– ç”±ä¸Šå¯çŸ¥ä¸å”¯ä¸€ï¼Œæ‰€ä»¥éœ€è¦å¯¹æƒé‡çŸ©é˜µè¿›è¡Œçº¦æŸï¼Œå¸¸ç”¨çš„çº¦æŸæ–¹å¼æ˜¯æ­£åˆ™åŒ– ä¸€ç§æ–°çš„å¼•å…¥æ­£åˆ™åŒ–é¡¹çš„æ€è·¯ ğŸ¤” ===\u003e express our preference or å…ˆéªŒçš„çŸ¥è¯†ç‚¹ ä½™ä¸‹ä¸¤ç§è§‚ç‚¹ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ ","date":"2024-09-23","objectID":"/l3-linear-classifiers/:4:0","tags":null,"title":"L3-linear classifiers","uri":"/l3-linear-classifiers/"},{"categories":["UMich-EECS-498"],"content":"é‡æ–°ç†è§£L1 / L2æ­£åˆ™åŒ– L1æ­£åˆ™åŒ– : å€¾å‘äºæƒé‡é›†ä¸­ L2æ­£åˆ™åŒ– : å€¾å‘äºæƒé‡å‡åŒ€ ","date":"2024-09-23","objectID":"/l3-linear-classifiers/:4:1","tags":null,"title":"L3-linear classifiers","uri":"/l3-linear-classifiers/"},{"categories":["UMich-EECS-498"],"content":"Lecture 2: Image Classification ","date":"2024-09-23","objectID":"/l2-image-classification/:0:0","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"Introduction Image classification is the task of assigning a labelâ€¦ can be a building-block for many applications ","date":"2024-09-23","objectID":"/l2-image-classification/:1:0","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"More robust, data-driven approaches ","date":"2024-09-23","objectID":"/l2-image-classification/:2:0","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"Understanding the dataset ç®€å•ä»‹ç»ä¸€ä¸‹ç±»ä¼¼äºMNIST, CIFAR-100ç­‰æ•°æ®é›†çš„åŸºæœ¬ç»“æ„ æå‡ºOmniglotæ•°æ®é›†çš„æ¦‚å¿µ few-shot learning ","date":"2024-09-23","objectID":"/l2-image-classification/:2:1","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"Choosing a model Nearest Neighbor find the distance metric between the test image and all the training images memorize the training images and their corresponding labels predict the label of the test image based on the nearest training image With N examplesâ€¦ training time: O(1) or O(N), depending on the copying strategy testing time: O(N) there are more knnâ€¦ see here å†³ç­–è¾¹ç•Œå¹³æ»‘åŒ– more neighboring examplesï¼Œ k ğŸ†™ change the metric ","date":"2024-09-23","objectID":"/l2-image-classification/:2:2","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"Evaluating the model è¯¦è§DATA-100è¯¾ç¨‹ train / validation / test setçš„åˆ’åˆ† \u0026 k-fold cross-validationçš„ä»‹ç» ","date":"2024-09-23","objectID":"/l2-image-classification/:2:3","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"é€šç”¨è¿‘ä¼¼å®šç† knnå¯ä»¥æ‹Ÿåˆä»»æ„çš„è¿ç»­å‡½æ•° ","date":"2024-09-23","objectID":"/l2-image-classification/:3:0","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"ç»´åº¦ç¾éš¾ ","date":"2024-09-23","objectID":"/l2-image-classification/:4:0","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["UMich-EECS-498"],"content":"æ€»ç»“ ","date":"2024-09-23","objectID":"/l2-image-classification/:5:0","tags":null,"title":"L2-image classification","uri":"/l2-image-classification/"},{"categories":["CMU-10-414-714"],"content":"Auto Differentiation Implementation ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:0:0","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"Basic Knowledge ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:1:0","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"OOP in Python class call method åœ¨Pythonä¸­ï¼Œ__call__æ–¹æ³•æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•ï¼Œå®ƒå…è®¸ä¸€ä¸ªç±»çš„å®ä¾‹è¡¨ç°å¾—åƒä¸€ä¸ªå‡½æ•°ã€‚å½“ä½ å®šä¹‰äº†ä¸€ä¸ªç±»ï¼Œå¹¶åœ¨è¯¥ç±»ä¸­å®ç°äº†__call__æ–¹æ³•ï¼Œä½ å°±å¯ä»¥é€šè¿‡ç›´æ¥è°ƒç”¨å®ä¾‹æ¥æ‰§è¡Œè¿™ä¸ªæ–¹æ³•ï¼Œå°±åƒè°ƒç”¨ä¸€ä¸ªå‡½æ•°ä¸€æ ·ã€‚ è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜__call__æ–¹æ³•çš„ç”¨æ³•ï¼š class Greeter: def __init__(self, name): self.name = name def __call__(self): return f\"Hello, {self.name}!\" # åˆ›å»ºGreeterç±»çš„å®ä¾‹ greeter = Greeter(\"Kimi\") # è°ƒç”¨å®ä¾‹ï¼Œå°±åƒå®ƒæ˜¯ä¸€ä¸ªå‡½æ•° print(greeter()) # è¾“å‡º: Hello, Kimi! åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒGreeterç±»æœ‰ä¸€ä¸ª__init__æ–¹æ³•æ¥åˆå§‹åŒ–å®ä¾‹ï¼Œè¿˜æœ‰ä¸€ä¸ª__call__æ–¹æ³•æ¥å®šä¹‰å½“å®ä¾‹è¢«è°ƒç”¨æ—¶åº”è¯¥æ‰§è¡Œçš„æ“ä½œã€‚å½“æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªGreeterçš„å®ä¾‹å¹¶è°ƒç”¨å®ƒæ—¶ï¼Œå®é™…ä¸Šæ˜¯è°ƒç”¨äº†__call__æ–¹æ³•ï¼Œå®ƒè¿”å›äº†ä¸€ä¸ªé—®å€™è¯­ã€‚ __call__æ–¹æ³•é€šå¸¸ç”¨äºåˆ›å»ºå¯è°ƒç”¨çš„å¯¹è±¡ï¼Œè¿™åœ¨æŸäº›è®¾è®¡æ¨¡å¼ä¸­éå¸¸æœ‰ç”¨ï¼Œæ¯”å¦‚å·¥å‚æ¨¡å¼ã€å•ä¾‹æ¨¡å¼ç­‰ã€‚æ­¤å¤–ï¼Œå®ƒä¹Ÿå¸¸ç”¨äºè£…é¥°å™¨ä¸­ï¼Œå…è®¸è£…é¥°å™¨è¿”å›çš„å¯¹è±¡èƒ½å¤Ÿè¢«è°ƒç”¨ã€‚ new method åœ¨Pythonä¸­ï¼Œ__new__æ–¹æ³•æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„é™æ€æ–¹æ³•ï¼Œç”¨äºåˆ›å»ºä¸€ä¸ªç±»çš„æ–°å®ä¾‹ã€‚å®ƒæ˜¯åœ¨__init__æ–¹æ³•ä¹‹å‰è¢«è°ƒç”¨çš„ï¼Œå¹¶ä¸”æ˜¯åˆ›å»ºå¯¹è±¡å®ä¾‹çš„ç¬¬ä¸€ä¸ªæ­¥éª¤ã€‚__new__æ–¹æ³•ä¸»è¦è´Ÿè´£åˆ›å»ºä¸€ä¸ªå¯¹è±¡ï¼Œè€Œ__init__æ–¹æ³•åˆ™ç”¨äºåˆå§‹åŒ–è¿™ä¸ªå¯¹è±¡ã€‚ __new__æ–¹æ³•é€šå¸¸ç”¨äºä»¥ä¸‹æƒ…å†µï¼š ç»§æ‰¿ä¸å¯å˜ç±»å‹ï¼šæ¯”å¦‚å…ƒç»„ã€å­—ç¬¦ä¸²ç­‰ï¼Œå®ƒä»¬æ˜¯ä¸å¯å˜çš„ï¼Œä¸èƒ½ä½¿ç”¨__init__è¿›è¡Œåˆå§‹åŒ–ï¼Œå› ä¸ºå®ƒä»¬åœ¨åˆ›å»ºæ—¶å°±å·²ç»å®Œæˆäº†åˆå§‹åŒ–ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥é€šè¿‡é‡å†™__new__æ–¹æ³•æ¥åˆ›å»ºæ–°çš„å®ä¾‹ã€‚ æ§åˆ¶å®ä¾‹çš„åˆ›å»ºï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½æƒ³è¦æ§åˆ¶å¯¹è±¡çš„åˆ›å»ºè¿‡ç¨‹ï¼Œæ¯”å¦‚å•ä¾‹æ¨¡å¼ï¼Œæˆ–è€…åœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œä¸€äº›ç‰¹æ®Šçš„å¤„ç†ã€‚ ç»§æ‰¿è‡ªå†…ç½®ç±»å‹ï¼šå½“ä½ æƒ³è¦ç»§æ‰¿è‡ªPythonçš„å†…ç½®ç±»å‹æ—¶ï¼Œä½ éœ€è¦é‡å†™__new__æ–¹æ³•æ¥åˆ›å»ºå®ä¾‹ï¼Œå› ä¸ºå†…ç½®ç±»å‹é€šå¸¸ä¸æä¾›__init__æ–¹æ³•ã€‚ __new__æ–¹æ³•çš„åŸºæœ¬è¯­æ³•å¦‚ä¸‹ï¼š class MyClass(metaclass=type): def __new__(cls, *args, **kwargs): # åˆ›å»ºå®ä¾‹çš„ä»£ç  instance = super(MyClass, cls).__new__(cls) # å¯ä»¥åœ¨è¿™é‡Œè¿›è¡Œä¸€äº›åˆå§‹åŒ–æ“ä½œ return instance åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ__new__æ–¹æ³•é¦–å…ˆè°ƒç”¨super()æ¥åˆ›å»ºç±»çš„å®ä¾‹ï¼Œç„¶åå¯ä»¥è¿›è¡Œä¸€äº›é¢å¤–çš„æ“ä½œï¼Œæœ€åè¿”å›è¿™ä¸ªå®ä¾‹ã€‚æ³¨æ„ï¼Œ__new__æ–¹æ³•å¿…é¡»è¿”å›ä¸€ä¸ªå®ä¾‹å¯¹è±¡ã€‚ è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨__new__æ–¹æ³•ï¼š class Singleton: _instance = None def __new__(cls, *args, **kwargs): if not cls._instance: cls._instance = super(Singleton, cls).__new__(cls) return cls._instance # æµ‹è¯•å•ä¾‹æ¨¡å¼ s1 = Singleton() s2 = Singleton() print(s1 is s2) # è¾“å‡º Trueï¼Œè¯´æ˜s1å’Œs2æ˜¯åŒä¸€ä¸ªå®ä¾‹ åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒSingletonç±»é€šè¿‡é‡å†™__new__æ–¹æ³•å®ç°äº†å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿äº†å…¨å±€åªæœ‰ä¸€ä¸ªå®ä¾‹ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:1:1","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"Data Structure in NEEDLE Lazy or Eager Evaluation è‡ªåŠ¨å¾®åˆ†çš„lazyï¼ˆæƒ°æ€§ï¼‰æ¨¡å¼å’Œeagerï¼ˆæ€¥åˆ‡ï¼‰æ¨¡å¼æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å¤„ç†è®¡ç®—å›¾çš„ä¸¤ç§ä¸åŒæ–¹å¼ã€‚å®ƒä»¬å„æœ‰ä¼˜åŠ£ï¼Œé€‚ç”¨äºä¸åŒçš„åœºæ™¯ã€‚ Lazyæ¨¡å¼ï¼š ä¼˜ç‚¹ï¼š çµæ´»æ€§é«˜ï¼Œå¯ä»¥åŠ¨æ€åœ°æ„å»ºè®¡ç®—å›¾ï¼Œæ”¯æŒæ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯ç­‰æ§åˆ¶æµç»“æ„ã€‚ è°ƒè¯•å‹å¥½ï¼Œå› ä¸ºæ“ä½œæ˜¯æŒ‰éœ€æ‰§è¡Œçš„ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ä¼ ç»Ÿçš„Pythonè°ƒè¯•å·¥å…·ã€‚ é€‚åˆäºç ”ç©¶å’Œå¼€å‘é˜¶æ®µï¼Œå› ä¸ºå¯ä»¥å³æ—¶çœ‹åˆ°æ¯ä¸ªæ“ä½œçš„æ•ˆæœã€‚ ç¼ºç‚¹ï¼š æ€§èƒ½å¯èƒ½ä¸å¦‚eageræ¨¡å¼ï¼Œå› ä¸ºå®ƒä¸æ”¯æŒä¸€äº›ä¼˜åŒ–ï¼Œå¦‚æ“ä½œèåˆã€‚ å†…å­˜æ¶ˆè€—å¯èƒ½æ›´é«˜ï¼Œå› ä¸ºä¸ä¼šè¿›è¡Œä¸€äº›ä¼˜åŒ–æ¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚ Eageræ¨¡å¼ï¼š ä¼˜ç‚¹ï¼š æ€§èƒ½é€šå¸¸æ›´å¥½ï¼Œå› ä¸ºå®ƒå…è®¸åœ¨æ‰§è¡Œå‰å¯¹è®¡ç®—å›¾è¿›è¡Œä¼˜åŒ–ï¼Œå¦‚æ“ä½œèåˆå’Œå¸¸é‡æŠ˜å ã€‚ å¯ä»¥å‡å°‘è¿è¡Œæ—¶çš„å†…å­˜æ¶ˆè€—ï¼Œå› ä¸ºä¼˜åŒ–åçš„è®¡ç®—å›¾å¯èƒ½æ›´é«˜æ•ˆã€‚ é€‚åˆäºç”Ÿäº§ç¯å¢ƒï¼Œå› ä¸ºå®ƒæä¾›äº†æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦ã€‚ ç¼ºç‚¹ï¼š çµæ´»æ€§è¾ƒä½ï¼Œä¸æ”¯æŒåŠ¨æ€å›¾ç»“æ„ï¼Œå› ä¸ºè®¡ç®—å›¾æ˜¯åœ¨è¿è¡Œå‰æ„å»ºçš„ã€‚ è°ƒè¯•å¯èƒ½ä¸å¦‚lazyæ¨¡å¼æ–¹ä¾¿ï¼Œå› ä¸ºéœ€è¦è€ƒè™‘è®¡ç®—å›¾çš„ç»“æ„ã€‚ åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€‰æ‹©å“ªç§æ¨¡å¼å–å†³äºå…·ä½“çš„éœ€æ±‚ã€‚ä¾‹å¦‚ï¼ŒPyTorché»˜è®¤ä½¿ç”¨eageræ¨¡å¼ï¼Œå› ä¸ºå®ƒçš„åŠ¨æ€æ€§å’Œæ˜“ç”¨æ€§é€‚åˆäºç ”ç©¶å’Œå¼€å‘ã€‚è€ŒTensorFlowåœ¨æ—©æœŸç‰ˆæœ¬ä¸­ä½¿ç”¨é™æ€å›¾ï¼Œä½†åæ¥å¼•å…¥äº†eager executionæ¥æä¾›æ›´çµæ´»çš„ç¼–ç¨‹ä½“éªŒã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œé€šå¸¸ä¼šä½¿ç”¨å›¾æ¨¡å¼æ¥ä¼˜åŒ–æ€§èƒ½ã€‚ æ ¹æ®æœç´¢ç»“æœï¼ŒPyTorchçš„eageræ¨¡å¼å…è®¸å³æ—¶æ‰§è¡Œæ“ä½œï¼Œä½¿å¾—è°ƒè¯•æ›´åŠ ç›´è§‚ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨Pythonicçš„æ§åˆ¶æµç»“æ„ï¼Œè€Œä¸æ˜¯é¢„å…ˆå®šä¹‰çš„é™æ€å›¾ ã€‚è€ŒTensorFlowçš„eageræ¨¡å¼åˆ™æä¾›äº†ä¸PyTorchç±»ä¼¼çš„åŠ¨æ€å›¾è®¡ç®—æ¨¡å¼ï¼Œä½¿å¾—æ“ä½œå¯ä»¥ç«‹å³æ‰§è¡Œï¼Œè€Œä¸æ˜¯å…ˆæ„å»ºè®¡ç®—å›¾ ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—eageræ¨¡å¼åœ¨æŸäº›æƒ…å†µä¸‹æ›´åŠ æ–¹ä¾¿ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¿«é€Ÿè¿­ä»£å’Œè°ƒè¯•çš„ç ”ç©¶ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œå¯¹äºéœ€è¦é«˜æ€§èƒ½çš„ç”Ÿäº§ç¯å¢ƒï¼Œå›¾æ¨¡å¼é€šå¸¸æ›´å—é’çï¼Œå› ä¸ºå®ƒå¯ä»¥é€šè¿‡å„ç§ä¼˜åŒ–æ‰‹æ®µæ¥æé«˜æ‰§è¡Œæ•ˆç‡ ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:1:2","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"Details in math ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:2:0","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"summation è¦ä»æ•°å­¦çš„è§’åº¦æ¨å¯¼å‡º Summation çš„æ¢¯åº¦ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦ç†è§£ summation æ“ä½œçš„åŸºæœ¬åŸç†ä»¥åŠå®ƒå¯¹è¾“å…¥å¼ é‡çš„å½±å“ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¼ é‡ aï¼Œå¹¶ä¸”æˆ‘ä»¬é€šè¿‡ sum æ“ä½œå¯¹å…¶è¿›è¡Œæ±‚å’Œï¼Œé‚£ä¹ˆï¼š Summation æ“ä½œï¼š å®šä¹‰ä¸€ä¸ªå¼ é‡ aï¼Œå…¶å½¢çŠ¶ä¸º (a_1, a_2, ..., a_n)ã€‚å½“æˆ‘ä»¬å¯¹ a çš„æŸäº›è½´ï¼ˆaxesï¼‰æ‰§è¡Œæ±‚å’Œæ“ä½œæ—¶ï¼Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶å°†å˜å°ï¼Œä¸¢å¤±æ‰é‚£äº›è¢«æ±‚å’Œçš„ç»´åº¦ã€‚ä¾‹å¦‚ï¼š å¦‚æœå¯¹æ‰€æœ‰ç»´åº¦æ±‚å’Œï¼Œè¾“å‡ºå°†æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚ å¦‚æœåªå¯¹æŸäº›ç»´åº¦æ±‚å’Œï¼Œè¾“å‡ºçš„å¼ é‡å½¢çŠ¶ä¼šä¿æŒä¸å˜ï¼Œä½†ä¼šä¸¢å¤±é‚£äº›è¢«æ±‚å’Œçš„ç»´åº¦ã€‚ æ¢¯åº¦çš„æ¨å¯¼ï¼š æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨å¯¼ f(a) = sum(a) å¯¹è¾“å…¥ a çš„æ¢¯åº¦ã€‚æ¢å¥è¯è¯´ï¼Œç»™å®š sum(a) å¯¹è¾“å…¥ a çš„è¾“å‡º out å’Œè¾“å‡ºçš„æ¢¯åº¦ out_gradï¼Œæˆ‘ä»¬è¦è®¡ç®— f å¯¹ a çš„æ¢¯åº¦ã€‚è¿™ä¸ªæ¢¯åº¦è¡¨ç¤ºçš„å®é™…ä¸Šæ˜¯ åå‘ä¼ æ’­ä¸­å¦‚ä½•å°†æ¢¯åº¦ä» out_grad ä¼ æ’­å›è¾“å…¥ aã€‚ ä¸è€ƒè™‘è½´çš„æƒ…å†µï¼š å¯¹äºæ²¡æœ‰æŒ‡å®šè½´çš„ç®€å•æ€»å’Œæ“ä½œ sum(a)ï¼Œå³å¯¹æ‰€æœ‰å…ƒç´ æ±‚å’Œçš„æƒ…å†µï¼š [ f(a) = \\sum_{i} a_i ] æ±‚å’Œæ“ä½œçš„æ¢¯åº¦å¯¹äºæ¯ä¸ªå…ƒç´ æ˜¯å‡åŒ€çš„ã€‚å¦‚æœæˆ‘ä»¬å¯¹ out = f(a) çš„æ ‡é‡æœ‰ä¸€ä¸ªæ¢¯åº¦ out_gradï¼Œåˆ™å¯¹æ¯ä¸ªè¾“å…¥å…ƒç´  a_i çš„æ¢¯åº¦æ˜¯ç›¸åŒçš„ï¼Œä¹Ÿå°±æ˜¯ out_gradã€‚å› æ­¤ï¼Œå¯¹ a çš„æ¢¯åº¦æ˜¯ä¸€ä¸ªä¸ a å½¢çŠ¶ç›¸åŒçš„å¼ é‡ï¼Œæ¯ä¸ªä½ç½®çš„å€¼éƒ½æ˜¯ out_gradã€‚ è€ƒè™‘ç‰¹å®šè½´çš„æƒ…å†µï¼š å¦‚æœæˆ‘ä»¬åªå¯¹ a çš„æŸäº›è½´ axes è¿›è¡Œæ±‚å’Œï¼Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ä¼šå˜å°ï¼Œä¸¢å¤±æ‰è¢«æ±‚å’Œçš„ç»´åº¦ã€‚è¦æŠŠ out_grad ä¼ æ’­å›åˆ°åŸå§‹è¾“å…¥å¼ é‡ aï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ å¹¿æ’­ï¼ˆbroadcastingï¼‰ æ¥æ‰©å±• out_grad çš„å½¢çŠ¶ï¼Œä½¿å…¶ä¸ a çš„å½¢çŠ¶ç›¸åŒã€‚è¿™æ˜¯é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°çš„ï¼š é¦–å…ˆï¼Œç¡®å®šå“ªäº›è½´è¢«æ±‚å’Œï¼ˆå³ axesï¼‰ã€‚ æ¥ç€ï¼Œæˆ‘ä»¬å°† out_grad å½¢çŠ¶æ‰©å±•ä¸ºä¸åŸå§‹è¾“å…¥ a çš„å½¢çŠ¶åŒ¹é…ã€‚é€šè¿‡ reshape å’Œ broadcast æ“ä½œï¼Œå¯ä»¥å°† out_grad çš„å½¢çŠ¶è°ƒæ•´ä¸ºä¸ a çš„å½¢çŠ¶å…¼å®¹ã€‚ è¿™æ„å‘³ç€æˆ‘ä»¬å°† out_grad çš„å€¼å¤åˆ¶åˆ°æ‰€æœ‰æ±‚å’Œçš„è½´ä¸Šã€‚ å…·ä½“çš„æ¢¯åº¦æ“ä½œä¸ºï¼š [ \\text{grad}_a = \\text{broadcast_to}(\\text{reshape}(out_grad, \\text{expanded_shape}), \\text{original_shape}) ] å…¶ä¸­ï¼Œexpanded_shape æ˜¯å°† out_grad çš„å½¢çŠ¶åœ¨æ±‚å’Œçš„è½´ä¸Šæ‰©å±•ä¸º 1ï¼Œç„¶åé€šè¿‡å¹¿æ’­å°†å…¶åŒ¹é…åŸå§‹è¾“å…¥å¼ é‡çš„å½¢çŠ¶ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:2:1","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"æ¢¯åº¦çš„æ¨å¯¼æ€»ç»“ï¼š å½“æˆ‘ä»¬å¯¹æ‰€æœ‰ç»´åº¦æ±‚å’Œæ—¶ï¼Œæ¢¯åº¦ä¼šå‡åŒ€åœ°åˆ†å¸ƒåˆ°æ¯ä¸ªè¾“å…¥å…ƒç´ ä¸Šï¼Œæ¯ä¸ªä½ç½®çš„æ¢¯åº¦éƒ½æ˜¯ out_gradã€‚ å½“å¯¹æŸäº›ç»´åº¦æ±‚å’Œæ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°† out_grad æ‰©å±•åˆ°ä¸è¾“å…¥ç›¸åŒçš„å½¢çŠ¶ï¼Œè¿™é€šè¿‡ reshape å’Œ broadcast å®ç°ï¼Œä½¿å¾—æ±‚å’Œæ“ä½œçš„åå‘ä¼ æ’­èƒ½å¤Ÿæ­£ç¡®ä¼ æ’­æ¢¯åº¦ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:2:2","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"reshape Reshape æ“ä½œçš„æ¢¯åº¦æ¨å¯¼å…¶å®ç›¸å¯¹ç›´è§‚ã€‚é‡å¡‘ï¼ˆreshapeï¼‰æ“ä½œä¸ä¼šæ”¹å˜æ•°æ®æœ¬èº«ï¼Œåªæ˜¯æ”¹å˜æ•°æ®åœ¨å†…å­˜ä¸­çš„æ’åˆ—æ–¹å¼ã€‚å› æ­¤ï¼Œåœ¨åå‘ä¼ æ’­æ—¶ï¼Œreshape æ“ä½œçš„æ¢¯åº¦å¯ä»¥ç›´æ¥æŒ‰ç…§åå‘çš„å½¢çŠ¶å˜åŒ–æ¥è¿›è¡Œé‡æ–°æ’åˆ—ã€‚ Reshape æ“ä½œçš„åŸºæœ¬æ¦‚å¿µï¼š Reshape æ“ä½œçš„ç›®çš„æ˜¯å°†å¼ é‡ a çš„å½¢çŠ¶ä»åŸå§‹çš„å½¢çŠ¶ input_shape è½¬æ¢ä¸ºç›®æ ‡å½¢çŠ¶ target_shapeï¼Œä½†ä¿æŒå…ƒç´ çš„é¡ºåºä¸å˜ã€‚æ•°æ®çš„å†…å­˜å¸ƒå±€ä¿æŒä¸å˜ï¼Œåªæ˜¯æ›´æ”¹äº†å®ƒçš„å½¢çŠ¶ã€‚ Reshape çš„æ¢¯åº¦è®¡ç®—ï¼š å› ä¸º reshape æ“ä½œåªæ˜¯æ”¹å˜äº†å¼ é‡çš„å½¢çŠ¶è€Œä¸æ”¹å˜å…¶æ•°æ®å†…å®¹ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦çš„è®¡ç®—éå¸¸ç®€å•ã€‚å¯¹äºä¸€ä¸ªé€šè¿‡ reshape äº§ç”Ÿçš„è¾“å‡ºå¼ é‡çš„æ¢¯åº¦ out_gradï¼Œå…¶æ¢¯åº¦ä¼ æ’­åˆ°è¾“å…¥å¼ é‡æ—¶ï¼Œä»…éœ€å°† out_grad é‡å¡‘å›è¾“å…¥å¼ é‡çš„å½¢çŠ¶ã€‚ å…·ä½“æ¥è¯´ï¼Œå‡è®¾ï¼š è¾“å…¥å¼ é‡çš„å½¢çŠ¶ä¸º input_shapeï¼Œå³ a.shape = input_shapeï¼› é€šè¿‡ reshape æ“ä½œï¼Œå¼ é‡å˜æˆäº† target_shapeï¼› æˆ‘ä»¬åœ¨åå‘ä¼ æ’­ä¸­å¾—åˆ°äº† out_gradï¼Œå®ƒçš„å½¢çŠ¶ä¸º target_shapeã€‚ æˆ‘ä»¬è¦è®¡ç®—è¾“å…¥å¼ é‡ a çš„æ¢¯åº¦ã€‚ç”±äº reshape å¹¶æ²¡æœ‰æ”¹å˜æ•°æ®å†…å®¹ï¼Œåªéœ€å°† out_grad çš„å½¢çŠ¶é‡å¡‘ä¸º input_shapeã€‚ å®ç°æ¢¯åº¦çš„é€»è¾‘ï¼š å› æ­¤ï¼ŒReshape æ“ä½œçš„æ¢¯åº¦å®ç°éå¸¸ç®€å•ï¼Œåªéœ€è¦å°† out_grad é‡æ–°å˜æ¢å›åŸå§‹è¾“å…¥å¼ é‡çš„å½¢çŠ¶å³å¯ã€‚è¿™å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å®ç°ï¼š def gradient(self, out_grad, node): # è·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶ input_shape = node.inputs[0].shape # å°†è¾“å‡ºæ¢¯åº¦ reshaped å›åŸå§‹è¾“å…¥çš„å½¢çŠ¶ return reshape(out_grad, input_shape) ç›´è§‚ç†è§£ï¼š åœ¨å‰å‘è¿‡ç¨‹ä¸­ï¼Œreshape åªæ˜¯é‡æ–°æ’åˆ—æ•°æ®çš„å½¢çŠ¶ï¼Œè€Œä¸æ”¹å˜æ•°æ®çš„å€¼ã€‚ åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦å°† out_grad çš„å½¢çŠ¶é‡æ–°æ’åˆ—ä¸ºè¾“å…¥å¼ é‡çš„å½¢çŠ¶ï¼Œä»¥ä¾¿æ¢¯åº¦èƒ½å¤Ÿæ­£ç¡®ä¼ æ’­åˆ°è¾“å…¥ã€‚ å› ä¸º reshape æ“ä½œæœ¬èº«ä¸ä¼šå½±å“æ•°æ®å€¼ï¼Œæ¢¯åº¦ä¼ æ’­è¿‡ç¨‹ä¹Ÿæ˜¯ç®€å•çš„å½¢çŠ¶å˜æ¢ï¼Œå› æ­¤ Reshape æ“ä½œçš„æ¢¯åº¦æ¨å¯¼éå¸¸ç›´æ¥ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:2:3","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"broadcast ä½ æä¾›çš„ä»£ç æ˜¯ BroadcastTo æ“ä½œçš„æ¢¯åº¦å®ç°ï¼Œå®ƒæ­£ç¡®åœ°å¤„ç†äº†å¹¿æ’­æ“ä½œçš„åå‘ä¼ æ’­é—®é¢˜ã€‚è®©æˆ‘ä»¬é€æ­¥åˆ†æå®ƒçš„é€»è¾‘ï¼š è¾“å…¥ä¸å‰å‘ä¼ æ’­ï¼š BroadcastTo æ“ä½œçš„ç›®çš„æ˜¯å°†è¾“å…¥å¼ é‡ a æ‰©å±•ä¸ºä¸€ä¸ªæ–°çš„å½¢çŠ¶ self.shapeã€‚å‰å‘ä¼ æ’­ä¸­ï¼Œé€šè¿‡è°ƒç”¨ array_api.broadcast_to(a, self.shape) å°†è¾“å…¥ a å¹¿æ’­åˆ° self.shapeã€‚ åå‘ä¼ æ’­çš„ç›®æ ‡ï¼š åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°†æ¢¯åº¦ out_grad ä¼ æ’­å›è¾“å…¥å¼ é‡ aï¼Œå³å½¢çŠ¶ä¸º node.inputs[0].shape çš„å¼ é‡ã€‚åœ¨å¹¿æ’­è¿‡ç¨‹ä¸­ï¼ŒæŸäº›ç»´åº¦å¯èƒ½æ˜¯é€šè¿‡æ‰©å±•ä¸ºæ›´å¤§å€¼å®ç°çš„ï¼Œè¿™äº›æ‰©å±•çš„ç»´åº¦éœ€è¦åœ¨åå‘ä¼ æ’­ä¸­è¿›è¡Œå¤„ç†ï¼ˆé€šè¿‡æ±‚å’Œæ¢å¤åˆ°åŸå§‹ç»´åº¦çš„å¤§å°ï¼‰ã€‚ ä¸»è¦æ­¥éª¤ï¼š Step 1: ç¡®å®šè¾“å…¥å½¢çŠ¶å’Œç›®æ ‡å½¢çŠ¶çš„å·®å¼‚ shape = list(node.inputs[0].shape) # è·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶ shape = [1] * (len(self.shape) - len(shape)) + shape # å¯¹è¾“å…¥å½¢çŠ¶è¿›è¡Œæ‰©å±•ï¼Œä¿æŒä¸ç›®æ ‡å½¢çŠ¶çš„é•¿åº¦ä¸€è‡´ node.inputs[0].shape æ˜¯åŸå§‹è¾“å…¥å¼ é‡çš„å½¢çŠ¶ã€‚ self.shape æ˜¯å¹¿æ’­åçš„å½¢çŠ¶ã€‚ å¦‚æœ self.shape çš„é•¿åº¦å¤§äº node.inputs[0].shape çš„é•¿åº¦ï¼Œé‚£ä¹ˆéœ€è¦åœ¨å‰é¢è¡¥ä¸Š 1 æ¥åŒ¹é…ç»´åº¦çš„æ•°é‡ã€‚è¿™æ˜¯å› ä¸ºå¹¿æ’­å…è®¸åœ¨é«˜ç»´åº¦çš„å‰é¢æ’å…¥ 1 ä»¥é€‚åº”ç›®æ ‡å½¢çŠ¶ã€‚ Step 2: æ‰¾åˆ°éœ€è¦æ±‚å’Œçš„è½´ ä½ æä¾›çš„ä»£ç æ˜¯ BroadcastTo æ“ä½œçš„æ¢¯åº¦å®ç°ï¼Œå®ƒæ­£ç¡®åœ°å¤„ç†äº†å¹¿æ’­æ“ä½œçš„åå‘ä¼ æ’­é—®é¢˜ã€‚è®©æˆ‘ä»¬é€æ­¥åˆ†æå®ƒçš„é€»è¾‘ï¼š è¾“å…¥ä¸å‰å‘ä¼ æ’­ï¼š BroadcastTo æ“ä½œçš„ç›®çš„æ˜¯å°†è¾“å…¥å¼ é‡ a æ‰©å±•ä¸ºä¸€ä¸ªæ–°çš„å½¢çŠ¶ self.shapeã€‚å‰å‘ä¼ æ’­ä¸­ï¼Œé€šè¿‡è°ƒç”¨ array_api.broadcast_to(a, self.shape) å°†è¾“å…¥ a å¹¿æ’­åˆ° self.shapeã€‚ åå‘ä¼ æ’­çš„ç›®æ ‡ï¼š åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°†æ¢¯åº¦ out_grad ä¼ æ’­å›è¾“å…¥å¼ é‡ aï¼Œå³å½¢çŠ¶ä¸º node.inputs[0].shape çš„å¼ é‡ã€‚åœ¨å¹¿æ’­è¿‡ç¨‹ä¸­ï¼ŒæŸäº›ç»´åº¦å¯èƒ½æ˜¯é€šè¿‡æ‰©å±•ä¸ºæ›´å¤§å€¼å®ç°çš„ï¼Œè¿™äº›æ‰©å±•çš„ç»´åº¦éœ€è¦åœ¨åå‘ä¼ æ’­ä¸­è¿›è¡Œå¤„ç†ï¼ˆé€šè¿‡æ±‚å’Œæ¢å¤åˆ°åŸå§‹ç»´åº¦çš„å¤§å°ï¼‰ã€‚ ä¸»è¦æ­¥éª¤ï¼š Step 1: ç¡®å®šè¾“å…¥å½¢çŠ¶å’Œç›®æ ‡å½¢çŠ¶çš„å·®å¼‚ shape = list(node.inputs[0].shape) # è·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶ shape = [1] * (len(self.shape) - len(shape)) + shape # å¯¹è¾“å…¥å½¢çŠ¶è¿›è¡Œæ‰©å±•ï¼Œä¿æŒä¸ç›®æ ‡å½¢çŠ¶çš„é•¿åº¦ä¸€è‡´ node.inputs[0].shape æ˜¯åŸå§‹è¾“å…¥å¼ é‡çš„å½¢çŠ¶ã€‚ self.shape æ˜¯å¹¿æ’­åçš„å½¢çŠ¶ã€‚ å¦‚æœ self.shape çš„é•¿åº¦å¤§äº node.inputs[0].shape çš„é•¿åº¦ï¼Œé‚£ä¹ˆéœ€è¦åœ¨å‰é¢è¡¥ä¸Š 1 æ¥åŒ¹é…ç»´åº¦çš„æ•°é‡ã€‚è¿™æ˜¯å› ä¸ºå¹¿æ’­å…è®¸åœ¨é«˜ç»´åº¦çš„å‰é¢æ’å…¥ 1 ä»¥é€‚åº”ç›®æ ‡å½¢çŠ¶ã€‚ Step 2: æ‰¾åˆ°éœ€è¦æ±‚å’Œçš„è½´ axes = [] for i, s in enumerate(self.shape): if i \u003e= len(shape) or s != shape[i]: axes.append(i) self.shape æ˜¯å¹¿æ’­åçš„å½¢çŠ¶ï¼Œshape æ˜¯ç»è¿‡æ‰©å±•çš„åŸå§‹è¾“å…¥å½¢çŠ¶ã€‚ éå†ç›®æ ‡å½¢çŠ¶ self.shapeï¼Œå¦‚æœç›®æ ‡å½¢çŠ¶å’Œæ‰©å±•åçš„è¾“å…¥å½¢çŠ¶åœ¨æŸä¸ªç»´åº¦ä¸ç›¸åŒï¼ˆå³å¹¿æ’­å‘ç”Ÿï¼‰ï¼Œåˆ™å°†è¯¥ç»´åº¦çš„ç´¢å¼•æ·»åŠ åˆ° axes åˆ—è¡¨ä¸­ã€‚ è¿™äº›è½´æ˜¯éœ€è¦åœ¨åå‘ä¼ æ’­æ—¶è¿›è¡Œæ±‚å’Œçš„è½´ï¼Œå› ä¸ºè¿™äº›è½´åœ¨å‰å‘ä¼ æ’­æ—¶é€šè¿‡å¹¿æ’­æ‰©å±•äº†ã€‚ Step 3: æ±‚å’Œå¹¶è°ƒæ•´å½¢çŠ¶ return reshape(summation(out_grad, tuple(axes)), node.inputs[0].shape) æ±‚å’Œï¼šåœ¨æŒ‡å®šçš„è½´ axes ä¸Šå¯¹ out_grad è¿›è¡Œæ±‚å’Œï¼Œä»¥æ¶ˆé™¤å¹¿æ’­çš„æ•ˆæœï¼Œæ¢å¤åˆ°å¹¿æ’­å‰çš„å½¢çŠ¶ã€‚ è°ƒæ•´å½¢çŠ¶ï¼šæ±‚å’Œåï¼Œä½¿ç”¨ reshape å°†æ±‚å’Œåçš„å¼ é‡è°ƒæ•´ä¸ºè¾“å…¥å¼ é‡çš„åŸå§‹å½¢çŠ¶ node.inputs[0].shapeï¼Œç¡®ä¿æ¢¯åº¦çš„å½¢çŠ¶ä¸è¾“å…¥å¼ é‡åŒ¹é…ã€‚ ä»£ç åˆ†ææ€»ç»“ï¼š å¹¿æ’­ç»´åº¦åŒ¹é…ï¼šé€šè¿‡åœ¨è¾“å…¥å½¢çŠ¶å‰é¢è¡¥ 1 æ¥å¤„ç†è¾“å…¥å¼ é‡å’Œç›®æ ‡å¼ é‡ç»´åº¦æ•°é‡ä¸åŒ¹é…çš„æƒ…å†µã€‚ è¯†åˆ«éœ€è¦æ±‚å’Œçš„è½´ï¼šé€šè¿‡æ¯”è¾ƒç›®æ ‡å½¢çŠ¶å’Œæ‰©å±•åçš„è¾“å…¥å½¢çŠ¶ï¼Œæ‰¾å‡ºå“ªäº›ç»´åº¦æ˜¯å¹¿æ’­å¯¼è‡´æ‰©å±•çš„ï¼Œå¹¶åœ¨è¿™äº›ç»´åº¦ä¸Šè¿›è¡Œæ±‚å’Œã€‚ é‡å¡‘æ¢¯åº¦ï¼šæœ€ç»ˆå°†ç»è¿‡æ±‚å’Œçš„æ¢¯åº¦é‡å¡‘å›è¾“å…¥å¼ é‡çš„å½¢çŠ¶ï¼Œä»¥ä¾¿æ­£ç¡®åœ°åå‘ä¼ æ’­æ¢¯åº¦ã€‚ ç›´è§‚ç†è§£ï¼š åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œå¹¿æ’­æ˜¯å°†è¾ƒå°å½¢çŠ¶çš„å¼ é‡æ‰©å±•ä¸ºæ›´å¤§å½¢çŠ¶ã€‚ åœ¨åå‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬è¦é€†è½¬è¿™ç§æ‰©å±•ï¼Œå³å°†æ‰©å±•çš„ç»´åº¦çš„æ¢¯åº¦â€œåˆå¹¶â€ï¼Œè¿™å°±æ˜¯é€šè¿‡åœ¨è¿™äº›ç»´åº¦ä¸Šæ±‚å’Œå®ç°çš„ã€‚ è¿™æ®µä»£ç å¾ˆå¥½åœ°å®ç°äº† BroadcastTo æ“ä½œçš„æ¢¯åº¦è®¡ç®—ï¼Œæ—¢è€ƒè™‘äº†è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶çš„å·®å¼‚ï¼Œä¹Ÿæ­£ç¡®å¤„ç†äº†å¹¿æ’­æ‰©å±•çš„ç»´åº¦çš„åå‘æ±‚å’Œã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:2:4","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"list, tuple, and dict åœ¨Pythonä¸­ï¼Œlistï¼ˆåˆ—è¡¨ï¼‰ã€tupleï¼ˆå…ƒç»„ï¼‰å’Œdictï¼ˆå­—å…¸ï¼‰æ˜¯ä¸‰ç§å¸¸ç”¨çš„æ•°æ®ç»“æ„ï¼Œå®ƒä»¬å„è‡ªæœ‰ç‹¬ç‰¹çš„ç‰¹æ€§å’Œç”¨é€”ã€‚ä¸‹é¢æ˜¯å®ƒä»¬çš„åŒºåˆ«ä¸è”ç³»ï¼š ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:3:0","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"Listï¼ˆåˆ—è¡¨ï¼‰ ç±»å‹ï¼šå¯å˜åºåˆ—ã€‚ å…ƒç´ ï¼šå¯ä»¥åŒ…å«ä»»ä½•ç±»å‹çš„å…ƒç´ ï¼ŒåŒ…æ‹¬å¦ä¸€ä¸ªåˆ—è¡¨ã€‚ ç´¢å¼•ï¼šé€šè¿‡ç´¢å¼•è®¿é—®å…ƒç´ ï¼Œç´¢å¼•ä»0å¼€å§‹ã€‚ æ“ä½œï¼šå¯ä»¥è¿›è¡Œå¢åŠ ã€åˆ é™¤ã€ä¿®æ”¹ç­‰æ“ä½œã€‚ ç”¨é€”ï¼šå½“ä½ éœ€è¦ä¸€ä¸ªå¯ä»¥æ”¹å˜å¤§å°çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:3:1","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"Tupleï¼ˆå…ƒç»„ï¼‰ ç±»å‹ï¼šä¸å¯å˜åºåˆ—ã€‚ å…ƒç´ ï¼šå¯ä»¥åŒ…å«ä»»ä½•ç±»å‹çš„å…ƒç´ ï¼ŒåŒ…æ‹¬å¦ä¸€ä¸ªå…ƒç»„ã€‚ ç´¢å¼•ï¼šé€šè¿‡ç´¢å¼•è®¿é—®å…ƒç´ ï¼Œç´¢å¼•ä»0å¼€å§‹ã€‚ æ“ä½œï¼šä¸€æ—¦åˆ›å»ºï¼Œä¸èƒ½ä¿®æ”¹ï¼ˆä¸èƒ½å¢åŠ ã€åˆ é™¤æˆ–ä¿®æ”¹å…ƒç´ ï¼‰ã€‚ ç”¨é€”ï¼šå½“ä½ éœ€è¦ä¸€ä¸ªä¸éœ€è¦æ”¹å˜çš„åºåˆ—æ—¶ä½¿ç”¨ï¼Œé€šå¸¸ç”¨äºä¿æŠ¤æ•°æ®ä¸è¢«æ”¹å˜ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:3:2","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"Dictï¼ˆå­—å…¸ï¼‰ ç±»å‹ï¼šå¯å˜å®¹å™¨ã€‚ å…ƒç´ ï¼šå­˜å‚¨é”®å€¼å¯¹ï¼ˆkey-value pairsï¼‰ï¼Œé”®å¿…é¡»æ˜¯ä¸å¯å˜ç±»å‹ï¼Œå€¼å¯ä»¥æ˜¯ä»»ä½•ç±»å‹ã€‚ ç´¢å¼•ï¼šé€šè¿‡é”®è®¿é—®å…ƒç´ ï¼Œè€Œä¸æ˜¯ç´¢å¼•ã€‚ æ“ä½œï¼šå¯ä»¥æ·»åŠ ã€åˆ é™¤æˆ–ä¿®æ”¹é”®å€¼å¯¹ã€‚ ç”¨é€”ï¼šå½“ä½ éœ€è¦å­˜å‚¨å…³è”æ•°æ®æ—¶ä½¿ç”¨ï¼Œä¾‹å¦‚ï¼Œå­˜å‚¨å¯¹è±¡çš„å±æ€§ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:3:3","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"è”ç³» åºåˆ—ï¼šlist å’Œ tuple éƒ½æ˜¯åºåˆ—ç±»å‹ï¼Œå¯ä»¥è¿›è¡Œè¿­ä»£ï¼Œå¹¶ä¸”æ”¯æŒè®¸å¤šç›¸ä¼¼çš„æ“ä½œï¼Œå¦‚ç´¢å¼•ã€åˆ‡ç‰‡ç­‰ã€‚ å¯è¿­ä»£ï¼šlistã€tuple å’Œ dict éƒ½æ˜¯å¯è¿­ä»£çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯ä»¥ç”¨äºå¾ªç¯å’Œå…¶ä»–æœŸæœ›å¯è¿­ä»£å¯¹è±¡çš„åœºåˆã€‚ å†…ç½®æ–¹æ³•ï¼šå®ƒä»¬éƒ½æœ‰è®¸å¤šå†…ç½®æ–¹æ³•æ¥æ”¯æŒå¸¸è§çš„æ“ä½œï¼Œå¦‚æ·»åŠ ã€åˆ é™¤ã€æŸ¥æ‰¾ç­‰ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:3:4","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"åŒºåˆ« å¯å˜æ€§ï¼šlist æ˜¯å¯å˜çš„ï¼Œè€Œ tuple æ˜¯ä¸å¯å˜çš„ã€‚dict ä¹Ÿæ˜¯å¯å˜çš„ã€‚ å…ƒç´ ç±»å‹ï¼šdict å­˜å‚¨çš„æ˜¯é”®å€¼å¯¹ï¼Œè€Œ list å’Œ tuple å­˜å‚¨çš„æ˜¯å…ƒç´ åºåˆ—ã€‚ æ€§èƒ½ï¼šå¯¹äºéœ€è¦é¢‘ç¹ä¿®æ”¹å…ƒç´ çš„åœºæ™¯ï¼Œlist æ›´åˆé€‚ï¼›å¯¹äºä¸éœ€è¦ä¿®æ”¹çš„åœºæ™¯ï¼Œtuple æ›´åˆé€‚ï¼Œå› ä¸ºå®ƒçš„ä¸å¯å˜æ€§å¯ä»¥æé«˜æ€§èƒ½ã€‚ å­˜å‚¨æ•ˆç‡ï¼šç”±äº tuple çš„ä¸å¯å˜æ€§ï¼Œå®ƒé€šå¸¸æ¯” list åœ¨å­˜å‚¨ä¸Šæ›´é«˜æ•ˆã€‚ è®¿é—®æ–¹å¼ï¼šdict é€šè¿‡é”®è®¿é—®å…ƒç´ ï¼Œè€Œ list å’Œ tuple é€šè¿‡ç´¢å¼•è®¿é—®ã€‚ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:3:5","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"ç¤ºä¾‹ # List my_list = [1, 2, 3] my_list.append(4) # å¯å˜ # Tuple my_tuple = (1, 2, 3) # my_tuple[0] = 4 # ä¸å¯å˜ï¼Œä¼šæŠ¥é”™ # Dict my_dict = {'name': 'Kimi', 'age': 30} my_dict['age'] = 31 # å¯å˜ ","date":"2024-09-17","objectID":"/lec5-automatic-differentiation-implementation/:3:6","tags":null,"title":"Lec5-Automatic Differentiation Implementation","uri":"/lec5-automatic-differentiation-implementation/"},{"categories":["CMU-10-414-714"],"content":"Automatic Differentiation ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:0:0","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"æ•°å€¼å¾®åˆ† å¸Œæœ›è¯¯å·®é˜¶æ•°ä¸º $O(h^2)$ äº‹å®ä¸Šå¹¶éé‡‡å–è¿™ç§æ–¹å¼è®¡ç®—ï¼Œåªæ˜¯ç”¨æ¥test side note: $\\delta^T$ : pick a small vector $\\delta$ from unit ball $\\nabla_{x} f(x)$ : gradient of $f(x)$ at $x$, é€šå¸¸æ˜¯å…¶ä»–æ–¹æ³•è®¡ç®—çš„ å³æ‰‹è¾¹åˆ™æ˜¯æ•°å€¼è®¡ç®—ï¼Œç„¶åçœ‹ä¸¤è€…æ˜¯å¦è¿‘ä¼¼ç›¸ç­‰ ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:1:0","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"ç¬¦å·å¾®åˆ† è®¸å¤šé‡å¤çš„çš„è®¡ç®—ä¸IOï¼Œä½†æ˜¯å¯ä»¥ä½œä¸ºè‡ªåŠ¨å¾®åˆ†çš„å¼•å…¥ ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:2:0","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"è‡ªåŠ¨å¾®åˆ† ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:3:0","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"è®¡ç®—å›¾ æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œç‚¹åŒ…å«æ•°å€¼ï¼Œè¾¹è¡¨ç¤ºè¿ç®— ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:3:1","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"å‰å‘æ¨¡å¼ Forward Mode AD ç®—æ³• éå†å›¾ï¼Œä»è¾“å…¥å¼€å§‹ï¼Œè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºï¼Œç„¶åå°†ç»“æœä¼ æ’­åˆ°åç»­èŠ‚ç‚¹ $$ \\Large \\begin{aligned} Define: \u0026 \\quad vâ€™_{1i} = \\frac{\\partial v_i}{\\partial x_1} \\ \\end{aligned} $$ then compute $v_iâ€™$ iteratively, in the forward topological order é™åˆ¶ å¦‚æœnå°kå¤§ï¼Œé‚£ä¹ˆè·‘å¾ˆå°‘çš„passå°±å¯ä»¥å¾—åˆ°æƒ³è¦çš„æ¢¯åº¦ï¼Œä½†æ˜¯äº‹å®ä¸Šæƒ…å†µç›¸å ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:3:2","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"åå‘æ¨¡å¼ Reverse Mode AD ç®—æ³• éå†å›¾ï¼Œä»è¾“å‡ºå¼€å§‹ï¼Œè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥ï¼Œç„¶åå°†ç»“æœä¼ æ’­åˆ°å‰ç»­èŠ‚ç‚¹ $$ \\Large \\begin{aligned} Define\\ adjoints: \u0026 \\quad \\bar{v}_{i} = \\frac{\\partial y}{\\partial v_i} \\ \\end{aligned} $$ then compute $\\bar{v}_i$ iteratively, in the reverse topological order æ„Ÿè§‰BFSå’ŒDFSéƒ½å‘¼ä¹‹æ¬²å‡ºäº†ï¼Œä½†æ˜¯DFSæ›´ç›´è§‚ä¸€äº›? BFSæ›´é€‚åˆè®¡ç®—å›¾çš„ç»“æ„? ğŸ¤” implementation äº‹å®ä¸Šæˆ‘ä»¬åœ¨è¿™é‡Œæ˜¯ æ‹“å±•äº†è®¡ç®—å›¾ ğŸ¤” side note: è¿™é‡Œid == identity functionï¼Œå³è¾“å…¥è¾“å‡ºç›¸åŒï¼Œåªæ˜¯ä¸ºäº†æ‰©å±•è®¡ç®—å›¾è€Œå·² ä½¿å¾—æˆ‘ä»¬å¯ä»¥è®¡ç®—ä»»æ„èŠ‚ç‚¹çš„å¯¼æ•°ï¼Œè€Œä¸ä»…ä»…æ˜¯è¾“å…¥èŠ‚ç‚¹çš„å¯¼æ•°ï¼Œå¹¶ä¸”è¾“å…¥èŠ‚ç‚¹çš„å€¼å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå¸¦è¿›å»è·‘ä¸€è¾¹å›¾å³å¯ ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:3:3","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"Reverse Mode AD vs Backpropagation Gradient of Gradient ä¼˜åŒ– GPTï¼š å³è¾¹çš„å›¾å±•ç¤ºäº†é€šè¿‡æ‰©å±•è®¡ç®—å›¾å®ç°çš„åå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†ï¼ˆReverse Mode ADï¼‰ï¼Œè€Œè¿™åœ¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­è¢«å¹¿æ³›é‡‡ç”¨ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„åå‘ä¼ æ’­ï¼ˆBackpropï¼‰æ–¹æ³•æœ‰ä¸€äº›ä¼˜åŠ¿ã€‚ ä¸»è¦åŸå› å¦‚ä¸‹ï¼š å›¾ç»“æ„çš„ä¼˜åŒ–ï¼š åœ¨åå‘ä¼ æ’­ä¸­ï¼Œåå‘è®¡ç®—æ˜¯åœ¨å‰å‘è®¡ç®—å›¾çš„åŸºç¡€ä¸Šå¤ç”¨çš„ï¼ˆå·¦å›¾ï¼‰ã€‚ä½†æ˜¯ï¼ŒReverse Mode AD ä¼šä¸ºæ¯ä¸ªæ“ä½œçš„æ¢¯åº¦è®¡ç®—ç”Ÿæˆå•ç‹¬çš„å›¾èŠ‚ç‚¹ï¼ˆå³å›¾ï¼‰ï¼Œä»è€Œæä¾›æ›´æ¸…æ™°çš„å›¾ç»“æ„ï¼Œå…è®¸æ›´çµæ´»çš„æ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–ã€‚ æ›´æ˜“äºå¤æ‚æ“ä½œçš„å¤„ç†ï¼š æ‰©å±•çš„å›¾ç»“æ„èƒ½å¤Ÿæ›´å¥½åœ°è¡¨ç¤ºå¤æ‚æ“ä½œçš„æ¢¯åº¦è®¡ç®—ï¼Œå°¤å…¶æ˜¯åœ¨ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œè¿™ç§çµæ´»æ€§ä½¿å¾—å¤„ç†å¤æ‚çš„ç¥ç»ç½‘ç»œç»“æ„æ›´åŠ é«˜æ•ˆã€‚ æ›´å¥½çš„æ¡†æ¶æ”¯æŒï¼š ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚TensorFlowã€PyTorchç­‰ï¼‰é€šå¸¸é‡‡ç”¨è¿™ç§Reverse Mode ADçš„æ–¹å¼ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒè‡ªåŠ¨å¾®åˆ†ï¼Œå°¤å…¶æ˜¯åœ¨æ›´å¤æ‚çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚åå‘ä¼ æ’­ï¼ˆBackpropï¼‰æ›´å¤šè¢«ç”¨åœ¨æ—©æœŸçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼Œå¦‚Caffeå’ŒCuda-convnetï¼Œè€Œè¿™äº›æ¡†æ¶ç›¸å¯¹å·²ç»è¿‡æ—¶ã€‚ æ€»ç»“ï¼šå³è¾¹çš„æ–¹æ³•èƒ½å¤Ÿé€šè¿‡æ„å»ºå•ç‹¬çš„è®¡ç®—å›¾èŠ‚ç‚¹ï¼Œä½¿æ¢¯åº¦è®¡ç®—æ›´çµæ´»å’Œé«˜æ•ˆï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶æ›´å€¾å‘äºä½¿ç”¨è¿™ç§æ–¹æ³•ã€‚ ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:3:4","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"æ‹“å±•åˆ°å¤šå…ƒå‡½æ•° Tensors Differentiable Programming æ¨å¹¿åˆ°æ•°æ®ç»“æ„ä¸Šé¢å» ","date":"2024-09-16","objectID":"/lec4-automatic-differentiation/:3:5","tags":null,"title":"Lec4-Automatic Differentiation","uri":"/lec4-automatic-differentiation/"},{"categories":["CMU-10-414-714"],"content":"Manual Neural Network I ","date":"2024-09-15","objectID":"/lec3-manual-neural-networks-backprop/:0:0","tags":null,"title":"Lec3-Manual Neural Networks Backprop","uri":"/lec3-manual-neural-networks-backprop/"},{"categories":["CMU-10-414-714"],"content":"å›é¡¾ä¸€ä¸‹é€šç”¨è¿‘ä¼¼å®šç† æ³¨æ„äº‹å®ä¸Šå¤šé¡¹å¼ã€Nearest Neighborç­‰éƒ½æ˜¯åŸºäºé€šç”¨è¿‘ä¼¼å®šç†çš„ã€‚ ","date":"2024-09-15","objectID":"/lec3-manual-neural-networks-backprop/:1:0","tags":null,"title":"Lec3-Manual Neural Networks Backprop","uri":"/lec3-manual-neural-networks-backprop/"},{"categories":["CMU-10-414-714"],"content":"éçº¿æ€§ReLUå‡½æ•°æ–°ç†è§£ ç”»å›¾æŠ˜çº¿ ","date":"2024-09-15","objectID":"/lec3-manual-neural-networks-backprop/:2:0","tags":null,"title":"Lec3-Manual Neural Networks Backprop","uri":"/lec3-manual-neural-networks-backprop/"},{"categories":["CMU-10-414-714"],"content":"åå‘ä¼ æ’­ no one else can do this awful math happily ğŸ˜€ so here comes the backpropagation algorithm \u0026 autograd æ½œåœ¨çš„æ¨¡å—åŒ–å’Œè‡ªåŠ¨æ±‚å¯¼ğŸ¤” ","date":"2024-09-15","objectID":"/lec3-manual-neural-networks-backprop/:3:0","tags":null,"title":"Lec3-Manual Neural Networks Backprop","uri":"/lec3-manual-neural-networks-backprop/"},{"categories":["CMU-10-414-714"],"content":"ML refresher and Softmax Regression ","date":"2024-09-15","objectID":"/lec2-ml-refresher-softmax-regression/:0:0","tags":null,"title":"Lec2-ML Refresher Softmax Regression","uri":"/lec2-ml-refresher-softmax-regression/"},{"categories":["CMU-10-414-714"],"content":"Introduction ","date":"2024-09-15","objectID":"/lec2-ml-refresher-softmax-regression/:1:0","tags":null,"title":"Lec2-ML Refresher Softmax Regression","uri":"/lec2-ml-refresher-softmax-regression/"},{"categories":["CMU-10-414-714"],"content":"Softmax Regression æ ‡è®°ä¸€ä¸‹æœ‰æ„æ€çš„æ¨å¯¼ ","date":"2024-09-15","objectID":"/lec2-ml-refresher-softmax-regression/:2:0","tags":null,"title":"Lec2-ML Refresher Softmax Regression","uri":"/lec2-ml-refresher-softmax-regression/"},{"categories":["CMU-10-414-714"],"content":"ä»¥å‰ç°å®æ‰§è¡Œæ¢¯åº¦è®¡ç®—çš„åœºæ™¯ å½“ä½œæ ‡é‡å…ˆæ±‚åå¯¼ï¼Œç„¶åæ ¹æ®sizeè€ƒè™‘è½¬ä¸è½¬ç½®å‡‘å‡ºæ¥ğŸ˜‹ ","date":"2024-09-15","objectID":"/lec2-ml-refresher-softmax-regression/:3:0","tags":null,"title":"Lec2-ML Refresher Softmax Regression","uri":"/lec2-ml-refresher-softmax-regression/"},{"categories":["Tools"],"content":"Condaé…ç½®å¤‡å¿˜å½• ","date":"2024-08-15","objectID":"/tools/conda/:0:0","tags":null,"title":"Condaé…ç½®å¤‡å¿˜å½•","uri":"/tools/conda/"},{"categories":["Tools"],"content":"å¯¼å‡ºä¸åˆ›å»ºç¯å¢ƒ ä½¿ç”¨YAMLæ–‡ä»¶åˆ›å»ºCondaç¯å¢ƒçš„æ­¥éª¤å¦‚ä¸‹ï¼š åˆ›å»ºYAMLæ–‡ä»¶ï¼šé¦–å…ˆï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªYAMLé…ç½®æ–‡ä»¶ï¼Œé€šå¸¸å‘½åä¸ºenvironment.ymlã€‚åœ¨è¯¥æ–‡ä»¶ä¸­ï¼Œæ‚¨å¯ä»¥å®šä¹‰ç¯å¢ƒçš„åç§°ã€æ‰€éœ€çš„é€šé“ä»¥åŠä¾èµ–çš„åŒ…ã€‚ä¾‹å¦‚ï¼š name: my_conda_env channels: - conda-forge dependencies: - numpy=1.19.5 - pandas=1.2.3 - scikit-learn=0.23.2 åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªåä¸ºmy_conda_envçš„ç¯å¢ƒï¼Œå¹¶æŒ‡å®šäº†è¦å®‰è£…çš„åŒ…åŠå…¶ç‰ˆæœ¬ã€‚ ä½¿ç”¨Condaåˆ›å»ºç¯å¢ƒï¼šåœ¨å‘½ä»¤è¡Œä¸­ï¼Œå¯¼èˆªåˆ°åŒ…å«environment.ymlæ–‡ä»¶çš„ç›®å½•ï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š conda env create -f environment.yml è¿™æ¡å‘½ä»¤ä¼šæ ¹æ®YAMLæ–‡ä»¶ä¸­çš„å®šä¹‰åˆ›å»ºæ–°çš„Condaç¯å¢ƒã€‚ æ¿€æ´»æ–°ç¯å¢ƒï¼šåˆ›å»ºå®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¿€æ´»æ–°ç¯å¢ƒï¼š conda activate my_conda_env ç°åœ¨ï¼Œæ‚¨å¯ä»¥åœ¨æ–°ç¯å¢ƒä¸­å®‰è£…ã€è¿è¡Œå’Œæµ‹è¯•è½¯ä»¶åŒ…ã€‚ å¯¼å‡ºç°æœ‰ç¯å¢ƒï¼šå¦‚æœæ‚¨æƒ³å°†å½“å‰ç¯å¢ƒå¯¼å‡ºä¸ºYAMLæ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š conda env export \u003e environment.yml è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«å½“å‰ç¯å¢ƒæ‰€æœ‰åŒ…åŠå…¶ç‰ˆæœ¬ä¿¡æ¯çš„YAMLæ–‡ä»¶ï¼Œæ–¹ä¾¿åœ¨å…¶ä»–è®¡ç®—æœºä¸Šé‡å»ºç›¸åŒçš„ç¯å¢ƒ ","date":"2024-08-15","objectID":"/tools/conda/:1:0","tags":null,"title":"Condaé…ç½®å¤‡å¿˜å½•","uri":"/tools/conda/"},{"categories":["Tools"],"content":"å•çº¯åˆ›å»ºã€åˆ‡æ¢ã€åˆ é™¤ç¯å¢ƒ conda create -n YOUR_ENV_NAME python=3.8 conda activate YOUR_ENV_NAME conda remove -n YOUR_ENV_NAME --all æŸ¥çœ‹ç¯å¢ƒ conda info -e ","date":"2024-08-15","objectID":"/tools/conda/:2:0","tags":null,"title":"Condaé…ç½®å¤‡å¿˜å½•","uri":"/tools/conda/"},{"categories":["Tools"],"content":"pip install ç¬¬ä¸‰æ–¹åº“ ä»¥topologylayer==0.0.0ä¸ºä¾‹ï¼Œç›´æ¥pipå°†ä¼šæŠ¥é”™ï¼Œå› ä¸ºè¯¥åº“æ²¡æœ‰å‘å¸ƒåˆ°condaä¸­ git clone https://github.com/bruel-gabrielsson/TopologyLayer.git cd TopologyLayer # æ³¨æ„æ‰¾åˆ°setup.pyæ–‡ä»¶ pip install -e . # pip install . ","date":"2024-08-15","objectID":"/tools/conda/:3:0","tags":null,"title":"Condaé…ç½®å¤‡å¿˜å½•","uri":"/tools/conda/"},{"categories":["NNDL"],"content":"DDPM â…  ","date":"2024-08-14","objectID":"/papers/ddpm/:0:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"diffusion modelå¦‚ä½•è¿ä½œ? sampleä¸€å¼ æ‚è®¯å›¾ æ”¾è¿›å»denoiseæ¨¡å— åå¤è¿­ä»£ï¼Œå¾—åˆ°æ›´æ¸…æ™°çš„å›¾ç‰‡ step xxx â†’ step 1, reverse process ","date":"2024-08-14","objectID":"/papers/ddpm/:1:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"denoiseæ¨¡å— åå¤åº”ç”¨åŒä¸€ä¸ªæ¨¡å— åƒå›¾ç‰‡ åƒå™ªå£°ä¸¥é‡ç¨‹åº¦ï¼ˆæ•°å­—ï¼‰ Denoiseå†…éƒ¨ ","date":"2024-08-14","objectID":"/papers/ddpm/:2:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"å¦‚ä½•è®­ç»ƒnoise predictor? Forward/Diffusion process è¾“å…¥ï¼šæ‚è®¯å›¾ and å™ªå£°å¼ºåº¦ ","date":"2024-08-14","objectID":"/papers/ddpm/:3:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"Text to Image? æœ‰å¾ˆå¤šè¯­è¨€å’Œå›¾ç‰‡æˆå¯¹å‡ºç°çš„è®­ç»ƒèµ„æ–™ æ­¤æ—¶å¢åŠ ä¸€ä¸ªæ–‡æœ¬è¾“å…¥å³å¯ DDPM â…¡ ","date":"2024-08-14","objectID":"/papers/ddpm/:4:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"text to imageçš„framework ä¸‰è€…åˆ†å¼€è®­ç»ƒ ","date":"2024-08-14","objectID":"/papers/ddpm/:5:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"text encoder \u0026 generation model ç»“è®ºï¼šæ–‡å­—encoderå¤§ä¸€ç‚¹æ¯”è¾ƒå¥½, diffusion modelæ²¡æœ‰å¤ªå¤§å½±å“ side note: FID, å³feature-wise inception distance, è¡¡é‡ä¸¤ä¸ªGaussianåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ CLIP, Contrastive Language-Image Pre-training, è¡¡é‡ä¸¤ä¸ªæ–‡æœ¬å’Œå›¾ç‰‡ä¹‹é—´çš„å·®å¼‚ ","date":"2024-08-14","objectID":"/papers/ddpm/:6:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"decoder ä¸éœ€è¦æ–‡å­—èµ„æ–™è®­ç»ƒ ä¸­é—´äº§ç‰©ä¸º å°å›¾ $\\rightarrow$ åŸå›¾ ä¸­é—´äº§ç‰©ä¸º Latent Representation $\\rightarrow$ åŸå›¾, auto-encoder ","date":"2024-08-14","objectID":"/papers/ddpm/:7:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"äº‹å®ä¸Šçš„å™ªéŸ³æ·»åŠ  diffusion modelçš„æ•°å­¦åŸç†â…  ","date":"2024-08-14","objectID":"/papers/ddpm/:8:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"VAE vs. DDPM VAEæ˜¯â€œå˜åˆ†è‡ªç¼–ç å™¨â€ï¼ˆVariational Auto Encoderï¼‰çš„ç¼©å†™, VAEæ¨¡å‹åŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤ä¸ªéƒ¨åˆ†ï¼šç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥æ•°æ®å‹ç¼©æˆä¸€ä¸ªæ½œåœ¨å‘é‡ï¼Œè§£ç å™¨åˆ™ä»è¿™ä¸ªæ½œåœ¨å‘é‡é‡æ„åŸå§‹æ•°æ® Diffusion çš„ noise ä¸éœ€è¦learn ","date":"2024-08-14","objectID":"/papers/ddpm/:9:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"ç®—æ³•åˆ†æ Training ä¸æ˜¯ä¸€ç‚¹ä¸€ç‚¹, è€Œæ˜¯ä¸€æ¬¡è§£å†³ Inference diffusion modelçš„æ•°å­¦åŸç†â…¡ ","date":"2024-08-14","objectID":"/papers/ddpm/:10:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"æ˜ åƒç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šçš„ç›®æ ‡ åŠ äº†æ–‡å­—ç›¸å½“äºåŠ äº†ä¸€ä¸ªæ¡ä»¶condition, ä»¥ä¸‹è®¨è®ºå¿½ç•¥ä¹‹ MLEæ¥è¡¡é‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ æ‰¾åˆ°$\\theta^*$ä½¿å¾—$p_{\\theta}(x)$å’Œ$p_{data}(x)$ä¹‹é—´çš„KLæ•£åº¦æœ€å° side note: $\\approx$ æˆç«‹æ˜¯å› ä¸ºæ•°æ®é‡å¾ˆå¤§? ","date":"2024-08-14","objectID":"/papers/ddpm/:11:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"VAEçš„æ•°å­¦åŸç† def $P_{\\theta}(x|z)$ asm $G(Z)$ æ˜¯ä¸€ä¸ªGaussianåˆ†å¸ƒçš„ $\\mu$ side note: $f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ (æ³¨æ„$\\theta$è¢«çœç•¥äº†) æ ¹æ®MLE, éœ€è¦ä½¿å¾— $logP_{\\theta}(x)$ å¤§ ","date":"2024-08-14","objectID":"/papers/ddpm/:12:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"DDPMçš„æ•°å­¦åŸç† side note: $P_{\\theta}(x_{t-1}|x_{t}) \\propto exp(-||G(x_{t}) - x_{t-1}||_{2})$ $x_1:x_T$ æ˜¯ä» $x_1$ åˆ° $x_T$ çš„å˜é‡ç§¯åˆ† $P_{\\theta}(x)$ å’Œ $P(x)$ åŒºåˆ«! æœ‰ $\\theta$ æ„å‘³ç€é€šè¿‡ $\\theta$ æ±‚å¾— è®¡ç®— $q(x_t|x_{t-1})$ é€’æ¨å’Œå½’çº³æ³•, æ³¨æ„iidçš„Gaussianåˆ†å¸ƒçº¿æ€§ç»„åˆ, ç”¨ç›¸åŒæ–¹å·®å’Œå‡å€¼ä»£æ›¿ å›åˆ° $P_{\\theta}(x)$ ä¸‹ç•Œ, çœç•¥äº† $\\theta$ ? æ”¾å¤§æ¥çœ‹ ç”±å¯†åº¦å‡½æ•°æ‹†å¼€æ¥ç¡¬æ•´, æ‰€ä»¥æœ‰ side note: $\\bar \\alpha_t$ å’Œ $\\alpha_t$ ä¹‹é—´æœ‰ä¹˜æ³•å…³ç³»çº¦æŸ!æ³¨æ„å‰é¢çš„æ¢å…ƒ! mean: å–å†³äº $x_t$ å’Œ $x_{0}$ , ç¬¦åˆç›´è§‰ å›åˆ°ä¼˜åŒ–ç›®æ ‡ æœ‰è§£æè§£, ä½†æ˜¯æ³¨æ„åˆ°ä¸¤ä¸ªGaussianåˆ†å¸ƒçš„ç‰¹ç‚¹, ç›´æ¥é è¿‘meanå³å¯ $G(x_t) \\rightarrow LHS$, æ³¨æ„åˆ° $x_t = \\sqrt{\\bar \\alpha_t}x_0 + \\sqrt{1-\\bar \\alpha_t}\\epsilon$, å¹¶ä¸”æ²¡æœ‰ç”¨åˆ° $x_{t-1}$ éœ€è¦learnçš„æ˜¯ $\\epsilon$ æ³¨æ„ $\\alpha_t$ åºåˆ—æ²¡æœ‰æ·±å…¥æ¢è®¨! diffusion modelçš„æ•°å­¦åŸç†â…¢ ","date":"2024-08-14","objectID":"/papers/ddpm/:13:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"æ¥ä¸Šå»DDPMçš„æ•°å­¦åŸç†â€”â€”ä¸ºä»€ä¹ˆè¦åŠ å…¥éšæœºæ€§? è¾“å‡ºçš„ä¸å…¨æ˜¯mean, è¿˜å åŠ äº† $\\sigma_t \\mathbf{z}$ å¯èƒ½çš„åŸå› ï¼š ç±»æ¯”LLM å‡ ç‡å¤§ â‰  æ•ˆæœå¥½ ç±»æ¯”è¯­éŸ³åˆæˆçš„dropoutå½¢æˆsampling ","date":"2024-08-14","objectID":"/papers/ddpm/:14:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"diffusion modelçš„æˆåŠŸå…³é”® ","date":"2024-08-14","objectID":"/papers/ddpm/:15:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"auto-regressiveçš„æ€æƒ³ non-auto-regressive + auto-regressive ","date":"2024-08-14","objectID":"/papers/ddpm/:15:1","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["NNDL"],"content":"å…¶ä»–åº”ç”¨ è¯­éŸ³åˆæˆ Text Noise on latent space mask-predict, ","date":"2024-08-14","objectID":"/papers/ddpm/:16:0","tags":null,"title":"DDPM","uri":"/papers/ddpm/"},{"categories":["CS186"],"content":"distributed == parallel shared nothing architecture ","date":"2024-08-14","objectID":"/databasel19/:0:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"Intro ","date":"2024-08-14","objectID":"/databasel19/:1:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"Distributed Locking each nodes has lock table locally, can manage the pages/tuples easily, but when it comes to Table, there should be a global lock table ï¼ˆor distributed lock tablesï¼‰and a coordinator to manage the access to the table. ","date":"2024-08-14","objectID":"/databasel19/:2:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"Distributed Deadlocks Detection åˆå¹¶å…¨å±€waits ","date":"2024-08-14","objectID":"/databasel19/:3:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"Distributed Commit å…¨å±€æŠ•ç¥¨ ","date":"2024-08-14","objectID":"/databasel19/:4:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"2PC ","date":"2024-08-14","objectID":"/databasel19/:4:1","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"The Recovery Processes crash situations ","date":"2024-08-14","objectID":"/databasel19/:5:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"2PC, Locking and Availability 2PC + Strict 2PL locking what if a node is down? some locks can still be held by other nodesâ€¦â€¦ ","date":"2024-08-14","objectID":"/databasel19/:6:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"Summary ","date":"2024-08-14","objectID":"/databasel19/:7:0","tags":null,"title":"CS186-L19:  Distributed Transactions","uri":"/databasel19/"},{"categories":["CS186"],"content":"Scaling Relational Databases isnâ€™t always the best option including partitioning and replication BUT, consistency is hard to enforce! ","date":"2024-08-14","objectID":"/databasel20/:1:0","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"Taxonomy of NoSQL Data Models ","date":"2024-08-14","objectID":"/databasel20/:2:0","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"Key-Value Stores Map\u003cKey, Value\u003e get/put Distribution / Partitioning, just using hash function if no replication, key k is stored on $h(k)$ node if multi-way replication, key k is stored on $h_i(k), i=1,2,â€¦,n$ nodes ","date":"2024-08-14","objectID":"/databasel20/:2:1","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"Extensible Record Stores the idea is that do not use a simple key to lookup ğŸ¤” ","date":"2024-08-14","objectID":"/databasel20/:2:2","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"Document Stores JSON Documents using JSON as example do not store replicated key! JSON is a Tree ğŸªµ, Self-describing ğŸ’¬, and Flexible ğŸ”¥ can store Json in RDBMS SELECT # FROM people WHERE person @\u003e '{\"name\": \"John Doe\", \"age\": 30}'; mapping between JSON and Relational Data Relational Data Model ===\u003e JSON Document easy, note that replicated key can be handled by using a array [ ] JSON Document ===\u003e Relational Data Model using NULL to represent missing values nested or replicated data? hard to handle! multi-tables may help ğŸ¤” ","date":"2024-08-14","objectID":"/databasel20/:2:3","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"Introduction to MongoDB åŸºæœ¬è¯­æ³• ","date":"2024-08-14","objectID":"/databasel20/:3:0","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"select and find db.collection.find(\u003cpredicate\u003e, optional\u003cprojection\u003e) db.inventory.find({}) // return all documents ","date":"2024-08-14","objectID":"/databasel20/:3:1","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"Limit and sort ","date":"2024-08-14","objectID":"/databasel20/:3:2","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"MQL Aggregations and Updates ","date":"2024-08-14","objectID":"/databasel20/:4:0","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"unwind ","date":"2024-08-14","objectID":"/databasel20/:4:1","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"update ","date":"2024-08-14","objectID":"/databasel20/:4:2","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"MongoDB internals ","date":"2024-08-14","objectID":"/databasel20/:5:0","tags":null,"title":"CS186-L20: NoSQL","uri":"/databasel20/"},{"categories":["CS186"],"content":"Motivation only scaling up relational databases is challenging :s ","date":"2024-08-14","objectID":"/databasel21/:1:0","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"MapReduce Data and Programming Model Target ","date":"2024-08-14","objectID":"/databasel21/:2:0","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Map phase map function will not keep the state of the intermediate results, so it can be parallelized easily ","date":"2024-08-14","objectID":"/databasel21/:2:1","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Reduce phase for example, wanna count the number of occurrences of each word in the input data, we can use the reduce function to sum up the values of the same key ","date":"2024-08-14","objectID":"/databasel21/:2:2","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Implementation of MapReduce ","date":"2024-08-14","objectID":"/databasel21/:3:0","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"fault tolerance by writing intermediate results to diskâ€¦ mappers can write their output to local disk reducers can read the output of mappers from local disk and combine them, if the reduce task is restarted, the reduce task is restarted on another server ","date":"2024-08-14","objectID":"/databasel21/:3:1","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"implementation how to handle the stragglers? ","date":"2024-08-14","objectID":"/databasel21/:3:2","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Implementing Relational Operators ","date":"2024-08-14","objectID":"/databasel21/:4:0","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Introduction to Spark why MR sucks? hard to write more complex queries slow for writing all intermediate results to disk ","date":"2024-08-14","objectID":"/databasel21/:5:0","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Programming in Spark collections in spark JavaSparkContext s = SparkSession.builder().appName(\"MyApp\").getOrCreate(); JavaRDD\u003cString\u003e lines = s.read().textFile(\"input.txt\"); JavaRDD\u003cString\u003e errors = lines.filter(line -\u003e line.contains(\"error\")); // lazy errors.collect() // eager similar steps in spark and MR ","date":"2024-08-14","objectID":"/databasel21/:6:0","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Persistence API in Java ","date":"2024-08-14","objectID":"/databasel21/:6:1","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Spark 2.0 has DataFrame API ğŸ˜² and have Datasets API ğŸ˜² like DATA100 python! ","date":"2024-08-14","objectID":"/databasel21/:7:0","tags":null,"title":"CS186-L21: MapReduce and Spark","uri":"/databasel21/"},{"categories":["CS186"],"content":"Functional Dependencies big picture ","date":"2024-08-14","objectID":"/databasel16/:1:0","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Def X -\u003e Y means X determines Y, X and Y can be a single column or multiple columns F+ means that to be the set of all FDs that are implied by F ","date":"2024-08-14","objectID":"/databasel16/:1:1","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"terminology ","date":"2024-08-14","objectID":"/databasel16/:1:2","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Anomalies å¯ä»¥ç”¨FDåˆ†è§£relationä»è€Œé¿å…å†—ä½™ ","date":"2024-08-14","objectID":"/databasel16/:2:0","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Armstrongs Axioms ","date":"2024-08-14","objectID":"/databasel16/:3:0","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Attribute Closure wanna check if X-\u003eY is in F+ ","date":"2024-08-14","objectID":"/databasel16/:4:0","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"BCNF and other Normal Forms ","date":"2024-08-14","objectID":"/databasel16/:5:0","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Basic Normal Form NF is a def of data model! ","date":"2024-08-14","objectID":"/databasel16/:5:1","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Boyce-Codd Normal Form ","date":"2024-08-14","objectID":"/databasel16/:5:2","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Lossless Join Decompositions Def: decomposition wonâ€™t create new attributes, and will cover the original attributes (ä¸æ˜¯å®Œå…¨æ— é‡å åˆ†å‰²) ","date":"2024-08-14","objectID":"/databasel16/:6:0","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Problems with Decompositions can loss info and unable to reconstruct the original data do not loss data actually, in fact, we gain some dirty data Dependency check may require joins some queries may be more expensive, since join is required ","date":"2024-08-14","objectID":"/databasel16/:6:1","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Lossless Join Decompositions å®šä¹‰ å®šç† ","date":"2024-08-14","objectID":"/databasel16/:6:2","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Dependency Preservation and BCNF Decomposition Def: Projection of set of FDs F: Def: Dependency Preserving Decomposition ","date":"2024-08-14","objectID":"/databasel16/:7:0","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"BCNF Decomposition æ²¡æœ‰å¬æ‡‚ ä½†æ˜¯dependencyæ²¡æœ‰ä¿ç•™ æ‰€ä»¥BCNFå¯ä»¥losslessï¼Œä½†æ˜¯ä¸ä¸€å®šä¿ç•™æ‰€æœ‰çš„dependency ","date":"2024-08-14","objectID":"/databasel16/:7:1","tags":null,"title":"CS186-L16: DB Design: FDs and Normalization","uri":"/databasel16/"},{"categories":["CS186"],"content":"Need for Atomicity and Durability, SQL support for Transactions ","date":"2024-08-14","objectID":"/databasel17/:1:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Strawman Solution No Steal/Force policy seem like no a good choice for recovery not scalable in buffer if crash in 2a, inconsistencies will occur ","date":"2024-08-14","objectID":"/databasel17/:2:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"STEAL / NO FORCE, UNDO and REDO ","date":"2024-08-14","objectID":"/databasel17/:3:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"STEAL/NO FORCE no force: problem: sys crash before dirty page of a committed transaction is written to disk solution: flush as little as possible, in a convenient space, prior to commit. allows REDOing modifications STEAL: must remember the old value of flushed pages to support undo ","date":"2024-08-14","objectID":"/databasel17/:3:1","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"pattern ","date":"2024-08-14","objectID":"/databasel17/:3:2","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Intro to Write-Ahead Logging (WAL) Log: a ordered list of log records to allow redo/undo log records: \u003cXID, pageID, offset, length, old data, new data\u003e and other info Write-Ahead Logging (WAL): force the log record before the data page is written to disk force all log records before a transaction is committed #1 with UNDO guarantee Atomicity and #2 with REDO guarantee Durability ===\u003e steal/no force implementation å¯¹äºæ¯ä¸ªlog recordï¼Œæœ‰ä¸€ä¸ªå¯¹åº”çš„Log Sequence Number (LSN)æ¥æ ‡è¯†å®ƒåœ¨æ—¥å¿—ä¸­çš„ä½ç½®ï¼Œæˆ‘ä»¬å¯¹æœ€è¿‘ï¼ˆlatelyï¼‰çš„LSNä»¬æ„Ÿå…´è¶£ï¼ŒflushedLSN, pageLSNç­‰ç­‰ ","date":"2024-08-14","objectID":"/databasel17/:4:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Undo logging Rule: å’ŒWALæœ‰ç‚¹ä¸ä¸€æ ·ï¼Œæ³¨æ„U2ï¼ŒCOMMITæ”¾åœ¨æœ€åï¼ presudo code: ","date":"2024-08-14","objectID":"/databasel17/:5:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Redo logging No steal/no force from beginning to end, redo all log records that are committed incomplete? do nothing! ä¸¤è€…å¯¹æ¯” ","date":"2024-08-14","objectID":"/databasel17/:6:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"ARIES logging Log records format belike: æ³¨æ„æœ‰è®¸å¤šä¸åŒçš„log recordç§ç±»ï¼Œæ„æˆä¸Šæœ‰ä¸ä¸€æ ·çš„åœ°æ–¹ prevLSN ===\u003e linkedlist dirty page ===\u003e as long as it is in memory, it is dirty ğŸ˜€ ç‰©ç†ç©ºé—´çš„æŠ½è±¡åˆ†å¸ƒ ","date":"2024-08-14","objectID":"/databasel17/:7:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"ARIES and Checkpointing Checkpoint: idea: save the DB state periodically to disk, so that we donâ€™t need to always process entire log records during recovery During a checkpoint: stop accepting new transactions wait for all active transactions to commit or abort flush log to disk flush dirty pages to disk write a checkpoint record to the log, flush log again At this point, changes by committed transactions are written to disk, and aborted transactions are rolled back. Fuzzy Checkpoint: save state of all txns and pages status some txns can be running during checkpoint and dirty pages not flushed yet data structure and idea: write-ahead log ğŸ¤” ","date":"2024-08-14","objectID":"/databasel17/:8:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"ARIES logging during normal execution, commit and abort ","date":"2024-08-14","objectID":"/databasel17/:9:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Normal Execution åŸºæœ¬ä¸Šå°±æ˜¯æŠŠä¸Šè¿°çš„é“¾è¡¨å®ç°ä¸€éï¼Œæ³¨æ„(WAL)æ¦‚å¿µå’Œå®šä¹‰å³å¯ ","date":"2024-08-14","objectID":"/databasel17/:9:1","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Commit and Abort åŒæ ·æ³¨æ„ç®¡ç†å››ä¸ªåŒºåŸŸï¼šTransaction Table, Log, Buffer Pool, and Dirty Page Table æ¯”è¾ƒç»å…¸çš„example ","date":"2024-08-14","objectID":"/databasel17/:9:2","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"ARIES Recovery, Overview and Analysis, Redo and Undo Phases ","date":"2024-08-14","objectID":"/databasel17/:10:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Analysis Phase: scan forward from the last checkpoint, END record? rm Xact form Xact table, do not care anymore UPDATE record? if its page P not in DPT, add it to DPT, set its recLSN = LSN !END record? if not in X table, add it to X table set lastLSN = LSN change Xâ€™s state to COMMITTED if see a commit record, ABORTED if see an abort record at the end of Analysis Phase, for any X in X table in COMMITTED state, write a corresponding END record to the log record and rm X from X table now X table can tell which X were active at time of crash change status of running X to ABORTED and write an ABORT record to the log DPT says which dirty pages might not have been flushed to disk yet ","date":"2024-08-14","objectID":"/databasel17/:10:1","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Redo Phase: scan forward from the smallest recLSN in DPT, For each update log record or CLR with a given LSN, REDO the action unless: affected page is not in DPT affected page is in DPT, but has recLSN \u003e LSN pageLSN in DB \u003e= LSN to REDO an action: reapply logged action set pageLSN in DB to LSN. NO additional logging, no forcing ","date":"2024-08-14","objectID":"/databasel17/:10:2","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Undo Phase: scan backward from the CRASH point, simple solution do at once toUndo = {lastLSNs of all X in X table} while !toUndo.isEmpty(): thisLR = toUndo.find_and_remove_largest_LSN() if thisLR.type == CLR: if thisLR.undoNextLSN != NULL: toUndo.add(thisLR.undoNextLSN) else: write an END record for thisLR.xid in the log else: if thisLR.type == UPDATE: write a CLR record for the undo in the log undo the update in the database if this.LR.type != NULL: toUndo.add(thisLR.undoPrevLSN) elif thisLR.type == NULL: write an END record for thisLR.xid in the log ","date":"2024-08-14","objectID":"/databasel17/:10:3","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"FAQ ","date":"2024-08-14","objectID":"/databasel17/:11:0","tags":null,"title":"CS186-L17: Recovery","uri":"/databasel17/"},{"categories":["CS186"],"content":"Intro to Parallelism ","date":"2024-08-14","objectID":"/databasel18/:1:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"Architectures and Software Structures we will focus on the shared-nothing here ğŸ˜‹ ","date":"2024-08-14","objectID":"/databasel18/:2:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"Kinds of Query Parallelism side note: intra: single inter: multiple at the same level ","date":"2024-08-14","objectID":"/databasel18/:3:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"Parallel Data Acceess ","date":"2024-08-14","objectID":"/databasel18/:4:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"Data Partitioning across Machines Round robin means that each machine haves the same shuffled data ","date":"2024-08-14","objectID":"/databasel18/:4:1","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"parallel scans scan and merge $\\sigma_p$ : an operator that skip entire sites that have no matching tuples in range or hash partitioning ","date":"2024-08-14","objectID":"/databasel18/:4:2","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"lookup by key if data partitioned on function of key, then Route lookup only to the relevant nodes otherwise, broadcast lookup to all nodes ","date":"2024-08-14","objectID":"/databasel18/:4:3","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"insert if on function of key, insert only to the relevant nodes else insert to any nodes insert an unique key seems to be same ","date":"2024-08-14","objectID":"/databasel18/:4:4","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"parallel hash join naive hash join grace hash join Pass one is like hashing above, but do it 2xâ€“ once for each relation being joined Pass two is local grace hash join per node ","date":"2024-08-14","objectID":"/databasel18/:4:5","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"sort-merge join å›åˆ°å‡åˆ†é—®é¢˜äº† ç„¶åå’Œä¸Šé¢ä¸€æ ·è¯»å–åˆ†é…ä¸¤æ¬¡for join ","date":"2024-08-14","objectID":"/databasel18/:5:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"parallel aggregation/grouping naive group by: ","date":"2024-08-14","objectID":"/databasel18/:6:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"Symmetric Hash Joins sort and hash can break the pipelineâ€¦â€¦ ","date":"2024-08-14","objectID":"/databasel18/:7:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"one-sided and Broadcast Joins ","date":"2024-08-14","objectID":"/databasel18/:8:0","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"one-sided joins one is sorted/hashed ","date":"2024-08-14","objectID":"/databasel18/:8:1","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":"broadcast joins one is small ","date":"2024-08-14","objectID":"/databasel18/:8:2","tags":null,"title":"CS186-L18: Parallel Query Processing","uri":"/databasel18/"},{"categories":["CS186"],"content":" ğŸ‰ ","date":"2024-08-14","objectID":"/databasel13/:0:0","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Intro transactionâ€™s principle ACID ","date":"2024-08-14","objectID":"/databasel13/:1:0","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Isolation (Concurrency) however, do not consider serial execution ğŸ˜… ","date":"2024-08-14","objectID":"/databasel13/:1:1","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Atomicity and Durability ","date":"2024-08-14","objectID":"/databasel13/:1:2","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Consistency ","date":"2024-08-14","objectID":"/databasel13/:1:3","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Concurrency Control åŸºæœ¬ç¬¦å·è¡¨è¾¾ åºåˆ—ç­‰ä»·æ€§ï¼š $Def1:$ Serial Schedule each transaction executes in a serial order, one after the other, without any intervening $Def2:$ schedules Equivalent involve same transaction each transactionâ€™s actions are the same order both transactions have the same effect on the databaseâ€™s final state $Def3:$ Serializable if a schedule is serializable, then it is equivalent to some serial schedule ","date":"2024-08-14","objectID":"/databasel13/:2:0","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Conflict Serializability conflict operations? Intuitive Understanding of Conflict Serializable Conflict Dependency Graph ","date":"2024-08-14","objectID":"/databasel13/:2:1","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"View Serializability ","date":"2024-08-14","objectID":"/databasel13/:2:2","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Conclusion Neither definition allows all schedules that are actually serializable. because they can not check the meaning of the operation ğŸ˜ˆ ","date":"2024-08-14","objectID":"/databasel13/:3:0","tags":null,"title":"CS186-L13: Transactions \u0026 Concurrency I","uri":"/databasel13/"},{"categories":["CS186"],"content":"Two Phase Locking (2PL) ","date":"2024-08-14","objectID":"/databasel14/:1:0","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"Strict 2PL same as 2PL, but with stricter definition of release locks at once pink area is the Strict 2PL ","date":"2024-08-14","objectID":"/databasel14/:2:0","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"Lock Management there is a lock manager, which maintains a hash table keyed on names of objects being locked ","date":"2024-08-14","objectID":"/databasel14/:3:0","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"Deadlocks ğŸ¤” ","date":"2024-08-14","objectID":"/databasel14/:4:0","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"why happen? side note: prioritize upgrades can avoid #2 unlike the OS which can have a fixed order of required sourcesâ€¦â€¦ ","date":"2024-08-14","objectID":"/databasel14/:4:1","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"avoiding deadlocks timeout first, TIMEOUT is a not so bad idea ğŸ¤” two patterns thenâ€¦ why can work out? young at best wait, at worst die order by age, so no circle if re-start or â€œhurt/woundâ€, time stamp wonâ€™t change deadlock detection try to construct a graph of all the locks and wait-for relationships ğŸ¤” as long as there is a cycle, deadlock happens! exp: most cycle is 2-3 nodes ","date":"2024-08-14","objectID":"/databasel14/:4:2","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"Lock Granularity at which level should i lock? tuples? pages? tables? or database? multi-granularity locking is helpful! æ•°æ®åº“é”çš„å…¼å®¹æ€§çŸ©é˜µï¼ˆLock Compatibility Matrixï¼‰ æ•°æ®åº“é”çš„å…¼å®¹æ€§çŸ©é˜µç”¨äºå®šä¹‰ä¸åŒç±»å‹çš„é”åœ¨æ•°æ®åº“ç³»ç»Ÿä¸­èƒ½å¦åŒæ—¶å…±å­˜ï¼Œé¿å…åœ¨å¤šç”¨æˆ·å¹¶å‘ç¯å¢ƒä¸‹äº§ç”Ÿæ•°æ®ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™ä¸ªçŸ©é˜µå±•ç¤ºäº†å½“ä¸€ä¸ªäº‹åŠ¡æŒæœ‰æŸç§ç±»å‹çš„é”æ—¶ï¼Œå¦ä¸€ä¸ªäº‹åŠ¡èƒ½å¦åœ¨åŒä¸€èµ„æºä¸Šè·å¾—å¦ä¸€ç§ç±»å‹çš„é”ã€‚ é”ç±»å‹çš„åŸºæœ¬å«ä¹‰ï¼š IS (Intent Share)ï¼š æ„å›¾åœ¨æ›´ç»†ç²’åº¦ï¼ˆå¦‚è®°å½•çº§åˆ«ï¼‰ä¸Šè·å–å…±äº«é”ï¼ˆSé”ï¼‰ã€‚ IX (Intent Exclusive)ï¼š æ„å›¾åœ¨æ›´ç»†ç²’åº¦ä¸Šè·å–æ’ä»–é”ï¼ˆXé”ï¼‰ã€‚ S (Share)ï¼š å…±äº«é”ï¼Œå…è®¸å¤šä¸ªäº‹åŠ¡åŒæ—¶è¯»å–æ•°æ®ï¼Œä½†ä¸å…è®¸ä¿®æ”¹ã€‚ X (Exclusive)ï¼š æ’ä»–é”ï¼Œç‹¬å é”ï¼Œç¦æ­¢å…¶ä»–äº‹åŠ¡è¯»å–æˆ–ä¿®æ”¹æ•°æ®ã€‚ SIX (Shared Intent Exclusive)ï¼š åŒæ—¶æŒæœ‰Sé”å’ŒIXé”ã€‚äº‹åŠ¡å¸Œæœ›å¯¹è¾ƒç²—ç²’åº¦çš„å¯¹è±¡è¿›è¡Œå…±äº«è®¿é—®ï¼ŒåŒæ—¶åœ¨æ›´ç»†ç²’åº¦çš„å¯¹è±¡ä¸Šè¿›è¡Œæ’ä»–ä¿®æ”¹ã€‚ 1. IS é”çš„å…¼å®¹æ€§ï¼š IS vs IS: trueï¼Œå¤šä¸ªäº‹åŠ¡å¯ä»¥åœ¨åŒä¸€èµ„æºä¸Šæ”¾ç½®ISé”ï¼Œè¡¨ç¤ºå®ƒä»¬éƒ½æœ‰æ„åœ¨æ›´ç»†ç²’åº¦ä¸Šè·å¾—Sé”ã€‚ IS vs IX: trueï¼ŒISå’ŒIXé”å¯ä»¥åŒæ—¶å­˜åœ¨ï¼Œæ„å‘³ç€ä¸€ä¸ªäº‹åŠ¡æ„å›¾è·å–å…±äº«é”ï¼Œè€Œå¦ä¸€ä¸ªäº‹åŠ¡æ„å›¾è·å–æ’ä»–é”ï¼Œä¸¤è€…åœ¨ç²—ç²’åº¦ä¸Šå¹¶ä¸å†²çªã€‚ IS vs S: trueï¼ŒISé”ä¸Sé”å…¼å®¹ï¼Œä¸€ä¸ªäº‹åŠ¡å¯ä»¥æœ‰æ„è·å–æ›´ç»†ç²’åº¦çš„å…±äº«é”ï¼Œè€Œå¦ä¸€ä¸ªäº‹åŠ¡åœ¨å½“å‰ç²’åº¦ä¸ŠæŒæœ‰å…±äº«é”ã€‚ IS vs SIX: trueï¼ŒSIXé”æœ¬è´¨ä¸Šæ˜¯Sé”å’ŒIXé”çš„ç»„åˆï¼ŒISä¸è¿™ä¸¤ç§é”éƒ½å…¼å®¹ï¼Œå› æ­¤ISä¸SIXå…¼å®¹ã€‚ IS vs X: falseï¼ŒISé”ä¸Xé”ä¸å…¼å®¹ï¼ŒXé”æ˜¯æ’ä»–é”ï¼Œä¸å…è®¸å…¶ä»–ä»»ä½•é”å…±å­˜ã€‚ 2. IX é”çš„å…¼å®¹æ€§ï¼š IX vs IS: trueï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ IX vs IX: trueï¼Œå¤šä¸ªäº‹åŠ¡å¯ä»¥åŒæ—¶è¡¨ç¤ºå®ƒä»¬æœ‰æ„åœ¨æ›´ç»†ç²’åº¦ä¸Šè·å–æ’ä»–é”ã€‚ IX vs S: falseï¼ŒIXé”ä¸Sé”ä¸å…¼å®¹ï¼Œæ’ä»–é”æ„å›¾ä¸å…±äº«é”å‘ç”Ÿå†²çªã€‚ IX vs SIX: falseï¼ŒSIXé”åŒ…å«å…±äº«é”éƒ¨åˆ†ï¼Œä¸IXé”ä¸å…¼å®¹ã€‚ IX vs X: falseï¼Œæ’ä»–é”ä¹‹é—´æ— æ³•å…±å­˜ã€‚ 3. S é”çš„å…¼å®¹æ€§ï¼š S vs IS: trueï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ S vs IX: falseï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ S vs S: trueï¼Œå¤šä¸ªäº‹åŠ¡å¯ä»¥å…±äº«è¯»å–èµ„æºï¼Œå› æ­¤Sé”å½¼æ­¤å…¼å®¹ã€‚ S vs SIX: falseï¼Œå› ä¸ºSIXé”ä¸­åŒ…å«IXé”éƒ¨åˆ†ï¼Œè€ŒSä¸IXä¸å…¼å®¹ã€‚ S vs X: falseï¼Œå…±äº«é”ä¸æ’ä»–é”ä¸å…¼å®¹ã€‚ 4. SIX é”çš„å…¼å®¹æ€§ï¼š SIX vs IS: trueï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ SIX vs IX: falseï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ SIX vs S: falseï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ SIX vs SIX: falseï¼Œä¸¤ä¸ªSIXé”ä¸å…¼å®¹ï¼Œå› ä¸ºSIXé”åŒ…å«æ’ä»–æ„å›¾ã€‚ SIX vs X: falseï¼Œæ’ä»–é”ä¸SIXé”ä¸å…¼å®¹ã€‚ 5. X é”çš„å…¼å®¹æ€§ï¼š X vs IS: falseï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ X vs IX: falseï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ X vs S: falseï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ X vs SIX: falseï¼Œè§ä¸Šæ–‡è§£é‡Šã€‚ X vs X: falseï¼Œä¸¤ä¸ªæ’ä»–é”ä¸èƒ½å…±å­˜ã€‚ ","date":"2024-08-14","objectID":"/databasel14/:5:0","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"Summary ","date":"2024-08-14","objectID":"/databasel14/:6:0","tags":null,"title":"CS186-L14: Transactions \u0026 Concurrency II","uri":"/databasel14/"},{"categories":["CS186"],"content":"Overview how to design a database instead of DBMS! ğŸ¤” this class mainly focuses on the conceptual design ","date":"2024-08-14","objectID":"/databasel15/:1:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Data Models and Relational Levels of Abstraction ","date":"2024-08-14","objectID":"/databasel15/:2:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"def in Data model ","date":"2024-08-14","objectID":"/databasel15/:2:1","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"abstractions example ","date":"2024-08-14","objectID":"/databasel15/:2:2","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Data Independence ","date":"2024-08-14","objectID":"/databasel15/:3:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Entity-Relationship Model (ERM) ","date":"2024-08-14","objectID":"/databasel15/:4:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Def Entity: a real-world object that can be described and identified by a set of attributes Entity Set: a collection of similar entities all entities in an entity set have the same attributes each entity set has a key each attribute has a domain Relationship: a connection between two entity sets ","date":"2024-08-14","objectID":"/databasel15/:4:1","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Key and Participation Constraints ","date":"2024-08-14","objectID":"/databasel15/:5:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Key Constraints ","date":"2024-08-14","objectID":"/databasel15/:5:1","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Participation Constraints ","date":"2024-08-14","objectID":"/databasel15/:5:2","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Weak Entity ","date":"2024-08-14","objectID":"/databasel15/:6:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Alternative Notation and Terminology decoder graph ğŸ˜… math notation ğŸ˜… ","date":"2024-08-14","objectID":"/databasel15/:7:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Binary vs Ternary Relationships ä¸Šé¢çš„æ›´åŠ ç´§å¯†å¹¶ä¸”å¯ä»¥è®°å½•qty ","date":"2024-08-14","objectID":"/databasel15/:8:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Aggregation and Ternary Relationships ","date":"2024-08-14","objectID":"/databasel15/:9:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Entities vs Attributes Remember: attributes can not have nested attributes (atomic attributes only), if you want to represent nested attributes, use entities instead ğŸ¤” entity or attribute? depends on the context! ","date":"2024-08-14","objectID":"/databasel15/:10:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Entities vs Relationships å¿…è¦æ—¶æ‹†å‡ºæ¥æ–°çš„å®ä½“æ¥æ„å»ºæ–°çš„å…³ç³» ","date":"2024-08-14","objectID":"/databasel15/:11:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Converting ER to Relational Entity Set: table many-to-many Relationship Set: keys for participating entities, forming a superkey for the relation all other attributes Key Constraints: think carefully about the uniqueness of the primary key! Participation Constraints: usually using NOT NULL Weak Entity Set: ","date":"2024-08-14","objectID":"/databasel15/:12:0","tags":null,"title":"CS186-L15: DB Design: Entity-Relationship Models","uri":"/databasel15/"},{"categories":["CS186"],"content":"Intro relational operator: tuples(in other way, relations) in, tuples out abstract class Iterator { // set up the children and the dataflow graph void setup(List\u003cIterator\u003e inputs); void init(args); // state tuple next(); // returns the next tuple void close(); } ","date":"2024-08-14","objectID":"/databasel10/:1:0","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"presudo code select on the fly ğŸ¤” init() { child.init(); pred = predicate; current = null; } next() { while (current != EOF \u0026\u0026 !pred(current)) { current = child.next(); } } close() { child.close(); } heap scan want to find out the empty record id init(relation) { heap = open heap file for the relation; cur_page = heap.first_page(); cur_slot = cur_page.first_slot(); } next() { if (cur_page == null) return EOF; current = [cur_page, cur_slot]; // return the id // advance to the next slot cur_slot = cur_page.next_slot(cur_slot); if (cur_slot == null) { // advance to the next page, first slot cur_page = heap.next_page(cur_page); if (cur_page != null) cur_slot = cur_page.first_slot(); } return current; } close() { heap.close(); } sort (two pass) Group By assume that already sorted, and notice that only contain ONE tuple at a time ===\u003e memory efficient ","date":"2024-08-14","objectID":"/databasel10/:1:1","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"A single thread side note: how does the block operator work with the streaming operator Sort use disk internally we do not store the operator output in disk ===\u003e stream through the call stack ","date":"2024-08-14","objectID":"/databasel10/:1:2","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Join operators ","date":"2024-08-14","objectID":"/databasel10/:2:0","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Simple Nested Loops Join see the course note, not that hard to understand $[R] + [R]|S|$ $[S] + [S]|R|$ é¡ºåºå¾ˆé‡è¦ï¼ ","date":"2024-08-14","objectID":"/databasel10/:2:1","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Pages Nested Loops Join $[R]+[R][S]$ ","date":"2024-08-14","objectID":"/databasel10/:2:2","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Chunk Nested Loops Join $[R] + \\lceil(\\frac{[R]}{B-2})\\rceil[S]$ ","date":"2024-08-14","objectID":"/databasel10/:2:3","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Index Nested Loops Join $[R] + |R|*(cost\\ of\\ index\\ lookup)$ cost of index lookup unclustered: (# of matching s tuples for each r tuple) $\\times$ (access cost of per s tuple) clustered: (# of matching s tuples for each r pages) $\\times$ (access cost of per s page) ","date":"2024-08-14","objectID":"/databasel10/:2:4","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Sort-Merge Join ä¾æ¬¡æ»šä¸¤ä¸ªçº¸å¸¦ï¼Œå¯¹é½ï¼Œå½’å¹¶ã€‚ $Sort(R) + Sort(S) + ([R]+[S])$ worst $|R|[S]$ , too many dups a refinement of the sort-merge join note that if join and sort, will cost around 9500 \u003e 7500 so sort and join can allow us to get the ORDER BY free ğŸ¤” here comes the refinement é‡ç‚¹åœ¨äºå¯¹sortingæœ€åä¸€æ¬¡mergeçš„ä¼˜åŒ–ï¼Œå› ä¸ºå¯ä»¥ track Rå’ŒSçš„æœ€å°å€¼ï¼Œäºæ˜¯å¼€å§‹joinçš„æ­¥éª¤å³å¯ ","date":"2024-08-14","objectID":"/databasel10/:2:5","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Naive Hash Join Requires equality predicate: equi-join and natural join assume that $R$ is small enough to fit in memory algorithm: hash $R$ into hash table scan $S$ (can be huge file) and probe $R$ requires $R$ \u003c (B-2)*hash_fill_factor ","date":"2024-08-14","objectID":"/databasel10/:2:6","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Grace Hash Join Requires equality predicate: equi-join and natural join algorithm: partition tuples from $R$ and $S$ Build \u0026 Probe a separate hash table for each partition assume that each partition is small enough to fit in memory recurse if necessary cost: $3([R]+[S])$ so it is a good choice for large $S$ and small $R$ Hybrid Hash Join is not included ğŸ¤” ","date":"2024-08-14","objectID":"/databasel10/:2:7","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Summary ","date":"2024-08-14","objectID":"/databasel10/:3:0","tags":null,"title":"CS186-L10: Iterators \u0026 Joins","uri":"/databasel10/"},{"categories":["CS186"],"content":"Intro ??? I am here right now ğŸ˜„ çœæµï¼šæ‹¿è¿›æ¥sqlï¼Œè½¬æ¢æˆrelational algebraï¼Œç„¶åæ ‘å½¢å›¾å±•ç¤ºï¼Œè€ƒè™‘ä¸åŒçš„å®ç°æ–¹å¼ï¼Œç„¶åç»™å‡ºä¼˜åŒ–æ–¹æ¡ˆã€‚ è€ƒè™‘ä¸åŒçš„å®ç°æ–¹å¼ plan space: how many plans? cost estimation search strategy ","date":"2024-08-14","objectID":"/databasel11/:1:0","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"algebra equivalences ","date":"2024-08-14","objectID":"/databasel11/:2:0","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"selections, projections and Cartesian products åœ¨æ•°æ®åº“æŸ¥è¯¢çš„å…³ç³»ä»£æ•°ä¸­ï¼Œprojectionsï¼ˆæŠ•å½±ï¼‰ æ˜¯ä¸€ç§æ“ä½œï¼Œå®ƒç”¨äºä»ä¸€ä¸ªå…³ç³»ä¸­é€‰æ‹©ç‰¹å®šçš„å±æ€§åˆ—ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æŠ•å½±çš„æ€§è´¨ä»¥åŠå¦‚ä½•ç†è§£å®ƒã€‚æŠ•å½±ï¼ˆProjectionsï¼‰çš„æ€§è´¨è¡¨æ˜äº†å®ƒä»¬å¯ä»¥è¢« çº§è”ï¼ˆcascadeï¼‰ ä½¿ç”¨ã€‚è¿™æ„å‘³ç€å¤šä¸ªæŠ•å½±æ“ä½œå¯ä»¥é€æ­¥åº”ç”¨äºå…³ç³» R ä¸Šï¼Œæ¯ä¸€æ­¥é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªå±æ€§åˆ—ï¼š [ \\pi_{a_1}(R) \\equiv \\pi_{a_1}(â€¦(\\pi_{a_1, â€¦, a_{n-1}}(R))â€¦) ] çº§è”ï¼ˆCascadeï¼‰ çº§è”çš„å«ä¹‰æ˜¯æŒ‡å¤šä¸ªæŠ•å½±æ“ä½œå¯ä»¥æŒ‰é¡ºåºåº”ç”¨ã€‚åœ¨çº§è”çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¤šä¸ªæŠ•å½±æ“ä½œç»“åˆèµ·æ¥ï¼Œä¾æ¬¡å‡å°‘å…³ç³»çš„å±æ€§ã€‚ ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå…³ç³» Rï¼Œå®ƒæœ‰å±æ€§é›†åˆ {a_1, a_2, a_3, a_4}ã€‚å¦‚æœæˆ‘ä»¬åº”ç”¨ (\\pi_{a_1, a_2}(R))ï¼Œæˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯ä»…æœ‰ a_1 å’Œ a_2 å±æ€§çš„å…³ç³»ã€‚ æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªç»“æœä¸Šå†æ¬¡åº”ç”¨ (\\pi_{a_1}(R))ï¼Œæœ€ç»ˆå¾—åˆ°åªæœ‰ a_1 å±æ€§çš„å…³ç³»ã€‚ è¿™ç§çº§è”æ€§è´¨å¯ä»¥è¢«ç†è§£ä¸ºå°†å¤šä¸ªæŠ•å½±æ“ä½œåˆå¹¶æˆä¸€ä¸ªæ“ä½œã€‚ ","date":"2024-08-14","objectID":"/databasel11/:2:1","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"joins ç»“åˆå¾‹ï¼Ÿäº¤æ¢å¾‹ï¼Ÿ ç»“åˆå¾‹ä¸€èˆ¬ä¸æˆç«‹ï¼Œäº¤æ¢å¾‹æ˜¯æˆç«‹çš„ï¼ˆç”±å‰é¢çš„Cartesian productå¯çŸ¥ï¼‰ã€‚æœ‰ä¸€ä¸ªç†è§£ç‚¹ï¼šå‰ç§¯ $\\times$ çš„èŒƒå›´å˜å¤§äº† ","date":"2024-08-14","objectID":"/databasel11/:2:2","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"Some Heuristics ","date":"2024-08-14","objectID":"/databasel11/:3:0","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"selections selection is cheap, while join is expensive ğŸ˜‹ apply selections as soon as you have relevant columns ","date":"2024-08-14","objectID":"/databasel11/:3:1","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"projections keep the number of columns as small as possible, and avoid unnecessary columns ","date":"2024-08-14","objectID":"/databasel11/:3:2","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"joins always try to avoid cross-product joins, use appropriate indexes ","date":"2024-08-14","objectID":"/databasel11/:3:3","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"Physical Equivalences base table access: with single-table selections and projections heap scan index scan equi-joins: ç­‰å€¼è¿æ¥ï¼Œåœ¨ç­‰å€¼è¿æ¥ä¸­ï¼Œæˆ‘ä»¬å°†ä¸¤ä¸ªå…³ç³»ï¼ˆè¡¨ï¼‰ä¸­çš„è¡Œåˆå¹¶ï¼Œå‰ææ˜¯å®ƒä»¬çš„æŒ‡å®šåˆ—å…·æœ‰ç›¸åŒçš„å€¼ã€‚ chunk nested loops join index nested loops join sort-merge join grace hash join non-equi-joins: block nested loops join ","date":"2024-08-14","objectID":"/databasel11/:4:0","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"example åµŒå¥—å¾ªç¯è¿æ¥ï¼ˆNested Loop Joinï¼‰ çš„å·¥ä½œæ–¹å¼æ˜¯åœ¨å¤–å±‚å¾ªç¯ä¸­å¯¹ä¸€ä¸ªè¡¨ï¼ˆé€šå¸¸æ˜¯è¾ƒå°çš„è¡¨ï¼‰é€è¡Œæ‰«æï¼Œç„¶ååœ¨å†…å±‚å¾ªç¯ä¸­å¯¹å¦ä¸€ä¸ªè¡¨ï¼ˆé€šå¸¸æ˜¯è¾ƒå¤§çš„è¡¨ï¼‰è¿›è¡ŒåŒ¹é…ã€‚æ‰§è¡Œè®¡åˆ’çš„å³ä¾§å›¾ç¤ºæ„äº†é€‰æ‹©æ¡ä»¶ä¸‹æ¨åˆ°åµŒå¥—å¾ªç¯çš„å†…å±‚ã€‚ å…³é”®åŸå› ï¼š ä¸‹æ¨åçš„é€‰æ‹©å¹¶æœªå‡å°‘å†…å±‚æ‰«æçš„æ¬¡æ•°ï¼šåœ¨åµŒå¥—å¾ªç¯è¿æ¥ä¸­ï¼Œå³ä½¿é€‰æ‹©æ¡ä»¶ Ïƒ_bid=100 è¢«æ¨å…¥å†…å±‚å¾ªç¯ï¼Œä»ç„¶éœ€è¦å¯¹ Reserves è¡¨ï¼ˆå³å³ä¾§è¡¨ï¼‰è¿›è¡Œå®Œæ•´æ‰«æï¼Œä»¥æŸ¥æ‰¾ç¬¦åˆ sid è¿æ¥æ¡ä»¶çš„æ‰€æœ‰è¡Œã€‚ç”±äºå†…å±‚å¾ªç¯éœ€è¦å¯¹ Reserves è¡¨è¿›è¡Œæ‰«ææ¥åŒ¹é…å¤–å±‚è¡¨ Sailors çš„æ¯ä¸€è¡Œï¼Œé€‰æ‹©æ¡ä»¶ä¸‹æ¨å¹¶ä¸ä¼šå‡å°‘å†…å±‚è¡¨çš„æ‰«ææ¬¡æ•°ã€‚ é€‰æ‹©æ¡ä»¶çš„ä¸‹æ¨ç­‰åŒäºåœ¨è¿æ¥ä¹‹ååº”ç”¨é€‰æ‹©ï¼šè¿™æ„å‘³ç€åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ¡ä»¶çš„ä¸‹æ¨ä¸åœ¨è¿æ¥æ“ä½œä¹‹åå†åº”ç”¨é€‰æ‹©æ¡ä»¶çš„æ•ˆæœå‡ ä¹ç›¸åŒã€‚åœ¨è¿™ç§ç‰¹å®šçš„æƒ…å†µä¸‹ï¼Œé€‰æ‹©æ¡ä»¶ä¸ä¼šå‡å°‘ Reserves è¡¨çš„æ‰«æï¼Œå› ä¸ºæ— è®ºå¦‚ä½•éƒ½å¿…é¡»æ‰«ææ•´ä¸ªè¡¨ä»¥åŒ¹é… sidã€‚ äº¤æ¢ä½ç½® å¼•å…¥ç‰©åŒ–è§†å›¾ï¼ˆMaterialized Viewï¼‰å¯ä»¥å‡å°‘æ‰«ææ¬¡æ•°ï¼Œå¢åŠ å°‘é‡ç‰©åŒ–å¼€é”€ æ›´æ¢joinç§ç±» ä¸‹æ”¾projectionsï¼Œå¯èƒ½ä¼šå‡ºç°ä¸€ä¸ªbufferæå®šå·¦è¾¹çš„æƒ…å†µ è€ƒè™‘physical equivalence clusterä½¿å¾—æ•°æ®ä»¥pageä¸ºå•ä½è¯»å– ","date":"2024-08-14","objectID":"/databasel11/:5:0","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"Summary ","date":"2024-08-14","objectID":"/databasel11/:6:0","tags":null,"title":"CS186-L11:  Query Opt: Plan Space","uri":"/databasel11/"},{"categories":["CS186"],"content":"Intro An important property of query optimization is that we have no way of knowing how many I/Os a plan will cost until we execute that plan. see note 9 ","date":"2024-08-14","objectID":"/databasel12/:1:0","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"example background ","date":"2024-08-14","objectID":"/databasel12/:2:0","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"plan space overview å¯¹physical propertiesçš„æè¿°ï¼Œå…³æ³¨çš„ç‚¹åœ¨äºä¸åŒè¿‡ç¨‹å¯¹hash or sortçš„è¦æ±‚ ","date":"2024-08-14","objectID":"/databasel12/:3:0","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"Cost Estimation è®¨è®ºçš„å‡è®¾ æ³¨æ„predicateä¹‹é—´ç‹¬ç«‹ï¼Œä»¥åŠåªæ˜¯å…³æ³¨IO é‡åŒ–çš„ä¿¡æ¯ç‚¹ï¼Œcatalog ","date":"2024-08-14","objectID":"/databasel12/:4:0","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"selectivity formula side note: ç­‰å€¼joinçš„ç†è§£è§ä¸‹å›¾çš„bunny ğŸ° histograms for selectivity estimation ä½†æ˜¯è¿™èŠ‚è¯¾ç”¨çš„æ˜¯ç­‰å®½çš„ç›´æ–¹å›¾ğŸ˜ï¼Œç„¶åæ‰€æœ‰çš„æ¡ä»¶ç›¸äº’ç‹¬ç«‹ï¼Œselectivityæ ¹æ®ç›´æ–¹å›¾æ˜¾ç¤ºçš„é¢‘ç‡è®¡ç®—å¾—å‡º selectivity for join query $$ \\Large R\\Join_{p} \\sigma_{q}(S) \\equiv \\sigma_{p}(R \\times \\sigma_{q}(S)) \\equiv \\sigma_{p \\land q}(R \\times S) $$ æ‰€ä»¥ $ s = s_p s_q $æ³¨æ„å‰ç§¯äº§ç”Ÿçš„sizeå˜åŒ– ","date":"2024-08-14","objectID":"/databasel12/:4:1","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"Search Algorithm ","date":"2024-08-14","objectID":"/databasel12/:5:0","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"single table plan æ²¡æœ‰å¬æ‡‚ï¼Ÿï¼Ÿï¼Ÿ ååˆ†ç²—ç³™çš„ä¼°è®¡ æ²¡æœ‰å¬æ‡‚ï¼Ÿï¼Ÿï¼Ÿ ","date":"2024-08-14","objectID":"/databasel12/:5:1","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"multi-table plan å€Ÿé‰´DPæ€æƒ³ï¼Œå°†joinçš„è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œæ¯ä¸ªå­é—®é¢˜éƒ½å¯ä»¥å•ç‹¬ä¼°è®¡costï¼Œç„¶åå°†å­é—®é¢˜çš„costç´¯åŠ èµ·æ¥ ä½†æ˜¯å¯èƒ½å‡ºç°å­è®¡åˆ’å¹¶éæœ€ä¼˜çš„æƒ…å†µï¼Œç„¶è€Œå¯¹å…¨å±€æœ€ä¼˜ ä¸­é—´ç»“æœé™„åŠ äº†Orderï¼Œå¯èƒ½å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç§¯æå½±å“ï¼Œæ•…è€ƒè™‘ä¿ç•™ åˆ—ä¸¾planæŠ€å·§ å¯¹äºå¤šè¿‡ç¨‹çš„planæšä¸¾ï¼ŒDP tableåŒæ ·å¯ä»¥å»¶å±• ","date":"2024-08-14","objectID":"/databasel12/:5:2","tags":null,"title":"CS186-L12: Query Opt: Costs \u0026 Search","uri":"/databasel12/"},{"categories":["CS186"],"content":"Relational Algebra Intro Relational algebra ğŸ˜µ algebra on sets â€”\u003e mean no duplicates! operational description of transformations Relational Calculus ğŸ˜µ basic of SQL ä»¥ä¸Šä¸¤è€…è¢«è¯æ˜ç­‰ä»·å¹¶ä¸”ä½¿å¾—SQLèƒ½å¤Ÿè¢«ç¼–è¯‘â€“long storyâ€¦â€¦ ","date":"2024-08-14","objectID":"/databasel8/:1:0","tags":null,"title":"CS186-L8: Relational Algebra","uri":"/databasel8/"},{"categories":["CS186"],"content":"Relational Algebra Operators ","date":"2024-08-14","objectID":"/databasel8/:2:0","tags":null,"title":"CS186-L8: Relational Algebra","uri":"/databasel8/"},{"categories":["CS186"],"content":"Unary Operators Selection (Ïƒ) : select rows that satisfy a condition Projection (Ï€) : select specific columns å¦‚æœç»“æœæ˜¯multiset, å»é‡! è™½ç„¶real systemä¸ä¼šè‡ªåŠ¨å»é‡ Rename (Ï) : rename a column Union (âˆª) : tuples in either r1 or r2 two input relations, must be compatible (same number of columns or â€œfieldsâ€, and have same data types corresponding to the same field) UNION in SQL Difference (âˆ’) : tuples in r1, but not in r2 same with union, both input relations must be compatible EXCEPT in SQL Cross Product (Ã—) : cartesian product of two relations how many rows in the result? $|R_1|*|R_2|$ none duplicate tuples ","date":"2024-08-14","objectID":"/databasel8/:2:1","tags":null,"title":"CS186-L8: Relational Algebra","uri":"/databasel8/"},{"categories":["CS186"],"content":"Compound Operators Join (â‹ˆ / $â‹ˆ_\\theta$) : join two relations on common attributes avoid cross product as we can! natural join Intersection (âˆ©) : tuples that appear in both r1 and r2 $S_1 \\cap S_2 = S_1 - (S_1 - S_2)$ ","date":"2024-08-14","objectID":"/databasel8/:2:2","tags":null,"title":"CS186-L8: Relational Algebra","uri":"/databasel8/"},{"categories":["CS186"],"content":"Grouping and Aggregation Operators Group By / Aggregation Operators($\\gamma$) $\\gamma_{age, AVG(rating)}$ (Sailors) with selection (HAVING) $\\gamma_{age, AVG(rating), COUNT(*)\u003e2}$(Sailors) ","date":"2024-08-14","objectID":"/databasel8/:2:3","tags":null,"title":"CS186-L8: Relational Algebra","uri":"/databasel8/"},{"categories":["CS186"],"content":"Summary ","date":"2024-08-14","objectID":"/databasel8/:3:0","tags":null,"title":"CS186-L8: Relational Algebra","uri":"/databasel8/"},{"categories":["CS186"],"content":"intro ","date":"2024-08-14","objectID":"/databasel7/:1:0","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"å¯èƒ½éœ€è¦å¤„ç†çš„é—®é¢˜ dirty pages pages replacement ","date":"2024-08-14","objectID":"/databasel7/:1:1","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"state of buffer management ","date":"2024-08-14","objectID":"/databasel7/:1:2","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"Page replacement terminology a page in â€œuseâ€ : Page pin count if full. which should be replaced: page replacement policy requestè¯·æ±‚å‘å‡ºæ¥ï¼Œè½¬æ¥åˆ°buffer managerâ€¦â€¦ ","date":"2024-08-14","objectID":"/databasel7/:2:0","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"Page replacement policies ","date":"2024-08-14","objectID":"/databasel7/:3:0","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"LRU (Least Recently Used) æœ€è¿‘æœ€å°‘ä½¿ç”¨ä½¿ç”¨çš„æ˜¯ æ—¶é—´ pin count == 0! Priority heap data structure can help! like $O(logN)$ ","date":"2024-08-14","objectID":"/databasel7/:3:1","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"CLOCK ä¸€ç§è¿‘ä¼¼LRUçš„ç®—æ³•ï¼Œæ—¨åœ¨ä¸éœ€è¦ç»´æŠ¤æ¯ä¸ªé¡µé¢çš„è®¿é—®æ—¶é—´æˆ³ï¼Œä»è€Œå‡å°‘äº†é¢å¤–çš„å¼€é”€ã€‚ Clock policyå°†ç¼“å†²åŒºä¸­çš„é¡µé¢è§†ä¸ºä¸€ä¸ªå¾ªç¯åˆ—è¡¨ï¼Œä½¿ç”¨ä¸€ä¸ªâ€œæ—¶é’ŸæŒ‡é’ˆâ€æ¥è·Ÿè¸ªå½“å‰è€ƒè™‘æ›¿æ¢çš„é¡µé¢ã€‚æ¯ä¸ªé¡µé¢éƒ½æœ‰ä¸€ä¸ªâ€œå¼•ç”¨ä½â€ï¼ˆref bitï¼‰ï¼Œç”¨äºæŒ‡ç¤ºè¯¥é¡µé¢æ˜¯å¦è¢«æœ€è¿‘è®¿é—®è¿‡ã€‚ å·¥ä½œæµç¨‹ åˆå§‹åŒ–ï¼šå½“ç¼“å†²åŒºç®¡ç†å™¨å¯åŠ¨æ—¶ï¼Œæ—¶é’ŸæŒ‡é’ˆæŒ‡å‘ç¬¬ä¸€ä¸ªæœªå›ºå®šï¼ˆunpinnedï¼‰çš„é¡µé¢ï¼Œå¹¶å°†è¯¥é¡µé¢çš„å¼•ç”¨ä½è®¾ç½®ä¸º1ï¼Œå½“é¡µé¢è¢«è¯»å…¥æ—¶ã€‚ é¡µé¢æ›¿æ¢ï¼š å½“éœ€è¦æ›¿æ¢é¡µé¢æ—¶ï¼Œç¼“å†²åŒºç®¡ç†å™¨ä»æ—¶é’ŸæŒ‡é’ˆå¼€å§‹ï¼Œéå†ç¼“å†²åŒºä¸­çš„é¡µé¢ã€‚ å¯¹äºæ¯ä¸ªé¡µé¢ï¼Œå¦‚æœè¯¥é¡µé¢çš„å¼•ç”¨ä½ä¸º1ï¼Œåˆ™å°†å…¶å¼•ç”¨ä½é‡ç½®ä¸º0ï¼Œå¹¶å°†æ—¶é’ŸæŒ‡é’ˆç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªé¡µé¢ã€‚ å¦‚æœæ‰¾åˆ°ä¸€ä¸ªå¼•ç”¨ä½ä¸º0çš„é¡µé¢ï¼Œåˆ™å¯ä»¥å°†å…¶æ›¿æ¢ã€‚æ­¤æ—¶ï¼Œå¦‚æœè¯¥é¡µé¢æ˜¯â€œè„é¡µâ€ï¼ˆdirty pageï¼‰ï¼Œåˆ™éœ€è¦å°†å…¶å†™å›ç£ç›˜ï¼Œç„¶åè¯»å–æ–°çš„é¡µé¢å¹¶å°†å…¶å¼•ç”¨ä½è®¾ç½®ä¸º1ã€‚ ","date":"2024-08-14","objectID":"/databasel7/:3:2","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"LRU-Clock bad behavior Sequential Flooding! ","date":"2024-08-14","objectID":"/databasel7/:3:3","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"MRU (Most Recently Used) General case: SeqScan + MRU $B$ buffers $N\u003eB$ pages in file Improvements for sequential scan: prefetch hybrid? LRU wins for random access MRU wins for repeated sequential scan Two General Approaches: ğŸ¤” ","date":"2024-08-14","objectID":"/databasel7/:3:4","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"System perspective ","date":"2024-08-14","objectID":"/databasel7/:4:0","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"summary ","date":"2024-08-14","objectID":"/databasel7/:5:0","tags":null,"title":"CS186-L7: Buffer Management","uri":"/databasel7/"},{"categories":["CS186"],"content":"Why Sort? Rendezvous ä¸ºäº†â€œé›†åˆâ€ eliminating duplicates (DISTINCT) Grouping for summarization (GROUP BY) Upcoming sort-merge join algorithms Ordering sometimes output must be in a specific order First step in bulk loading Tree indexes Problem: sort 100GB of data with 1GB of RAM why not virtual memory? â€“ random IO access, too slow ğŸ˜¢ ","date":"2024-08-14","objectID":"/databasel9/:1:0","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Out-of-Core Algorithms core == RAM back in the day ","date":"2024-08-14","objectID":"/databasel9/:2:0","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Single Streaming data passing through the memory MapReduceâ€™s â€œMapâ€ ğŸ˜ ","date":"2024-08-14","objectID":"/databasel9/:2:1","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Better: Double Buffering 1. ä¸»è¦çº¿ç¨‹å¤„ç† I/O ç¼“å†²åŒºä¸­çš„æ•°æ® ä¸»çº¿ç¨‹è´Ÿè´£åœ¨ä¸€ä¸ªI/Oç¼“å†²åŒºå¯¹ï¼ˆå³è¾“å…¥ç¼“å†²åŒºå’Œè¾“å‡ºç¼“å†²åŒºï¼‰ä¸Šè¿è¡Œf(x)å‡½æ•°ã€‚ ä¸»çº¿ç¨‹å®Œæˆè®¡ç®—åå‡†å¤‡å¤„ç†æ–°çš„ç¼“å†²åŒºæ•°æ®æ—¶ï¼Œä¼šè¿›è¡Œç¼“å†²åŒºçš„äº¤æ¢ï¼ˆSwapï¼‰ã€‚ 2. ç¬¬äºŒä¸ª I/O çº¿ç¨‹å¹¶è¡Œå¤„ç†æœªä½¿ç”¨çš„ I/O ç¼“å†²åŒº ç¬¬äºŒä¸ªI/Oçº¿ç¨‹å¹¶è¡Œæ“ä½œï¼Œç”¨äºæ¸…ç©ºå·²æ»¡çš„è¾“å‡ºç¼“å†²åŒºå¹¶å¡«å……æ–°çš„è¾“å…¥ç¼“å†²åŒºã€‚ è¿™ç§å¹¶è¡Œæ€§èƒ½å¤Ÿæé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œå› ä¸ºI/Oæ“ä½œé€šå¸¸è¾ƒä¸ºè€—æ—¶ï¼Œè€Œé€šè¿‡å¹¶è¡Œå¤„ç†å¯ä»¥å‡å°‘ä¸»çº¿ç¨‹çš„ç­‰å¾…æ—¶é—´ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°åˆ©ç”¨CPUèµ„æºã€‚ 3. ä¸ºä»€ä¹ˆå¹¶è¡Œå¤„ç†æ˜¯å¯è¡Œçš„ï¼Ÿ åŸå› ï¼šé€šå¸¸æƒ…å†µä¸‹ï¼ŒI/Oæ“ä½œæ¯”è¾ƒæ…¢ï¼Œå› æ­¤éœ€è¦å ç”¨å•ç‹¬çš„çº¿ç¨‹æ¥å¤„ç†ï¼Œä»¥é¿å…é˜»å¡ä¸»çº¿ç¨‹ã€‚ ä¸»é¢˜ï¼šI/Oå¤„ç†é€šå¸¸éœ€è¦ç‹¬ç«‹çš„çº¿ç¨‹æ¥ç®¡ç†ï¼Œä»¥æé«˜æ•´ä½“å¤„ç†æ•ˆç‡ã€‚ 4. å›¾è§£è¯´æ˜ å›¾ä¸­æ˜¾ç¤ºäº†åŒç¼“å†²æœºåˆ¶ä¸‹çš„å¤„ç†æµç¨‹ï¼šè¾“å…¥ç¼“å†²åŒºå’Œè¾“å‡ºç¼“å†²åŒºæˆå¯¹å‡ºç°ï¼Œå…¶ä¸­ä¸€å¯¹ç¼“å†²åŒºåœ¨ä¸»çº¿ç¨‹ä¸­å¤„ç†ï¼Œè€Œå¦ä¸€å¯¹ç¼“å†²åŒºåœ¨I/Oçº¿ç¨‹ä¸­å¤„ç†ã€‚å½“ä¸»çº¿ç¨‹å¤„ç†å®Œå½“å‰ç¼“å†²åŒºå¯¹æ—¶ï¼Œä¸¤ä¸ªçº¿ç¨‹ä¼šè¿›è¡Œç¼“å†²åŒºäº¤æ¢ã€‚ æ€»ç»“ ç›¸æ¯”å•ç¼“å†²çš„å•æ¬¡æµå¼å¤„ç†ï¼ŒåŒç¼“å†²é€šè¿‡å¹¶è¡Œå¤„ç†I/Oæ“ä½œï¼Œå¯ä»¥æ˜¾è‘—æé«˜å¤„ç†æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨I/Oæ“ä½œè¾ƒæ…¢çš„æƒ…å†µä¸‹ã€‚ä¸»çº¿ç¨‹å¯ä»¥ä¸“æ³¨äºè®¡ç®—ï¼Œè€Œä¸å¿…ç­‰å¾…I/Oæ“ä½œå®Œæˆï¼Œè¿›ä¸€æ­¥æå‡äº†ç³»ç»Ÿçš„å¹¶è¡Œæ€§å’Œæ€§èƒ½ã€‚ double buffering applies to all streams! assume that you have RAM buffers to spare! ","date":"2024-08-14","objectID":"/databasel9/:2:2","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Sorting and Hashing ","date":"2024-08-14","objectID":"/databasel9/:3:0","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Formal Specs a file $F$: a multiset of records $R$ consuming $N$ blocks of storage two â€œscratchâ€ disks each with Â» $N$ blocks of free storage a fixed amount of space in RAM memory capacity equivalent to $B$ blocks of disk As for sorting: produce an output file $F_S$ with content $R$ stored in order by a given sorting criterion As for hashing: produce an output file $F_H$ with content $R$, arranged on disk so that no 2 records that have the same hash value are separated by a record with a different hash value i.e., consecutively stored on disk ","date":"2024-08-14","objectID":"/databasel9/:3:1","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Sorting Strawman Algorithm æ³¨æ„å·¦ä¾§æ˜¯æ²¡æœ‰sortçš„ï¼Œå³ä¾§æ˜¯sortä¹‹åçš„ã€‚ General External Merge Sort åŸºäºRAMè¿œè¿œä¸å¤Ÿå­˜æ”¾æ‰€æœ‰è¦æ’åºçš„æ•°æ®æ¥è®¨è®º side note: PASSæ„å‘³ç€ä»æ‰€æœ‰çš„æ•°æ®æµdiskæµå‘å¦ä¸€ä¸ªdiskï¼Œå¯ä»¥è®¤ä¸ºæ˜¯IOï¼›RUNæŒ‡çš„æ˜¯ a sequence of sorted pages. see length = $B$ï¼Œæœ€åä¸€ä¸ªæ˜¯å˜é•¿çš„block $B$ pages/blocks â€”\u003e $B-1$ merge (æœ‰ä¸€ä¸ªbufferæ˜¯ä¸ºäº†å†™å…¥) äº‹å®ä¸Šå¾ˆåƒä¸€ä¸ªTreeï¼Œåˆ†è€Œæ²»ä¹‹ç„¶åä¸æ–­åˆå¹¶ä¸­é—´ç»“æœ ","date":"2024-08-14","objectID":"/databasel9/:3:2","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Hashing ideal Divide and Conquer é€šè¿‡ $2N$ passå°†æ•°æ®æ ¹æ® $h_p$ äº§ç”Ÿçš„å“ˆå¸Œå€¼åˆ†å‰²æˆ $N$ ä¸ªblock å¯¹äºæ¯ä¸ªåˆ†å¥½å¤§ç±»åˆ«çš„blockï¼Œé‡æ–°hashä»è€Œå®ç°ç›¸åŒå†…å®¹çš„recordè¿ç»­å­˜å‚¨ï¼Œ $2N$ pass æ‰€ä»¥costçº¦ä¸º $4N$ pass recursive partitioning for External Hashing å½“divideæ—¶å‡ºç°æŸä¸ªblockçš„recordæ•°ç›®å¤ªå¤šæ—¶ checkä¸åŒç§ç±»çš„hashæ•°é‡ï¼ŒåŸºäºæ–°çš„ $h_{r_1}$ ç”Ÿæˆhash å¦‚æœæ•°é‡ä¸ºä¸€ï¼Œåœæ­¢åˆ†å‰²ï¼Œå†™å…¥ç£ç›˜ å¦‚æœæ•°é‡å¤§äºä¸€ï¼Œç»§ç»­åˆ†å‰²ï¼Œç›´åˆ°æ•°é‡å°äºç­‰äº $B$ ","date":"2024-08-14","objectID":"/databasel9/:3:3","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"hash and sort duality hash: divide-conquer sort: conquer-merge cost around $4N$ pass å¯¹äºä¸€æ¬¡å®Œæˆå®¹é‡ä¸º $X$ çš„ä»»åŠ¡ï¼Œbuffer è¦æ±‚çº¦ä¸º $\\sqrt{X}$ ","date":"2024-08-14","objectID":"/databasel9/:4:0","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"parallel sorting and hashing parallel hashing: å¤šäº†ä¸€ä¸ª $h_n$ ç„¶åå¿«äº† parallel sorting: å¤šäº†ä¸€ä¸ª range å¦‚ä½•ä¿è¯å„ä¸ªè®¡ç®—æœºå·¥ä½œé‡å¤§è‡´ç›¸åŒï¼Ÿ===\u003e å¿«é€Ÿä¼°è®¡æ•°æ®é›†çš„åˆ†å¸ƒ ","date":"2024-08-14","objectID":"/databasel9/:5:0","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"Summary Hashing pros: for duplicate elimination, scales with # of values delete dups in the first pass VS. sort scales with # of items easy to shuffle in parallel Sorting pros: if need to be sorted Not sensitive to duplicates or â€œbadâ€ hash functions (eg. many dups in data) ","date":"2024-08-14","objectID":"/databasel9/:6:0","tags":null,"title":"CS186-L9: Sorting and Hashing","uri":"/databasel9/"},{"categories":["CS186"],"content":"General Notes issues to consider in any index structure (not just in B+ tree) query support: what class of queries can be supported? choice of search key affects how we write the query data entry storage affect performance of the index variable-length keys tricks affect performance of the index cost model for Index vs Heap vs Sorted File ","date":"2024-08-14","objectID":"/databasel6/:1:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"query support ","date":"2024-08-14","objectID":"/databasel6/:2:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Indexes basic selection \u003ckey\u003e\u003cop\u003e\u003cconstant\u003e è¯¸å¦‚=ï¼ŒBETWEENï¼Œ\u003eï¼Œ\u003cï¼Œ\u003e=ï¼Œ\u003c= more selection ç»´åº¦ç¾éš¾ğŸ˜² ä½†æ˜¯è¿™èŠ‚è¯¾æˆ‘ä»¬åªæ˜¯å…³æ³¨1-d range search, equalityï¼Œ B+ tree ","date":"2024-08-14","objectID":"/databasel6/:2:1","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Search Key and Ordering æ³¨æ„lexicographic! ä»¥ä¸‹ç»™å‡ºäº†ä¸€ä¸ªå®šä¹‰Composite Keysï¼Œå¤šåˆ—ï¼Œå‰ç­‰ï¼Œå°¾å”¯ä¸€range æ³¨æ„å¯¹Lexicographic Rangeçš„å¼ºè°ƒ ","date":"2024-08-14","objectID":"/databasel6/:3:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Data Entry Storage ","date":"2024-08-14","objectID":"/databasel6/:4:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"intro the representation of data? itself or pointers to it? how data is stored? clustered or unclustered? ","date":"2024-08-14","objectID":"/databasel6/:4:1","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"representation alt. 1 index entry: (key, value) alt. 2 index entry: (key, recordID), remember recordID isâ€¦â€¦ alt. 3 index entry: (key, list of recordIDs) ","date":"2024-08-14","objectID":"/databasel6/:4:2","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"clustered vs unclustered index clustered is more efficient for IOs ğŸ¤”, range search and supports â€œcompressionâ€ ğŸ¤” ","date":"2024-08-14","objectID":"/databasel6/:4:3","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Variable-length keys tricks é‡æ–°å®šä¹‰ Occupancy Invariant ï¼ˆå½“ä¸æ˜¯ç”¨æ•´æ•°æ¥indexæ—¶å€™ï¼‰ get more index entries to shorten the tree (avoiding long-time IOs) prefix key compression (only in leaf level ğŸ¤”, slightly change the order of keys?) suffix key compression ","date":"2024-08-14","objectID":"/databasel6/:5:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"B+ Tree Costs è¿™é‡Œå¼•å…¥æ–°çš„å‡è®¾ï¼š store by ref (see in alt. 2) clustered index with 2/3 full heap file pages clustered -\u003e heapfile is initially sorted fanout is larger ~ $O(Ref)$ assume static index ç¬¦å·è¡¨è¾¾å¦‚ä¸‹ï¼š $ B $ : num of full data blocks (why full? recall previous lecture) $ R $ : num of records per blocks $ D $ : Average time to r/w disk block $ F $ : avg internal node fanout $ E $ : avg num of data entries per leaf side note: Scan all records: $3/2$æ¥è‡ªä¸å æœ‰ç‡2/3ï¼Œ $\\frac{2}{3}Bâ€™ = B \\Rightarrow Bâ€™ = \\frac{3}{2}B \\Rightarrow Bâ€™D = \\frac{3}{2}B D$ Equality Search: $1 \\Rightarrow 2$ !! æ¥è‡ªäºä»pageä¸­è¯»å–slotä»è€Œè·å¾—å…·ä½“çš„indexå¹¶ä¸”è¯»å–æ•°å€¼, $log_F(BR/E)$ æ˜¯æœç´¢page Range Search: åº”è¯¥æ˜¯ $(log_F(BR/E)+1+3*pages)*D$ Insert\u0026Delete: åº”è¯¥æ˜¯ $(log_F(BR/E)+4)*D$, index 1ï¼Œè¯»å–æ•°å€¼ 1ï¼Œæ”¹å˜æ•°å€¼ 1ï¼Œæ”¹å˜index 1 big-O notation: ğŸ˜¸ ","date":"2024-08-14","objectID":"/databasel6/:6:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["CS186"],"content":"Summary time-stamp: 01h42m07s ","date":"2024-08-14","objectID":"/databasel6/:7:0","tags":null,"title":"CS186-L6: Indices \u0026 B+ Tree Refinements","uri":"/databasel6/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab13.ipynb\") Lab 13: Decision Trees and Random Forests # Run this cell to set up your notebook import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import seaborn as sns from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import tree # you may get a warning from importing ensemble. It is OK to ignore said warning from sklearn import ensemble plt.style.use('fivethirtyeight') ","date":"2024-08-13","objectID":"/datalab13/:0:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Objectives In this assignment, we will have you train a multi-class classifier with three different models (one-vs-rest logistic regression, decision trees, random forests) and compare the accuracies and decision boundaries created by each. ","date":"2024-08-13","objectID":"/datalab13/:1:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"[Tutorial] Dataset, EDA, and Classification Task Weâ€™ll be looking at a dataset of per-game stats for all NBA players in the 2018-19 season. This dataset comes from basketball-reference.com. # just run this cell nba_data = pd.read_csv(\"nba18-19.csv\") nba_data.head(5) Rk\rPlayer\rPos\rAge\rTm\rG\rGS\rMP\rFG\rFGA\r...\rFT%\rORB\rDRB\rTRB\rAST\rSTL\rBLK\rTOV\rPF\rPTS\r0\r1\rÃlex Abrines\\abrinal01\rSG\r25\rOKC\r31\r2\r19.0\r1.8\r5.1\r...\r0.923\r0.2\r1.4\r1.5\r0.6\r0.5\r0.2\r0.5\r1.7\r5.3\r1\r2\rQuincy Acy\\acyqu01\rPF\r28\rPHO\r10\r0\r12.3\r0.4\r1.8\r...\r0.700\r0.3\r2.2\r2.5\r0.8\r0.1\r0.4\r0.4\r2.4\r1.7\r2\r3\rJaylen Adams\\adamsja01\rPG\r22\rATL\r34\r1\r12.6\r1.1\r3.2\r...\r0.778\r0.3\r1.4\r1.8\r1.9\r0.4\r0.1\r0.8\r1.3\r3.2\r3\r4\rSteven Adams\\adamsst01\rC\r25\rOKC\r80\r80\r33.4\r6.0\r10.1\r...\r0.500\r4.9\r4.6\r9.5\r1.6\r1.5\r1.0\r1.7\r2.6\r13.9\r4\r5\rBam Adebayo\\adebaba01\rC\r21\rMIA\r82\r28\r23.3\r3.4\r5.9\r...\r0.735\r2.0\r5.3\r7.3\r2.2\r0.9\r0.8\r1.5\r2.5\r8.9\r5 rows Ã— 30 columns Our goal will be to predict a playerâ€™s position given several other features. The 5 positions in basketball are PG, SG, SF, PF, and C (which stand for point guard, shooting guard, small forward, power forward, and center; Wikipedia). This information is contained in the Pos column: nba_data['Pos'].value_counts() Pos\rSG 176\rPF 147\rPG 139\rC 120\rSF 118\rPF-SF 2\rSF-SG 2\rSG-PF 1\rC-PF 1\rSG-SF 1\rPF-C 1\rName: count, dtype: int64\rThere are several features we could use to predict this position; check the Basketball statistics page of Wikipedia for more details on the statistics themselves. nba_data.columns Index(['Rk', 'Player', 'Pos', 'Age', 'Tm', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%',\r'3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%',\r'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS'],\rdtype='object')\rIn this lab, we will restrict our exploration to two inputs: Rebounds (TRB) and Assists (AST). Two-input feature models will make our 2-D visualizations more straightforward. ","date":"2024-08-13","objectID":"/datalab13/:2:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"3-class classification While we could set out to try and perform 5-class classification, the results (and visualizations) are slightly more interesting if we try and categorize players into 1 of 3 categories: Guard, Forward, and Center. The below code will take the Pos column of our dataframe and use it to create a new column Pos3 that consist of values 'G', 'F', and 'C' (which stand for Guard, Forward, and Center). # just run this cell def basic_position(pos): if 'F' in pos: return 'F' elif 'G' in pos: return 'G' return 'C' nba_data['Pos3'] = nba_data['Pos'].apply(basic_position) nba_data['Pos3'].value_counts() Pos3\rG 315\rF 273\rC 120\rName: count, dtype: int64\r","date":"2024-08-13","objectID":"/datalab13/:2:1","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Data Cleaning and Visualization Furthermore, since there are many players in the NBA (in the 2018-19 season there were 530 unique players), our visualizations can get noisy and messy. Letâ€™s restrict our data to only contain rows for players that averaged 10 or more points per game. # just run this cell nba_data = nba_data[nba_data['PTS'] \u003e 10] Now, letâ€™s look at a scatterplot of Rebounds (TRB) vs. Assists (AST). sns.scatterplot(data = nba_data, x = 'AST', y = 'TRB', hue = 'Pos3'); As you can see, when using just rebounds and assists as our features, we see pretty decent cluster separation. That is, Guards, Forward, and Centers appear in different regions of the plot. ","date":"2024-08-13","objectID":"/datalab13/:2:2","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 1: Evaluating Split Quality We will explore different ways to evaluate split quality for classification and regression trees in this question. ","date":"2024-08-13","objectID":"/datalab13/:3:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 1a: Entropy In lecture we defined the entropy $S$ of a node as: $$ S = -\\sum_{C} p_C \\log_{2} p_C $$ where $p_C$ is the proportion of data points in a node with label $C$. This function is a measure of the unpredictability of a node in a decision tree. Implement the entropy function, which outputs the entropy of a node with a given set of labels. The labels parameter is a list of labels in our dataset. For example, labels could be ['G', 'G', 'F', 'F', 'C', 'C']. def entropy(labels): _, counts = np.unique(labels, return_counts=True) ps = counts / counts.sum() return -np.sum(ps * np.log2(ps)) entropy(nba_data['Pos3']) np.float64(1.521555567956027)\rgrader.check(\"q1a\") ","date":"2024-08-13","objectID":"/datalab13/:3:1","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 1b: Gini impurity Another metric for determining the quality of a split is Gini impurity. This is defined as the chance that a sample would be misclassified if randomly assigned at this point. Gini impurity is a popular alternative to entropy for determining the best split at a node, and it is in fact the default criterion for scikit-learnâ€™s DecisionTreeClassifier. We can calculate the Gini impurity of a node with the formula ($p_C$ is the proportion of data points in a node with label $C$): $$ G = 1 - \\sum_{C} {p_C}^2 $$ Note that no logarithms are involved in the calculation of Gini impurity, which can make it faster to compute compared to entropy. Implement the gini_impurity function, which outputs the Gini impurity of a node with a given set of labels. The labels parameter is defined similarly to the previous part. def gini_impurity(labels): _, counts = np.unique(labels, return_counts=True) ps = counts / counts.sum() return 1 - np.sum(ps**2) gini_impurity(nba_data['Pos3']) np.float64(0.6383398017253514)\rgrader.check(\"q1b\") As an optional exercise in probability, try to think of a way to derive the formula for Gini impurity. ","date":"2024-08-13","objectID":"/datalab13/:3:2","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"[Tutorial] Variance Are there other splitting metrics beyond entropy and Gini impurity? Yes! A third metric is variance (yes, that variance), which is often used for regression trees, or decision tree regressors, which split data based on a continuous response variable. It makes little sense to use entropy/Gini impurity for regression, as both metrics assume that there are discrete probabilities of responses (and therefore are more suited to classification). Recall that the variance is defined as: $$ \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2 $$ where $\\mu$ is the mean, $N$ is the total number of data points, and $x_i$ is the value of each data point. Run the below cell to define the variance function. # just run this cell def variance(values): return np.mean((values - np.mean(values)) ** 2) # if we were predicting # points scored per player (regression) variance(nba_data['PTS']) np.float64(21.023148263588652)\r","date":"2024-08-13","objectID":"/datalab13/:3:3","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 1c: Weighted Metrics In lecture, we used weighted entropy as a loss function to help us determine the best split. Recall that the weighted entropy is given by: $$ L = \\frac{N_1 S(X) + N_2 S(Y)}{N_1 + N_2} $$ $N_1$ is the number of samples in the left node $X$, and $N_2$ is the number of samples in the right node $Y$. This notion of a weighted average can be extended to other metrics such as Gini impurity and variance simply by changing the $S$ (entropy) function to $G$ (Gini impurity) or $\\sigma^2$ (variance). First, implement the weighted_metric function. The left parameter is a list of labels or values in the left node $X$, and the right parameter is a list of labels or values in the right node $Y$. The metric parameter is a function which can be entropy, gini_impurity, or variance. For entropy and gini_impurity, you may assume that left and right contain discrete labels. For variance, you may assume that left and right contain continuous values. Then, assign we_pos3_age_30 to the weighted entropy (in the Pos3 column) of a split that partitions nba_data into two groups: a group with players who are 30 years old or older and a group with players who are younger than 30 years old. nba_data['Pos3'] 3 C\r7 C\r10 C\r19 F\r21 F\r..\r695 G\r698 F\r699 G\r700 C\r703 C\rName: Pos3, Length: 223, dtype: object\rdef weighted_metric(left, right, metric): return (len(left) * metric(left) + len(right) * metric(right)) / (len(left) + len(right)) we_pos3_age_30 = weighted_metric(nba_data.loc[nba_data['Age']\u003e=30, 'Pos3'], nba_data.loc[nba_data['Age'] \u003c 30, 'Pos3'], entropy) we_pos3_age_30 np.float64(1.521489768014793)\rgrader.check(\"q1c\") We will not go over the entire decision tree fitting process in this assignment, but you now have the basic tools to fit a decision tree. As an optional exercise, try to think about how you would extend these tools to fit a decision tree from scratch. Question 2: Classification Letâ€™s switch gears to classification. Before fitting any models, letâ€™s first split nba_data into a training set and test set. # just run this cell nba_train, nba_test = train_test_split(nba_data, test_size=0.25, random_state=100) nba_train = nba_train.sort_values(by='Pos') nba_test = nba_test.sort_values(by='Pos') ","date":"2024-08-13","objectID":"/datalab13/:3:4","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"One-vs-Rest Logistic Regression We only discussed binary logistic regression in class, but there is a natural extension to binary logistic regression called one-vs-rest logistic regression for multiclass classification. In essence, one-vs-rest logistic regression simply builds one binary logistic regression classifier for each of the $N$ classes (in this scenario $N = 3$). We then predict the class corresponding to the classifier that gives the highest probability among the $N$ classes. ","date":"2024-08-13","objectID":"/datalab13/:4:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 2a In the cell below, set logistic_regression_model to be a one-vs-rest logistic regression model. Then, fit that model using the AST and TRB columns (in that order) from nba_train as our features, and Pos3 as our response variable. Remember, sklearn.linear_model.LogisticRegression (documentation) has already been imported for you. There is an optional parameter multi_class you need to specify in order to make your model a multi-class one-vs-rest classifier. See the documentation for more details. logistic_regression_model = LogisticRegression(multi_class='ovr') logistic_regression_model.fit(nba_train[['AST', 'TRB']], nba_train['Pos3']) grader.check(\"q2a\") q2a passed! ğŸ™Œ ","date":"2024-08-13","objectID":"/datalab13/:4:1","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"[Tutorial] Visualizing Performance To see our classifier in action, we can use logistic_regression_model.predict and see what it outputs. # just run this cell nba_train['Predicted (OVRLR) Pos3'] = logistic_regression_model.predict(nba_train[['AST', 'TRB']]) nba_train[['AST', 'TRB', 'Pos3', 'Predicted (OVRLR) Pos3']].head(15) AST\rTRB\rPos3\rPredicted (OVRLR) Pos3\r655\r1.4\r8.6\rC\rC\r644\r2.0\r10.2\rC\rC\r703\r0.8\r4.5\rC\rF\r652\r1.6\r7.2\rC\rF\r165\r1.4\r7.5\rC\rC\r122\r2.4\r8.4\rC\rC\r353\r7.3\r10.8\rC\rC\r367\r1.4\r8.6\rC\rC\r408\r1.2\r4.9\rC\rF\r161\r3.9\r12.0\rC\rC\r647\r3.4\r12.4\rC\rC\r308\r4.2\r6.7\rC\rG\r362\r3.0\r11.4\rC\rC\r146\r3.6\r8.2\rC\rC\r233\r4.4\r7.9\rC\rC\rOur model does decently well here, as you can see visually above. Below, we compute the training accuracy; remember that model.score() computes accuracy. lr_training_accuracy = logistic_regression_model.score(nba_train[['AST', 'TRB']], nba_train['Pos3']) lr_training_accuracy 0.7964071856287425\rWe can compute the test accuracy as well by looking at nba_test instead of nba_train: lr_test_accuracy = logistic_regression_model.score(nba_test[['AST', 'TRB']], nba_test['Pos3']) lr_test_accuracy 0.6428571428571429\rNow, letâ€™s draw the decision boundary for this logistic regression classifier, and see how the classifier performs on both the training and test data. # just run this cell to save the helper function def plot_decision_boundaries(model, nba_dataset, title=None, ax=None): sns_cmap = ListedColormap(np.array(sns.color_palette())[0:3, :]) xx, yy = np.meshgrid(np.arange(0, 12, 0.02), np.arange(0, 16, 0.02)) Z_string = model.predict(np.c_[xx.ravel(), yy.ravel()]) categories, Z_int = np.unique(Z_string, return_inverse = True) Z_int = Z_int.reshape(xx.shape) if ax is None: plt.figure() ax = plt.gca() ax.contourf(xx, yy, Z_int, cmap = sns_cmap) sns.scatterplot(data = nba_dataset, x = 'AST', y = 'TRB', hue = 'Pos3', ax=ax) if title is not None: ax.set_title(title) # just run this cell plot_decision_boundaries(logistic_regression_model, nba_train, \"Logistic Regression on nba_train\") d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\rwarnings.warn(\r# just run this cell plot_decision_boundaries(logistic_regression_model, nba_test, \"Logistic Regression on nba_test\") d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\rwarnings.warn(\rOur one-vs-rest logistic regression was able to find a linear decision boundary between the three classes. It generally classifies centers as players with a lot of rebounds, forwards as players with a medium number of rebounds and a low number of assists, and guards as players with a low number of rebounds. Note: In practice we would use many more features â€“ we only used 2 here just so that we could visualize the decision boundary. ","date":"2024-08-13","objectID":"/datalab13/:4:2","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Decision Trees ","date":"2024-08-13","objectID":"/datalab13/:5:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 2b Letâ€™s now create a decision tree classifier on the same training data nba_train, and look at the resulting decision boundary. In the following cell, first, use tree.DecisionTreeClassifier (documentation) to fit a model using the same features and response as above, and call this model decision_tree_model. Set the random_state and criterion parameters to 42 and entropy, respectively. Hint: Your code will be mostly the same as the previous part. decision_tree_model = tree.DecisionTreeClassifier(random_state=42, criterion='entropy') decision_tree_model.fit(nba_train[['AST', 'TRB']], nba_train['Pos3']) # logistic_regression_model = LogisticRegression(multi_class='ovr') # logistic_regression_model.fit(nba_train[['AST', 'TRB']], nba_train['Pos3']) grader.check(\"q2b\") q2b passed! ğŸŒŸ ","date":"2024-08-13","objectID":"/datalab13/:5:1","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"[Tutorial] Decision Tree Performance Now, letâ€™s draw the decision boundary for this decision tree classifier, and see how the classifier performs on both the training and test data. # just run this cell plot_decision_boundaries(decision_tree_model, nba_train, \"Decision Tree on nba_train\") d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\rwarnings.warn(\r# just run this cell plot_decision_boundaries(decision_tree_model, nba_test, \"Decision Tree on nba_test\") d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\rwarnings.warn(\rWe compute the training and test accuracies of the decision tree model below. dt_training_accuracy = decision_tree_model.score(nba_train[['AST', 'TRB']], nba_train['Pos3']) dt_test_accuracy = decision_tree_model.score(nba_test[['AST', 'TRB']], nba_test['Pos3']) dt_training_accuracy, dt_test_accuracy (0.9940119760479041, 0.5714285714285714)\r","date":"2024-08-13","objectID":"/datalab13/:5:2","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Random Forests ","date":"2024-08-13","objectID":"/datalab13/:6:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 2c Letâ€™s now create a random forest classifier on the same training data nba_train and look at the resulting decision boundary. In the following cell, use ensemble.RandomForestClassifier (documentation) to fit a model using the same features and response as above, and call this model random_forest_model. Use 20 trees in your random forest classifier; set the random_state and criterion parameters to 42 and entropy, respectively. Hint: Your code for both parts will be mostly the same as the first few parts of this question. Hint: Look at the n_estimators parameter of ensemble.RandomForestClassifier. random_forest_model = ensemble.RandomForestClassifier(n_estimators=20, random_state=42, criterion='entropy') random_forest_model.fit(nba_train[['AST', 'TRB']], nba_train['Pos3']) grader.check(\"q2c\") q2c passed! ğŸ™Œ ","date":"2024-08-13","objectID":"/datalab13/:6:1","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"[Tutorial] Random Forest Performance Now, letâ€™s draw the decision boundary for this random forest classifier, and see how the classifier performs on both the training and test data. # just run this cell plot_decision_boundaries(random_forest_model, nba_train, \"Random Forest on nba_train\") d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\rwarnings.warn(\r# just run this cell plot_decision_boundaries(random_forest_model, nba_test, \"Random Forest on nba_test\") d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\rwarnings.warn(\rWe compute the training and test accuracies of the random forest model below. # just run this cell rf_train_accuracy = random_forest_model.score(nba_train[['AST', 'TRB']], nba_train['Pos3']) rf_test_accuracy = random_forest_model.score(nba_test[['AST', 'TRB']], nba_test['Pos3']) rf_train_accuracy, rf_test_accuracy (0.9760479041916168, 0.6964285714285714)\r","date":"2024-08-13","objectID":"/datalab13/:6:2","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Compare/Contrast How do the three models you created (multiclass one-vs-rest logistic regression, decision tree, random forest) compare to each other?) Decision boundaries: Run the below cell for your convenience. It overlays the decision boundaries for the train and test sets for each of the models you created. # just run this cell fig, axs = plt.subplots(2, 3, figsize=(12, 6)) for j, (model, title) in enumerate([(logistic_regression_model, \"Logistic Regression\"), (decision_tree_model, \"Decision Tree\"), (random_forest_model, \"Random Forest\")]): axs[0, j].set_title(title) for i, nba_dataset in enumerate([nba_train, nba_test]): plot_decision_boundaries(model, nba_dataset, ax=axs[i, j]) # reset leftmost ylabels axs[0, 0].set_ylabel(\"nba_train\\nTRB\") axs[1, 0].set_ylabel(\"nba_test\\nTRB\") fig.tight_layout() d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\rwarnings.warn(\rd:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\rwarnings.warn(\rd:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\rwarnings.warn(\rd:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\rwarnings.warn(\rd:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\rwarnings.warn(\rd:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\rwarnings.warn(\rPerformance Metrics: Run the below cell for your convenience. It summarizes the train and test accuracies for the three models you created. # just run this cell train_accuracy = [lr_training_accuracy, lr_test_accuracy, dt_training_accuracy, dt_test_accuracy, rf_train_accuracy, rf_test_accuracy] index = ['OVR Logistic Regression', 'Decision Tree', 'Random Forest'] df = pd.DataFrame([(lr_training_accuracy, lr_test_accuracy), (dt_training_accuracy, dt_test_accuracy), (rf_train_accuracy, rf_test_accuracy)], columns=['Training Accuracy', 'Test Accuracy'], index=index) df.plot.bar(); plt.legend().remove() # remove legend from plot itself plt.gcf().legend(loc='lower right') # and add legend to bottom of figure \u003cmatplotlib.legend.Legend at 0x21314df30e0\u003e\r","date":"2024-08-13","objectID":"/datalab13/:7:0","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":"Question 2d Looking at the three models, which model performed the best on the training set, and which model performed the best on the test set? How are the training and test accuracy related for the three models, and how do the decision boundaries generated for each of the three models relate to the modelâ€™s performance? DT OVRLR [ungraded] Question 3: Regression Trees In Project 1, we used linear regression to predict housing prices in Cook County, Illinois. However, what would happen if we tried to use a different prediction method? Try fitting a regression tree (also known as a decision tree regressor) to predict housing prices. Hereâ€™s one in sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html What do you notice about the training error and the test error for the decision tree regressor? Is one significantly larger than the other? If so, what methods could we use to make this error lower? Now, try fitting a random forest regressor instead of a single decision tree. What do you notice about the training error and the test error for the random forest, and how does this compare to the training and test error of a single decision tree? see in project 1 Congratulations! You finished the lab! ","date":"2024-08-13","objectID":"/datalab13/:7:1","tags":["Scikit-Learn"],"title":"DATA100-lab13: Decision Trees and Random Forests","uri":"/datalab13/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab14.ipynb\") Ungraded Lab 14: Clustering In this lab you will explore K-Means, Agglomerative Clustering, and Spectral Clustering. Spectral Clustering is out of scope for Spring 2022.ref. Note: This is an ungraded assignment. There is no Gradescope submission for this assignment. As this is a bonus and ungraded assignment, there will also be more limited staff office hours devoted to this ungraded homework. We may prioritize students who have other questions. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn import cluster # more readable exceptions %pip install --quiet iwut %load_ext iwut %wut on Note: you may need to restart the kernel to use updated packages.\rIn the first part of this lab, we work with three different toy datasets, all with different clustering characteristics. In the second part, we explore a real-world dataset from the World Bank. ","date":"2024-08-13","objectID":"/datalab14/:0:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Toy Data 1: Balanced Clusters Let us begin with a toy dataset with three groups that are completely separated with the variables given. There are the same number of points per group and the same variance within each group. # just run this cell np.random.seed(1337) c1 = np.random.normal(size = (25, 2)) c2 = np.array([2, 8]) + np.random.normal(size = (25, 2)) c3 = np.array([8, 4]) + np.random.normal(size = (25, 2)) x1 = np.vstack((c1, c2, c3)) sns.scatterplot(x = x1[:, 0], y = x1[:, 1]); Below, we create a cluster.KMeans object (documentation) which implements the K-Means algorithm. # just run this cell kmeans = cluster.KMeans(n_clusters = 3, random_state = 42).fit(x1) sns.scatterplot(x = x1[:, 0], y = x1[:, 1], hue = kmeans.labels_) sns.scatterplot(x = kmeans.cluster_centers_[:, 0], y = kmeans.cluster_centers_[:, 1], color = 'blue', marker = 'x', s = 300, linewidth = 5); We observe that K-Means is able to accurately pick out the three initial clusters. ","date":"2024-08-13","objectID":"/datalab14/:1:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Question 1: Initial Centers In the previous example, the K-Means algorithm was able to accurately find the three initial clusters. However, changing the starting centers for K-Means can change the final clusters that K-Means gives us. Change the initial centers to the points [0, 1], [1, 1], and [2, 2]; and fit a cluster.KMeans object (documentation) called kmeans_q1 on the toy dataset from the previous example. Keep the random_state parameter as 42 and the n_clusters parameter as 3. Hint: You will need to change the init and n_init parameters in cluster.KMeans. kmeans_q1 = cluster.KMeans(n_clusters = 3, random_state = 42, init=[[0,1],[1,1],[2,2]], n_init=1).fit(x1) grader.check(\"q1\") q1 passed! ğŸ€ Running the K-Means algorithm with these centers gives us a different result from before, and this particular run of K-Means was unable to accurately find the three initial clusters. sns.scatterplot(x = x1[:, 0], y = x1[:, 1], hue = kmeans_q1.labels_) sns.scatterplot(x = kmeans_q1.cluster_centers_[:, 0], y = kmeans_q1.cluster_centers_[:, 1], color = 'blue', marker = 'x', s = 300, linewidth = 5); ","date":"2024-08-13","objectID":"/datalab14/:2:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Toy Data 2: Clusters of Different Sizes Sometimes, K-Means will have a difficult time finding the â€œcorrectâ€ clusters even with ideal starting centers. For example, consider the data below. # just run this cell np.random.seed(1337) c1 = 0.5 * np.random.normal(size = (25, 2)) c2 = np.array([10, 10]) + 3 * np.random.normal(size = (475, 2)) x2 = np.vstack((c1, c2)) sns.scatterplot(x = x2[:, 0], y = x2[:, 1]); There are two groups of different sizes in two different senses: variability (i.e., spread) and number of datapoints. The smaller group has both smaller variability and has fewer datapoints, and the larger of the two groups is more diffuse and populated. ","date":"2024-08-13","objectID":"/datalab14/:3:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Question 2 ","date":"2024-08-13","objectID":"/datalab14/:4:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Question 2a: K-Means Fit a cluster.KMeans object (documentation) called kmeans_q2a on the dataset above with two clusters and a random_state parameter of 42. kmeans_q2a = cluster.KMeans(n_clusters=2, random_state=42).fit(x2) grader.check(\"q2a\") q2a passed! ğŸ€ (For notational simplicity we will call the initial cluster on the bottom left $A$ and the initial cluster on the top right $B$. We will call the bottom left cluster found by K-Means as cluster $a$ and the top right cluster found by K-Means as cluster $b$.) As seen below, K-Means is unable to find the two intial clusters because cluster $A$ includes points from cluster $B$. Recall that K-Means attempts to minimize inertia (æƒ¯æ€§ï¼Ÿ), so it makes sense that points in the bottom left of cluster $B$ would prefer to be in cluster $A$ rather than cluster $B$. If these points were in cluster $B$ instead, then the resulting cluster assignments would have a larger distortion(æ­ªæ›²). # just run this cell sns.scatterplot(x = x2[:, 0], y = x2[:, 1], hue = kmeans_q2a.labels_) sns.scatterplot(x = kmeans_q2a.cluster_centers_[:, 0], y = kmeans_q2a.cluster_centers_[:, 1], color = 'red', marker = 'x', s = 300, linewidth = 5); ","date":"2024-08-13","objectID":"/datalab14/:4:1","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Agglomerative Clustering: The Linkage Criterion It turns out agglomerative clustering works better for this task, as long as we choose the right definition of distance between two clusters. Recall that agglomerative clustering starts with every data point in its own cluster and iteratively joins the two closest clusters until there are $k$ clusters remaining. However, the â€œdistanceâ€ between two clusters is ambiguous. In lecture, we used the maximum distance between a point in the first cluster and a point in the second as this notion of distance, but there are other ways to define the distance between two clusters. Our choice of definition for the distance is sometimes called the â€œlinkage criterion.â€ We will discuss three linkage criteria, each of which is a different definition of â€œdistanceâ€ between two clusters: Complete linkage considers the distance between two clusters as the maximum distance between a point in the first cluster and a point in the second. This is what you will see in Lecture 26. Single linkage considers the distance between two clusters as the minimum distance between a point in the first cluster and a point in the second. Average linkage considers the distance between two clusters as the average distance between a point in the first cluster and a point in the second. Below, we fit a cluster.AgglomerativeClustering object (documentation) called agg_complete on the dataset above with two clusters, using the complete linkage criterion. # just run this cell agg_complete = cluster.AgglomerativeClustering(n_clusters = 2, linkage = 'complete').fit(x2) Below we visualize the results: # just run this cell sns.scatterplot(x = x2[:, 0], y = x2[:, 1], hue = agg_complete.labels_); It looks like complete linkage agglomerative clustering has the same issue as K-Means! The bottom left cluster found by complete linkage agglomerative clustering includes points from the top right cluster. However, we can remedy this by picking a different linkage criterion. ","date":"2024-08-13","objectID":"/datalab14/:4:2","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Question 2b: Agglomerative Clustering Now, use the single linkage criterion to fit a cluster.AgglomerativeClustering object called agg_single on the dataset above with two clusters. agg_single = cluster.AgglomerativeClustering(n_clusters = 2, linkage = 'single').fit(x2) grader.check(\"q2b\") q2b passed! ğŸŒŸ Finally, we see that single linkage agglomerative clustering is able to find the two initial clusters. sns.scatterplot(x = x2[:, 0], y = x2[:, 1], hue = agg_single.labels_); You might be curious why single linkage â€œworksâ€ while complete linkage does not in this scenario; we will leave this as an exercise for students who are interested. ","date":"2024-08-13","objectID":"/datalab14/:4:3","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Toy Data 3: Oddly Shaped Clusters Another example when k-means fails is when the clusters have odd shapes. For example, look at the following dataset. np.random.seed(100) data = np.random.normal(0, 7, size = (1000, 2)) lengths = np.linalg.norm(data, axis = 1, ord = 2) x3 = data[(lengths \u003c 2) | ((lengths \u003e 5) \u0026 (lengths \u003c 7)) | ((lengths \u003e 11) \u0026 (lengths \u003c 15))] sns.scatterplot(x = x3[:, 0], y = x3[:, 1]); Looking at this data, we might say there are 3 clusters, corresponding to each of the 3 concentric circles, with the same center. However, k-means will fail. kmeans_q3 = cluster.KMeans(n_clusters = 3, random_state = 42).fit(x3) sns.scatterplot(x = x3[:, 0], y = x3[:, 1], hue = kmeans_q3.labels_) sns.scatterplot(x = kmeans_q3.cluster_centers_[:, 0], y = kmeans_q3.cluster_centers_[:, 1], color = 'red', marker = 'x', s = 300, linewidth = 5); ","date":"2024-08-13","objectID":"/datalab14/:5:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"(Bonus) Question 3: Spectral Clustering (Note in Spring 2022 we did not go over Spectral Clustering. Spectral Clustering is out of scope for exams.) Letâ€™s try spectral clustering instead. In the cell below, create and fit a cluster.SpectralClustering object (documentation), and assign it to spectral. Use 3 clusters, and make sure you set affinity to \"nearest_neighbors\" and a random_state of 10. Note: Ignore any warnings about the graph not being fully connected. spectral = cluster.SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=10).fit(x3) d:\\miniconda3\\envs\\ds100\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\rwarnings.warn(\rgrader.check(\"q3\") q3 passed! âœ¨ Below, we see that spectral clustering is able to find the three rings, when k-means does not. sns.scatterplot(x = x3[:, 0], y = x3[:, 1], hue = spectral.labels_); ","date":"2024-08-13","objectID":"/datalab14/:6:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"The World Bank Dataset In the previous three questions, we looked at clustering on two dimensional datasets. However, we can easily use clustering on data which have more than two dimensions. For this, let us turn to a World Bank dataset, containing various features for the worldâ€™s countries. This data comes from https://databank.worldbank.org/source/world-development-indicators#. world_bank_data = pd.read_csv(\"world_bank_data.csv\", index_col = 'country') world_bank_data.head(5) Age dependency ratio (% of working-age population)\rAge dependency ratio, old (% of working-age population)\rAge dependency ratio, young (% of working-age population)\rBird species, threatened\rBusiness extent of disclosure index (0=less disclosure to 10=more disclosure)\rContributing family workers, female (% of female employment) (modeled ILO estimate)\rContributing family workers, male (% of male employment) (modeled ILO estimate)\rContributing family workers, total (% of total employment) (modeled ILO estimate)\rCost of business start-up procedures (% of GNI per capita)\rCost of business start-up procedures, female (% of GNI per capita)\r...\rUnemployment, youth total (% of total labor force ages 15-24) (modeled ILO estimate)\rUrban population\rUrban population (% of total population)\rUrban population growth (annual %)\rVulnerable employment, female (% of female employment) (modeled ILO estimate)\rVulnerable employment, male (% of male employment) (modeled ILO estimate)\rVulnerable employment, total (% of total employment) (modeled ILO estimate)\rWage and salaried workers, female (% of female employment) (modeled ILO estimate)\rWage and salaried workers, male (% of male employment) (modeled ILO estimate)\rWage and salaried workers, total (% of total employment) (modeled ILO estimate)\rcountry\rAlgeria\r57.508032\r10.021442\r47.486590\r15.0\r4.0\r2.720000\r1.836\r1.978000\r0.0\r11.8\r...\r29.952999\r30670086.0\r72.629\r2.804996\r24.337001\r27.227001\r26.762000\r73.734001\r68.160004\r69.056000\rAfghanistan\r84.077656\r4.758273\r79.319383\r16.0\r8.0\r71.780998\r9.606\r31.577999\r0.0\r6.4\r...\r2.639000\r9477100.0\r25.495\r3.350383\r95.573997\r85.993001\r89.378998\r4.282000\r13.292000\r10.108000\rAlbania\r45.810037\r20.041214\r25.768823\r8.0\r9.0\r37.987000\r20.795\r28.076000\r0.0\r11.3\r...\r30.979000\r1728969.0\r60.319\r1.317162\r54.663000\r54.994001\r54.854000\r44.320999\r41.542999\r42.720001\rAmerican Samoa\rNaN\rNaN\rNaN\r8.0\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\rNaN\r48339.0\r87.153\r-0.299516\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rAndorra\rNaN\rNaN\rNaN\r3.0\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\rNaN\r67813.0\r88.062\r-0.092859\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r5 rows Ã— 209 columns There are some missing values. For the sake of convenience and of keeping the lab short, we will fill them all with zeros. world_bank_data = world_bank_data.fillna(0) Like with PCA, it sometimes makes sense to center and scale our data so that features with higher variance donâ€™t dominate the analysis. For example, without standardization, statistics like population will completely dominate features like â€œpercent of total population that live in urban areas.â€ This is because the range of populations is on the order of billions, whereas percentages are always between 0 and 100. The ultimate effect is that many of our columns are not really considered by our clustering algorithm. ","date":"2024-08-13","objectID":"/datalab14/:7:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":"Question 4 Below, fit a cluster.KMeans object called kmeans_q4 with four clusters and a random_state parameter of 42. Make sure you should use a centered and scaled version of the world bank data. By centered and scaled we mean that the mean in each column should be zero and the variance should be 1. world_bank_data = (world_bank_data - world_bank_data.mean(axis=0)) / world_bank_data.std(axis=0) kmeans_q4 = cluster.KMeans(n_clusters = 4, random_state = 42).fit(world_bank_data) sorted(np.unique(kmeans_q4.labels_, return_counts = True)[1]) [np.int64(3), np.int64(23), np.int64(85), np.int64(106)]\rbelow is very interesting, note that in ref. all tests passed. â€”\u003e maybe Cluster itself changed? grader.check(\"q4\") q4 results: q4 - 1 result: âŒ Test case failed\rTrying:\rsorted(np.unique(kmeans_q4.labels_, return_counts = True)[1]) == [3, 25, 90, 99]\rExpecting:\rTrue\r**********************************************************************\rLine 1, in q4 0\rFailed example:\rsorted(np.unique(kmeans_q4.labels_, return_counts = True)[1]) == [3, 25, 90, 99]\rExpected:\rTrue\rGot:\rFalse\rLooking at these new clusters, we see that they seem to correspond to: 0: Very small countries. 1: Developed countries. 2: Less developed countries. 3: Huge countries. # just run this cell labeled_world_bank_data_q4 = pd.Series(kmeans_q4.labels_, name = \"cluster\", index = world_bank_data.index).to_frame() for c in range(4): print(f\"\u003e\u003e\u003e Cluster {c}:\") print(list(labeled_world_bank_data_q4.query(f'cluster == {c}').index)) print() \u003e\u003e\u003e Cluster 0:\r['Afghanistan', 'Angola', 'Bangladesh', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Botswana', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Central African Republic', 'Chad', 'Comoros', 'Congo, Dem. Rep.', 'Congo, Rep.', \"Cote d'Ivoire\", 'Djibouti', 'Ecuador', 'Egypt, Arab Rep.', 'Equatorial Guinea', 'Eritrea', 'Eswatini', 'Ethiopia', 'Fiji', 'Gabon', 'Gambia, The', 'Ghana', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Indonesia', 'Iraq', 'Kenya', 'Kiribati', 'Kyrgyz Republic', 'Lao PDR', 'Lesotho', 'Liberia', 'Madagascar', 'Malawi', 'Mali', 'Mauritania', 'Micronesia, Fed. Sts.', 'Mongolia', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Nicaragua', 'Niger', 'Nigeria', 'Pakistan', 'Papua New Guinea', 'Paraguay', 'Philippines', 'Rwanda', 'Samoa', 'Sao Tome and Principe', 'Senegal', 'Sierra Leone', 'Solomon Islands', 'Somalia', 'South Sudan', 'Sudan', 'Syrian Arab Republic', 'Tajikistan', 'Tanzania', 'Timor-Leste', 'Togo', 'Tonga', 'Uganda', 'Uzbekistan', 'Vanuatu', 'Venezuela, RB', 'Vietnam', 'West Bank and Gaza', 'Yemen, Rep.', 'Zambia', 'Zimbabwe']\r\u003e\u003e\u003e Cluster 1:\r['China', 'India', 'United States']\r\u003e\u003e\u003e Cluster 2:\r['American Samoa', 'Andorra', 'Bermuda', 'British Virgin Islands', 'Cayman Islands', 'Dominica', 'Faroe Islands', 'Gibraltar', 'Greenland', 'Isle of Man', 'Kosovo', 'Liechtenstein', 'Marshall Islands', 'Monaco', 'Nauru', 'Northern Mariana Islands', 'Palau', 'San Marino', 'Sint Maarten (Dutch part)', 'St. Kitts and Nevis', 'St. Martin (French part)', 'Turks and Caicos Islands', 'Tuvalu']\r\u003e\u003e\u003e Cluster 3:\r['Algeria', 'Albania', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas, The', 'Bahrain', 'Barbados', 'Belarus', 'Belgium', 'Bosnia and Herzegovina', 'Brazil', 'Brunei Darussalam', 'Bulgaria', 'Cabo Verde', 'Canada', 'Channel Islands', 'Chile', 'Colombia', 'Costa Rica', 'Croatia', 'Cuba', 'Curacao', 'Cyprus', 'Czech Republic', 'Denmark', 'Dominican Republic', 'El Salvador', 'Estonia', 'Finland', 'France', 'French Polynesia', 'Georgia', 'Germany', 'Greece', 'Grenada', 'Guam', 'Hong Kong SAR, China', 'Hungary', 'Iceland', 'Iran, Islamic Rep.', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Korea, Dem. Peopleâ€™s Rep.', 'Korea, Rep.', 'Kuwait', 'Latvia', 'Lebanon', 'Libya', 'Lithuania', 'Luxembourg', 'Macao SAR, China', 'Malaysia', 'Maldives', 'Malta', 'Mauritius', 'Mexico', 'Moldova', 'Montenegro', 'Netherlan","date":"2024-08-13","objectID":"/datalab14/:8:0","tags":["Scikit-Learn"],"title":"DATA100-lab14: Clustering","uri":"/datalab14/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab11.ipynb\") Lab 11: Principal Component Analysis In this lab assignment, we will walk through two examples that use Principal Component Analysis (PCA): one involving a dataset of iris plants, and another involving an artificial â€œsurfboardâ€ 3D dataset. # Run this cell to set up your notebook from sklearn.datasets import load_iris import matplotlib.pyplot as plt import numpy as np import pandas as pd import plotly.express as px import seaborn as sns plt.style.use('fivethirtyeight') # Use plt.style.available to see more styles sns.set() sns.set_context(\"talk\") %matplotlib inline In lecture we discussed how Principal Component Analysis (PCA) can be used for dimensionality reduction. Specifically, given a high dimensional dataset, PCA allows us to: Understand the rank of the data. If $k$ principal components capture almost all of the variance, then the data is roughly rank $k$. Create 2D scatterplots of the data. Such plots are a rank 2 representation of our data, and allow us to visually identify clusters of similar observations. In this lab, youâ€™ll learn how to perform PCA using the np.linalg package (Part 1), and youâ€™ll also build a geometric intuition of PCA to help you understand its strengths (Part 2). We work with low-dimensional datasets for now to focus on the basics; in the homework, youâ€™ll explore how PCA works on a high-dimensional dataset. Part 1: The Iris Dataset To begin, run the following cell to load the dataset into this notebook. iris_features will contain a numpy array of 4 attributes for 150 different plants (shape 150 x 4). iris_target will contain the class of each plant. There are 3 classes of plants in the dataset: Iris-Setosa, Iris-Versicolour, and Iris-Virginica. The class names will be stored in iris_target_names. iris_feature_names will be a list of 4 names, one for each attribute in iris_features. # just run this cell from sklearn.datasets import load_iris iris_data = load_iris() # Loading the dataset # Unpacking the data into arrays iris_features = iris_data['data'] iris_target = iris_data['target'] iris_feature_names = iris_data['feature_names'] iris_target_names = iris_data['target_names'] # Convert iris_target to string labels instead of int labels currently (0, 1, 2) for the classes iris_target = iris_target_names[iris_target] Letâ€™s explore the data by creating a scatter matrix of our iris features. To do this, weâ€™ll create 2D scatter plots for every possible pair of our four features. This should result in six total scatter plots in our scatter matrix with the classes labeled in distinct colors for each plot. # just run this cell fig = plt.figure(figsize=(14, 10)) plt.suptitle(\"Scatter Matrix of Iris Features\", fontsize=20) plt.subplots_adjust(wspace=0.3, hspace=0.3) for i in range(1, 4): for j in range(i): plot_index = 3*j + i plt.subplot(3, 3, plot_index) sns.scatterplot(x=iris_features[:, i], y=iris_features[:, j], hue=iris_target, legend=(plot_index == 1)) plt.xlabel(iris_feature_names[i]) plt.ylabel(iris_feature_names[j]) if plot_index == 1: plt.legend().remove() fig.legend(loc='lower left') # same legend for all subplots fig.tight_layout() plt.show() ","date":"2024-08-13","objectID":"/datalab11/:0:0","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 1: Standardization and SVD ","date":"2024-08-13","objectID":"/datalab11/:1:0","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 1a To apply PCA, we will first need to center the data so that the mean of each feature is 1. We could go further and create standardized features, where we scale features such that they are centered with standard deviation 1. (Weâ€™ll explore simple centered data in Part 2.) Compute the columnwise mean of iris_features in the cell below and store it in iris_mean, and compute the columnwise standard deviation of iris_features and store it in iris_std. Each should be a numpy array of 4 means, 1 for each feature. Then, subtract iris_mean from iris_features and divide by iris_std, and finally, save the result in iris_standardized. Hints: Use np.mean (documentation) or np.average (documentation) to compute iris_mean, and pay attention to the axis argument. Same for np.std (documentation). åˆ¶å®šå¥½axiså‚æ•°å¾ˆé‡è¦!Because PCA will capture the bigger stddev features, we want to standardize them or will lead to artificial error! If you are confused about how numpy deals with arithmetic operations between arrays of different shapes, see this note about broadcasting for explanations/examples. å¹¿æ’­æœºåˆ¶! iris_mean = np.mean(iris_features, axis=0) iris_std = np.std(iris_features, axis=0) iris_standardized = (iris_features - iris_mean) / iris_std iris_mean, iris_std (array([5.84333333, 3.05733333, 3.758 , 1.19933333]),\rarray([0.82530129, 0.43441097, 1.75940407, 0.75969263]))\rgrader.check(\"q1a\") ","date":"2024-08-13","objectID":"/datalab11/:1:1","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 1b As you may recall from lecture, PCA is a specific application of the singular value decomposition (SVD) for matrices. In the following cell, letâ€™s use the np.linalg.svd function (documentation) to compute the SVD of our iris_standardized matrix. Store the left singular vectors $U$, singular values $\\Sigma$, and (transposed) right singular vectors $V^T$ in u, s, and vt, respectively. Set the full_matrices argument of np.linalg.svd to False. Notice: PCA is a application of SVD on centered data matrix. ğŸ¤” u1, s1, vt1 = np.linalg.svd(iris_standardized, full_matrices=False) u1.shape, s1, vt1.shape ((150, 4), array([20.92306556, 11.7091661 , 4.69185798, 1.76273239]), (4, 4))\rgrader.check(\"q1b\") ","date":"2024-08-13","objectID":"/datalab11/:1:2","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 1c What can we learn from the singular values in s? As discussed in lecture, the total variance of the data is also equal to the sum of the squares of the singular values divided by the number of data points, that is: $$\\text{Var}(X) = \\frac{\\sum_{i=1}^d{\\sigma_i^2}}{N} = \\sum_{i=1}^d \\frac{\\sigma_i^2}{N}$$ where for data $X$ with $N$ datapoints and $d$ features, $\\sigma_i$ is the singular value corresponding to the $i$-th principal component, and $\\text{Var}(X)$ is the total variance of the data. The right-hand side implies that the expression $\\sigma_i^2/N$ is the amount of variance captured by the $i$-th principal component. Compute the total variance of our data below by summing the square of each singular value in s and dividing the result by the total number of data points. Store the result in the variable iris_total_variance. iris_total_variance = np.sum(s1**2) / len(iris_standardized) print(\"iris_total_variance: {:.3f} should approximately equal the sum of the feature variances: {:.3f}\" .format(iris_total_variance, np.sum(np.var(iris_standardized, axis=0)))) iris_total_variance: 4.000 should approximately equal the sum of the feature variances: 4.000\rgrader.check(\"q1c\") As you can see, iris_total_variance is equal to the sum of the standardized feature variances. Since our features are standardized (i.e., have variance 1), iris_total_variance is equal to the number of original features. ","date":"2024-08-13","objectID":"/datalab11/:1:3","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 2 ","date":"2024-08-13","objectID":"/datalab11/:2:0","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 2a Letâ€™s now use only the first two principal components to see what a 2D version of our iris data looks like. First, construct the 2D version of the iris data by multiplying our iris_standardized array with the first two right singular vectors in $V$. Because the first two right singular vectors are directions for the first two principal components, this will project the iris data down from a 4D subspace to a 2D subspace. Hints: To matrix-multiply two numpy arrays, use @ or np.dot. In case youâ€™re interested, the matmul documentation contrasts the two methods. Note that in Question 1b, you computed vt (SVD decomposition is $U\\Sigma V^T$). The first two right singular vectors in $V$ will be the two rows of vt, transposed to be column vectors instead of row vectors. Since we want to obtain a 2D version of our iris dataset, the shape of iris_2d should be (150, 2). iris_2d = iris_standardized @ vt1[0:2, :].T np.sum(iris_2d[:, 0]) np.float64(-2.5579538487363607e-13)\rgrader.check(\"q2a\") Now, run the cell below to create the scatter plot of our 2D version of the iris data, iris_2d. # just run this cell plt.figure(figsize = (9, 6)) plt.title(\"PC2 vs. PC1 for Iris Data\") plt.xlabel(\"Iris PC1\") plt.ylabel(\"Iris PC2\") sns.scatterplot(x = iris_2d[:, 0], y = iris_2d[:, 1], hue = iris_target); ","date":"2024-08-13","objectID":"/datalab11/:2:1","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 2b What do you observe about the plot above? If you were given a point in the subspace defined by PC1 and PC2, how well would you be able to classify the point as one of the three Iris types? ","date":"2024-08-13","objectID":"/datalab11/:2:2","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 2c What proportion of the total variance is accounted for when we project the iris data down to two dimensions? Compute this quantity in the cell below by dividing the variance captured by the first two singular values (also known as component scores) in s by the iris_total_variance you calculated previously. Store the result in iris_2d_variance. iris_2d_variance = np.sum(s1[:2]**2) / len(iris_standardized) / iris_total_variance iris_2d_variance np.float64(0.9581320720000164)\rgrader.check(\"q2c\") Most of the variance in the data is explained by the two-dimensional projection! ","date":"2024-08-13","objectID":"/datalab11/:2:3","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 3 As a last step, we will create a scree plot to visualize the weight of each principal component. In the cell below, create a scree plot by creating a line plot of the component scores (variance captured by each principal component) vs. the principal component number (1st, 2nd, 3rd, or 4th). Your graph should match the image below: Hint: Be sure to label your axes appropriately! You may find plt.xticks() (documentation) helpful for formatting. # your plot here plt.xticks([1, 2, 3, 4]) plt.xlabel(\"Principal Component\") plt.ylabel(\"Variance (Component Scores)\") plt.title(\"Scree Plot of Iris Principle Components\") plt.plot([1,2,3,4], np.square(s1) / len(iris_standardized)); Part 2: PCA on 3D Data In Part 2, our goal is to see visually how PCA is simply the process of rotating the coordinate axes of our data. The code below reads in a 3D dataset. We have named the DataFrame surfboard because the data resembles a surfboard when plotted in 3D space. # just run this cell surfboard = pd.read_csv(\"data/data3d.csv\") surfboard.head(5) x\ry\rz\r0\r0.005605\r2.298191\r1.746604\r1\r-1.093255\r2.457522\r0.170309\r2\r0.060946\r0.473669\r-0.003543\r3\r-1.761945\r2.151108\r3.132426\r4\r1.950637\r-0.194469\r-2.101949\r","date":"2024-08-13","objectID":"/datalab11/:3:0","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"[Tutorial 1] Visualize the Data The cell below will allow you to view the data as a 3D scatterplot. Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure. You should see that the data is an ellipsoid that looks roughly like a surfboard or a hashbrown patty. That is, it is pretty long in one direction, pretty wide in another direction, and relatively thin along its third dimension. We can think of these as the â€œlengthâ€, â€œwidthâ€, and â€œthicknessâ€ of the surfboard data. Observe that the surfboard is not aligned with the x/y/z axes. If you get an error that your browser does not support webgl, you may need to restart your kernel and/or browser. # just run this cell fig = px.scatter_3d(surfboard, x='x', y='y', z='z', range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10]) fig.show() Visualize the Data (Colorized) To give the figure a little more visual pop, the following cell does the same plot, but also assigns a pre-determined color value (that weâ€™ve arbitrarily chosen) to each point. These colors do not mean anything important; theyâ€™re simply there as a visual aid. You might find it useful to use the colorize_surfboard_data method later in this lab. # just run this cell (the colorized version of previous cell) def colorize_surfboard_data(df): colors = pd.read_csv(\"data/surfboard_colors.csv\", header = None).values df_copy = df.copy() df_copy.insert(loc = 3, column = \"color\", value = colors) return df_copy fig = px.scatter_3d(colorize_surfboard_data(surfboard), x='x', y='y', z='z', range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], color = \"color\", color_continuous_scale = 'RdBu') fig.show() ","date":"2024-08-13","objectID":"/datalab11/:3:1","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 4: Centering and SVD ","date":"2024-08-13","objectID":"/datalab11/:4:0","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 4a In Part 1, we standardized the Iris data prior to performing SVD, i.e., we made features zero-mean and unit-variance. In this part, weâ€™ll try just centering our data so that each feature is zero-mean and variance is unchanged. Compute the columnwise mean of surfboard in the cell below, and store the result in surfboard_mean. You can choose to make surfboard_mean a numpy array or a series, whichever is more convenient for you. Regardless of what data type you use, surfboard_mean should have 3 means: 1 for each attribute, with the x coordinate first, then y, then z. Then, subtract surfboard_mean from surfboard, and save the result in surfboard_centered. The order of the columns in surfboard_centered should be x, then y, then z. surfboard_mean = np.mean(surfboard, axis=0) surfboard_centered = surfboard - surfboard_mean grader.check(\"q4a\") ","date":"2024-08-13","objectID":"/datalab11/:4:1","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 4b In the following cell, compute the singular value decomposition (SVD) of surfboard_centered as $U\\Sigma V^T$, and store the left singular vectors $U$, singular values $\\Sigma$, and (transposed) right singular vectors $V^T$ in u, s, and vt, respectively. Your code should be very similar to Part 1, Question 1b up above. u2, s2, vt2 = np.linalg.svd(surfboard_centered, full_matrices=False) u2, s2, vt2 (array([[-0.02551985, 0.02108339, 0.03408865],\r[-0.02103979, 0.0259219 , -0.05432967],\r[-0.00283413, 0.00809889, -0.00204459],\r...,\r[ 0.01536972, 0.00483066, -0.05673824],\r[-0.00917593, -0.0345672 , -0.03491181],\r[-0.01701236, -0.02743128, 0.01966704]]),\rarray([103.76854043, 40.38357469, 21.04757518]),\rarray([[ 0.38544534, -0.67267377, -0.63161847],\r[ 0.5457216 , 0.7181477 , -0.43180066],\r[ 0.74405633, -0.17825229, 0.64389929]]))\rgrader.check(\"q4b\") ","date":"2024-08-13","objectID":"/datalab11/:4:2","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 4c: Total Variance In Part 1 Question 1c, we considered standardized features (each with unit variance), whose total variance was simply the count of features. Now, weâ€™ll show that the same relationship holds between singular values s and the variance of our (unstandardized) data. In the cell below, compute the total variance as the sum of the squares of the singular values $\\sigma_i$ divided by the number of datapoints $N$. Hereâ€™s that formula again from Question 1c: $$\\text{Var}(X) = \\frac{\\sum_{i=1}^d{\\sigma_i^2}}{N} = \\sum_{i=1}^d \\frac{\\sigma_i^2}{N}$$ total_variance_computed_from_singular_values = np.sum(s2**2) / len(surfboard_centered) total_variance_computed_from_singular_values np.float64(12.841743509780112)\rgrader.check(\"q4c\") Your total_variance_computed_from_singular_values result should be very close to the total variance of the original surfboard data: # run this cell np.var(surfboard, axis=0) x 2.330704\ry 5.727527\rz 4.783513\rdtype: float64\rThe total variance of our dataset is given by the sum of these numbers. # run this cell total_variance_computed_from_surfboard = sum(np.var(surfboard, axis=0)) total_variance_computed_from_surfboard 12.841743509780109\rNote: The variances are the same for both surfboard_centered and surfboard (why?), so we show only one to avoid redundancy. ","date":"2024-08-13","objectID":"/datalab11/:4:3","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 4d: Variance Explained by First Principal Component In the cell below, set variance_explained_by_1st_pc to the proportion of the total variance explained by the 1st principal component. Your answer should be a number between 0 and 1. variance_explained_by_1st_pc = np.sum(s2[0:1]**2)/len(surfboard_centered) / total_variance_computed_from_surfboard variance_explained_by_1st_pc np.float64(0.8385084140449135)\rgrader.check(\"q4d\") We can also create a scree plot that shows the proportion of variance explained by all of our principal components, ordered from most to least. You already constructed a scree plot for the Iris data, so weâ€™ll leave the surfboard scree plot for you to do on your own time. Instaed, letâ€™s try to visualize why PCA is simply a rotation of the coordinate axes (i.e., features) of our data. ","date":"2024-08-13","objectID":"/datalab11/:4:4","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Question 5: V as a Rotation Matrix In lecture, we saw that the first column of $XV$ contained the first principal component values for each observation, the second column of $XV$ contained the second principal component values for each observation, and so forth. Letâ€™s give this matrix a name: $P = XV$ is sometimes known as the â€œprincipal component matrixâ€. Compute the $P$ matrix for the surfboard dataset and store it in the variable surfboard_pcs. Hint: What does $X$ represent here: surfboard or surfboard_centered? Why? surfboard_pcs = surfboard_centered @ vt2.T grader.check(\"q1e\") ","date":"2024-08-13","objectID":"/datalab11/:5:0","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"[Tutorial 2] Visualizing the Principal Component Matrix In some sense, we can think of $P$ as an output of the PCA procedure. $P$ is a rotation of the data such that the data will now appear â€œaxis alignedâ€. Specifically, for a 3d dataset, if we plot PC1, PC2, and PC3 along the x, y, and z axes of our plot, then the greatest amount of variation happens along the x-axis, the second greatest amount along the y-axis, and the smallest amount along the z-axis. To visualize this, run the cell below, which will show our data now projected onto the principal component space. Compare with your original figure (from Tutorial 1 in this part), and observe that the data is exactly the sameâ€”only it is now rotated. # just run this cell surfboard_pcs = surfboard_pcs.rename(columns = {0: \"pc1\", 1: \"pc2\", 2: \"pc3\"}) fig = px.scatter_3d(colorize_surfboard_data(surfboard_pcs), x='pc1', y='pc2', z='pc3', range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], color = 'color', color_continuous_scale = 'RdBu'); fig.show(); We can also create a 2D scatterplot of our surfboard data as well. Note that the resulting is just the 3D plot as viewed from directly â€œoverheadâ€. # just run this cell fig = plt.figure(figsize=(10, 6)) plt.subplot(1, 2, 1) # just run this cell sns.scatterplot(data = colorize_surfboard_data(surfboard_pcs), x = 'pc1', y = 'pc2', hue = \"color\", palette = \"RdBu\", legend = False) plt.xlim(-10, 10); plt.ylim(-10, 10); plt.title(\"Top-Down View of $P$\") plt.show() ","date":"2024-08-13","objectID":"/datalab11/:5:1","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":"Part 2 Summary Above, we saw that the principal component matrix $P$ is simply the original data rotated in space so that it appears axis-aligned. Whenever we do a 2D scatter plot of only the first 2 columns of $P$, we are simply looking at the data from â€œaboveâ€, i.e. so that the 3rd (or higher) PC is invisible to us. Congratulations! You finished the lab! ","date":"2024-08-13","objectID":"/datalab11/:5:2","tags":["Numpy","Scikit-Learn"],"title":"DATA100-lab11: Principal Component Analysis","uri":"/datalab11/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab12.ipynb\") Lab 12: Logistic Regression # Run this cell to set up your notebook import numpy as np import pandas as pd import sklearn import sklearn.datasets import matplotlib.pyplot as plt import seaborn as sns import plotly.offline as py import plotly.graph_objs as go import plotly.figure_factory as ff %matplotlib inline sns.set() sns.set_context(\"talk\") ","date":"2024-08-13","objectID":"/datalab12/:0:0","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Objectives In Questions 1 and 2 of this lab, we will manually construct the logistic regression model and minimize cross-entropy loss using scipy.minimize. This structure mirrors the linear regression labs from earlier in the semester and lets us dive deep into how logistic regression works. Lastly, in Question 3 we introduce the sklearn.linear_model.LogisticRegression module that you would use in practice, and we explore performance metrics for classification. We will explore a breast cancer dataset from the University of Wisconsin (source). This dataset can be loaded using the sklearn.datasets.load_breast_cancer() method. # just run this cell data = sklearn.datasets.load_breast_cancer() # data is actually a dictionnary print(data.keys()) print(data.DESCR) dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\r.. _breast_cancer_dataset:\rBreast cancer wisconsin (diagnostic) dataset\r--------------------------------------------\r**Data Set Characteristics:**\r:Number of Instances: 569\r:Number of Attributes: 30 numeric, predictive attributes and the class\r:Attribute Information:\r- radius (mean of distances from center to points on the perimeter)\r- texture (standard deviation of gray-scale values)\r- perimeter\r- area\r- smoothness (local variation in radius lengths)\r- compactness (perimeter^2 / area - 1.0)\r- concavity (severity of concave portions of the contour)\r- concave points (number of concave portions of the contour)\r- symmetry\r- fractal dimension (\"coastline approximation\" - 1)\rThe mean, standard error, and \"worst\" or largest (mean of the three\rworst/largest values) of these features were computed for each image,\rresulting in 30 features. For instance, field 0 is Mean Radius, field\r10 is Radius SE, field 20 is Worst Radius.\r- class:\r- WDBC-Malignant\r- WDBC-Benign\r:Summary Statistics:\r===================================== ====== ======\rMin Max\r===================================== ====== ======\rradius (mean): 6.981 28.11\rtexture (mean): 9.71 39.28\rperimeter (mean): 43.79 188.5\rarea (mean): 143.5 2501.0\rsmoothness (mean): 0.053 0.163\rcompactness (mean): 0.019 0.345\rconcavity (mean): 0.0 0.427\rconcave points (mean): 0.0 0.201\rsymmetry (mean): 0.106 0.304\rfractal dimension (mean): 0.05 0.097\rradius (standard error): 0.112 2.873\rtexture (standard error): 0.36 4.885\rperimeter (standard error): 0.757 21.98\rarea (standard error): 6.802 542.2\rsmoothness (standard error): 0.002 0.031\rcompactness (standard error): 0.002 0.135\rconcavity (standard error): 0.0 0.396\rconcave points (standard error): 0.0 0.053\rsymmetry (standard error): 0.008 0.079\rfractal dimension (standard error): 0.001 0.03\rradius (worst): 7.93 36.04\rtexture (worst): 12.02 49.54\rperimeter (worst): 50.41 251.2\rarea (worst): 185.2 4254.0\rsmoothness (worst): 0.071 0.223\rcompactness (worst): 0.027 1.058\rconcavity (worst): 0.0 1.252\rconcave points (worst): 0.0 0.291\rsymmetry (worst): 0.156 0.664\rfractal dimension (worst): 0.055 0.208\r===================================== ====== ======\r:Missing Attribute Values: None\r:Class Distribution: 212 - Malignant, 357 - Benign\r:Creator: Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\r:Donor: Nick Street\r:Date: November, 1995\rThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\rhttps://goo.gl/U2Uwz2\rFeatures are computed from a digitized image of a fine needle\raspirate (FNA) of a breast mass. They describe\rcharacteristics of the cell nuclei present in the image.\rSeparating plane described above was obtained using\rMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\rConstruction Via Linear Programming.\" Proceedings of the 4th\rMidwest Artificial Intelligence and Cognitive Science Society,\rpp. 97-101, 1992], a classification method which uses linear\rprogramming to construct a decision tree. Relevant features\rwere selected using an exhaustive search in the space of 1-4\rfeatures and 1-3 separating planes.\rThe actual linear program used to obtain the s","date":"2024-08-13","objectID":"/datalab12/:1:0","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 1 ","date":"2024-08-13","objectID":"/datalab12/:2:0","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 1a Using the above definition for $\\mathbb{X}$, we can also construct a matrix representation of our Logistic Regression model, just like we did for OLS. Noting that $\\theta = (\\theta_0, \\theta_1, \\dots, \\theta_p$), the vector $\\hat{\\mathbb{Y}}$ is: $$\\Large \\hat{\\mathbb{Y}} = \\sigma(\\mathbb{X} \\theta) $$ Then the $i$-th element of $\\hat{\\mathbb{Y}}$ is the probability that the $i$-th observation belongs to class 1, given the feature vector is the $i$-th row of design matrix $\\mathbb{X}$ and the parameter vector is $\\theta$. Below, implement the lr_model function to evaluate this expression. You should use @ or np.dot. def sigmoid(z): \"\"\" The sigmoid function, defined for you \"\"\" return 1 / (1 + np.exp(-z)) def lr_model(theta, X): \"\"\" Return the logistic regression model as defined above. You should not need to use a for loop; use @ or np.dot. Args: theta: The model parameters. Dimension (p+1,). X: The design matrix. Dimension (n, p+1). Return: Probabilities that Y = 1 for each datapoint. Dimension (n,). \"\"\" return sigmoid(X @ theta) grader.check(\"q1a\") ","date":"2024-08-13","objectID":"/datalab12/:2:1","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 1b: Compute Empirical Risk Now letâ€™s try to analyze the cross-entropy loss from logistic regression. Suppose for a single observation, we predict probability $p$ that the true response $y$ is in class 1 (otherwise the prediction is 0 with probability $1 - p$). The cross-entropy loss is: $$ - \\left( y \\log(p) + (1 - y) \\log(1 - p) \\right)$$ For the logistic regression model, the empirical risk is therefore defined as the average cross-entropy loss across all $n$ datapoints: $$\\Large R(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n \\left( y_i \\log(\\sigma(X_i^T \\theta)) + (1 - y_i) \\log(1 - \\sigma(X_i^T \\theta)) \\right) $$ Where $y_i$ is the $i-$th response in our dataset, $\\theta$ are the parameters of our model, $X_i$ is the iâ€™th row of our design matrix $\\mathbb{X}$, and $\\sigma(X_i^T \\theta)$ is the probability that the response is 1 given input $X_i$. Note: In this class, when performing linear algebra operations we interpret both rows and columns as column vectors. So if we wish to calculate the dot product between row $X_i$ and a vector $v$, we write $X_i^Tv$. Below, implement the function lr_loss that computes empirical risk over the dataset. Feel free to use functions defined in the previous part. def lr_avg_loss(theta, X, Y): ''' Compute the average cross entropy loss using X, Y, and theta. You should not need to use a for loop. Args: theta: The model parameters. Dimension (p+1,) X: The design matrix. Dimension (n, p+1). Y: The label. Dimension (n,). Return: The average cross entropy loss. ''' n = X.shape[0] return -1/n * np.sum(Y * np.log(sigmoid(X @ theta)) + (1-Y) * np.log(1-sigmoid(X @ theta))) grader.check(\"q1b\") Below is a plot showing the average training cross-entropy loss for various values of $\\theta_0$ and $\\theta_1$ (respectively x and y in the plot). # just run this cell with np.errstate(invalid='ignore', divide='ignore'): uvalues = np.linspace(-8,8,70) vvalues = np.linspace(-5,5,70) (u,v) = np.meshgrid(uvalues, vvalues) thetas = np.vstack((u.flatten(),v.flatten())) lr_avg_loss_values = np.array([lr_avg_loss(t, X_intercept_train, Y_train) for t in thetas.T]) lr_loss_surface = go.Surface(name=\"Logistic Regression Loss\", x=u, y=v, z=np.reshape(lr_avg_loss_values,(len(uvalues), len(vvalues))), contours=dict(z=dict(show=True, color=\"gray\", project=dict(z=True))) ) fig = go.Figure(data=[lr_loss_surface]) fig.update_layout( scene = dict( xaxis_title='theta_0', yaxis_title='theta_1', zaxis_title='Loss'), width=700, margin=dict(r=20, l=10, b=10, t=10)) py.iplot(fig) with np.errstate(invalid='ignore', divide='ignore'): uvalues = np.linspace(-8,8,70) vvalues = np.linspace(-5,5,70) (u,v) = np.meshgrid(uvalues, vvalues) thetas = np.vstack((u.flatten(),v.flatten())) lr_loss_values = np.array([lr_avg_loss(t, X_intercept_train, Y_train) for t in thetas.T]) lr_loss_surface = go.Surface(name=\"Logistic Regression Loss\", x=u, y=v, z=np.reshape(lr_loss_values,(len(uvalues), len(vvalues))), contours=dict(z=dict(show=True, color=\"gray\", project=dict(z=True))) ) fig = go.Figure(data=[lr_loss_surface]) # fig.update_layout( # scene = dict( # xaxis_title='theta_1', # yaxis_title='theta_2', # zaxis_title='Loss'), # width=700, # margin=dict(r=20, l=10, b=10, t=10)) py.iplot(fig) ","date":"2024-08-13","objectID":"/datalab12/:2:2","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 1c Describe one interesting observation about the loss plot above. Part 2: Fit and Predict ","date":"2024-08-13","objectID":"/datalab12/:2:3","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"[Tutorial] scipy.optimize.minimize The next two cells call the minimize function from scipy on the lr_avg_loss function you defined in the previous part. We pass in the training data to args (documentation) to find the theta_hat that minimizes the average cross-entropy loss over the train set. # just run this cell from scipy.optimize import minimize min_result = minimize(lr_avg_loss, x0=np.zeros(X_intercept_train.shape[1]), args=(X_intercept_train, Y_train)) # args: Extra arguments passed to the objective function and its derivatives (fun, jac and hess functions). min_result message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: 0.3123767645005891\rx: [-1.387e+01 9.372e-01]\rnit: 16\rjac: [-4.210e-07 -7.335e-06]\rhess_inv: [[ 7.478e+02 -5.212e+01]\r[-5.212e+01 3.683e+00]]\rnfev: 57\rnjev: 19\r# just run this cell theta_hat = min_result['x'] theta_hat array([-13.87178941, 0.93723937])\rBecause our design matrix $\\mathbb{X}$ leads with a column of all ones, theta_hat has two elements: $\\hat{\\theta}_0$ is the estimate of the intercept/bias term, and $\\hat{\\theta}_1$ is the estimate of the slope of our single feature. Itâ€™s time for a recap: For logistic regression with parameter $\\theta$, $P(Y = 1 | x) = \\sigma(x^T \\theta)$, where $\\sigma$ is the sigmoid function and $x$ is a feature vector. Therefore $\\sigma(x^T \\theta)$ is the Bernoulli probability that the response is 1 given the feature is $x$ Otherwise the response is 0 with probability $P(Y = 0 | x) = 1 - \\sigma(x^T \\theta)$. The $\\hat{\\theta}$ that minimizes average log-entropy loss of our training data also maximizes the likelihood of observing the training data according to the logistic regression model (check out lecture for more details). The main takeaway is that logistic regression models probabilities of classifying datapoints as 1 or 0. Next, we use this takeaway to implement model predictions. ","date":"2024-08-13","objectID":"/datalab12/:2:4","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 2 Using the theta_hat estimate above, we can construct a decision rule for classifying a datapoint with observation $x$. Let $P(Y = 1 | x) = \\sigma(x^T \\hat{\\theta})$: $$\\Large \\text{classify}(x) = \\begin{cases} 1, \u0026 P(Y = 1 | x) \\geq 0.5 \\ 0, \u0026 P(Y = 1 | x) \u003c 0.5 \\end{cases}$$ This decision rule has a decision threshold $T = 0.5$. This threshold means that we treat the classes $0$ and $1$ â€œequally.â€ Lower thresholds mean that we are more likely to predict $1$, whereas higher thersholds mean that we are more likely to predict $0$. Implement the lr_predict function below, which returns a vector of predictions according to the logistic regression model. The function takes a design matrix of observations X, parameter estimate theta, and decision threshold threshold with default value 0.5. def lr_predict(theta, X, threshold=0.5): ''' Classification using a logistic regression model with a probability threshold. Args: theta: The model parameters. Dimension (p+1,) X: The design matrix. Dimension (n, p+1). threshold: decision rule threshold for predicting class 1. Return: A vector of predictions. ''' return (sigmoid(X @ theta) \u003e= threshold).astype(int) # è½¬æ¢æ•°æ®ç±»å‹ï¼ # do not modify below this line Y_train_pred = lr_predict(theta_hat, X_intercept_train) Y_train_pred array([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\r0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\r0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\r0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\r0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\r0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\r0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\r0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\r0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\r0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\r1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\r0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\r1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\r1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\r1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\r0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\r1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\r1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\r0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\r0, 1, 0, 0, 0, 0, 0, 0])\rgrader.check(\"q2\") ","date":"2024-08-13","objectID":"/datalab12/:3:0","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"[Tutorial] Linearly separable data How do these predicted classifications compare to the true responses $\\mathbb{Y}$? Run the below cell to visualize our predicted responses, the true responses, and the probabilities we used to make predictions. We use sns.stripplot which introduces some jitter to avoid overplotting. # just run this cell plot_df = pd.DataFrame({\"X\": np.squeeze(X_train), \"Y\": Y_train, \"Y_pred\": Y_train_pred, \"correct\": (Y_train == Y_train_pred)}) sns.stripplot(data=plot_df, x=\"X\", y=\"Y\", orient='h', alpha=0.5, hue=\"correct\") plt.xlabel('mean radius, $x$') plt.ylabel('$y$') plt.yticks(ticks=[0, 1], labels=['0:\\nbenign', '1:\\nmalignant']) plt.title(\"Predictions for decision threshold T = 0.5\") plt.show() Because we are using a decision threshold $T = 0.5$, we predict $1$ for all $x$ where $\\sigma(x^T\\theta) \\geq 0.5$, which happens when $x^T\\theta = 0$. For the single mean radius feature, we can use algebra to solve for the boundary to be approximately $x \\approx 14.8$. In other words, will always predict $0$ (benign) if the mean radius feature is less than 14.8, and $1$ (malignant) otherwise. However, in our training data there are datapoints with large mean radii that are benign, and vice versa. Our data is not linearly separable by a vertical line. The above visualization is useful when we have just one feature. In practice, however, we use other performance metrics to diagnose our model performance. Next, we will explore several such metrics: accuracy, precision, recall, and confusion matrices. Part 3: Quantifying Performance ","date":"2024-08-13","objectID":"/datalab12/:3:1","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"[Tutorial] sklearnâ€™s LogisticRegression Instead of using the model structure we built manually in the previous questions, we will instead use sklearnâ€™s LogisticRegression model, which operates similarly to the sklearn OLS, Ridge, and LASSO models. Letâ€™s first fit a logistic regression model to the training data. Some notes: Like with linear models, the fit_intercept argument specifies if model includes an intercept term. We therefore pass in the original matrix X_train in the call to lr.fit(). sklearn fits a regularized logistic regression model as specified by the solver='lbfgs' argument; read the documentation for more details. # just run this cell from sklearn.linear_model import LogisticRegression lr = LogisticRegression( fit_intercept=True, solver = 'lbfgs') lr.fit(X_train, Y_train) lr.intercept_, lr.coef_ (array([-13.75518968]), array([[0.92897696]]))\rNote that because we are now fitting a regularized logistic regression model, the estimated coefficients above deviate slightly from our numerical findings in Question 1. Like with linear models, we can call lr.predict(x_train) to classify our training data with our fitted model. # just run this cell lr.predict(X_train) array([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\r0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\r0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\r0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\r0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\r0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\r0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\r0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\r0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\r0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\r1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\r0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\r1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\r1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\r1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\r0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\r1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\r1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\r0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\r0, 1, 0, 0, 0, 0, 0, 0])\rNote that for a binary classification task, the sklearn model uses an unadjustable decision rule of 0.5. If youâ€™re interested in manually adjusting this threshold, check out the documentation for lr.predict_proba(). ","date":"2024-08-13","objectID":"/datalab12/:3:2","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 3 ","date":"2024-08-13","objectID":"/datalab12/:4:0","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 3a: Accuracy Fill in the code below to compute the training and testing accuracy, defined as: $$ \\text{Training Accuracy} = \\frac{1}{n_{train_set}} \\sum_{i \\in {train_set}} {\\mathbb{1}_{y_i == \\hat{y_i}}} $$ $$ \\text{Testing Accuracy} = \\frac{1}{n_{test_set}} \\sum_{i \\in {test_set}} {\\mathbb{1}_{y_i == \\hat{y_i}}} $$ where for the $i$-th observation in the respective dataset, $\\hat{y_i}$ is the predicted response and $y_i$ the true response. $\\mathbb{1}_{y_i == \\hat{y_i}}$ is an indicator function which is $1$ if ${y_i} = \\hat{y_i}$ and $ 0$ otherwise. train_accuracy = np.sum(Y_train == lr.predict(X_train)) / len(Y_train) test_accuracy = np.sum(Y_test == lr.predict(X_test)) / len(Y_test) print(f\"Train accuracy: {train_accuracy:.4f}\") print(f\"Test accuracy: {test_accuracy:.4f}\") Train accuracy: 0.8709\rTest accuracy: 0.9091\rgrader.check(\"q3a\") ","date":"2024-08-13","objectID":"/datalab12/:4:1","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 3b: Precision and Recall It seems we can get a very high test accuracy. What about precision and recall? Precision (also called positive predictive value) is the fraction of true positives among the total number of data points predicted as positive. çœŸé˜³æ€§ / (çœŸé˜³æ€§ + å‡é˜³æ€§) Recall (also known as sensitivity) is the fraction of true positives among the total number of data points with positive labels. çœŸé˜³æ€§ / (çœŸé˜³æ€§ + å‡é˜´æ€§) Precision measures the ability of our classifier to not predict negative samples as positive (i.e., avoid false positives), while recall is the ability of the classifier to find all the positive samples (i.e., avoid false negatives). Below is a graphical illustration of precision and recall, modified slightly from Wikipedia: Mathematically, Precision and Recall are defined as: $$ \\text{Precision} = \\frac{n_{true_positives}}{n_{true_positives} + n_{false_positives}} $$ $$ \\text{Recall} = \\frac{n_{true_positives}}{n_{true_positives} + n_{false_negatives}} $$ Use the formulas above to compute the precision and recall for the test set using the lr model trained using sklearn. Y_test_pred = lr.predict(X_test) # æ³¨æ„boolç´¢å¼•ï¼ precision = np.sum(Y_test_pred[Y_test == 1] == 1) / np.sum(Y_test_pred == 1) recall = np.sum(Y_test_pred[Y_test == 1] == 1) / np.sum(Y_test == 1) print(f'precision = {precision:.4f}') print(f'recall = {recall:.4f}') precision = 0.9184\rrecall = 0.8333\rgrader.check(\"q3b\") ","date":"2024-08-13","objectID":"/datalab12/:4:2","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 3c Our precision is fairly high, while our recall is a bit lower. Consider the following plots, which display the distribution of the response variable $\\mathbb{Y}$ in the training and testing sets (sometimes called the target, or true response). Recall class labels are 0: benign, 1: malignant. fig, axes = plt.subplots(1, 2) sns.countplot(x=Y_train, ax=axes[0]); sns.countplot(x=Y_test, ax=axes[1]); axes[0].set_title('Train') axes[1].set_title('Test') plt.tight_layout(); Based on the above distribution, what might explain the observed difference between our precision and recall metrics? This is due to the class imbalance between M and B in both train and test sets. ","date":"2024-08-13","objectID":"/datalab12/:4:3","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"[Tutorial] Confusion Matrices To understand the link between precision and recall, itâ€™s useful to create a confusion matrix of our predictions. Luckily, sklearn.metrics provides us with such a function! The confusion_matrix function (documentation) categorizes counts of datapoints based if their true and predicted values match. For the 143-datapoint test dataset: # just run this cell from sklearn.metrics import confusion_matrix Y_test_pred = lr.predict(X_test) cnf_matrix = confusion_matrix(Y_test, Y_test_pred) cnf_matrix array([[85, 4],\r[ 9, 45]])\rWeâ€™ve implemented the following function to better visualize these four counts against the true and predicted categories: # just run this cell def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. \"\"\" import itertools plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) plt.grid(False) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, np.round(cm[i, j], 2), horizontalalignment=\"center\", color=\"white\" if cm[i, j] \u003e thresh else \"black\") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') class_names = ['False', 'True'] plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization') ","date":"2024-08-13","objectID":"/datalab12/:4:4","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":"Question 3d: Normalized Confusion Matrix To better interpret these counts, assign cnf_matrix_norm to a normalized confusion matrix by the count of each true label category. In other words, build a 2-D NumPy array constructed by normalizing cnf_matrix by the count of datapoints in each row. For example, the top-left quadrant of cnf_matrix_norm should represent the proportion of true negatives over the total number of datapoints with negative labels. Hint: In array broadcasting, you may encounter issues dividing 2-D NumPy arrays by 1-D NumPy arrays. Check out the keepdims parameter in np.sum (documentation). cnf_matrix_norm = cnf_matrix / np.sum(cnf_matrix, axis=1, keepdims=True) # do not modify below this line plot_confusion_matrix(cnf_matrix_norm, classes=class_names, title='Normalized confusion matrix') grader.check(\"q3d\") Compare the normalized confusion matrix to the values you computed for precision and recall earlier: # just run this cell for your convenience print(f'precision = {precision:.4f}') print(f'recall = {recall:.4f}') precision = 0.9184\rrecall = 0.8333\rBased on the definitions of precision and recall, why does only recall appear in the normalized confusion matrix? Why doesn't precision appear? (No answer required for this part; just something to think about.)\rCongratulations! You finished the lab! ","date":"2024-08-13","objectID":"/datalab12/:4:5","tags":["Scikit-Learn","Numpy"],"title":"DATA100-lab12: Logistic Regression","uri":"/datalab12/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab10.ipynb\") Lab 10: SQL In this lab, we are going to practice viewing, sorting, grouping, and merging tables with SQL. We will explore two datasets: A â€œminifiedâ€ version of the Internet Movie Database (IMDb). This SQLite database (~10MB) is a tiny sample of the much larger database (more than a few GBs). As a result, disclaimer that we may get wildly different results than if we use the whole database! The money donated during the 2016 election using the Federal Election Commission (FEC)â€™s public records. You will be connecting to a SQLite database containing the data. The data we will be working with in this lab is quite small (~16MB); however, it is a sample taken from a much larger database (more than a few GBs). # Run this cell to set up your notebook import numpy as np import pandas as pd import plotly.express as px import sqlalchemy from ds100_utils import fetch_and_cache from pathlib import Path %load_ext sql # Unzip the data. !unzip -o data.zip Archive: data.zip\rinflating: imdbmini.db inflating: fec_nyc.db ","date":"2024-08-13","objectID":"/datalab10/:0:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"SQL Query Syntax Throughout this lab, you will become familiar with the following syntax for the SELECT query: SELECT [DISTINCT] {* | expr [[AS] c_alias] {,expr [[AS] c_alias] ...}} FROM tableref {, tableref} [[INNER | LEFT ] JOIN table_name ON qualification_list] [WHERE search_condition] [GROUP BY colname {,colname...}] [HAVING search condition] [ORDER BY column_list] [LIMIT number] [OFFSET number of rows]; Part 0 [Tutorial]: Writing SQL in Jupyter Notebooks ","date":"2024-08-13","objectID":"/datalab10/:1:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"1. %%sql cell magic In lecture, we used the sql extension to call %%sql cell magic, which enables us to connect to SQL databses and issue SQL commands within Jupyter Notebooks. Run the below cell to connect to a mini IMDb database. %sql sqlite:///imdbmini.db Above, prefixing our single-line command with %sql means that the entire line will be treated as a SQL command (this is called â€œline magicâ€). In this class we will most often write multi-line SQL, meaning we need â€œcell magicâ€, where the first line has %%sql (note the double % operator). The database imdbmini.db includes several tables, one of which is Title. Running the below cell will return first 5 lines of that table. Note that %%sql is on its own line. Weâ€™ve also included syntax for single-line comments, which are surrounded by --. %%sql /* * This is a * multi-line comment. */ -- This is a single-line/inline comment. -- SELECT * FROM Name LIMIT 5; * sqlite:///imdbmini.db\rDone.\rnconst\rprimaryName\rbirthYear\rdeathYear\rprimaryProfession\r1\rFred Astaire\r1899\r1987\rsoundtrack,actor,miscellaneous\r2\rLauren Bacall\r1924\r2014\ractress,soundtrack\r3\rBrigitte Bardot\r1934\rNone\ractress,soundtrack,music_department\r4\rJohn Belushi\r1949\r1982\ractor,soundtrack,writer\r5\rIngmar Bergman\r1918\r2007\rwriter,director,actor\r","date":"2024-08-13","objectID":"/datalab10/:2:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"2. The Pandas command pd.read_sql As of 2022, the %sql magic for Jupyter Notebooks is still in development (check out its GitHub. It is still missing many features that would justify real-world use with Python. In particular, its returned tables are not Pandas dataframes (for example, the query result from the above cell is missing an index). The rest of this section describes how data scientists use SQL and Python in practice, using the Pandas command pd.read_sql (documentation). You will see both %sql magic and pd.read_sql in this course. The below cell connects to the same database using the SQLAlchemy Python library, which can connect to several different database management systems, including sqlite3, MySQL, PostgreSQL, and Oracle. The library also supports an advanced feature for generating queries called an object relational mapper or ORM, which we wonâ€™t discuss in this course but is quite useful for application development. # important!!! run this cell import sqlalchemy # create a SQL Alchemy connection to the database engine = sqlalchemy.create_engine(\"sqlite:///imdbmini.db\") connection = engine.connect() With the SQLAlchemy object connection, we can then call pd.read_sql which takes in a query string. Note the \"\"\" to define our multi-line string, which allows us to have a query span multiple lines. The resulting df DataFrame stores the results of the same SQL query from the previous section. # just run this cell query = \"\"\" SELECT * FROM Title LIMIT 5; \"\"\" df = pd.read_sql(query, engine) df tconst\rtitleType\rprimaryTitle\roriginalTitle\risAdult\rstartYear\rendYear\rruntimeMinutes\rgenres\r0\r417\rshort\rA Trip to the Moon\rLe voyage dans la lune\r0\r1902\rNone\r13\rAction,Adventure,Comedy\r1\r4972\rmovie\rThe Birth of a Nation\rThe Birth of a Nation\r0\r1915\rNone\r195\rDrama,History,War\r2\r10323\rmovie\rThe Cabinet of Dr. Caligari\rDas Cabinet des Dr. Caligari\r0\r1920\rNone\r76\rFantasy,Horror,Mystery\r3\r12349\rmovie\rThe Kid\rThe Kid\r0\r1921\rNone\r68\rComedy,Drama,Family\r4\r13442\rmovie\rNosferatu\rNosferatu, eine Symphonie des Grauens\r0\r1922\rNone\r94\rFantasy,Horror\rLong error messages: Given that the SQL query is now in the string, the errors become more unintelligible. Consider the below (incorrect) query, which has a semicolon in the wrong place. # uncomment the below code and check out the error # query = \"\"\" # SELECT * # FROM Title; # LIMIT 5 # \"\"\" # pd.read_sql(query, engine) ---------------------------------------------------------------------------\rProgrammingError Traceback (most recent call last)\rFile d:\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1967, in Connection._exec_single_context(self, dialect, context, statement, parameters)\r1966 if not evt_handled:\r-\u003e 1967 self.dialect.do_execute(\r1968 cursor, str_statement, effective_parameters, context\r1969 )\r1971 if self._has_events or self.engine._has_events:\rFile d:\\miniconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:924, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\r923 def do_execute(self, cursor, statement, parameters, context=None):\r--\u003e 924 cursor.execute(statement, parameters)\rProgrammingError: You can only execute one statement at a time.\rThe above exception was the direct cause of the following exception:\rProgrammingError Traceback (most recent call last)\rCell In[7], line 8\r1 # uncomment the below code and check out the error\r3 query = \"\"\"\r4 SELECT *\r5 FROM Title;\r6 LIMIT 5\r7 \"\"\"\r----\u003e 8 pd.read_sql(query, engine)\rFile d:\\miniconda3\\Lib\\site-packages\\pandas\\io\\sql.py:734, in read_sql(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\r724 return pandas_sql.read_table(\r725 sql,\r726 index_col=index_col,\r(...)\r731 dtype_backend=dtype_backend,\r732 )\r733 else:\r--\u003e 734 return pandas_sql.read_query(\r735 sql,\r736 index_col=index_col,\r737 params=params,\r738 coerce_float=coerce_float,\r739 parse_dates=parse_dates,\r740 chunksize=chunksize,\r741 dtype_backend=dtype_backend,\r742 dtype=dtype,\r743 )\rFile d:\\miniconda3\\Lib\\","date":"2024-08-13","objectID":"/datalab10/:2:1","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"3. A suggested workflow for writing SQL in Jupyter Notebooks Which approach is better, %sql magic or pd.read_sql? The SQL database generally contains much more data than what you would analyze in detail. As a Python-fluent data scientist, you will often query SQL databases to perform initial exploratory data analysis, a subset of which you load into Python for further processing. In practice, you would likely use a combination of the two approaches. First, youâ€™d try out some SQL queries with %sql magic to get an interesting subset of data. Then, youâ€™d copy over the query into a pd.read_sql command for visualization, modeling, and export with Pandas, sklearn, and other Python libraries. For SQL assignments in this course, to minimize unruly error messages while maximizing Python compatibility, we suggest the following â€œsandboxedâ€ workflow: Create a %%sql magic cell below the answer cell. You can copy in the below code: %% sql -- This is a comment. Put your code here... -- Work on the SQL query in the %%sql cell; e.g., SELECT ... ; Then, once youâ€™re satisfied with your SQL query, copy it into the multi-string query in the answer cell (the one that contains the pd.read_sql call). You donâ€™t have to follow the above workflow to get full credit on assignments, but we suggest it to reduce debugging headaches. Weâ€™ve created the scratchwork %%sql cells for you in this assignment, but do not add cells between this %%sql cell and the Python cell right below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file. Part 1: The IMDb (mini) Dataset Letâ€™s explore a miniature version of the IMDb Dataset. This is the same dataset that we will use for the upcoming homework. Letâ€™s load in the database in two ways (using both Python and cell magic) so that we can flexibly explore the database. engine = sqlalchemy.create_engine(\"sqlite:///imdbmini.db\") connection = engine.connect() %sql sqlite:///imdbmini.db %%sql SELECT * FROM sqlite_master WHERE type='table'; * sqlite:///imdbmini.db\rDone.\rtype\rname\rtbl_name\rrootpage\rsql\rtable\rTitle\rTitle\r2\rCREATE TABLE \"Title\" ( \"tconst\" INTEGER, \"titleType\" TEXT, \"primaryTitle\" TEXT, \"originalTitle\" TEXT, \"isAdult\" TEXT, \"startYear\" TEXT, \"endYear\" TEXT, \"runtimeMinutes\" TEXT, \"genres\" TEXT )\rtable\rName\rName\r12\rCREATE TABLE \"Name\" ( \"nconst\" INTEGER, \"primaryName\" TEXT, \"birthYear\" TEXT, \"deathYear\" TEXT, \"primaryProfession\" TEXT )\rtable\rRole\rRole\r70\rCREATE TABLE \"Role\" ( tconst INTEGER, ordering TEXT, nconst INTEGER, category TEXT, job TEXT, characters TEXT )\rtable\rRating\rRating\r41\rCREATE TABLE \"Rating\" ( tconst INTEGER, averageRating TEXT, numVotes TEXT )\rFrom running the above cell, we see the database has 4 tables: Name, Role, Rating, and Title. [Click to Expand] See descriptions of each table's schema.\rName -Contains the following information for names of people. nconst (text) - alphanumeric unique identifier of the name/person primaryName (text)-name by which the person is most often credited birthYear (integer) -in YYYY format deathYear (integer) -in YYYY format Role -Contains the principal cast/crew for titles. tconst (text) - alphanumeric unique identifier of the title ordering (integer) -a number to uniquely identify rows for a given tconst nconst (text) - alphanumeric unique identifier of the name/person category (text) - the category of job that person was in characters (text) - the name of the character played if applicable, else â€˜\\Nâ€™ Rating -Contains the IMDb rating and votes information for titles. tconst (text) - alphanumeric unique identifier of the title averageRating (real) -weighted average of all the individual user ratings numVotes (integer) - number of votes (i.e., ratings) the title has received Title - Contains the following information for titles. tconst (text) - alphanumeric unique identifier of the title titleType (text) - the type/format of the title primaryTitle (text) - the more popular title / the title used by the filmma","date":"2024-08-13","objectID":"/datalab10/:2:2","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 1 What are the different kinds of titleTypes included in the Title table? Write a query to find out all the unique titleTypes of films using the DISTINCT keyword. (You may not use GROUP BY.) %%sql /* * Code in this scratchwork cell is __not graded.__ * Copy over any SQL queries you write here into the below Python cell. * Do __not__ insert any new cells in between the SQL/Python cells! * Doing so may break the autograder. */ -- Write below this comment. -- SELECT DISTINCT titleType FROM Title; * sqlite:///imdbmini.db\rDone.\rtitleType\rshort\rmovie\rtvSeries\rtvMovie\rtvMiniSeries\rvideo\rvideoGame\rtvEpisode\rtvSpecial\rquery_q1 = \"\"\" SELECT DISTINCT titleType FROM Title; \"\"\" res_q1 = pd.read_sql(query_q1, engine) res_q1 titleType\r0\rshort\r1\rmovie\r2\rtvSeries\r3\rtvMovie\r4\rtvMiniSeries\r5\rvideo\r6\rvideoGame\r7\rtvEpisode\r8\rtvSpecial\rgrader.check(\"q1\") q1 passed! ğŸ€ ","date":"2024-08-13","objectID":"/datalab10/:3:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 2 Before we proceed we want to get a better picture of the kinds of jobs that exist. To do this examine the Role table by computing the number of records with each job category. Present the results in descending order by the total counts. The top of your table should look like this (however, you should have more rows): category total 0 actor 21665 1 writer 13830 2 â€¦ â€¦ %%sql /* * Code in this scratchwork cell is __not graded.__ * Copy over any SQL queries you write here into the below Python cell. * Do __not__ insert any new cells in between the SQL/Python cells! * Doing so may break the autograder. */ -- Write below this comment. -- SELECT category, COUNT(*) AS total FROM Role GROUP BY category ORDER BY total DESC; * sqlite:///imdbmini.db\rDone.\rcategory\rtotal\ractor\r21665\rwriter\r13830\ractress\r12175\rproducer\r11028\rdirector\r6995\rcomposer\r4123\rcinematographer\r2747\reditor\r1558\rself\r623\rproduction_designer\r410\rarchive_footage\r66\rarchive_sound\r6\rquery_q2 = \"\"\" SELECT category, COUNT(*) AS total FROM Role GROUP BY category ORDER BY total DESC; \"\"\" res_q2 = pd.read_sql(query_q2, engine) res_q2 category\rtotal\r0\ractor\r21665\r1\rwriter\r13830\r2\ractress\r12175\r3\rproducer\r11028\r4\rdirector\r6995\r5\rcomposer\r4123\r6\rcinematographer\r2747\r7\reditor\r1558\r8\rself\r623\r9\rproduction_designer\r410\r10\rarchive_footage\r66\r11\rarchive_sound\r6\rgrader.check(\"q2\") q2 passed! ğŸ€ If we computed the results correctly we should see a nice horizontal bar chart of the counts per category below:\r# just run this cell px.bar(res_q2, x=\"total\", y=\"category\", orientation='h') ","date":"2024-08-13","objectID":"/datalab10/:4:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 3 Now that we have a better sense of the basics of our data, we can ask some more interesting questions. The Rating table has the numVotes and the averageRating for each title. Which 10 films have the most ratings? Write a SQL query that outputs three fields: the title, numVotes, and averageRating for the 10 films that have the highest number of ratings. Sort the result in descending order by the number of votes. Hint: The numVotes in the Rating table is not an integer! Use CAST(Rating.numVotes AS int) AS numVotes to convert the attribute to an integer. %%sql /* * Code in this scratchwork cell is __not graded.__ * Copy over any SQL queries you write here into the below Python cell. * Do __not__ insert any new cells in between the SQL/Python cells! * Doing so may break the autograder. */ -- Write below this comment. -- SELECT primaryTitle, CAST(numVotes AS int) AS numVotes, averageRating FROM Rating, Title WHERE Rating.tconst = Title.tconst ORDER BY numVotes DESC; query_q3 = \"\"\" SELECT primaryTitle AS title, CAST(numVotes AS int) AS numVotes, averageRating FROM Rating, Title WHERE Rating.tconst = Title.tconst ORDER BY numVotes DESC LIMIT 10; \"\"\" res_q3 = pd.read_sql(query_q3, engine) res_q3 title\rnumVotes\raverageRating\r0\rThe Shawshank Redemption\r2462686\r9.3\r1\rThe Dark Knight\r2417875\r9.0\r2\rInception\r2169255\r8.8\r3\rFight Club\r1939312\r8.8\r4\rPulp Fiction\r1907561\r8.9\r5\rForrest Gump\r1903969\r8.8\r6\rGame of Thrones\r1874040\r9.2\r7\rThe Matrix\r1756469\r8.7\r8\rThe Lord of the Rings: The Fellowship of the Ring\r1730296\r8.8\r9\rThe Lord of the Rings: The Return of the King\r1709023\r8.9\rgrader.check(\"q3\") Part 2: Election Donations in New York City Finally, letâ€™s analyze the Federal Election Commission (FEC)â€™s public records. We connect to the database in two ways (using both Python and cell magic) so that we can flexibly explore the database. # important!!! run this cell and the next one import sqlalchemy # create a SQL Alchemy connection to the database engine = sqlalchemy.create_engine(\"sqlite:///fec_nyc.db\") connection = engine.connect() %sql sqlite:///fec_nyc.db ","date":"2024-08-13","objectID":"/datalab10/:5:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Table Descriptions Run the below cell to explore the schemas of all tables saved in the database. If youâ€™d like, you can consult the below linked FEC pages for the descriptions of the tables themselves. cand (link): Candidates table. Contains names and party affiliation. comm (link): Committees table. Contains committee names and types. indiv_sample_nyc (link): All individual contributions from New York City . %%sql /* just run this cell */ SELECT sql FROM sqlite_master WHERE type='table'; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rsql\rCREATE TABLE \"cand\" ( cand_id character varying(9), cand_name text, cand_pty_affiliation character varying(3), cand_election_yr integer, cand_office_st character varying(2), cand_office character(1), cand_office_district integer, cand_ici character(1), cand_status character(1), cand_pcc character varying(9), cand_st1 text, cand_st2 text, cand_city text, cand_st character varying(2), cand_zip character varying(10) )\rCREATE TABLE \"comm\"( \"cmte_id\" TEXT, \"cmte_nm\" TEXT, \"tres_nm\" TEXT, \"cmte_st1\" TEXT, \"cmte_st2\" TEXT, \"cmte_city\" TEXT, \"cmte_st\" TEXT, \"cmte_zip\" TEXT, \"cmte_dsgn\" TEXT, \"cmte_tp\" TEXT, \"cmte_pty_affiliation\" TEXT, \"cmte_filing_freq\" TEXT, \"org_tp\" TEXT, \"connected_org_nm\" TEXT, \"cand_id\" TEXT )\rCREATE TABLE indiv_sample_nyc ( cmte_id character varying(9), amndt_ind character(1), rpt_tp character varying(3), transaction_pgi character(5), image_num bigint, transaction_tp character varying(3), entity_tp character varying(3), name text, city text, state character(2), zip_code character varying(12), employer text, occupation text, transaction_dt character varying(9), transaction_amt integer, other_id text, tran_id text, file_num bigint, memo_cd text, memo_text text, sub_id bigint )\rLetâ€™s look at the indiv_sample_nyc table. The below cell displays individual donations made by residents of the state of New York. We use LIMIT 5 to avoid loading and displaying a huge table. %%sql /* just run this cell */ SELECT comm.cmte_id, cmte_nm, sum(transaction_amt) as total FROM indiv_sample_nyc, comm WHERE indiv_sample_nyc.cmte_id = comm.cmte_id AND name LIKE '%TRUMP%' AND name LIKE '%DONALD%' GROUP BY cmte_nm ORDER BY transaction_amt LIMIT 5; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rcmte_id\rcmte_nm\rtotal\rC00608489\rGREAT AMERICA PAC\r75\rC00369033\rTEXANS FOR SENATOR JOHN CORNYN INC\r1000\rC00494229\rHELLER FOR SENATE\r2000\rC00554949\rFRIENDS OF DAVE BRAT INC.\r2600\rC00230482\rGRASSLEY COMMITTEE INC\r5200\rYou can write a SQL query to return the id and name of the first five candidates from the Democratic party, as below: %%sql /* just run this cell */ SELECT cand_id, cand_name FROM cand WHERE cand_pty_affiliation = 'DEM' LIMIT 5; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rcand_id\rcand_name\rH0AL05049\rCRAMER, ROBERT E \"BUD\" JR\rH0AL07086\rSEWELL, TERRYCINA ANDREA\rH0AL07094\rHILLIARD, EARL FREDERICK JR\rH0AR01091\rGREGORY, JAMES CHRISTOPHER\rH0AR01109\rCAUSEY, CHAD\r","date":"2024-08-13","objectID":"/datalab10/:6:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"[Tutorial] Matching Text with LIKE First, letâ€™s look at 2016 election contributions made by Donald Trump, who was a New York (NY) resident during that year. The following SQL query returns the cmte_id, transaction_amt, and name for every contribution made by any donor with â€œDONALDâ€ and â€œTRUMPâ€ in their name in the indiv_sample_nyc table. Notes: We use the WHERE ... LIKE '...' to match fields with text patterns. The % wildcard represents at least zero characters. Compare this to what you know from regex! We use pd.read_sql syntax here because we will do some EDA on the result res. # just run this cell example_query = \"\"\" SELECT cmte_id, transaction_amt, name FROM indiv_sample_nyc WHERE name LIKE '%TRUMP%' AND name LIKE '%DONALD%'; \"\"\" example_res = pd.read_sql(example_query, engine) example_res cmte_id\rtransaction_amt\rname\r0\rC00230482\r2600\rDONALD, TRUMP\r1\rC00230482\r2600\rDONALD, TRUMP\r2\rC00014498\r9000\rTRUMP, DONALD\r3\rC00494229\r2000\rTRUMP, DONALD MR\r4\rC00571869\r2700\rTRUMP, DONALD J.\r...\r...\r...\r...\r152\rC00608489\r5\rDONALD J TRUMP FOR PRESIDENT INC\r153\rC00608489\r5\rDONALD J TRUMP FOR PRESIDENT INC\r154\rC00608489\r5\rDONALD J TRUMP FOR PRESIDENT INC\r155\rC00608489\r5\rDONALD J TRUMP FOR PRESIDENT INC\r156\rC00608489\r5\rDONALD J TRUMP FOR PRESIDENT INC\r157 rows Ã— 3 columns If we look at the list above, it appears that some donations were not by Donald Trump himself, but instead by an entity called â€œDONALD J TRUMP FOR PRESIDENT INCâ€. Fortunately, we see that our query only seems to have picked up one such anomalous name. # just run this cell example_res['name'].value_counts() name\rTRUMP, DONALD J. 133\rDONALD J TRUMP FOR PRESIDENT INC 15\rTRUMP, DONALD 4\rDONALD, TRUMP 2\rTRUMP, DONALD MR 1\rTRUMP, DONALD J MR. 1\rTRUMP, DONALD J MR 1\rName: count, dtype: int64\r","date":"2024-08-13","objectID":"/datalab10/:7:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 4 Revise the above query so that the 15 anomalous donations made by â€œDONALD J TRUMP FOR PRESIDENT INCâ€ do not appear. Your resulting table should have 142 rows. Hints: Consider using the above query as a starting point, or checking out the SQL query skeleton at the top of this lab. The NOT keyword may also be useful here. %%sql /* * Code in this scratchwork cell is __not graded.__ * Copy over any SQL queries you write here into the below Python cell. * Do __not__ insert any new cells in between the SQL/Python cells! * Doing so may break the autograder. */ -- Write below this comment. -- SELECT cmte_id, transaction_amt, name FROM indiv_sample_nyc WHERE name LIKE '%TRUMP%' AND name LIKE '%DONALD%' AND name NOT LIKE '%DONALD J TRUMP FOR PRESIDENT INC%'; query_q4 = \"\"\" SELECT cmte_id, transaction_amt, name FROM indiv_sample_nyc WHERE name LIKE '%TRUMP%' AND name LIKE '%DONALD%' AND name NOT LIKE '%DONALD J TRUMP FOR PRESIDENT INC%'; \"\"\" res_q4 = pd.read_sql(query_q4, engine) res_q4 cmte_id\rtransaction_amt\rname\r0\rC00230482\r2600\rDONALD, TRUMP\r1\rC00230482\r2600\rDONALD, TRUMP\r2\rC00014498\r9000\rTRUMP, DONALD\r3\rC00494229\r2000\rTRUMP, DONALD MR\r4\rC00571869\r2700\rTRUMP, DONALD J.\r...\r...\r...\r...\r137\rC00580100\r9752\rTRUMP, DONALD J.\r138\rC00580100\r2574\rTRUMP, DONALD J.\r139\rC00580100\r23775\rTRUMP, DONALD J.\r140\rC00580100\r2000000\rTRUMP, DONALD J.\r141\rC00580100\r2574\rTRUMP, DONALD J.\r142 rows Ã— 3 columns grader.check(\"q4\") q4 passed! ","date":"2024-08-13","objectID":"/datalab10/:8:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 5: JOINing Tables Letâ€™s explore the other two tables in our database: cand and comm. The cand table contains summary financial information about each candidate registered with the FEC or appearing on an official state ballot for House, Senate or President. %%sql /* just run this cell */ SELECT * FROM indiv_sample_nyc LIMIT 5; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rcmte_id\ramndt_ind\rrpt_tp\rtransaction_pgi\rimage_num\rtransaction_tp\rentity_tp\rname\rcity\rstate\rzip_code\remployer\roccupation\rtransaction_dt\rtransaction_amt\rother_id\rtran_id\rfile_num\rmemo_cd\rmemo_text\rsub_id\rC00445015\rN\rQ1\rP 15951128130\r15\rIND\rSINGER, TRIPP MR.\rNEW YORK\rNY\r100214505\rATLANTIC MAILBOXES, INC.\rOWNER\r01302015\r1000\rA-CF13736\r1002485\r4041420151241812398\rC00510461\rN\rQ1\rP 15951129284\r15E\rIND\rSIMON, DANIEL A\rNEW YORK\rNY\r100237940\rN/A\rRETIRED\r03292015\r400\rC00401224\rVN8JBDDJBA8\r1002590\r* EARMARKED CONTRIBUTION: SEE BELOW\r4041420151241813640\rC00422410\rN\rQ1\rP 15970352211\r15\rIND\rABDUL RAUF, FEISAL\rNEW YORK\rNY\r101150010\rTHE CORDOBA INITIATIVE\rCHAIRMAN\r03042015\r250\rVN8A3DBSYG6\r1003643\r4041620151241914560\rC00510461\rN\rQ1\rP 15951129280\r15\rIND\rSCHWARZER, FRANK\rNEW YORK\rNY\r100145135\rMETRO HYDRAULIC JACK CO\rSALES\r01162015\r100\rVN8JBDAP4C4\r1002590\r* EARMARKED CONTRIBUTION: SEE BELOW\r4041420151241813630\rC00510461\rN\rQ1\rP 15951129281\r15\rIND\rSCHWARZER, FRANK\rNEW YORK\rNY\r100145135\rMETRO HYDRAULIC JACK CO\rSALES\r02162015\r100\rVN8JBDBRDG3\r1002590\r* EARMARKED CONTRIBUTION: SEE BELOW\r4041420151241813632\rThe comm table contains summary financial information about each committee registered with the FEC. Committees are organizations that spend money for political action or parties, or spend money for or against political candidates. %%sql /* just run this cell */ SELECT * FROM comm LIMIT 5; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rcmte_id\rcmte_nm\rtres_nm\rcmte_st1\rcmte_st2\rcmte_city\rcmte_st\rcmte_zip\rcmte_dsgn\rcmte_tp\rcmte_pty_affiliation\rcmte_filing_freq\rorg_tp\rconnected_org_nm\rcand_id\rC00000059\rHALLMARK CARDS PAC\rERIN BROWER\r2501 MCGEE\rMD#288\rKANSAS CITY\rMO\r64108\rU\rQ\rUNK\rM\rC\rC00000422\rAMERICAN MEDICAL ASSOCIATION POLITICAL ACTION COMMITTEE\rWALKER, KEVIN\r25 MASSACHUSETTS AVE, NW\rSUITE 600\rWASHINGTON\rDC\r20001\rB\rQ\rM\rM\rAMERICAN MEDICAL ASSOCIATION\rC00000489\rD R I V E POLITICAL FUND CHAPTER 886\rTOM RITTER\r3528 W RENO\rOKLAHOMA CITY\rOK\r73107\rU\rN\rQ\rL\rTEAMSTERS LOCAL UNION 886\rC00000547\rKANSAS MEDICAL SOCIETY POLITICAL ACTION COMMITTEE\rC. RICHARD BONEBRAKE, M.D.\r623 SW 10TH AVE\rTOPEKA\rKS\r66612\rU\rQ\rUNK\rQ\rT\rC00000638\rINDIANA STATE MEDICAL ASSOCIATION POLITICAL ACTION COMMITTEE\rVIDYA KORA, M.D.\r322 CANAL WALK, CANAL LEVEL\rINDIANAPOLIS\rIN\r46202\rU\rQ\rQ\rM\r","date":"2024-08-13","objectID":"/datalab10/:9:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 5a Notice that both the cand and comm tables have a cand_id column. Letâ€™s try joining these two tables on this column to print out committee information for candidates. List the first 5 candidate names (cand_name) in reverse lexicographic order by cand_name, along with their corresponding committee names. Only select rows that have a matching cand_id in both tables. Your output should look similar to the following: cand_name cmte_nm 0 ZUTLER, DANIEL PAUL MR CITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT 1 ZUMWALT, JAMES ZUMWALT FOR CONGRESS â€¦ â€¦ â€¦ Consider starting from the following query skeleton, which uses the AS keyword to rename the cand and comm tables to c1 and c2, respectively. Which join is most appropriate? SELECT ...\rFROM cand AS c1\r[INNER | {LEFT |RIGHT | FULL } {OUTER}] JOIN comm AS c2\rON ...\r...\r...;\r%%sql /* * Code in this scratchwork cell is __not graded.__ * Copy over any SQL queries you write here into the below Python cell. * Do __not__ insert any new cells in between the SQL/Python cells! * Doing so may break the autograder. */ -- Write below this comment. -- SELECT cand_name, cmte_nm FROM cand AS c1 INNER JOIN comm AS c2 ON c1.cand_id = c2.cand_id ORDER BY cand_name DESC; query_q5a = \"\"\" SELECT cand_name, cmte_nm FROM cand AS c1 INNER JOIN comm AS c2 ON c1.cand_id = c2.cand_id ORDER BY cand_name DESC LIMIT 5; \"\"\" res_q5a = pd.read_sql(query_q5a, engine) res_q5a cand_name\rcmte_nm\r0\rZUTLER, DANIEL PAUL MR\rCITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT\r1\rZUMWALT, JAMES\rZUMWALT FOR CONGRESS\r2\rZUKOWSKI, ANDREW GEORGE\rZUKOWSKI FOR CONGRESS\r3\rZUCCOLO, JOE\rJOE ZUCCOLO FOR CONGRESS\r4\rZORN, ROBERT ERWIN\rCONSTITUTIONAL COMMITTEE\rgrader.check(\"q5a\") q5a passed! ğŸ‰ ","date":"2024-08-13","objectID":"/datalab10/:9:1","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 5b Suppose we modify the query from the previous part to include all candidates, including those that donâ€™t have a committee. List the first 5 candidate names (cand_name) in reverse lexicographic order by cand_name, along with their corresponding committee names. If the candidate has no committee in the comm table, then cmte_nm should be NULL (or None in the Python representation). Your output should look similar to the following: cand_name cmte_nm 0 ZUTLER, DANIEL PAUL MR CITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT â€¦ â€¦ â€¦ 4 ZORNOW, TODD MR None Hint: Start from the same query skeleton as the previous part. Which join is most appropriate? %%sql /* * Code in this scratchwork cell is __not graded.__ * Copy over any SQL queries you write here into the below Python cell. * Do __not__ insert any new cells in between the SQL/Python cells! * Doing so may break the autograder. */ -- Write below this comment. -- SELECT cand_name, cmte_nm FROM cand AS c1 LEFT JOIN comm AS c2 ON c1.cand_id = c2.cand_id ORDER BY cand_name DESC LIMIT 5; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rcand_name\rcmte_nm\rZUTLER, DANIEL PAUL MR\rCITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT\rZUMWALT, JAMES\rZUMWALT FOR CONGRESS\rZUKOWSKI, ANDREW GEORGE\rZUKOWSKI FOR CONGRESS\rZUCCOLO, JOE\rJOE ZUCCOLO FOR CONGRESS\rZORNOW, TODD MR\rNone\rquery_q5b = \"\"\" SELECT cand_name, cmte_nm FROM cand AS c1 LEFT JOIN comm AS c2 ON c1.cand_id = c2.cand_id ORDER BY cand_name DESC LIMIT 5; \"\"\" res_q5b = pd.read_sql(query_q5b, engine) res_q5b cand_name\rcmte_nm\r0\rZUTLER, DANIEL PAUL MR\rCITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT\r1\rZUMWALT, JAMES\rZUMWALT FOR CONGRESS\r2\rZUKOWSKI, ANDREW GEORGE\rZUKOWSKI FOR CONGRESS\r3\rZUCCOLO, JOE\rJOE ZUCCOLO FOR CONGRESS\r4\rZORNOW, TODD MR\rNone\rgrader.check(\"q5b\") q5b passed! ğŸš€ ","date":"2024-08-13","objectID":"/datalab10/:9:2","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":"Question 6: Subqueries and Grouping If we return to our results from Question 4, we see that many of the contributions were to the same committee: # Your SQL query result from Question 4 # reprinted for your convenience res_q4 cmte_id\rtransaction_amt\rname\r0\rC00230482\r2600\rDONALD, TRUMP\r1\rC00230482\r2600\rDONALD, TRUMP\r2\rC00014498\r9000\rTRUMP, DONALD\r3\rC00494229\r2000\rTRUMP, DONALD MR\r4\rC00571869\r2700\rTRUMP, DONALD J.\r...\r...\r...\r...\r137\rC00580100\r9752\rTRUMP, DONALD J.\r138\rC00580100\r2574\rTRUMP, DONALD J.\r139\rC00580100\r23775\rTRUMP, DONALD J.\r140\rC00580100\r2000000\rTRUMP, DONALD J.\r141\rC00580100\r2574\rTRUMP, DONALD J.\r142 rows Ã— 3 columns Create a new SQL query that returns the total amount that Donald Trump contributed to each committee. Your table should have four columns: cmte_id, total_amount (total amount contributed to that committee), num_donations (total number of donations), and cmte_nm (name of the committee). Your table should be sorted in decreasing order of total_amount. This is a hard question! Donâ€™t be afraid to reference the lecture slides, or the overall SQL query skeleton at the top of this lab. Here are some other hints: Note that committee names are not available in indiv_sample_nyc, so you will have to obtain information somehow from the comm table (perhaps a JOIN would be useful). Remember that you can compute summary statistics after grouping by using aggregates like COUNT(*), SUM() as output fields. A subquery may be useful to break your question down into subparts. Consider the following query skeleton, which uses the WITH operator to store a subqueryâ€™s results in a temporary table named donations. WITH donations AS (\rSELECT ...\r...\r)\rSELECT ...\rFROM donations\rGROUP BY ...\rORDER BY ...;\r%%sql /* just run this cell */ SELECT comm.cmte_id, sum(transaction_amt) as total_amount, COUNT(*) as num_donations, cmte_nm FROM indiv_sample_nyc, comm WHERE indiv_sample_nyc.cmte_id = comm.cmte_id AND name LIKE '%TRUMP%' AND name LIKE '%DONALD%' GROUP BY cmte_nm ORDER BY total_amount DESC LIMIT 10; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rcmte_id\rtotal_amount\rnum_donations\rcmte_nm\rC00580100\r18633157\r131\rDONALD J. TRUMP FOR PRESIDENT, INC.\rC00055582\r10000\r1\rNY REPUBLICAN FEDERAL CAMPAIGN COMMITTEE\rC00014498\r9000\r1\rREPUBLICAN PARTY OF IOWA\rC00571869\r5400\r2\rDONOVAN FOR CONGRESS\rC00230482\r5200\r2\rGRASSLEY COMMITTEE INC\rC00034033\r5000\r1\rSOUTH CAROLINA REPUBLICAN PARTY\rC00136457\r5000\r1\rNEW HAMPSHIRE REPUBLICAN STATE COMMITTEE\rC00554949\r2600\r1\rFRIENDS OF DAVE BRAT INC.\rC00494229\r2000\r1\rHELLER FOR SENATE\rC00369033\r1000\r1\rTEXANS FOR SENATOR JOHN CORNYN INC\r%%sql /* * Code in this scratchwork cell is __not graded.__ * Copy over any SQL queries you write here into the below Python cell. * Do __not__ insert any new cells in between the SQL/Python cells! * Doing so may break the autograder. */ -- Write below this comment. -- WITH donations AS ( SELECT c1.cmte_id,transaction_amt, cmte_nm FROM indiv_sample_nyc AS c1, comm AS c2 WHERE c1.cmte_id = c2.cmte_id AND name LIKE '%TRUMP%' AND name LIKE '%DONALD%' AND transaction_amt \u003e 0 ) SELECT cmte_id, SUM(transaction_amt) AS total_amount, count(*) AS num_donations, cmte_nm FROM donations GROUP BY cmte_nm ORDER BY total_amount LIMIT 10; * sqlite:///fec_nyc.db\rsqlite:///imdbmini.db\rDone.\rcmte_id\rtotal_amount\rnum_donations\rcmte_nm\rC00608489\r75\r15\rGREAT AMERICA PAC\rC00369033\r1000\r1\rTEXANS FOR SENATOR JOHN CORNYN INC\rC00494229\r2000\r1\rHELLER FOR SENATE\rC00554949\r2600\r1\rFRIENDS OF DAVE BRAT INC.\rC00136457\r5000\r1\rNEW HAMPSHIRE REPUBLICAN STATE COMMITTEE\rC00034033\r5000\r1\rSOUTH CAROLINA REPUBLICAN PARTY\rC00230482\r5200\r2\rGRASSLEY COMMITTEE INC\rC00571869\r5400\r2\rDONOVAN FOR CONGRESS\rC00014498\r9000\r1\rREPUBLICAN PARTY OF IOWA\rC00055582\r10000\r1\rNY REPUBLICAN FEDERAL CAMPAIGN COMMITTEE\rquery_q6 = \"\"\" SELECT comm.cmte_id, sum(transaction_amt) as total_amount, COUNT(*) as num_donations, cmte_nm FROM indiv_sample_nyc, comm WHERE indiv_sample_nyc.cmte_id = comm.cmte_id AND name LIKE '%TRUMP%' AND name LIKE '","date":"2024-08-13","objectID":"/datalab10/:10:0","tags":["Pandas","sqlite3","sqlalchemy"],"title":"DATA100-lab10: SQL","uri":"/datalab10/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab09.ipynb\") Lab 9: Probability and Modeling In this lab, you will explore estimators and modeling in two parts: You will explore if the â€œsample maxâ€ is a biased estimator for the true max of a population. Given a sample (and no access to the population), you will bootstrap the sample correlation estimator to infer properties of the population correlation of two features. # Run this cell to set up your notebook import csv import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats import seaborn as sns from IPython.display import FileLink, FileLinks FileLink('path_to_file/filename.extension') %matplotlib inline sns.set() sns.set_context(\"talk\") import warnings warnings.filterwarnings('ignore') from IPython.display import display, Latex, Markdown Part 1: Estimator for Population Max Berkeley Half Marathon is an annual weekend-long race here in Berkeley. On race day, you want to know how many people participated in this yearâ€™s race, but you donâ€™t have access to the official head count. So you want to find an estimator for this population maximum (Note: we artificially generated this data, but assume it is the population data as described). The bib numbers of each participant (i.e., racer) are in order of registrationâ€”integers from $1$ to the total unknown number of participants. You decide to construct a sample by recording the bib number of every racer you see on the street in a given time period, and use the maximum bib number in your sample as an estimator for the true maximum bib number (i.e., total number of particpants, assuming everyone who registered participated). Assume that a racerâ€™s bib number has no relation to their racing experience, so that you are equally likely to see any of bib number in your sample. Is the sample maximum a good estimator for the population maximum? Weâ€™ll use simulation to explore in this part of the lab. Note: In Part 1 of this lab, we assume that we have access to the true population of racers (and therefore the parameter, the true population maximum), so that we can evaluate the sample maximum across all possible samples. However, in practice we donâ€™t have access to the population, nor the population parameter. In Part 2, weâ€™ll explore the analysis we can do if just provided a single sample. ","date":"2024-08-13","objectID":"/datalab9/:0:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 1 Letâ€™s first assume that we have access to the total number of participants (in practice we donâ€™t!). The dataset marathon.csv includes information for all racers who registered for the Berkeley Half Marathon. Load the dataset marathon.csv into the DataFrame marathon, assign true_max to the true maximum bib number of the population. (Recall that max bib number is our proxy for total number of participants.) marathon = pd.read_csv('marathon.csv') marathon.head() true_max = int(marathon['Bib Number'].max()) true_max 50732\rgrader.check(\"q1\") q1 passed! ğŸš€ # just run this cell marathon.describe() Bib Number\rcount\r50732.000000\rmean\r25366.500000\rstd\r14645.211265\rmin\r1.000000\r25%\r12683.750000\r50%\r25366.500000\r75%\r38049.250000\rmax\r50732.000000\r","date":"2024-08-13","objectID":"/datalab9/:1:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 2 How would a sample maximum compare to the true maximum? Formally defined, let $X_1, \\dots, X_n$ be a sample of $n$ random variables drawn IID (independent and identically distributed) from the population. Define the sample max as the maximum value of the sample. $$\\text{samplemax} = \\max (X_1, \\dots, X_n)$$ Recall from Data 8 that we can get the empirical distribution of a statistic by simulating, or repeatedly sampling from the population. Suppose we compute the sample max as the maximum bib number from observing the bib numbers of $n = 200$ random racers. By repeating this process for many randomly selected samples, we get a simulated distributed of the sample max statistic. Assign sample_maxes to an array that contains 5,000 simulated sample maxes from samples of size 200, each sampled randomly with replacement from the population marathon. (Side note: We sample with replacement because while it suggests that we could see the same racer multiple times in our sample, it allows us to assume each individual in our sample is drawn IID from the population.) Some useful functions: df.sample (link), np.random.choice (link). Do not edit the np.random.seed call, which sets the pseudorandomness of the autograder. np.random.seed(2022) # do not change this line sample_maxes = [] for i in range(5000): sample = marathon['Bib Number'].sample(n=200, replace=True) sample_maxes.append(sample.max()) sample_maxes = np.array(sample_maxes) sample_maxes array([50641, 50724, 50486, ..., 50642, 50434, 50704])\rgrader.check(\"q2\") ","date":"2024-08-13","objectID":"/datalab9/:2:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 3 ","date":"2024-08-13","objectID":"/datalab9/:3:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 3a Plot the empirical distribution of the sample maximum that you generated in Question 2. Your plot should look like the below plot. It should include both the average sample maximum and the true population maximum as vertical lines. Visualization/plotting tips: To plot a vertical line with specific linestyles, see the plt.axvline documentation. To include a label in the legend, pass in label=... to the plot that youâ€™d like to label (example). BEGIN QUESTION name: q3a plt.figure(figsize = [10, 6]) bins = np.linspace(49000, 50750, 25) # for your plot avg_sample_maxes = np.mean(sample_maxes) plt.hist(sample_maxes, bins) plt.axvline(x = true_max, color ='black', label = 'True Maximum', linestyle = '--') plt.axvline(x = avg_sample_maxes, color = 'green', label = 'E[Sample Max]', linestyle = '--') plt.legend(); # show legend ","date":"2024-08-13","objectID":"/datalab9/:3:1","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 3b Recall from Spring 2022 Lecture 17 that an unbiased estimator is one where the expected value of the estimator is the parameter. For example, the sample mean $\\bar{X}_n$ is an unbiased estimator of the population mean $\\mu$ because $\\mathbb{E}[\\bar{X}_n] = \\mu$ by the Central Limit Theorem. Based on your analysis in Question 3a, assign q3b to the most correct option; then in the second cell, explain your choice. The sample maximum is an unbiased estimator of the population maximum. The sample maximum overestimates the population maximum. The sample maximum underestimates the population maximum. q3b = 3 grader.check(\"q3b\") q3b passed! ğŸš€ Part 2: Inference for the Population Correlation The previous part assumed that we had access to the population; from there, we simulated many different samples to understand properties of our estimator. In practice, however, we only have access to one sample (and therefore one value of our estimator); we will explore this next. In Spring 2022 Lecture 16, we defined population correlation as the expected product of standardized deviations from expectation: $$r(X, Y) = \\mathbb{E} \\left[\\left(\\frac{X - \\mathbb{E}[X]}{\\text{SD}(X)} \\right) \\left(\\frac{Y - \\mathbb{E}[Y]}{\\text{SD}(Y)}\\right)\\right]$$ Note that population correlation involves the population means $\\mathbb{E}[X]$ and $\\mathbb{E}[Y]$ and the population standard deviations $\\text{SD}(X)$ and $\\text{SD}(Y)$. Correlation provides us with important information about the linear relationship between variables. In this part, weâ€™ll explore the tips dataset once more, and we will compute the sample correlation statistic of two features: total bill and party size. We will then explore how the sample correlation estimates the true population correlation parameter. The below cell assigns data to our single sample collected about customer tipping behaviors. # just run this cell tips = sns.load_dataset(\"tips\") data = tips[['total_bill','size']] data total_bill\rsize\r0\r16.99\r2\r1\r10.34\r3\r2\r21.01\r3\r3\r23.68\r2\r4\r24.59\r4\r...\r...\r...\r239\r29.03\r3\r240\r27.18\r2\r241\r22.67\r2\r242\r17.82\r2\r243\r18.78\r2\r244 rows Ã— 2 columns ","date":"2024-08-13","objectID":"/datalab9/:3:2","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 4 To estimate the population correlation, weâ€™d like to use an estimator based on data from a simple random sample of our tips data set. For a sample $(X_1, Y_1), \\dots, (X_n, Y_n)$ generated IID from a population, define the sample correlation as follows: $$\\frac{\\sum\\limits_{i=1}^n\\left(X_i-\\overline{X}\\right)\\left(Y_i-\\overline{Y}\\right)}{\\sqrt{\\sum\\limits_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}\\sqrt{\\sum\\limits_{i=1}^n \\left(Y_i - \\overline{Y}\\right)^2}}$$ Note the similar structure to the true population correlation. If the $i$-th individual in our sample has â€œtotal billâ€ $X_i$ and â€œparty sizeâ€ $Y_i$, then $\\overline{X}, \\overline{Y}$ are the sample means of total bill and party size, respectively. Implement the sample_correlation function in the cell below to compute the sample correlation for sample, which has two columns: total_bill and size. def sample_correlation(sample): \"\"\"Compute sample correlation of x and y. sample: A DataFrame of dimension (n, 2) \"\"\" x, y = sample['total_bill'], sample['size'] x_bar = np.mean(x) y_bar = np.mean(y) n = len(sample) return np.sum((x - x_bar) * (y - y_bar)) / np.sqrt(np.sum((x - x_bar) ** 2) * np.sum((y - y_bar) ** 2)) sample_correlation(data) np.float64(0.5983151309049014)\rgrader.check(\"q4\") Let the sample correlation of data be the estimator for the population correlation. In other words: Parameter: Population correlation. Unknown, but fixed. Statistic: Sample correlation. Dependent on the random sample we obtained. Estimator: The sample correlation statistic corr_est is an estimator of the population correlation parameter. # just run this cell corr_est = sample_correlation(data) corr_est np.float64(0.5983151309049014)\rWhat can we infer about the population correlation given this estimate? Is it possible that the total bill and the party size are actually uncorrelated? We can perform bootstrapped hypothesis testing as follows:[å‡è®¾æ£€éªŒ] Null hypothesis: Total bill and party size are uncorrelated; the population correlation is 0. Alternate hypothesis: The population correlation is not 0. To test this hypothesis, we can bootstrap a $(1-p)$% confidence interval for the population correlation and check if 0 is in the interval. If 0 is in the interval, the data are consistent with the null hypothesis. If 0 is not in the interval, we reject the null hypothesis at the $p$% significance level. For more on the duality of the confidence interval and the p-value, see this StackExchange discussion. ","date":"2024-08-13","objectID":"/datalab9/:4:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 5 Implement the ci_correlation function in the cell below that returns a bootstrapped confidence interval at the $conf$% level. Your bootstrap should resample the sample dataframe with replacement m times to construct m bootstrapped sample correlations using the sample_correlation function you implemented in Question 4. Then, assign boot_ci to the bootstrapped 95% confidence interval for the tips data sample. def ci_correlation(sample, conf, m=5000): \"\"\"Compute a confidence interval for an estimator. sample: A DataFrame or Series estimator: A function that maps a sample DataFrame to an estimate (number) \"\"\" estimates = [] n = len(sample) for j in range(m): resample = sample.sample(n=n, replace=True) estimates.append(sample_correlation(resample)) lower = np.percentile(estimates, (100-conf)/2) upper = np.percentile(estimates, 100-(100-conf)/2) # ä¸€ä¸ªç”¨äºè®¡ç®—æ•°ç»„æ²¿æŒ‡å®šè½´çš„ç™¾åˆ†ä½æ•°çš„å‡½æ•° return (lower, upper) boot_ci = ci_correlation(data, conf=95) boot_ci (np.float64(0.5126429599326643), np.float64(0.6790547421163837))\rgrader.check(\"q5\") ","date":"2024-08-13","objectID":"/datalab9/:5:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 6 Now that we have the bootstrapped 95% confidence interval of the parameter based on a single sample of size 244, letâ€™s determine what we can conclude about our population correlation. ","date":"2024-08-13","objectID":"/datalab9/:6:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 6a Fill in the blanks for the sentence: By bootstrapping our sample data, our estimate of the population correlation is ________ with a ___ % confidence interval of ________. 0.5983, 95, (np.float64(0.5126429599326643), np.float64(0.6790547421163837)) ","date":"2024-08-13","objectID":"/datalab9/:6:1","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Question 6b In the cell below, interpret the statement in Question 6a. Can we reject the null hypothesis at the 5% significance level? What can we infer about the relationship between total bill and party size? æ‹’ç»åŸå‡è®¾ï¼Œæ¥å—å¤‡æ‹©å‡è®¾==\u003eæœ‰å…³è” ","date":"2024-08-13","objectID":"/datalab9/:6:2","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":"Submission Congratulations! You are finished with this assignment. ","date":"2024-08-13","objectID":"/datalab9/:7:0","tags":["Numpy","Pandas"],"title":"DATA100-lab9: Probability and Modeling","uri":"/datalab9/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab07.ipynb\") Lab 7: Gradient Descent and Feature Engineering In this lab, we will work through the process of: Defining loss functions Feature engineering Minimizing loss functions using numeric methods and analytical methods Understanding what happens if we use the analytical solution for OLS on a matrix with redundant features Computing a gradient for a nonlinear model Using gradient descent to optimize the nonline model This lab will continue using the toy tips calculation dataset used in Labs 5 and 6. Loading the Tips Dataset To begin, letâ€™s load the tips dataset from the seaborn library. This dataset contains records of tips, total bill, and information about the person who paid the bill. As earlier, weâ€™ll be trying to predict tips from the other data. import pandas as pd import numpy as np import seaborn as sns from sklearn.feature_extraction import DictVectorizer import matplotlib.pyplot as plt np.random.seed(42) plt.style.use('fivethirtyeight') sns.set() sns.set_context(\"talk\") %matplotlib inline data = sns.load_dataset(\"tips\") print(\"Number of Records:\", len(data)) data.head() Number of Records: 244\rtotal_bill\rtip\rsex\rsmoker\rday\rtime\rsize\r0\r16.99\r1.01\rFemale\rNo\rSun\rDinner\r2\r1\r10.34\r1.66\rMale\rNo\rSun\rDinner\r3\r2\r21.01\r3.50\rMale\rNo\rSun\rDinner\r3\r3\r23.68\r3.31\rMale\rNo\rSun\rDinner\r2\r4\r24.59\r3.61\rFemale\rNo\rSun\rDinner\r4\r","date":"2024-08-13","objectID":"/datalab7/:0:0","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Intro to Feature Engineering So far, weâ€™ve only considered models of the form $\\hat{y} = f_{\\theta}(x) = \\sum_{j=0}^d x_j\\theta_j$, where $\\hat{y}$ is quantitative continuous. We call this a linear model because it is a linear combination of the features (the $x_j$). However, our features donâ€™t need to be numbers: we could have categorical values such as names. Additionally, the true relationship doesnâ€™t have to be linear, as we could have a relationship that is quadratic, such as the relationship between the height of a projectile and time. In these cases, we often apply feature functions, functions that take in some value and output another value. This might look like converting a string into a number, combining multiple numeric values, or creating a boolean value from some filter. Then, if we call $\\phi$ (â€œphiâ€) our â€œphiâ€-ture function, our model takes the form $\\hat{y} = f_{\\theta}(x) = \\sum_{j=0}^d \\phi(x)_j\\theta_j$. ","date":"2024-08-13","objectID":"/datalab7/:1:0","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Example feature functions ç¼–ç ä¸€ç›´æ˜¯ä¸€ä¸ªå…ˆéªŒå·¥ç¨‹é—®é¢˜ï¼Ÿ vs AutoEncoders One-hot encoding converts a single categorical feature into many binary features, each of which represents one of the possible values in the original column each of the binary feature columns produced contains a 1 for rows that had that columnâ€™s label in the original column, and 0 elsewhere Polynomial features create polynomial combinations of features ","date":"2024-08-13","objectID":"/datalab7/:1:1","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 1: Defining the Model and Feature Engineering In Lab 6 we used the constant model. Now letâ€™s make a more complicated model that utilizes other features in our dataset. You can imagine that we might want to use the features with an equation that looks as shown below: $$ \\text{Tip} = \\theta_1 \\cdot \\text{total}_\\text{bill} + \\theta_2 \\cdot \\text{sex} + \\theta_3 \\cdot \\text{smoker} + \\theta_4 \\cdot \\text{day} + \\theta_5 \\cdot \\text{time} + \\theta_6 \\cdot \\text{size} $$ Unfortunately, thatâ€™s not possible because some of these features like â€œdayâ€ are not numbers, so it doesnâ€™t make sense to multiply by a numerical parameter. Letâ€™s start by converting some of these non-numerical values into numerical values. Before we do this, letâ€™s separate out the tips and the features into two separate variables. tips = data['tip'] X = data.drop(columns='tip') ","date":"2024-08-13","objectID":"/datalab7/:2:0","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 1a: Feature Engineering First, letâ€™s convert our features to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. For example, we can treat the day as a value from 1-7. However, one of the disadvantages in directly translating to a numeric value is that we unintentionally assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7, and Monday to the numeric value of 1. In our linear model, Sunday will have 7 times the influence of Monday, which can lower the accuracy of our model. Instead, letâ€™s use one-hot encoding to better represent these features! As you will learn in lecture, one-hot encoding is a way that we can produce a binary vector to indicate non-numeric features. In the tips dataset for example, we encode Sunday as the vector [0 0 0 1] because our dataset only contains bills from Thursday through Sunday. This assigns a more even weight across each category in non-numeric features. Complete the code below to one-hot encode our dataset. This dataframe holds our â€œfeaturizedâ€ data, which is also often denoted by $\\phi$. Hint: You may find the pd.get_dummies method or the DictVectorizer class useful when doing your one-hot encoding. def one_hot_encode(data): \"\"\" Return the one-hot encoded dataframe of our input data. Parameters ----------- data: a dataframe that may include non-numerical features Returns ----------- A one-hot encoded dataframe that only contains numeric features \"\"\" return pd.get_dummies(data, dtype=float) one_hot_X = one_hot_encode(X) one_hot_X.head() total_bill\rsize\rsex_Male\rsex_Female\rsmoker_Yes\rsmoker_No\rday_Thur\rday_Fri\rday_Sat\rday_Sun\rtime_Lunch\rtime_Dinner\r0\r16.99\r2\r0.0\r1.0\r0.0\r1.0\r0.0\r0.0\r0.0\r1.0\r0.0\r1.0\r1\r10.34\r3\r1.0\r0.0\r0.0\r1.0\r0.0\r0.0\r0.0\r1.0\r0.0\r1.0\r2\r21.01\r3\r1.0\r0.0\r0.0\r1.0\r0.0\r0.0\r0.0\r1.0\r0.0\r1.0\r3\r23.68\r2\r1.0\r0.0\r0.0\r1.0\r0.0\r0.0\r0.0\r1.0\r0.0\r1.0\r4\r24.59\r4\r0.0\r1.0\r0.0\r1.0\r0.0\r0.0\r0.0\r1.0\r0.0\r1.0\rgrader.check(\"q1a\") ","date":"2024-08-13","objectID":"/datalab7/:2:1","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 1b: Defining the Model Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 12 features instead of 6. Therefore, our linear model now looks like: $$ \\text{Tip} = \\theta_1 \\cdot \\text{size} + \\theta_2 \\cdot \\text{total}_\\text{bill} + \\theta_3 \\cdot \\text{day}_\\text{Thur} + \\theta_4 \\cdot \\text{day}_\\text{Fri} + â€¦ + \\theta_{11} \\cdot \\text{time}_\\text{Lunch} + \\theta_{12} \\cdot \\text{time}_\\text{Dinner} $$ We can represent the linear combination above as a matrix-vector product. Implement the linear_model function to evaluate this product. Below, we create a MyLinearModel class with two methods, predict and fit. When fitted, this model fails to do anything useful, setting all of its 12 parameters to zero. class MyLinearModel(): def predict(self, X): return X @ self._thetas def fit(self, X, y): number_of_features = X.shape[1] self._thetas = np.zeros(shape = (number_of_features, 1)) model = MyLinearModel() model.fit(one_hot_X, tips) model._thetas array([[0.],\r[0.],\r[0.],\r[0.],\r[0.],\r[0.],\r[0.],\r[0.],\r[0.],\r[0.],\r[0.],\r[0.]])\r","date":"2024-08-13","objectID":"/datalab7/:2:2","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 2: Fitting a Linear Model using scipy.optimize.minimize Methods Recall in Lab 5 and in lecture 12 we defined multiple loss functions and found the optimal theta using the scipy.optimize.minimize function. Adapt the code below to implement the fit method of the linear model. Note that weâ€™ve added a loss_function parameter where the model is fit using the desired loss function, i.e. not necssarily the L2 loss. Example loss function are given as l1 and l2. from scipy.optimize import minimize def l1(y, y_hat): return np.abs(y - y_hat) def l2(y, y_hat): return (y - y_hat)**2 class MyLinearModel(): def predict(self, X): return X @ self._thetas def fit(self, loss_function, X, y): \"\"\" Produce the estimated optimal _thetas for the given loss function, feature matrix X, and observations y. Parameters ----------- loss_function: either the squared or absolute loss functions defined above X: a 2D dataframe (or numpy array) of numeric features (one-hot encoded) y: a 1D vector of tip amounts Returns ----------- The estimate for the optimal theta vector that minimizes our loss \"\"\" number_of_features = X.shape[1] ## Notes on the following function call which you need to finish: # # 0. The starting guess should be some arbitrary array of the correct length. # Note the \"number of features\" variable above.\" # 1. The ... in \"lambda theta: ...\" should be replaced by the average loss if we # compute X @ theta. The loss is measured using the given loss function, # relative to the observations in the variable y. starting_guess = np.random.rand(number_of_features) self._thetas = minimize(lambda theta: loss_function(y, X @ theta).mean() , x0 = starting_guess)['x'] # Notice above that we extract the 'x' entry in the dictionary returned by `minimize`. # This entry corresponds to the optimal theta estimated by the function. Sorry # we know it's a little confusing, but 'x' is hard coded into the minimize function # because of the fact that in the optimization universe \"x\" is what you optimize over. # It'd be less confusing for DS100 students if they used \"theta\". # When you run the code below, you should get back some non zero thetas. model = MyLinearModel() model.fit(l2, one_hot_X, tips) model._thetas array([ 0.09448702, 0.17599315, 0.31373886, 0.34618029, -0.22256393,\r-0.13615575, 0.30569628, 0.46797868, 0.34653012, 0.44250887,\r0.1939605 , 0.1258012 ])\rgrader.check(\"q2\") q2 passed! ğŸ’¯ The MSE for your model above should be just slightly larger than 1: from sklearn.metrics import mean_squared_error mean_squared_error(model.predict(one_hot_X), tips) np.float64(1.0103535612506567)\r","date":"2024-08-13","objectID":"/datalab7/:3:0","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 3: Fitting the Model using Analytic Methods Letâ€™s also fit our model analytically for the L2 loss function. Recall from lecture that with a linear model, we are solving the following optimization problem for least squares: $$\\min_{\\theta} ||\\Bbb{X}\\theta - \\Bbb{y}||^2$$ We showed in Lecture 11 that the optimal $\\hat{\\theta}$ when $X^TX$ is invertible is given by the equation: $(X^TX)^{-1}X^TY$ ","date":"2024-08-13","objectID":"/datalab7/:4:0","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 3a: Analytic Solution Using Explicit Inverses For this problem, implement the analytic solution above using np.linalg.inv to compute the inverse of $X^TX$. Reminder: To compute the transpose of a matrix, you can use X.T or X.transpose() class MyAnalyticallyFitOLSModel(): def predict(self, X): return X @ self._thetas def fit(self, X, y): \"\"\" Sets _thetas using the analytical solution to the ordinary least squares problem Parameters ----------- X: a 2D dataframe (or numpy array) of numeric features (one-hot encoded) y: a 1D vector of tip amounts Returns ----------- The estimate for theta computed using the equation mentioned above \"\"\" xTx = X.T @ X xTy = X.T @ y self._thetas = np.linalg.inv(xTx) @ xTy Now, run the cell below to find the analytical solution for the tips dataset. Depending on the machine that you run your code on, you should either see a singular matrix error or end up with thetas that are nonsensical (magnitudes greater than 10^15). This is not good! # When you run the code below, you should get back some non zero thetas. model = MyAnalyticallyFitOLSModel() model.fit(one_hot_X, tips) analytical_thetas = model._thetas analytical_thetas array([ 9.66544413e+00, -1.89677732e+02, -8.30149679e+17, -8.30149679e+17,\r8.30149679e+17, 8.30149679e+17, -2.56000000e+02, 0.00000000e+00,\r-3.20000000e+01, 3.20000000e+01, -8.00000000e+00, 0.00000000e+00])\rIn the cell below, explain why we got the errorï¼ˆæŒ‡å‚æ•°ä¸å¯¹ï¼Ÿï¼‰ above when trying to calculate the analytical solution for our one-hot encoded tips dataset. æœ¬è´¨ä¸Šæ˜¯å› ä¸ºçŸ©é˜µ ä¸å¯é€†ï¼Œç‹¬çƒ­ç¼–ç æŸäº›çº¿æ€§ç»„åˆä¹‹åå¯ä»¥è½»æ˜“çœ‹å‡ºçŸ©é˜µ $X^TX$ ä¸æ˜¯æ»¡ç§©çš„ ","date":"2024-08-13","objectID":"/datalab7/:4:1","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 3b: Fixing our One-Hot Encoding Now, letâ€™s fix our one-hot encoding approach from question 1 so we donâ€™t get the error we saw in question 3a. Complete the code below to one-hot-encode our dataset such that one_hot_X_revised has no redundant features. def one_hot_encode_revised(data): \"\"\" Return the one-hot encoded dataframe of our input data, removing redundancies. Parameters ----------- data: a dataframe that may include non-numerical features Returns ----------- A one-hot encoded dataframe that only contains numeric features without any redundancies. \"\"\" columns = ['sex', 'smoker', 'day', 'time'] for column in columns: values = data[column].unique() for value in values[:-1]: # è¿™æ˜¯ç”¨[]åˆ‡ç‰‡çš„æŠ€å·§ï¼Œä»valuesä¸­å–é™¤äº†æœ€åä¸€ä¸ªå…ƒç´ çš„æ‰€æœ‰å…ƒç´  data[column + '=' + value] = (data[column] == value).astype(int) data = data.drop(column, axis=1) # åˆ é™¤åŸå§‹çš„åˆ— return data one_hot_X_revised = one_hot_encode_revised(X) numerical_model = MyLinearModel() numerical_model.fit(l2, one_hot_X_revised, tips) analytical_model = MyAnalyticallyFitOLSModel() analytical_model.fit(one_hot_X_revised, tips) print(\"Our numerical model's loss is: \", mean_squared_error(numerical_model.predict(one_hot_X_revised), tips)) print(\"Our analytical model's loss is: \", mean_squared_error(analytical_model.predict(one_hot_X_revised), tips)) Our numerical model's loss is: 1.0255082437778105\rOur analytical model's loss is: 1.0255082436053506\rgrader.check(\"q3b\") ","date":"2024-08-13","objectID":"/datalab7/:4:2","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 3c: Analyzing our new One-Hot Encoding Why did removing redundancies in our one-hot encoding fix the problem we had in 3a? ä¸æ˜¯å…¨éƒ¨è¿›è¡Œç‹¬çƒ­ç¼–ç æ“ä½œï¼Œé¿å…çº¿æ€§ç›¸å…³æ€§ Note: An alternate approach is to use np.linalg.solve instead of np.linalg.inv. For the example above, even with the redundant features, np.linalg.solve will work well. Though in general, itâ€™s best to drop redundant features anyway. In case you want to learn more, here is a relevant Stack Overflow post: https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li ","date":"2024-08-13","objectID":"/datalab7/:4:3","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 4: Gradient Descent # Run this cell to load the data for this problem df = pd.read_csv(\"lab7_data.csv\", index_col=0) df.head() x\ry\r0\r-5.000000\r-7.672309\r1\r-4.966555\r-7.779735\r2\r-4.933110\r-7.995938\r3\r-4.899666\r-8.197059\r4\r-4.866221\r-8.183883\rIf we plot this data, we see that there is a clear sinusoidal relationship between x and y. import plotly.express as px px.scatter(df, x = \"x\", y = \"y\") In this exercise, weâ€™ll show gradient descent is so powerful it can even optimize a nonlinear model. Specifically, weâ€™re going to model the relationship of our data by: $$\\Large{ f_{\\boldsymbol{\\theta(x)}} = \\theta_1x + sin(\\theta_2x) }$$ Our model is parameterized by both $\\theta_1$ and $\\theta_2$, which we can represent in the vector, $\\boldsymbol{\\theta}$. Note that a general sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. Here, weâ€™re assuming the amplitude $a$ is around 1, and the phase shifting parameter $c$ is around zero. We do not attempt to justify this assumption and youâ€™re welcome to see what happens if you ignore this assumption at the end of this lab. You might ask why we donâ€™t just create a linear model like we did earlier with a sinusoidal feature. The issue is that the theta is INSIDE the sin function. In other words, linear models use their parameters to adjust the scale of each feature, but $\\theta_2$ in this model adjusts the frequency of the feature. There are tricks we could play to use our linear model framework here, but we wonâ€™t attempt this in our lab. We define the sin_model function below that predicts $\\textbf{y}$ (the $y$-values) using $\\textbf{x}$ (the $x$-values) based on our new equation. def sin_model(x, theta): \"\"\" Predict the estimate of y given x, theta_1, theta_2 Keyword arguments: x -- the vector of values x theta -- a vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2 \"\"\" theta_1 = theta[0] theta_2 = theta[1] return theta_1 * x + np.sin(theta_2 * x) ","date":"2024-08-13","objectID":"/datalab7/:5:0","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 4a: Computing the Gradient of the MSE With Respect to Theta on the Sin Model Recall $\\hat{\\theta}$ is the value of $\\theta$ that minimizes our loss function. One way of solving for $\\hat{\\theta}$ is by computing the gradient of our loss function with respect to $\\theta$, like we did in lecture: https://docs.google.com/presentation/d/1j9ESgjn-aeZSOX5ON1wjkF5WBZHc4IN7XvTpYnX1pFs/edit#slide=id.gfc76b62ec3_0_27. Recall that the gradient is a column vector of two partial derivatives. Write/derive the expressions for following values and use them to fill in the functions below. $L(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2)$: our loss function, the mean squared error $\\frac{\\partial L }{\\partial \\theta_1}$: the partial derivative of $L$ with respect to $\\theta_1$ $\\frac{\\partial L }{\\partial \\theta_2}$: the partial derivative of $L$ with respect to $\\theta_2$ Recall that $L(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2) = \\frac{1}{n} \\sum_{i=1}^{n} (\\textbf{y}_i - \\hat{\\textbf{y}}_i)^2$ Specifically, the functions sin_MSE, sin_MSE_dt1 and sin_MSE_dt2 should compute $R$, $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ respectively. Use the expressions you wrote for $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ to implement these functions. In the functions below, the parameter theta is a vector that looks like $\\begin{bmatrix} \\theta_1 \\ \\theta_2 \\end{bmatrix}$. We have completed sin_MSE_gradient, which calls dt1 and dt2 and returns the gradient dt for you. Notes: Keep in mind that we are still working with our original set of data, df. To keep your code a bit more concise, be aware that np.mean does the same thing as np.sum divided by the length of the numpy array. *æ³¨æ„meançš„å±‚çº§åˆ« Another way to keep your code more concise is to use the function sin_model we defined which computes the output of the model. def sin_MSE(theta, x, y): \"\"\" Compute the numerical value of the l2 loss of our sinusoidal model given theta Keyword arguments: theta -- the vector of values theta x -- the vector of x values y -- the vector of y values \"\"\" return np.mean((y - sin_model(x, theta))**2) def sin_MSE_dt1(theta, x, y): \"\"\" Compute the numerical value of the partial of l2 loss with respect to theta_1 Keyword arguments: theta -- the vector of values theta x -- the vector of x values y -- the vector of y values \"\"\" return np.mean(-2 * (y - sin_model(x, theta)) * x) def sin_MSE_dt2(theta, x, y): \"\"\" Compute the numerical value of the partial of l2 loss with respect to theta_2 Keyword arguments: theta -- the vector of values theta x -- the vector of x values y -- the vector of y values \"\"\" return np.mean(-2*(y-sin_model(x, theta))*x*np.cos(theta[1]*x)) # This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you. def sin_MSE_gradient(theta, x, y): \"\"\" Returns the gradient of l2 loss with respect to vector theta Keyword arguments: theta -- the vector of values theta x -- the vector of x values y -- the vector of y values \"\"\" return np.array([sin_MSE_dt1(theta, x, y), sin_MSE_dt2(theta, x, y)]) grader.check(\"q4a\") ","date":"2024-08-13","objectID":"/datalab7/:5:1","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Question 4b: Implementing Gradient Descent and Using It to Optimize the Sin Model Letâ€™s now implement gradient descent. Note that the function youâ€™re implementing here is somewhat different than the gradient descent function we created in lecture. The version in lecture was gradient_descent(df, initial_guess, alpha, n), where df was the gradient of the function we are minimizing and initial_guess are the starting parameters for that function. Here our signature is a bit different (described below) than the gradient_descent implementation from lecture. def init_theta(): \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\" return np.array([0, 0]) def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1): \"\"\" Run gradient descent update for a finite number of iterations and static learning rate Keyword arguments: loss_f -- the loss function to be minimized (used for computing loss_history) gradient_loss_f -- the gradient of the loss function to be minimized theta -- the vector of values theta to use at first iteration data -- the data used in the model num_iter -- the max number of iterations alpha -- the learning rate (also called the step size) Return: theta -- the optimal value of theta after num_iter of gradient descent theta_history -- the series of theta values over each iteration of gradient descent loss_history -- the series of loss values over each iteration of gradient descent \"\"\" theta_history = [] loss_history = [] for i in range(num_iter): theta_history.append(theta) # å…ˆappendæ¯”è¾ƒå¥½ loss_history.append(loss_f(theta, data['x'], data['y'])) d_b = gradient_loss_f(theta, data['x'], data['y']) theta = theta - alpha * d_b return theta, theta_history, loss_history theta_start = init_theta() theta_hat, thetas_used, losses_calculated = grad_desc( sin_MSE, sin_MSE_gradient, theta_start, df, num_iter=20, alpha=0.1 ) for b, l in zip(thetas_used, losses_calculated): print(f\"theta: {b}, Loss: {l}\") theta: [0 0], Loss: 20.859191416422235\rtheta: [2.60105745 2.60105745], Loss: 9.285008173048666\rtheta: [0.90342728 2.59100602], Loss: 4.680169273815357\rtheta: [2.05633644 2.9631291 ], Loss: 2.6242517936325833\rtheta: [1.15892347 2.86687431], Loss: 1.4765157174727774\rtheta: [1.79388042 3.07275573], Loss: 0.9073271435862448\rtheta: [1.32157494 3.00146569], Loss: 0.541531643291128\rtheta: [1.64954491 3.02910866], Loss: 0.3775841142469479\rtheta: [1.42325294 2.98820303], Loss: 0.2969750688130759\rtheta: [1.58295041 3.01033846], Loss: 0.2590425421375732\rtheta: [1.47097255 2.98926519], Loss: 0.23973439443291833\rtheta: [1.55040965 3.0017442 ], Loss: 0.23034782416254634\rtheta: [1.49439132 2.99135194], Loss: 0.2255775832667724\rtheta: [1.5341564 2.99797824], Loss: 0.22321772191904068\rtheta: [1.50603995 2.99286671], Loss: 0.22202363967204045\rtheta: [1.52598919 2.99628665], Loss: 0.22142811500262397\rtheta: [1.51186655 2.99375531], Loss: 0.22112776381775168\rtheta: [1.52188208 2.99549617], Loss: 0.22097741373654575\rtheta: [1.51478773 2.99423497], Loss: 0.22090173185683032\rtheta: [1.51981739 2.99511516], Loss: 0.2208637810584589\rgrader.check(\"q4b\") If you pass the tests above, youâ€™re done coding for this lab, though there are some cool visualizations below weâ€™d like you to think about. Letâ€™s visually inspect our results of running gradient descent to optimize $\\boldsymbol\\theta$. The code below plots our $x$-values with our modelâ€™s predicted $\\hat{y}$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\\boldsymbol\\theta$. theta_init = init_theta() theta_est, thetas, loss = grad_desc(sin_MSE, sin_MSE_gradient, theta_init, df) Plotting our model output over our observaitons shows that gradient descent did a great job finding both the overall increase (slope) of the data, as well as the oscillation frequency. x, y = df['x'], df['y'] y_pred = sin_model(x, theta_est) plt.plot(x, y_pred, label='Model ($\\hat{y}$)') plt.scatter(x, y, alpha=0.5, label='Observati","date":"2024-08-13","objectID":"/datalab7/:5:2","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":"Visualizing Loss (Extra) Letâ€™s visualize our loss functions and gain some insight as to how gradient descent optimizes our model parameters. In the previous plot we saw the loss decrease with each iteration. In this part, weâ€™ll see the trajectory of the algorithm as it travels the loss surface? Run the following cells to see visualization of this trajectory. thetas = np.array(thetas).squeeze() loss = np.array(loss) thetas array([[0. , 0. ],\r[2.60105745, 2.60105745],\r[0.90342728, 2.59100602],\r[2.05633644, 2.9631291 ],\r[1.15892347, 2.86687431],\r[1.79388042, 3.07275573],\r[1.32157494, 3.00146569],\r[1.64954491, 3.02910866],\r[1.42325294, 2.98820303],\r[1.58295041, 3.01033846],\r[1.47097255, 2.98926519],\r[1.55040965, 3.0017442 ],\r[1.49439132, 2.99135194],\r[1.5341564 , 2.99797824],\r[1.50603995, 2.99286671],\r[1.52598919, 2.99628665],\r[1.51186655, 2.99375531],\r[1.52188208, 2.99549617],\r[1.51478773, 2.99423497],\r[1.51981739, 2.99511516]])\r# Run me to see a 3D plot (gradient descent with static alpha) from lab7_utils import plot_3d plot_3d(thetas[:, 0], thetas[:, 1], loss, mean_squared_error, sin_model, x, y) import plotly import plotly.graph_objs as go def contour_plot(title, theta_history, loss_function, model, x, y): \"\"\" The function takes the following as argument: theta_history: a (N, 2) array of theta history loss: a list or array of loss value loss_function: for example, l2_loss model: for example, sin_model x: the original x input y: the original y output \"\"\" theta_1_series = theta_history[:,0] # a list or array of theta_1 value theta_2_series = theta_history[:,1] # a list or array of theta_2 value ## In the following block of code, we generate the z value ## across a 2D grid theta1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1) theta2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1) x_s, y_s = np.meshgrid(theta1_s, theta2_s) data = np.stack([x_s.flatten(), y_s.flatten()]).T ls = [] for theta1, theta2 in data: l = loss_function(model(x, np.array([theta1, theta2])), y) ls.append(l) z = np.array(ls).reshape(50, 50) # Create trace of theta point # Create the contour theta_points = go.Scatter(name=\"theta Values\", x=theta_1_series, y=theta_2_series, mode=\"lines+markers\") lr_loss_contours = go.Contour(x=theta1_s, y=theta2_s, z=z, colorscale='Viridis', reversescale=True) plotly.offline.iplot(go.Figure(data=[lr_loss_contours, theta_points], layout={'title': title})) contour_plot('Gradient Descent with Static Learning Rate', thetas, mean_squared_error, sin_model, df[\"x\"], df[\"y\"]) As we can see, gradient descent is able to navigate even this fairly complex loss space and find a nice minimum. Congratulations! You finished the lab! ","date":"2024-08-13","objectID":"/datalab7/:6:0","tags":["Numpy","Pandas","Scikit-Learn","SciPy","Plotly","Seaborn"],"title":"DATA100-lab7: Gradient Descent and Feature Engineering","uri":"/datalab7/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab08.ipynb\") Lab 8: Model Selection, Regularization, and Cross-Validation In this lab, you will practice using scikit-learn to generate models of various complexity. Youâ€™ll then use the holdout method and K-fold cross-validation to select the models that generalize best. # Run this cell to set up your notebook import seaborn as sns import csv import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline sns.set() sns.set_context(\"talk\") from IPython.display import display, Latex, Markdown ","date":"2024-08-13","objectID":"/datalab8/:0:0","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Introduction For this lab, we will use a toy dataset to predict the house prices in Boston with data provided by the sklearn.datasets package. There are more interesting datasets in the package if you want to explore them during your free time! Run the following cell to load the data. load_boston() will return a dictionary object which includes keys for: data : the covariates (X) target : the response vector (Y) feature_names: the column names DESCR : a full description of the data filename: name of the csv file import pickle boston_data = pickle.load(open(\"boston_data.pickle\", \"rb\")) print(boston_data.keys()) sum(boston_data.data) dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])\rarray([1.82844292e+03, 5.75000000e+03, 5.63521000e+03, 3.50000000e+01,\r2.80675700e+02, 3.18002500e+03, 3.46989000e+04, 1.92029160e+03,\r4.83200000e+03, 2.06568000e+05, 9.33850000e+03, 6.40245000e+03])\rprint(boston_data['DESCR']) .. _boston_dataset:\rBoston house prices dataset\r---------------------------\r**Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 12 numeric/categorical predictive. Median Value (attribute 13) is usually the target.\r:Attribute Information (in order):\r- CRIM per capita crime rate by town\r- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\r- INDUS proportion of non-retail business acres per town\r- CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\r- NOX nitric oxides concentration (parts per 10 million)\r- RM average number of rooms per dwelling\r- AGE proportion of owner-occupied units built prior to 1940\r- DIS weighted distances to five Boston employment centres\r- RAD index of accessibility to radial highways\r- TAX full-value property-tax rate per $10,000\r- PTRATIO pupil-teacher ratio by town\r- LSTAT % lower status of the population\r- MEDV Median value of owner-occupied homes in $1000's\r:Missing Attribute Values: None\r:Creator: Harrison, D. and Rubinfeld, D.L.\rThis is a copy of UCI ML housing dataset.\rhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\rThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\rThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\rprices and the demand for clean air', J. Environ. Economics \u0026 Management,\rvol.5, 81-102, 1978. Used in Belsley, Kuh \u0026 Welsch, 'Regression diagnostics\r...', Wiley, 1980. N.B. Various transformations are used in the table on\rpages 244-261 of the latter.\rThe Boston house-price data has been used in many machine learning papers that address regression\rproblems. .. topic:: References\r- Belsley, Kuh \u0026 Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\r- Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\rA look at the DESCR attribute tells us the data contains these features: 1. CRIM per capita crime rate by town\r2. ZN proportion of residential land zoned for lots over 25,000 sq.ft.\r3. INDUS proportion of non-retail business acres per town\r4. CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\r5. NOX nitric oxides concentration (parts per 10 million)\r6. RM average number of rooms per dwelling\r7. AGE proportion of owner-occupied units built prior to 1940\r8. DIS weighted distances to five Boston employment centres\r9. RAD index of accessibility to radial highways\r10. TAX full-value property-tax rate per 10,000 USD\r11. PTRATIO pupil-teacher ratio by town\r12. LSTAT % lower status of the population\rLetâ€™s now convert this data into a pandas DataFrame. boston = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names']) boston.head() CRIM\rZN\rINDUS\rCHAS\rNOX\rRM\rAGE\rDIS\rRAD\rTAX\rPTRATIO\rLSTAT\r0\r0.00632\r18.0\r2.31\r0.0\r0.538\r6.575\r65.2\r4.0900\r1.0\r296.0\r15.3\r4.98\r1\r0.0273","date":"2024-08-13","objectID":"/datalab8/:0:1","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Question 1 Letâ€™s model this housing price data! Before we can do this, however, we need to split the data into training and test sets. Remember that the response vector (housing prices) lives in the target attribute. A random seed is set here so that we can deterministically generate the same splitting in the future if we want to test our result again and find potential bugs. Use the train_test_split function to split out 10% of the data for the test set. Call the resulting splits X_train, X_holdout, Y_train, Y_holdout. Here â€œholdoutâ€ refers to the fact that weâ€™re going to hold this data our when training our model. from sklearn.model_selection import train_test_split np.random.seed(45) X = boston Y = pd.Series(boston_data['target']) X_train, X_holdout, Y_train, Y_holdout = train_test_split(X, Y, test_size=0.1) grader.check(\"q1\") q1 passed! ğŸš€ ","date":"2024-08-13","objectID":"/datalab8/:0:2","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Question 2 As a warmup, fit a linear model to describe the relationship between the housing price and all available covariates. Weâ€™ve imported sklearn.linear_model as lm, so you can use that instead of typing out the whole module name. Fill in the cells below to fit a linear regression model to the covariates and create a scatter plot for our predictions vs. the true prices. import sklearn.linear_model as lm linear_model = lm.LinearRegression() # Fit your linear model linear_model.fit(X_train, Y_train) # Predict housing prices on the test set Y_pred = linear_model.predict(X_holdout) # Plot predicted vs true prices plt.scatter(Y_holdout, Y_pred, alpha=0.5) plt.xlabel(\"Prices $(y)$\") plt.ylabel(\"Predicted Prices $(\\hat{y})$\") plt.title(\"Prices vs Predicted Prices\"); \u003c\u003e:14: SyntaxWarning: invalid escape sequence '\\h'\r\u003c\u003e:14: SyntaxWarning: invalid escape sequence '\\h'\rC:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_6688\\1494534656.py:14: SyntaxWarning: invalid escape sequence '\\h'\rplt.ylabel(\"Predicted Prices $(\\hat{y})$\")\rBriefly analyze the scatter plot above. Do you notice any outliers? Write your answer in the cell below. ç†æƒ³æƒ…å†µåº”è¯¥æ˜¯å‡åŒ€ï¼Ÿä¸”è¾ƒçª„åˆ†å¸ƒäºy=xç›´çº¿ä¸Š Alternately, we can plot the residuals vs. our model predictions. Ideally theyâ€™d all be zero. Given the inevitably of noise, weâ€™d at least like them to be scatter randomly across the line where the residual is zero. By contrast, there appears to be a possible pattern, with our model consistently underestimating prices for both very low and very high values, and possibly consistently overestimating prices towards the middle range. plt.scatter(Y_pred, Y_holdout - Y_pred, alpha=0.5) plt.ylabel(\"Residual $(y - \\hat{y})$\") plt.xlabel(\"Predicted Prices $(\\hat{y})$\") plt.title(\"Residuals vs Predicted Prices\") plt.title(\"Residual of prediction for i'th house\") plt.axhline(y = 0, color='r'); \u003c\u003e:2: SyntaxWarning: invalid escape sequence '\\h'\r\u003c\u003e:3: SyntaxWarning: invalid escape sequence '\\h'\r\u003c\u003e:2: SyntaxWarning: invalid escape sequence '\\h'\r\u003c\u003e:3: SyntaxWarning: invalid escape sequence '\\h'\rC:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_6688\\2491234216.py:2: SyntaxWarning: invalid escape sequence '\\h'\rplt.ylabel(\"Residual $(y - \\hat{y})$\")\rC:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_6688\\2491234216.py:3: SyntaxWarning: invalid escape sequence '\\h'\rplt.xlabel(\"Predicted Prices $(\\hat{y})$\")\r","date":"2024-08-13","objectID":"/datalab8/:0:3","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Question 3 As we find from the scatter plot, our model is not perfect. If it were perfect, we would see the identity line (i.e. a line of slope 1). Compute the root mean squared error (RMSE) of the predicted responses: $$ \\textbf{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 } $$ Fill out the function below and compute the RMSE for our predictions on both the training data X_train and the held out set X_holdout. Your implementation should not use for loops. def rmse(actual_y, predicted_y): \"\"\" Args: predicted_y: an array of the prediction from the model actual_y: an array of the groudtruth label Returns: The root mean square error between the prediction and the groudtruth \"\"\" return np.sqrt(np.mean((predicted_y - actual_y)**2)) train_error = rmse(Y_train, linear_model.predict(X_train)) holdout_error = rmse(Y_holdout, Y_pred) print(\"Training RMSE:\", train_error) print(\"Holdout RMSE:\", holdout_error) Training RMSE: 4.633297105625516\rHoldout RMSE: 5.685160866583937\rgrader.check(\"q3\") q3 passed! ğŸ™Œ Is your training error lower than the error on the data the model never got to see? If so, why could this be happening? Answer in the cell below. ç¨å¾®è¿‡æ‹Ÿåˆï¼Ÿ ","date":"2024-08-13","objectID":"/datalab8/:0:4","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Overfitting Sometimes we can get even higher accuracy by adding more features. For example, the code below adds the square, square root, and hyperbolic tangent of every feature to the design matrix. Weâ€™ve chosen these bizarre features specifically to highlight overfitting. boston_with_extra_features = boston.copy() for feature_name in boston.columns: boston_with_extra_features[feature_name + \"^2\"] = boston_with_extra_features[feature_name] ** 2 boston_with_extra_features[\"sqrt\" + feature_name] = np.sqrt(boston_with_extra_features[feature_name]) boston_with_extra_features[\"tanh\" + feature_name] = np.tanh(boston_with_extra_features[feature_name]) boston_with_extra_features.head(5) CRIM\rZN\rINDUS\rCHAS\rNOX\rRM\rAGE\rDIS\rRAD\rTAX\r...\rtanhRAD\rTAX^2\rsqrtTAX\rtanhTAX\rPTRATIO^2\rsqrtPTRATIO\rtanhPTRATIO\rLSTAT^2\rsqrtLSTAT\rtanhLSTAT\r0\r0.00632\r18.0\r2.31\r0.0\r0.538\r6.575\r65.2\r4.0900\r1.0\r296.0\r...\r0.761594\r87616.0\r17.204651\r1.0\r234.09\r3.911521\r1.0\r24.8004\r2.231591\r0.999905\r1\r0.02731\r0.0\r7.07\r0.0\r0.469\r6.421\r78.9\r4.9671\r2.0\r242.0\r...\r0.964028\r58564.0\r15.556349\r1.0\r316.84\r4.219005\r1.0\r83.5396\r3.023243\r1.000000\r2\r0.02729\r0.0\r7.07\r0.0\r0.469\r7.185\r61.1\r4.9671\r2.0\r242.0\r...\r0.964028\r58564.0\r15.556349\r1.0\r316.84\r4.219005\r1.0\r16.2409\r2.007486\r0.999368\r3\r0.03237\r0.0\r2.18\r0.0\r0.458\r6.998\r45.8\r6.0622\r3.0\r222.0\r...\r0.995055\r49284.0\r14.899664\r1.0\r349.69\r4.324350\r1.0\r8.6436\r1.714643\r0.994426\r4\r0.06905\r0.0\r2.18\r0.0\r0.458\r7.147\r54.2\r6.0622\r3.0\r222.0\r...\r0.995055\r49284.0\r14.899664\r1.0\r349.69\r4.324350\r1.0\r28.4089\r2.308679\r0.999953\r5 rows Ã— 48 columns We split up our data again and refit the model. From this cell forward, we append 2 to the variable names X_train, X_holdout, Y_train, Y_holdout, train_error, holdout_error in order to maintain our original data. Make sure you use these variable names from this cell forward, at least until we get to the part where we create version 3 of each of these. np.random.seed(25) X = boston_with_extra_features X_train2, X_holdout2, Y_train2, Y_holdout2 = train_test_split(X, Y, test_size = 0.10) linear_model.fit(X_train2, Y_train2); Looking at our training and test RMSE, we see that they are lower than you computed earlier. This strange model is seemingly better, even though it includes seemingly useless features like the hyperbolic tangent of the average number of rooms per dwelling. train_error2 = rmse(Y_train2, linear_model.predict(X_train2)) holdout_error2 = rmse(Y_holdout2, linear_model.predict(X_holdout2)) print(\"Training RMSE:\", train_error2) print(\"Holdout RMSE:\", holdout_error2) Training RMSE: 3.3514483036916287\rHoldout RMSE: 5.410120414381265\rThe code below generates the training and holdout RMSE for 49 different models stores the results in a DataFrame. The first model uses only the first feature â€œCRIMâ€. The second model uses the first two features â€œCRIMâ€ and â€œZNâ€, and so forth. errors_vs_N = pd.DataFrame(columns = [\"N\", \"Training Error\", \"Holdout Error\"]) range_of_num_features = range(1, X_train2.shape[1] + 1) for N in range_of_num_features: X_train_first_N_features = X_train2.iloc[:, :N] linear_model.fit(X_train_first_N_features, Y_train2) train_error_overfit = rmse(Y_train2, linear_model.predict(X_train_first_N_features)) X_holdout_first_N_features = X_holdout2.iloc[:, :N] holdout_error_overfit = rmse(Y_holdout2, linear_model.predict(X_holdout_first_N_features)) errors_vs_N.loc[len(errors_vs_N)] = [N, train_error_overfit, holdout_error_overfit] errors_vs_N N\rTraining Error\rHoldout Error\r0\r1.0\r8.536340\r7.825177\r1\r2.0\r8.085693\r7.637465\r2\r3.0\r7.776942\r7.213870\r3\r4.0\r7.643897\r6.391482\r4\r5.0\r7.634894\r6.372166\r5\r6.0\r5.698878\r7.635694\r6\r7.0\r5.689554\r7.585860\r7\r8.0\r5.399034\r7.158563\r8\r9.0\r5.379679\r7.281769\r9\r10.0\r5.318218\r7.231629\r10\r11.0\r5.088829\r6.922974\r11\r12.0\r4.680294\r5.437528\r12\r13.0\r4.679671\r5.443388\r13\r14.0\r4.664717\r5.448438\r14\r15.0\r4.627661\r5.479720\r15\r16.0\r4.613226\r5.488425\r16\r17.0\r4.580971\r5.389309\r17\r18.0\r4.580622\r5.391183\r18\r19.0\r4.507301\r5.185114\r19\r20.0\r4.482925\r5.194924\r20\r21.0\r4.482412\r5.188007\r21\r22.0\r4.4824","date":"2024-08-13","objectID":"/datalab8/:1:0","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Regularization As an alternative and more realistic example, instead of using only the first N features, we can use various different regularization strengths. For example, for really low regularization strengths (e.g. $\\alpha = 10^{-3}$), we get a model that is very identical to our linear regression model. from sklearn.linear_model import Ridge regularized_model = Ridge(alpha = 10**-5) regularized_model.fit(X_train2, Y_train2) regularized_model.coef_ d:\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning:\rIll-conditioned matrix (rcond=6.11696e-19): result may not be accurate.\rarray([ 4.44044277e-01, -3.00268517e-02, 2.03776925e+00, 3.54247206e-01,\r-1.19704083e+02, 1.63780073e+01, -3.10555372e-01, -1.31182539e+01,\r2.87010751e+00, 7.68411439e-01, 2.43201974e+01, 2.09160420e+00,\r-1.17012738e-03, -5.60565882e+00, 6.79680723e+00, 1.02949752e-03,\r-1.31223400e+00, 6.99621340e+00, -3.55165065e-02, -7.66339676e+00,\r-2.53950130e+00, 3.54247186e-01, 3.54247186e-01, 2.69792455e-01,\r1.91778126e+00, 3.11293526e+02, -1.53815298e+02, 8.03364965e-01,\r-1.17792246e+02, 3.25883430e+02, 1.08476149e-03, 2.42998443e+00,\r2.52462516e+02, 3.55080093e-01, 3.78504405e+01, -8.11283072e+01,\r-5.18073808e-02, -8.51699934e+00, 1.14213610e+01, -2.86248788e-04,\r-2.10606164e+01, 0.00000000e+00, -1.85988225e-01, -1.54605184e+02,\r5.73422430e-06, -1.79546600e-02, -1.53342390e+01, -4.25637232e+01])\rlinear_model.fit(X_train2, Y_train2) linear_model.coef_ array([ 3.65647144e-01, 7.96329260e-02, 1.50196461e+00, 3.72759210e-01,\r-1.82281287e+03, 6.19862020e+02, -2.86690023e-01, -1.29491141e+01,\r1.68693762e+00, 7.86841735e-01, 1.62893036e+01, 1.95113824e+00,\r-9.11835586e-04, -5.02513063e+00, 5.90016774e+00, 6.12889765e-04,\r-2.21247181e+00, 8.90275845e+00, -2.73913970e-02, -5.40098561e+00,\r-4.23462112e+00, 3.72978675e-01, 3.72978861e-01, 2.84060205e-01,\r5.41748851e+02, 4.88274463e+02, 1.16998609e+03, -1.36350124e+01,\r-2.23299632e+03, 5.18647024e+04, 1.04162650e-03, 2.14549424e+00,\r4.31003519e+02, 3.51263646e-01, 3.77337190e+01, -8.06896603e+01,\r-2.88295129e-02, -4.52779826e+00, 8.15771554e+00, -2.99443268e-04,\r-2.14061912e+01, 3.63797881e-12, -1.15683673e-01, -1.07968511e+02,\r1.52846060e-03, -2.03166630e-02, -1.38532349e+01, -4.22894414e+01])\rHowever, if we pick a large regularization strength, e.g. $\\alpha = 10^4$, we see that the resulting parameters are much smaller in magnitude. from sklearn.linear_model import Ridge regularized_model = Ridge(alpha = 10**4) regularized_model.fit(X_train2, Y_train2) regularized_model.coef_ array([-2.64236947e-02, -9.32767913e-03, -2.42925745e-02, 5.47079848e-03,\r-2.54276859e-03, 1.92843599e-02, -5.85037883e-02, -2.06397155e-02,\r2.62611572e-02, -4.16712719e-02, -1.95840395e-03, -1.91841765e-01,\r-1.08846586e-03, -4.28805626e-03, 1.70791430e-03, 6.51767238e-04,\r1.71133790e-03, 1.07486010e-03, -1.19407955e-03, -7.15970642e-03,\r-7.29287455e-04, 5.47079848e-03, 5.47079848e-03, 4.16652815e-03,\r-3.60910235e-03, -1.50954020e-03, -1.59681172e-03, 3.35928833e-01,\r3.11186224e-03, -2.79750628e-06, 4.48782500e-04, -5.71759051e-03,\r2.22943575e-06, -6.59740404e-02, -7.01191670e-03, -1.58200606e-03,\r1.32454447e-03, 8.15878522e-03, 1.17645581e-03, 3.59660322e-05,\r-2.54207413e-03, 0.00000000e+00, -2.57499245e-02, -3.15683513e-04,\r-8.10128212e-15, -6.45893053e-03, -4.20286900e-02, -2.29035441e-04])\r","date":"2024-08-13","objectID":"/datalab8/:2:0","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Standard Scaling ","date":"2024-08-13","objectID":"/datalab8/:2:1","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"å½’ä¸€åŒ– Recall from lecture that in order to properly regularize a model, the features should be at the same scale. Otherwise the model has to spend more of its parameter budget to use â€œsmallâ€ features (e.g. lengths in inches) compared to â€œlargeâ€ features (e.g. lengths in kilometers). To do this we can use a Standard Scaler to create a new version of the DataFrame where every column has zero mean and a standard deviation of 1. from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(boston_with_extra_features) boston_with_extra_features_scaled = pd.DataFrame(ss.transform(boston_with_extra_features), columns = boston_with_extra_features.columns) boston_with_extra_features_scaled CRIM\rZN\rINDUS\rCHAS\rNOX\rRM\rAGE\rDIS\rRAD\rTAX\r...\rtanhRAD\rTAX^2\rsqrtTAX\rtanhTAX\rPTRATIO^2\rsqrtPTRATIO\rtanhPTRATIO\rLSTAT^2\rsqrtLSTAT\rtanhLSTAT\r0\r-0.419782\r0.284830\r-1.287909\r-0.272599\r-0.144217\r0.413672\r-0.120013\r0.140214\r-0.982843\r-0.666608\r...\r-4.863216\r-0.682024\r-0.644166\r0.0\r-1.458429\r-1.453573\r0.135095\r-0.789529\r-1.202689\r0.103530\r1\r-0.417339\r-0.487722\r-0.593381\r-0.272599\r-0.740262\r0.194274\r0.367166\r0.557160\r-0.867883\r-0.987329\r...\r-0.521299\r-0.866530\r-1.053383\r0.0\r-0.373078\r-0.266921\r0.179012\r-0.540454\r-0.399953\r0.128396\r2\r-0.417342\r-0.487722\r-0.593381\r-0.272599\r-0.740262\r1.282714\r-0.265812\r0.557160\r-0.867883\r-0.987329\r...\r-0.521299\r-0.866530\r-1.053383\r0.0\r-0.373078\r-0.266921\r0.179012\r-0.825825\r-1.429933\r-0.037847\r3\r-0.416750\r-0.487722\r-1.306878\r-0.272599\r-0.835284\r1.016303\r-0.809889\r1.077737\r-0.752922\r-1.106115\r...\r0.144191\r-0.925467\r-1.216415\r0.0\r0.057783\r0.139631\r0.179251\r-0.858040\r-1.726876\r-1.338649\r4\r-0.412482\r-0.487722\r-1.306878\r-0.272599\r-0.835284\r1.228577\r-0.511180\r1.077737\r-0.752922\r-1.106115\r...\r0.144191\r-0.925467\r-1.216415\r0.0\r0.057783\r0.139631\r0.179251\r-0.774228\r-1.124522\r0.116050\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r501\r-0.413229\r-0.487722\r0.115738\r-0.272599\r0.158124\r0.439316\r0.018673\r-0.625796\r-0.982843\r-0.803212\r...\r-4.863216\r-0.765138\r-0.813468\r0.0\r1.255407\r1.136187\r0.179299\r-0.498180\r-0.312324\r0.128400\r502\r-0.415249\r-0.487722\r0.115738\r-0.272599\r0.158124\r-0.234548\r0.288933\r-0.716639\r-0.982843\r-0.803212\r...\r-4.863216\r-0.765138\r-0.813468\r0.0\r1.255407\r1.136187\r0.179299\r-0.545089\r-0.410031\r0.128395\r503\r-0.413447\r-0.487722\r0.115738\r-0.272599\r0.158124\r0.984960\r0.797449\r-0.773684\r-0.982843\r-0.803212\r...\r-4.863216\r-0.765138\r-0.813468\r0.0\r1.255407\r1.136187\r0.179299\r-0.759808\r-1.057406\r0.121757\r504\r-0.407764\r-0.487722\r0.115738\r-0.272599\r0.158124\r0.725672\r0.736996\r-0.668437\r-0.982843\r-0.803212\r...\r-4.863216\r-0.765138\r-0.813468\r0.0\r1.255407\r1.136187\r0.179299\r-0.716638\r-0.884300\r0.127164\r505\r-0.415000\r-0.487722\r0.115738\r-0.272599\r0.158124\r-0.362767\r0.434732\r-0.613246\r-0.982843\r-0.803212\r...\r-4.863216\r-0.765138\r-0.813468\r0.0\r1.255407\r1.136187\r0.179299\r-0.631389\r-0.619088\r0.128327\r506 rows Ã— 48 columns Letâ€™s now regenerate the training and holdout sets using this new rescaled dataset. np.random.seed(25) X = boston_with_extra_features_scaled X_train3, X_holdout3, Y_train3, Y_holdout3 = train_test_split(X, Y, test_size = 0.10) Fitting our regularized model with $\\alpha = 10^4$ on this scaled data, we now see that our coefficients are of about the same magnitude. This is because all of our features are of around the same magnitude, whereas in the unscaled data, some of the features like TAX^2 were much larger than others. from sklearn.linear_model import Ridge regularized_model = Ridge(alpha = 10**2) regularized_model.fit(X_train3, Y_train3) regularized_model.coef_ array([-0.61501301, -0.04142115, -0.13765546, 0.11847529, -0.48559141,\r1.08393358, -0.11193453, -0.6446524 , 0.25956768, -0.41922265,\r-0.48366805, -1.23850023, -0.22227015, -0.51281683, 0.40952134,\r0.2537374 , -0.07390569, 0.06674777, 0.11386252, -0.32684806,\r-0.39658025, 0.11847529, 0.11847529, 0.11847529, -0.67728184,\r-0.385382 , -0.36114118, 1.652695 , 0.78959095, -1.09450355,\r-0.02430294, -0.14153645, 0.11511136, -0.41673","date":"2024-08-13","objectID":"/datalab8/:2:2","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Finding an Optimum Alpha In the cell below, write code that generates a DataFrame with the training and holdout error for the range of alphas given. Make sure youâ€™re using the 3rd training and holdout sets, which have been rescaled! Note: You should use all 48 features for every single model that you fit, i.e. youâ€™re not going to be keeping only the first N features. error_vs_alpha = pd.DataFrame(columns = [\"alpha\", \"Training Error\", \"Holdout Error\"]) range_of_alphas = 10**np.linspace(-5, 4, 40) # for N in range_of_num_features: # X_train_first_N_features = X_train2.iloc[:, :N] # linear_model.fit(X_train_first_N_features, Y_train2) # train_error_overfit = rmse(Y_train2, linear_model.predict(X_train_first_N_features)) # X_holdout_first_N_features = X_holdout2.iloc[:, :N] # holdout_error_overfit = rmse(Y_holdout2, linear_model.predict(X_holdout_first_N_features)) # errors_vs_N.loc[len(errors_vs_N)] = [N, train_error_overfit, holdout_error_overfit] for alpha in range_of_alphas: linear_model = Ridge(alpha=alpha) linear_model.fit(X_train3, Y_train3) training_error = rmse(Y_train3, linear_model.predict(X_train3)) holdout_error = rmse(Y_holdout3, linear_model.predict(X_holdout3)) error_vs_alpha.loc[len(error_vs_alpha)] = [alpha, training_error, holdout_error] error_vs_alpha alpha\rTraining Error\rHoldout Error\r0\r0.000010\r3.344803\r5.389722\r1\r0.000017\r3.344885\r5.362696\r2\r0.000029\r3.345093\r5.318839\r3\r0.000049\r3.345588\r5.249551\r4\r0.000084\r3.346672\r5.144906\r5\r0.000143\r3.348827\r4.997596\r6\r0.000242\r3.352670\r4.810448\r7\r0.000412\r3.358709\r4.603154\r8\r0.000702\r3.366898\r4.408047\r9\r0.001194\r3.376490\r4.252523\r10\r0.002031\r3.386611\r4.144918\r11\r0.003455\r3.396946\r4.077740\r12\r0.005878\r3.407582\r4.038919\r13\r0.010000\r3.418347\r4.018141\r14\r0.017013\r3.428713\r4.007542\r15\r0.028943\r3.438401\r4.001021\r16\r0.049239\r3.447793\r3.994133\r17\r0.083768\r3.457708\r3.984607\r18\r0.142510\r3.468839\r3.972858\r19\r0.242446\r3.481455\r3.962098\r20\r0.412463\r3.495804\r3.958457\r21\r0.701704\r3.512882\r3.971376\r22\r1.193777\r3.534575\r4.011992\r23\r2.030918\r3.562638\r4.086328\r24\r3.455107\r3.597518\r4.187414\r25\r5.878016\r3.638674\r4.296469\r26\r10.000000\r3.686303\r4.392487\r27\r17.012543\r3.742258\r4.458995\r28\r28.942661\r3.809021\r4.486227\r29\r49.238826\r3.889335\r4.478730\r30\r83.767764\r3.989339\r4.470314\r31\r142.510267\r4.121409\r4.524381\r32\r242.446202\r4.300992\r4.693465\r33\r412.462638\r4.541284\r4.968124\r34\r701.703829\r4.854189\r5.289802\r35\r1193.776642\r5.251478\r5.615333\r36\r2030.917621\r5.733147\r5.946439\r37\r3455.107295\r6.275742\r6.304280\r38\r5878.016072\r6.841884\r6.698886\r39\r10000.000000\r7.394722\r7.119279\rBelow we plot your training and holdout set error for the range of alphas given. You should see a figure similar to this one from lecture, where training error goes down as model complexity increases, but the error on the held out set is large for extreme values of alpha, and minimized for some intermediate value. Note that on your plot, the x-axis is in the inverse of complexity! In other words, small alpha models (on the left) are complex, because there is no regularization. Thatâ€™s why the training error is lowest on the left side of the plot, as this is where overfitting occurs. px.line(error_vs_alpha, x = \"alpha\", y = [\"Training Error\", \"Holdout Error\"], log_x=True) From the plot above, what is the best alpha to use? training errorå°½å¯èƒ½å°ï¼ŒåŒæ—¶hold-out errorå°½å¯èƒ½å° ==\u003e 0.01~1å·¦å³ ","date":"2024-08-13","objectID":"/datalab8/:2:3","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"REMINDER: Test Set vs. Validation Set (a.k.a. Development Set) In the plots above, we trained our models on a training set, and plotted the resulting RMSE on the training set in blue. We also held out a set of data, and plotted the error on this holdout set in red, calling it the â€œholdout set errorâ€. For the example above, since we used the holdout set to pick a hyperparameter, weâ€™d call the holdout set a â€œvalidation setâ€ or â€œdevelopment setâ€. These terms are exactly synonomous. It would not be accurate to call this line the â€œtest set errorâ€, because we did not use this dataset as a test set. While it is true that your code never supplied X_test3 or Y_test3 to the fit function of the ridge regression models, once you decide to use the holdout set to select between different models, different hyperparameters, or different sets of features, then we are not using that dataset as a â€œtest setâ€. That is, since weâ€™ve used this holdout set for picking alpha, the resulting errors are no longer unbiased predictors of our performance on unseen models â€“ the true error on an unseen dataset is likely to be somewhat higher than the validation set. After all, we trained 40 models and picked the best one! In many real world contexts, model builders will split their data into three sets: training, validation, and test sets, where the test set is only ever used once. That is, there are two holdout sets: One used as a development set (for model selection), and one used a test set (for providing an unbiased estimate of error). ","date":"2024-08-13","objectID":"/datalab8/:3:0","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"An Alternate Strategy for Hyper Parameter Selection: K-Fold Cross Validation Earlier we used the holdout method for model selection (the holdout method is also sometimes called â€œsimple cross validationâ€). Another approach is K-fold cross validation. This allows us to use more data for training instead of having to set aside some specifically for hyperparameter selection. However, doing so requires more computation resources as weâ€™ll have to fit K models per hyperparameter choice. In our course Data 100, thereâ€™s really no reason not to use cross validation. However, in environments where models are very expensive to train (e.g. deep learning), youâ€™ll typically prefer using a holdout set (simple cross validation) rather than K-fold cross validation. To emphasize what K-fold cross validation actually means, weâ€™re going to manually carry out the procedure. Recall the approach looks something like the figure below for 4-fold cross validation: When we use K-fold cross validation, rather than using a held out set for model selection, we instead use the training set for model selection. To select between various features, various models, or various hyperparameters, we split the training set further into multiple temporary train and validation sets (each split is called a â€œfoldâ€, hence k-fold cross validation). We will use the average validation error across all k folds to make our optimal feature, model, and hyperparameter choices. In this example, weâ€™ll only use this procedure for hyperparameter selection, specifically to choose the best alpha. ","date":"2024-08-13","objectID":"/datalab8/:4:0","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Question 4 é‡ç‚¹åœ¨äºæ€ä¹ˆåˆ‡åˆ†æ•°æ®é›†ï¼ Scikit-learn has built-in support for cross validation. However, to better understand how cross validation works complete the following function which cross validates a given model. Use the KFold.split function to get 4 splits on the training data. Note that split returns the indices of the data for that split. For each split: Select out the training and validation rows and columns based on the split indices and features. Compute the RMSE on the validation split. Return the average error across all cross validation splits. from sklearn.model_selection import KFold def compute_CV_error(model, X_train, Y_train): ''' Split the training data into 4 subsets. For each subset, fit a model holding out that subset compute the MSE on that subset (the validation set) You should be fitting 4 models total. Return the average MSE of these 4 folds. Args: model: an sklearn model with fit and predict functions X_train (data_frame): Training data Y_train (data_frame): Label Return: the average validation MSE for the 4 splits. ''' kf = KFold(n_splits=4) validation_errors = [] for train_idx, valid_idx in kf.split(X_train): # split the data split_X_train, split_X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx] split_Y_train, split_Y_valid = Y_train.iloc[train_idx], Y_train.iloc[valid_idx] # Fit the model on the training split model.fit(split_X_train, split_Y_train) # Compute the RMSE on the validation split error = rmse(model.predict(split_X_valid), split_Y_valid) validation_errors.append(error) return np.mean(validation_errors) grader.check(\"q4\") ","date":"2024-08-13","objectID":"/datalab8/:4:1","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Question 5 Use compute_CV_error to add a new column to error_vs_alpha which gives the 4-fold cross validation error for the given choice of alpha. cv_errors = [] range_of_alphas = 10**np.linspace(-5, 4, 40) for alpha in range_of_alphas: cv_error = compute_CV_error(Ridge(alpha=alpha), X_train3, Y_train3) cv_errors.append(cv_error) error_vs_alpha[\"CV Error\"] = cv_errors error_vs_alpha alpha\rTraining Error\rHoldout Error\rCV Error\r0\r0.000010\r3.344803\r5.389722\r10.763338\r1\r0.000017\r3.344885\r5.362696\r10.578003\r2\r0.000029\r3.345093\r5.318839\r10.254709\r3\r0.000049\r3.345588\r5.249551\r9.756308\r4\r0.000084\r3.346672\r5.144906\r9.054988\r5\r0.000143\r3.348827\r4.997596\r8.147759\r6\r0.000242\r3.352670\r4.810448\r7.069916\r7\r0.000412\r3.358709\r4.603154\r5.905299\r8\r0.000702\r3.366898\r4.408047\r4.810950\r9\r0.001194\r3.376490\r4.252523\r4.104387\r10\r0.002031\r3.386611\r4.144918\r4.080071\r11\r0.003455\r3.396946\r4.077740\r4.240810\r12\r0.005878\r3.407582\r4.038919\r4.224883\r13\r0.010000\r3.418347\r4.018141\r4.086858\r14\r0.017013\r3.428713\r4.007542\r3.956585\r15\r0.028943\r3.438401\r4.001021\r3.889772\r16\r0.049239\r3.447793\r3.994133\r3.867618\r17\r0.083768\r3.457708\r3.984607\r3.858856\r18\r0.142510\r3.468839\r3.972858\r3.850327\r19\r0.242446\r3.481455\r3.962098\r3.842001\r20\r0.412463\r3.495804\r3.958457\r3.837080\r21\r0.701704\r3.512882\r3.971376\r3.838459\r22\r1.193777\r3.534575\r4.011992\r3.848340\r23\r2.030918\r3.562638\r4.086328\r3.867120\r24\r3.455107\r3.597518\r4.187414\r3.893089\r25\r5.878016\r3.638674\r4.296469\r3.924624\r26\r10.000000\r3.686303\r4.392487\r3.962520\r27\r17.012543\r3.742258\r4.458995\r4.009721\r28\r28.942661\r3.809021\r4.486227\r4.070020\r29\r49.238826\r3.889335\r4.478730\r4.149246\r30\r83.767764\r3.989339\r4.470314\r4.257353\r31\r142.510267\r4.121409\r4.524381\r4.406670\r32\r242.446202\r4.300992\r4.693465\r4.607861\r33\r412.462638\r4.541284\r4.968124\r4.870040\r34\r701.703829\r4.854189\r5.289802\r5.203950\r35\r1193.776642\r5.251478\r5.615333\r5.617020\r36\r2030.917621\r5.733147\r5.946439\r6.099994\r37\r3455.107295\r6.275742\r6.304280\r6.625052\r38\r5878.016072\r6.841884\r6.698886\r7.158474\r39\r10000.000000\r7.394722\r7.119279\r7.665518\rThe code below shows the holdout error that we computed in the previous problem as well as the 4-fold cross validation error. Note that the cross validation error shows a similar dependency on alpha relative to the holdout error. This is because they are both doing the same thing, namely trying to estimate the expected error on unseen data drawn from distribution from which the training set was drawn. In other words, this figure compares the holdout method with 4-fold cross validation. Note: I donâ€™t know why the CV error is so much higher for very small (i.e. very complex) models. Let me know if you figure out why. I suspec tiâ€™ts just random noise. px.line(error_vs_alpha, x = \"alpha\", y = [\"Holdout Error\", \"CV Error\"], log_x=True) ","date":"2024-08-13","objectID":"/datalab8/:4:2","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Extra: Using GridSearchCV è‡ªä¸»æ‰¾åˆ°æœ€ä½³è¶…å‚æ•° Above, we manually performed a search of the space of possible hyperparameters. In this section weâ€™ll discuss how to use sklearn to automatically perform such a search. The code below automatically tries out all alpha values in the given range. from sklearn.model_selection import GridSearchCV params = {'alpha': 10**np.linspace(-5, 4, 40)} grid_search = GridSearchCV(Ridge(), params, cv = 4, scoring = \"neg_root_mean_squared_error\") grid_search.fit(X_train3, Y_train3) We can get the average RMSE for the four folds for each of the values of alpha with the code below. In other words, this array is the same as the one you computed earlier when you created the â€œCV Errorâ€ column. grid_search.cv_results_['mean_test_score'] array([-10.7633381 , -10.57800314, -10.25470921, -9.75630755,\r-9.05498816, -8.14775947, -7.06991566, -5.90529929,\r-4.8109505 , -4.10438693, -4.08007128, -4.24080956,\r-4.22488284, -4.08685828, -3.95658497, -3.88977241,\r-3.86761841, -3.85885628, -3.85032722, -3.8420014 ,\r-3.83707965, -3.83845914, -3.84833967, -3.86711956,\r-3.89308871, -3.92462404, -3.96251959, -4.00972106,\r-4.07002011, -4.14924607, -4.25735297, -4.4066697 ,\r-4.60786131, -4.87004045, -5.20394987, -5.61702004,\r-6.09999442, -6.62505185, -7.15847442, -7.66551837])\rWe can specifically see the lowest RMSE with best_score_: grid_search.best_score_ np.float64(-3.8370796510062055)\rAnd we can get the best model with best_estimator_, which youâ€™ll note is a Ridge regression model with alpha = 0.412. grid_search.best_estimator_ We can even add the errors from GridSearchCV to our error_vs_alpha DataFrame and compare the results of our manual 4-fold cross validation with sklearnâ€™s implementation: error_vs_alpha[\"sklearn CV Score\"] = grid_search.cv_results_['mean_test_score'] px.line(error_vs_alpha, x = \"alpha\", y = [\"CV Error\", \"sklearn CV Score\"], log_x=True) Youâ€™ll notice they are exactly the same except that the sklearn CV score is the negative of the error. This is because GridSearchCV is conceptualized as a â€œmaximizerâ€, where the goal is to get the highest possible score, whereas our code was a â€œminimizerâ€, where the goal was to get the lowest possible error. In other words, the error is just the negative of the score. é•œåƒç”±æ¥ ","date":"2024-08-13","objectID":"/datalab8/:4:3","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Extra: Examining the Residuals of our Optimal Alpha Model The code below plots the residuals of our best model (Ridge with alpha = 0.412) on the test set. Note that they now seem to be better distributed on either size of the line and are generally closer the line, though with a few more extreme outliers. plt.figure(figsize=(10, 6)) predicted_values_on_holdout3 = grid_search.best_estimator_.predict(X_holdout3) plt.scatter(predicted_values_on_holdout3, Y_holdout3 - predicted_values_on_holdout3, alpha = 0.5) plt.ylabel(\"Residual $(y - \\hat{y})$\") plt.xlabel(\"Predicted Prices $(\\hat{y})$\") plt.title(\"Residuals vs Predicted Prices\") plt.title(\"Residual of prediction for i'th house\") plt.axhline(y = 0, color='r'); \u003c\u003e:4: SyntaxWarning:\rinvalid escape sequence '\\h'\r\u003c\u003e:5: SyntaxWarning:\rinvalid escape sequence '\\h'\r\u003c\u003e:4: SyntaxWarning:\rinvalid escape sequence '\\h'\r\u003c\u003e:5: SyntaxWarning:\rinvalid escape sequence '\\h'\rC:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_6688\\3088448444.py:4: SyntaxWarning:\rinvalid escape sequence '\\h'\rC:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_6688\\3088448444.py:5: SyntaxWarning:\rinvalid escape sequence '\\h'\rLastly we can compute the RMSE on the test set. This gives the expected squared error on a new unseen data point that may come to us in the future from the same distribution as our training set. test_rmse = rmse(grid_search.best_estimator_.predict(X_holdout3), Y_holdout3) test_rmse np.float64(3.9584573514348387)\r","date":"2024-08-13","objectID":"/datalab8/:4:4","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Extra: LASSO Regression The code below finds an optimal Lasso model. Note that Lasso regression generalize behaves more poorly numerically, so youâ€™ll probably get a bunch of warnings. from sklearn.linear_model import Lasso params = {'alpha': 10**np.linspace(-5, 4, 40)} grid_search_lasso = GridSearchCV(Lasso(), params, cv = 4, scoring = \"neg_root_mean_squared_error\") grid_search_lasso.fit(X_train3, Y_train3) The best lasso model is below: grid_search_lasso.best_estimator_ Itâ€™s error on the same test set as our best Ridge model is shown below: test_rmse_lasso = rmse(grid_search_lasso.best_estimator_.predict(X_holdout3), Y_holdout3) test_rmse_lasso np.float64(4.054830916690993)\rNote that if we tried to use this test error to decide between Ridge and LASSO, then our holdout set is now being used as a validation set, not a test set!! In other words, you get to either use the holdout set to decide between models, or to provide an unbiased estimate of error, but not both! If we look at the best estimatorâ€™s parameters, weâ€™ll see that many of the parameters are zero, due to the inherent feature selecting nature of a LASSO model. grid_search_lasso.best_estimator_.coef_ array([-0.00000000e+00, -6.85384379e-01, 0.00000000e+00, 0.00000000e+00,\r-0.00000000e+00, -0.00000000e+00, -7.38599400e-02, -5.29374425e-02,\r5.54295757e-01, -0.00000000e+00, -0.00000000e+00, 0.00000000e+00,\r4.37063521e-01, -3.80592597e+00, 1.61080715e+00, 6.37366884e-01,\r-0.00000000e+00, 2.22834586e-01, 6.01812381e-03, -9.40700489e-02,\r-4.02630887e-01, 3.07990173e-01, 3.72360525e-14, 0.00000000e+00,\r-2.51811102e+00, 0.00000000e+00, 0.00000000e+00, 9.85248689e+00,\r-7.21033868e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\r1.36076846e-01, -0.00000000e+00, -2.00907909e+00, -1.68923341e+00,\r1.77833261e+00, 2.55936962e-01, 5.04076324e-01, 0.00000000e+00,\r-1.82804827e+00, 0.00000000e+00, -0.00000000e+00, -1.53173793e+00,\r-7.84893470e-03, 1.09000336e+00, -5.21734363e+00, -3.87962203e-01])\rWe can also stick these parameters in a Series showing us both the weights and the names: å¯è§£é‡Šæ€§å¼ºä¸€ç‚¹ç‚¹ lasso_weights = pd.Series(grid_search_lasso.best_estimator_.coef_, index = boston_with_extra_features_scaled.columns) lasso_weights CRIM -0.000000e+00\rZN -6.853844e-01\rINDUS 0.000000e+00\rCHAS 0.000000e+00\rNOX -0.000000e+00\rRM -0.000000e+00\rAGE -7.385994e-02\rDIS -5.293744e-02\rRAD 5.542958e-01\rTAX -0.000000e+00\rPTRATIO -0.000000e+00\rLSTAT 0.000000e+00\rCRIM^2 4.370635e-01\rsqrtCRIM -3.805926e+00\rtanhCRIM 1.610807e+00\rZN^2 6.373669e-01\rsqrtZN -0.000000e+00\rtanhZN 2.228346e-01\rINDUS^2 6.018124e-03\rsqrtINDUS -9.407005e-02\rtanhINDUS -4.026309e-01\rCHAS^2 3.079902e-01\rsqrtCHAS 3.723605e-14\rtanhCHAS 0.000000e+00\rNOX^2 -2.518111e+00\rsqrtNOX 0.000000e+00\rtanhNOX 0.000000e+00\rRM^2 9.852487e+00\rsqrtRM -7.210339e+00\rtanhRM -0.000000e+00\rAGE^2 -0.000000e+00\rsqrtAGE -0.000000e+00\rtanhAGE 1.360768e-01\rDIS^2 -0.000000e+00\rsqrtDIS -2.009079e+00\rtanhDIS -1.689233e+00\rRAD^2 1.778333e+00\rsqrtRAD 2.559370e-01\rtanhRAD 5.040763e-01\rTAX^2 0.000000e+00\rsqrtTAX -1.828048e+00\rtanhTAX 0.000000e+00\rPTRATIO^2 -0.000000e+00\rsqrtPTRATIO -1.531738e+00\rtanhPTRATIO -7.848935e-03\rLSTAT^2 1.090003e+00\rsqrtLSTAT -5.217344e+00\rtanhLSTAT -3.879622e-01\rdtype: float64\rOr sorting by the relative importance of each feature, we see that about a third of the parmaeters didnâ€™t end up getting used at all by the LASSO model. lasso_weights.sort_values(key = abs, ascending = False) RM^2 9.852487e+00\rsqrtRM -7.210339e+00\rsqrtLSTAT -5.217344e+00\rsqrtCRIM -3.805926e+00\rNOX^2 -2.518111e+00\rsqrtDIS -2.009079e+00\rsqrtTAX -1.828048e+00\rRAD^2 1.778333e+00\rtanhDIS -1.689233e+00\rtanhCRIM 1.610807e+00\rsqrtPTRATIO -1.531738e+00\rLSTAT^2 1.090003e+00\rZN -6.853844e-01\rZN^2 6.373669e-01\rRAD 5.542958e-01\rtanhRAD 5.040763e-01\rCRIM^2 4.370635e-01\rtanhINDUS -4.026309e-01\rtanhLSTAT -3.879622e-01\rCHAS^2 3.079902e-01\rsqrtRAD 2.559370e-01\rtanhZN 2.228346e-01\rtanhAGE 1.360768e-01\rsqrtINDUS -9.407005e-02\rAGE -7.385994e-02\rDIS -5.293744e-02\rtanhPTRATIO -7.848935e-03\rIN","date":"2024-08-13","objectID":"/datalab8/:4:5","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":"Submission Congratulations! You are finished with this assignment. ","date":"2024-08-13","objectID":"/datalab8/:5:0","tags":["Scikit-Learn"],"title":"DATA100-lab8: Model Selection, Regularization, and Cross-Validation","uri":"/datalab8/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab05.ipynb\") Lab 5: Modeling, Loss Functions, and Summary Statistics ","date":"2024-08-13","objectID":"/datalab5/:0:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Predicting Restaurant Tips In this lab, you will try to predict restaurant tips from a set of data in several ways: A. Without given any additional information, use a constant model with L2 loss to predict the tip $\\hat{y}$ as a summary statistic, $\\theta$. B. Given one piece of informationâ€”the total bill $x$ use a linear model with L2 loss to predict the tip $\\hat{y}$ as a linear function of $x$. C. See if a constant model with L1 loss changes our predictions. First, letâ€™s load in the data. # just run this cell import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') sns.set() sns.set_context(\"talk\") tips = sns.load_dataset(\"tips\") tips.head(5) total_bill\rtip\rsex\rsmoker\rday\rtime\rsize\r0\r16.99\r1.01\rFemale\rNo\rSun\rDinner\r2\r1\r10.34\r1.66\rMale\rNo\rSun\rDinner\r3\r2\r21.01\r3.50\rMale\rNo\rSun\rDinner\r3\r3\r23.68\r3.31\rMale\rNo\rSun\rDinner\r2\r4\r24.59\r3.61\rFemale\rNo\rSun\rDinner\r4\rQuick EDA: Note that this dataset is likely from the United States. The below plot graphs the distribution of tips in this dataset, both in absolute amounts ($) and as a fraction of the total bill (post-tax, but pre-tip). # just run this cell fig, ax = plt.subplots(ncols=2, figsize=(10, 4)) sns.histplot(tips['tip'], bins=20, stat=\"proportion\", ax=ax[0]) sns.histplot(tips['tip']/tips['total_bill'], bins=20, stat=\"proportion\", ax=ax[1]) ax[0].set_xlabel(\"Amount ($)\") ax[1].set_xlabel(\"Fraction of total bill\") ax[0].set_ylim((0, 0.35)) ax[1].set_ylim((0, 0.35)) ax[1].set_ylabel(\"\") # for cleaner visualization fig.suptitle(\"Restaurant Tips\") plt.show() In this lab weâ€™ll estimate the tip in absolute amounts ($). The above plot is just to confirm your expectations about the tips dataset. ","date":"2024-08-13","objectID":"/datalab5/:1:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Part A: Tips as a Summary Statistic Letâ€™s first predict any restaurant tip using one single number: in other words, letâ€™s try to find the best statistic $\\hat{\\theta}$ to represent (i.e., summarize) the tips from our dataset. Each actual tip in our dataset is $y$, which is what we call the observed value. We want to predict each observed value as $\\hat{y}$. Weâ€™ll save the observed tip values in a NumPy array y_tips: # just run this cell y_tips = np.array(tips['tip']) # array of observed tips y_tips.shape (244,)\rRecall the three-step process for modeling as covered in lecture: Define a model. Define a loss function and the associated risk on our training dataset (i.e., average loss). Find the best value of $\\theta$, known as $\\hat{\\theta}$, that minimizes risk. Weâ€™ll go through each step of this process next. ","date":"2024-08-13","objectID":"/datalab5/:2:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"A.1: Define the model We will define our model as the constant model: $$\\Large \\hat{y} = \\theta $$ In other words, regardless of any other details (i.e., features) about their meal, we will always predict our tip $\\hat{y}$ as one single value: $\\theta$. $\\theta$ is what we call a parameter. Our modeling goal is to find the value of our parameter(s) that best fit our data. We have choice over which $\\theta$ we pick (using the data at hand), but ultimately we can only pick one to report, so we want to find the optimal parameter(s) $\\hat{\\theta}$. We call the constant model a summary statistic, as we are determining one number that best â€œsummarizesâ€ a set of values. No code to write here! ","date":"2024-08-13","objectID":"/datalab5/:3:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"A.2: Define the loss function and risk Next, in order to pick our $\\theta$, we need to define a loss function, which is a measure of how well a model is able to predict the expected outcome. In other words, it measures the deviation of a predicted value $\\hat{y}$ from the observed value $y$. We will use squared loss (also known as the $L_2$ loss, pronounced â€œell-twoâ€). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_2$ loss of: $$\\Large L_2(y, \\hat{y}) = (y - \\hat{y})^2$$ ","date":"2024-08-13","objectID":"/datalab5/:4:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 1 In our constant model $\\hat{y} = \\theta$, we always predict the tip as $\\theta$. Therefore our $L_2$ loss for some actual, observed value $y$ can be rewritten as: $$\\Large L_2(y, \\theta) = (y - \\theta)^2$$ Use the function description below to implement the squared loss function for this single datapoint, assuming the constant model. Your answer should not use any loops. def squared_loss(y_obs, theta): \"\"\" Calculate the squared loss of the observed data and a summary statistic. Parameters ------------ y_obs: an observed value theta : some constant representing a summary statistic Returns ------------ The squared loss between the observation and the summary statistic. \"\"\" return (y_obs - theta)**2 grader.check(\"q1\") We just defined loss for a single datapoint. Letâ€™s extend the above loss function to our entire dataset by taking the average loss across the dataset. Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = {y_1, \\ldots, y_n}$, where $y_i$ is the $i^{th}$ tip (this is the y_tips array defined at the beginning of Part A). We can define the average loss (aka risk) over the dataset as: $$\\Large R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) $$ If we use $L_2$ loss per datapoint ($L = L_2$), then the risk is also known as mean squared error (MSE). For the constant model $\\hat{y}=\\theta$: $$\\Large R\\left(\\theta\\right) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta)^2 $$ ","date":"2024-08-13","objectID":"/datalab5/:5:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 2 Define the mse_tips_constant function which computes $R(\\theta)$ as the mean squared error on the tips data for a constant model with parameter $\\theta$. Notes/Hints: This function takes in one parameter, theta; data is defined for you as a NumPy array that contains the observed tips values in the data. Use the squared_loss function you defined in the previous question. def mse_tips_constant(theta): data = y_tips return sum(squared_loss(data, theta)) / len(data) mse_tips_constant(5.3) # arbitrarily pick theta = 5.3 np.float64(7.204529508196728)\rgrader.check(\"q2\") ","date":"2024-08-13","objectID":"/datalab5/:6:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"A.3: Find the $\\theta$ that minimizes risk ","date":"2024-08-13","objectID":"/datalab5/:7:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3 Now we can go about choosing our â€œbestâ€ value of $\\theta$, which we call $\\hat{\\theta}$, that minimizes our defined risk (which we defined as mean squared error). There are several approaches to computing $\\hat{\\theta}$ that weâ€™ll explore in this problem. ","date":"2024-08-13","objectID":"/datalab5/:8:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3a: Visual Solution In the cell below we plot the mean squared error for different thetas: # just run this cell theta_values = np.linspace(0, 6, 100) mse = [mse_tips_constant(theta) for theta in theta_values] plt.plot(theta_values, mse) plt.xlabel(r'$\\theta$') plt.ylabel('average L2 loss') plt.title(r'MSE for different values of $\\theta$'); Find the value of theta that minimizes the mean squared error via observation of the plot above. Round your answer to the nearest integer. min_observed_mse = 3 min_observed_mse 3\rgrader.check(\"q3a\") q3a passed! ğŸ’¯ ","date":"2024-08-13","objectID":"/datalab5/:8:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Numerically computing $\\hat{\\theta}$ scipy.optimize.minimize is a powerful method that can determine the optimal value of a variety of different functions. In practice, it is used to minimize functions that have no (or difficult to obtain) analytical solutions (it is a numerical method). It is overkill for our simple example, but nonetheless, we will show you how to use it, as it will become useful in the near future. The cell below plots some arbitrary 4th degree polynomial function. # just run this cell x_values = np.linspace(-4, 2.5, 100) def fx(x): return 0.1 * x**4 + 0.2*x**3 + 0.2 * x **2 + 1 * x + 10 plt.plot(x_values, fx(x_values)); plt.title(\"Arbitrary 4th degree polynomial\"); By looking at the plot, we see that the x that minimizes the function is slightly larger than -2. What if we want the exact value? We will demonstrate how to grab the minimum value and the optimal x in the following cell. The function minimize from scipy.optimize will attempt to minimize any function you throw at it. Try running the cell below, and you will see that minimize seems to get the answer correct. Note: For today, weâ€™ll let minimize work as if by magic. Weâ€™ll discuss how minimize works later in the course. # just run this cell from scipy.optimize import minimize minimize(fx, x0 = 1.1) message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: 8.728505719866614\rx: [-1.747e+00]\rnit: 6\rjac: [ 1.192e-07]\rhess_inv: [[ 5.088e-01]]\rnfev: 16\rnjev: 8\rNotes: [1] fun: the minimum value of the function. [2] x: the x which minimizes the function. We can index into the object returned by minimize to get these values. We have to add the additional [0] at the end because the minimizing x is returned as an array, but this is not necessarily the case for other attributes (i.e. fun), shown in the cell below. Note [2] means that minimize can also minimize multivariable functions, which weâ€™ll see in the second half of this lab. # just run this cell min_result = minimize(fx, x0 = 1.1) # è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬æœ€ä¼˜å€¼funå’Œæœ€ä¼˜è§£x min_of_fx = min_result['fun'] x_which_minimizes_fx = min_result['x'][0] min_of_fx, x_which_minimizes_fx (8.728505719866614, np.float64(-1.746827786380178))\rInitial guess: The parameter x0 that we passed to the minimize function is where the minimize function starts looking as it tries to find the minimum. For example, above, minimize started its search at x = 1.1 because thatâ€™s where we told it to start. For the function above, it doesnâ€™t really matter what x we start at because the function is nice and has only a single local minimum. More technically, the function is nice because it is convex, a property of functions that we will discuss later in the course. Local minima: minimize isnâ€™t perfect. For example, if we give it a function with many valleys (also known as local minima) it can get stuck. For example, consider the function below: # just run this cell w_values = np.linspace(-2, 10, 100) def fw(w): return 0.1 * w**4 - 1.5*w**3 + 6 * w **2 - 1 * w + 10 plt.plot(w_values, fw(w_values)); plt.title(\"Arbitrary function with local minima\"); If we start the minimization at w = 6.5, weâ€™ll get stuck in the local minimum at w = 7.03. Note that no matter what your actual variable is called in your function (w in this case), the minimize routine still expects a starting point parameter called x0. # just run this cell minimize(fw, x0 = 6.5) # initial w is 6.5 message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: 22.594302881719713\rx: [ 7.038e+00]\rnit: 4\rjac: [-3.815e-06]\rhess_inv: [[ 1.231e-01]]\rnfev: 12\rnjev: 6\r","date":"2024-08-13","objectID":"/datalab5/:8:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3b: Numerical Solution Using the minimize function, find the value of theta that minimizes the mean squared error for our tips dataset. In other words, you want to find the exact minimum of the plot that you saw in the previous part. Notes: You should use the function you defined earlier: mse_tips_constant. For autograding purposes, assign min_scipy to the value of theta that minimizes the MSE according to the minimize function, called with initial x0=0.0. # call minimize with initial x0 = 0.0 min_scipy = minimize(mse_tips_constant, x0=0.0)['x'][0] min_scipy np.float64(2.9982777037277204)\rgrader.check(\"q3b\") ","date":"2024-08-13","objectID":"/datalab5/:8:3","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 3c: Analytical Solution In lecture, we used calculus to show that the value of theta that minimizes the mean squared error for the constant model is the average (mean) of the data. Assign min_computed to the mean of the observed y_tips data, and compare this to the values you observed in questions 3a and 3b. min_computed = y_tips.mean() min_computed np.float64(2.99827868852459)\rgrader.check(\"q3c\") Reflecting on the lab so far, we used a 3-step approach to find the â€œbestâ€ summary statistic $\\theta$: Define the constant model $\\hat{y}=\\theta$. Define â€œbestâ€: Define loss per datapoint (L2 loss) and consequently define risk $R(\\theta)$ over a given data array as the mean squared error, or MSE. Find the $\\theta = \\hat{\\theta}$ that minimizes the MSE $R(\\theta)$ in several ways: Visually: Create a plot of $R(\\theta)$ vs. $\\theta$ and eyeball the minimizing $\\hat{\\theta}$. Numerically: Create a function that returns $R(\\theta)$, the MSE for the given data array for a given $\\theta$, and use the scipy minimize function to find the minimizing $\\hat{\\theta}$. Analytically: Simply compute $\\hat{\\theta}$ the mean of the given data array, since this minimizes the defined $R(\\theta)$. (a fourth analytical option) Use calculus to find $\\hat{\\theta}$ that minimizes MSE $R(\\theta)$. At this point, youâ€™ve hopefully convinced yourself that the mean of the data is the summary statistic that minimizes mean squared error. Our prediction for every mealâ€™s tip: # just run this cell def predict_tip_constant(): return min_computed # do not edit below this line bill = 20 print(f\"\"\"No matter what meal you have, Part A's modeling process predicts that you will pay a tip of ${predict_tip_constant():.2f}.\"\"\") No matter what meal you have, Part A's modeling process\rpredicts that you will pay a tip of $3.00.\r","date":"2024-08-13","objectID":"/datalab5/:8:4","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Part B: Tips as a Linear Function of Total Bill In this section, you will follow the exact same modeling process but instead use total bill to predict tip. Weâ€™ll save the observed total bill values (post-tax but pre-tip) and the observed tip values in two NumPy arrays, x_total_bills and y_tips: # just run this cell x_total_bills = np.array(tips['total_bill']) # array of total bill amounts y_tips = np.array(tips['tip']) # array of observed tips print(\"total bills\", x_total_bills.shape) print(\"tips\", y_tips.shape) total bills (244,)\rtips (244,)\r","date":"2024-08-13","objectID":"/datalab5/:9:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"B.1 Define the model We will define our model as the linear model that takes a single input feature, total_bill ($x$): $$\\Large \\hat{y} = a + b x $$ Our â€œparameterâ€ $\\theta$ is actually two parameters: $a$ and $b$. You may see this written as $\\theta = (a, b)$. Our modeling task is then to pick the best values $a = \\hat{a}$ and $b = \\hat{b}$ from our data. Then, given the total bill $x$, we can predict the tip as $\\hat{y} = \\hat{a} + \\hat{b} x$. No code to write here! ","date":"2024-08-13","objectID":"/datalab5/:10:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"B.2: Define the loss function and risk Next, weâ€™ll define our loss function $L(y, \\hat{y})$ and consequently our risk function $R(\\theta) = R(a, b)$. Similar to our approach to Part A, weâ€™ll use L2 Loss and Mean Squared Error. Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = {(x_1, y_1), \\ldots, (x_n, y_n)}$, where $(x_i, y_i)$ are the $i^{th}$ total bill and tip, respectively, in our dataset. Our L2 Loss and Mean Squared Error are therefore: \\begin{align} \\large L_2(y, \\hat{y}) = \\large (y - \\hat{y})^2 \u0026= \\large (y - (a + bx))^2 \\ \\large R(a, b) = \\large \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\hat{y_i}) \u0026= \\large \\frac{1}{n} \\sum_{i = 1}^n(y_i - (a + b x_i))^2 \\end{align} Notice that because our model is now the linear model $\\hat{y} = a + bx$, our final expressions for Loss and MSE are different from Part A. ","date":"2024-08-13","objectID":"/datalab5/:11:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 4 Define the mse_tips_linear function which computes $R(a, b)$ as the mean squared error on the tips data for a linear model with parameters $a$ and $b$. Notes: This function takes in two parameters a and b. You should use the NumPy arrays x_total_bills and y_tips defined at the beginning of Part B. Weâ€™ve included some skeleton code, but feel free to write your own as well. def mse_tips_linear(a, b): \"\"\" Returns average L2 loss between predicted y_hat values (using x_total_bills and parameters a, b) and actual y values (y_tips) \"\"\" y_hats = a + b * x_total_bills return ((y_hats - y_tips) ** 2).mean() ... mse_tips_linear(0.9, 0.1) # arbitrarily pick a = 0.9, b = 0.1 np.float64(1.052336405737705)\rgrader.check(\"q4\") ","date":"2024-08-13","objectID":"/datalab5/:12:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"B.3: Find the $\\theta$ that minimizes risk Similar to before, weâ€™d like to try out different approaches to finding the optimal parameters $\\hat{a}$ and $\\hat{b}$ that minimize MSE. ","date":"2024-08-13","objectID":"/datalab5/:13:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5: Analytical Solution In lecture, we derived the following expression for the line of best fit: $$\\Large \\hat{y_i} = \\bar{y} + r \\frac{SD(y)}{SD(x)} (x_i - \\bar{x})$$ where $\\bar{x}$, $\\bar{y}$, $SD(x)$, $SD(y)$ correspond to the means and standard deviations of $x$ and $y$, respectively, and $r$ is the correlation coefficient. ","date":"2024-08-13","objectID":"/datalab5/:14:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5a Assign x_bar, y_bar, std_x, std_y, and r, for our dataset. Note: Make sure to use np.std, and not \u003cSeries name\u003e.std(). Hint: Remember, in our case, y is y_tips, and x is x_total_bills. Hint: You may find np.corrcoef (documentation) handy in computing r. Note that the output of np.corrcoef is a matrix, not a number, so youâ€™ll need to collect the correlation coefficient by indexing into the returned matrix. x_bar = x_total_bills.mean() y_bar = y_tips.mean() std_x = np.std(x_total_bills) std_y = np.std(y_tips) r = np.corrcoef(x_total_bills, y_tips)[0, 1] grader.check(\"q5a\") ","date":"2024-08-13","objectID":"/datalab5/:14:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5b Now, set b_hat and a_hat correctly, in terms of the variables you defined above. Hints: Try and match the slope and intercept in $\\hat{y_i} = \\hat{a} + \\hat{b}x_i$ to the slope and intercept in $\\hat{y_i} = \\bar{y} + r \\frac{SD(y)}{SD(x)} (x_i - \\bar{x})$. You may want to define a_hat in terms of b_hat. b_hat = r*std_y/std_x a_hat = y_bar - b_hat*x_bar grader.check(\"q5b\") ","date":"2024-08-13","objectID":"/datalab5/:14:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 5c Now, use a_hat and b_hat to implement the predict_tip_linear function, which predicts the tip for a total bill amount of bill. def predict_tip_linear(bill): return a_hat + b_hat * bill # do not edit below this line bill = 20 print(f\"\"\"If you have a ${bill} bill, Part B's modeling process predicts that you will pay a tip of ${predict_tip_linear(bill):.2f}.\"\"\") If you have a $20 bill, Part B's modeling process\rpredicts that you will pay a tip of $3.02.\rgrader.check(\"q5c\") ","date":"2024-08-13","objectID":"/datalab5/:14:3","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Numerically computing $\\hat{\\theta}$ The minimize function we introduced earlier can also minimize functions of multiple variables (useful for numerically computing $\\hat{a}$ and $\\hat{b}$. Thereâ€™s one quirk, however, which is that the function has to accept its parameters as a single list. For example, consider the multivariate $f(u, v) = u^2 - 2 u v - 3 v + 2 v^2$. It turns out this functionâ€™s minimum is at $(1.5, 1.5)$. To minimize this function, we create f. # just run this cell def f(theta): u = theta[0] v = theta[1] return u**2 - 2 * u * v - 3 * v + 2 * v**2 minimize(f, x0 = [0.0, 0.0]) message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: -2.2499999999999982\rx: [ 1.500e+00 1.500e+00]\rnit: 3\rjac: [-5.960e-08 0.000e+00]\rhess_inv: [[ 1.000e+00 5.000e-01]\r[ 5.000e-01 5.000e-01]]\rnfev: 12\rnjev: 4\r","date":"2024-08-13","objectID":"/datalab5/:15:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 6: Numerical Solution ","date":"2024-08-13","objectID":"/datalab5/:16:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 6a Implement the mse_tips_linear_list function, which is exactly like mse_tips_linear defined in Question 4 except that it takes in a single list of 2 variables rather than two separate variables. For example mse_tips_linear_list([2, 3]) should return the same value as mse_tips_linear(2, 3). def mse_tips_linear_list(theta): \"\"\" Returns average L2 loss between predicted y_hat values (using x_total_bills and linear params theta) and actual y values (y_tips) \"\"\" y_hat = theta[0] + theta[1] * x_total_bills mse = sum((y_hat - y_tips)**2) / len(y_tips) return mse grader.check(\"q6a\") ","date":"2024-08-13","objectID":"/datalab5/:16:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 6b Now, set min_scipy_linear to the result of calling minimize to optimize the risk function you just implemented. Hint: Make sure to set x0, say, to [0.0, 0.0]. # call minimize with initial x0 = [0.0, 0.0] min_scipy_linear = minimize(mse_tips_linear_list, x0=[0.0, 0.0]) min_scipy_linear message: Optimization terminated successfully.\rsuccess: True\rstatus: 0\rfun: 1.036019442011604\rx: [ 9.203e-01 1.050e-01]\rnit: 3\rjac: [ 1.490e-08 0.000e+00]\rhess_inv: [[ 2.980e+00 -1.253e-01]\r[-1.253e-01 6.335e-03]]\rnfev: 15\rnjev: 5\rBased on the above output from your call to minimize, running the following cell will set and print the values of a_hat and b_hat. # just run this cell a_hat_scipy = min_scipy_linear['x'][0] b_hat_scipy = min_scipy_linear['x'][1] a_hat_scipy, b_hat_scipy (np.float64(0.9202707061277714), np.float64(0.1050244640398299))\rThe following cell will print out the values of a_hat and b_hat computed from both methods (â€œmanualâ€ refers to the analytical solution in Question 5; â€œscipyâ€ refers to the numerical solution in Question 6). If youâ€™ve done everything correctly, these should be very close to one another. # just run this cell print('a_hat_scipy: ', a_hat_scipy) print('a_hat_manual: ', a_hat) print('\\n') print('b_hat_scipy: ', b_hat_scipy) print('b_hat_manual: ', b_hat) a_hat_scipy: 0.9202707061277714\ra_hat_manual: 0.9202696135546735\rb_hat_scipy: 0.1050244640398299\rb_hat_manual: 0.10502451738435334\r","date":"2024-08-13","objectID":"/datalab5/:16:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"äº’åŠ¨å¯è§†åŒ– Visual Solution (not graded): Feel free to interact with the below plot and verify that the $\\hat{a}$ and $\\hat{b}$ you computed using either method above minimize the MSE. In the cell below we plot the mean squared error for different parameter values. Note that now that we have two parameters, we have a 3D MSE surface plot. Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure. If you get an error that your browser does not support webgl, you may need to restart your kernel and/or browser. # just run this cell import itertools import plotly.graph_objects as go a_values = np.linspace(-1, 1, 80) b_values = np.linspace(-1,1, 80) mse_values = [mse_tips_linear(a, b) \\ for a, b in itertools.product(a_values, b_values)] mse_values = np.reshape(mse_values, (len(a_values), len(b_values)), order='F') fig = go.Figure(data=[go.Surface(x=a_values, y=b_values, z=mse_values)]) fig.update_layout( title=r'MSE for different values of $a, b$', autosize=False, scene = dict( xaxis_title='x=a', yaxis_title='y=b', zaxis_title='z=MSE'), width=500, margin=dict(r=20, b=10, l=10, t=10)) fig.show() ","date":"2024-08-13","objectID":"/datalab5/:17:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Comparing Constant Model vs Linear Model At this point, we can actually compare our two models! Both the linear model and constant model were optimized using the same L2 loss function but predict different values for different tips. Run the cell below: sns.scatterplot(x = x_total_bills, y = y_tips, label='observed'); # the below plot expects you've run all of Question 5 plt.plot(x_total_bills, predict_tip_linear(x_total_bills), label='linear', color='g'); # the below function expects you've run the cell right before part B plt.axhline(y=predict_tip_constant(), label='constant', color='m', ls='--'); plt.legend() plt.xlabel(\"total bill\") plt.ylabel(\"tip\") plt.title(\"Tips: Linear vs Constant Models\"); plt.show() Note that while we plot tip by total bill, the constant model doesnâ€™t use the total bill in its prediction and therefore shows up as a horizontal line. Thought question: For predicting tip on this data, would you rather use the constant model or the linear model, assuming an L2 loss function for both? This might be more fun with a partner. Note, your answer will not be graded, so donâ€™t worry about writing a detailed answer. If you want to see our answer, see the very end of this lab notebook. In the not-so-distant future of this class, you will learn more quantitative metrics to compare model performance. Stay tuned! ","date":"2024-08-13","objectID":"/datalab5/:17:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Part C: Using a Different Loss Function In this last (short) section, weâ€™ll consider how the optimal parameters for the constant model would change if we used a different loss function. We will now use absolute loss (also known as the $L_1$ loss, pronounced â€œell-oneâ€). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_1$ loss of: $$\\Large L_1(y, \\hat{y}) = |y - \\hat{y}|$$ While we still define risk as average loss, since we now use $L_1$ loss per datapoint in our datset $\\mathcal{D} = {y_1, \\ldots, y_n}$, our risk is now known as mean absolute error (MAE). For the constant model $\\hat{y} = \\theta$ (i.e., we predict our tip as a summary statistic): \\begin{align} \\Large R\\left(\\theta\\right) \u0026= \\Large \\frac{1}{n} \\sum_{i=1}^n L_1(y_i, \\hat{y_i}) \\ \u0026= \\Large \\frac{1}{n} \\sum_{i=1}^n |y_i - \\theta| \\end{align} Note: the last line results from using the constant model for $\\hat{y}$. If we decided to use the linear model, we would have a different expression. ","date":"2024-08-13","objectID":"/datalab5/:18:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 7 ","date":"2024-08-13","objectID":"/datalab5/:19:0","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 7a Define the mae_tips_constant function which computes $R(\\theta)$ as the mean absolute error (MAE) on the tips data for a constant model with parameter $\\theta$. Hint: You may want to check out your solution from Question 2, which computed mean squared error (MSE). def mae_tips_constant(theta): data = y_tips return np.mean(np.abs(data - theta)) mae_tips_constant(5.3) # arbitrarily pick theta = 5.3 np.float64(2.4527868852459016)\rgrader.check(\"q7\") ","date":"2024-08-13","objectID":"/datalab5/:19:1","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Question 7b In lecture, we saw that the value of theta that minimizes mean absolute error for the constant model is the median of the data. Assign min_computed_mae to the median of the observed y_tips data. min_computed_mae = np.median(y_tips) min_computed_mae np.float64(2.9)\rgrader.check(\"q7b\") ","date":"2024-08-13","objectID":"/datalab5/:19:2","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":"Comparing MAE to MSE Now run the below cell to compare MAE with MSE on the constant model. # just run this cell fig, ax = plt.subplots(nrows=2, figsize=((6, 8))) theta_values = np.linspace(0, 6, 100) mse = [mse_tips_constant(theta) for theta in theta_values] ax[0].plot(theta_values, mse) ax[0].axvline(x=min_computed, linewidth=4, color='k', ls='--', label=r'$\\hat{\\theta}$') ax[0].legend() ax[0].set_ylabel(\"avg L2 loss (MSE)\") mae = [mae_tips_constant(theta) for theta in theta_values] ax[1].plot(theta_values, mae, color='orange') ax[1].axvline(x=min_computed_mae, linewidth=4, color='k', ls='--', label=r'$\\hat{\\theta}$') ax[1].legend() ax[1].set_ylabel(\"avg L1 loss (MAE)\") ax[1].set_xlabel(r'$\\theta$'); fig.suptitle(r\"MAE vs MSE (constant model) for different values of $\\theta$\"); Thought question You should see that the MAE plot (below) looks somewhat similar the MSE plot (above). Try to identify any key differences you observe and write them down below. This might be more fun with a partner. Note, your answer will not be graded, so donâ€™t worry about writing a detailed answer. If you want to see our answer, see the very end of this lab notebook. Write your answer here, replacing this text. Congratulations! You finished the lab! Extra Notes Our Observations on Constant Model vs Linear Model Earlier in this lab, we said weâ€™d describe our observations about whether to use Constant Model or Linear Model (both trained with MSE). Here are some thoughts: Recall that $r$ is the correlation coefficient, where values closer to -1 or 1 imply a very linear relationship: # you computed this in Q5a r np.float64(0.6757341092113641)\rThe relationship between $x$ and $y$ is somewhat linear; you can see this more clearly through the scatter plot, where there are many points that donâ€™t fall close to the linear model line. With this in mind: The linear model seems to work well for most bills. However, as bills get bigger, some datapoints seem to suggest that the constant model works better. In the wild, a tip predictor may use a combination of both the constant and linear models we trained: an average prediction, or a random coin flip to pick the model, or some heuristic decision to choose one model if the total bill exceeds a certain threshold. ï¼ˆé›†æˆå­¦ä¹ æ€æƒ³ï¼Ÿï¼‰ In the not-so-distant future of this class, you will learn more quantitative metrics to compare model performance. You will have an opportunity to explore your own models in a future assignment! Our Observations on Differences Between MAE vs. MSE Earlier in this lab, we said weâ€™d describe our observations about the differences between the MAE and MSE. There are three key differences that we identified between the plots of the MSE and MAE. The minimizing $\\theta = \\hat{\\theta}$ is different. The plot for MAE increases linearly instead of quadratically as we move far away from the minimizing $\\theta$. The plot for MAE is piecewise linear instead of smooth. Each change in slope happens at the same $\\theta$ value as a data point in our dataset. ","date":"2024-08-13","objectID":"/datalab5/:19:3","tags":["SciPy","Plotly"],"title":"DATA100-lab5: Modeling, Loss Functions, and Summary Statistics","uri":"/datalab5/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab06.ipynb\") Lab 6: Linear Regression ","date":"2024-08-13","objectID":"/datalab6/:0:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Objectives In this lab, you will review the details of linear regresison as described in Lectures 10 and 11. In particular: Matrix formulation and solution to Ordinary Least Squares sns.lmplot as a quick visual for simple linear regression scikit-learn, a real world data science tool that is more robust and flexible than analytical/scipy.optimize solutions You will also practice interpreting residual plots (vs. fitted values) and the Multiple $R^2$ metric used in Multiple Linear Regression. For the first part of this lab, you will predict fuel efficiency (mpg) of several models of automobiles using a single feature: engine power (horsepower). For the second part, you will perform feature engineering on multiple features to better predict fuel efficiency. First, letâ€™s load in the data. # Run this cell import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # Here, we load the fuel dataset, and drop any rows that have missing data vehicle_data = sns.load_dataset('mpg').dropna() vehicle_data = vehicle_data.sort_values('horsepower', ascending=True) vehicle_data.head(5) mpg\rcylinders\rdisplacement\rhorsepower\rweight\racceleration\rmodel_year\rorigin\rname\r102\r26.0\r4\r97.0\r46.0\r1950\r21.0\r73\reurope\rvolkswagen super beetle\r19\r26.0\r4\r97.0\r46.0\r1835\r20.5\r70\reurope\rvolkswagen 1131 deluxe sedan\r325\r44.3\r4\r90.0\r48.0\r2085\r21.7\r80\reurope\rvw rabbit c (diesel)\r326\r43.4\r4\r90.0\r48.0\r2335\r23.7\r80\reurope\rvw dasher (diesel)\r244\r43.1\r4\r90.0\r48.0\r1985\r21.5\r78\reurope\rvolkswagen rabbit custom diesel\rWe have 392 datapoints and 8 potential features (plus our observations, mpg). vehicle_data.shape (392, 9)\rLet us try to fit a line to the below plot, which shows mpg vs. horsepower for several models of automobiles. # just run this cell sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data); ","date":"2024-08-13","objectID":"/datalab6/:1:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1: Ordinary Least Squares Instead of using the SLR formulation, in this lab we will practice linear algebra with Ordinary Least Squares. Recall that the Simple Linear Regression model is written as follows: $$\\hat{y} = \\theta_0 + \\theta_1 x$$ We now use $\\theta = (\\theta_0, \\theta_1)$ so that the formulation more closely matches our multiple linear regression model: $$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_p x_p$$ We can rewrite our multiple linear regression model using matrix notation. Let $\\mathbb{Y}$ be a vector of all $n$ observations in our sample. Then our prediction vector $\\hat{\\mathbb{Y}}$ is $$\\Large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$ where $\\mathbb{X}$ is the design matrix representing the $p$ features for all $n$ datapoints in our sample. Note that for our SLR model, $p = 1$ and therefore the matrix notation seems rather silly. Nevertheless it is valuable to start small and build on our intuition. ","date":"2024-08-13","objectID":"/datalab6/:2:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1a: Construct $\\mathbb{X}$ with an intercept term Because we have an intercept term $\\theta_0$ in our parameter vector $\\theta$, our design matrix $\\mathbb{X}$ for $p$ features actually has dimension $$ \\Large \\mathbb{X} \\in \\mathbb{R}^{n \\times (p + 1)}$$ Therefore, the resulting matrix expression $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$ represents $n$ linear equations, where equation $i$ is $\\hat{y_i} = \\theta_0 \\cdot 1 + \\theta_1 \\cdot x_1 + \\dots + \\theta_p x_p$. The constant all-ones column of $\\mathbb{X}$ is sometimes called the bias feature; $\\theta_0$ is frequently called the bias or intercept term. Below, implement add_intercept, which computes a design matrix such that the first (left-most) column is all ones. The function has two lines: you are responsible for constructing the all-ones column bias_feature using the np.ones function (NumPy documentation). This is then piped into a call to np.concatenate (documentation), which weâ€™ve implemented for you. Note: bias_feature should be a matrix of dimension (n,1), not a vector of dimension (n,). def add_intercept(X): \"\"\" Return X with a bias feature. Parameters ----------- X: a 2D dataframe of p numeric features (may also be a 2D numpy array) of shape n x p Returns ----------- A 2D matrix of shape n x (p + 1), where the leftmost column is a column vector of 1's \"\"\" bias_feature = np.ones((X.shape[0], 1)) return np.concatenate([bias_feature, X], axis=1) # çŸ©é˜µæ‹¼æ¥ # Note the [[ ]] brackets below: the argument needs to be # a matrix (DataFrame), as opposed to a single array (Series). X = add_intercept(vehicle_data[['horsepower']]) X.shape (392, 2)\rgrader.check(\"q1a\") ","date":"2024-08-13","objectID":"/datalab6/:2:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1b: Define the OLS Model The predictions for all $n$ points in our data are (note $\\theta = (\\theta_0, \\theta_1, \\dots, \\theta_p)$) : $$ \\Large \\hat{\\mathbb{Y}} = \\mathbb{X}\\theta $$ Below, implement the linear_model function to evaluate this product. Hint: You can use np.dot, pd.DataFrame.dot, or the @ operator to multiply matrices/vectors. However, while the @ operator can be used to multiply numpy arrays, it generally will not work between two pandas objects, so keep that in mind when computing matrix-vector products! def linear_model(thetas, X): \"\"\" Return the linear combination of thetas and features as defined above. Parameters ----------- thetas: a 1D vector representing the parameters of our model ([theta1, theta2, ...]) X: a 2D dataframe of numeric features (may also be a 2D numpy array) Returns ----------- A 1D vector representing the linear combination of thetas and features as defined above. \"\"\" return np.dot(X, thetas) grader.check(\"q1b\") ","date":"2024-08-13","objectID":"/datalab6/:2:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1c: Least Squares Estimate, Analytically, å¸¸è§çš„çŸ©é˜µæ“ä½œ Recall from lecture that Ordinary Least Squares is when we fit a linear model with mean squared error, which is equivalent to the following optimization problem: $$\\Large \\min_{\\theta} ||\\Bbb{X}\\theta - \\Bbb{Y}||^2$$ We showed in Lecture that the optimal estimate $\\hat{\\theta}$ when $X^TX$ is invertible is given by the equation: $$ \\Large \\hat{\\theta} = (\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$$ Below, implement the analytic solution to $\\hat{\\theta}$ using np.linalg.inv (link) to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$. Reminder: To compute the transpose of a matrix, you can use X.T or X.transpose() (link). Note: You can also consider using np.linalg.solve (link) instead of np.linalg.inv because it is more robust (more on StackOverflow here). def get_analytical_sol(X, y): \"\"\" Computes the analytical solution to our least squares problem Parameters ----------- X: a 2D dataframe (or numpy array) of numeric features y: a 1D vector of tip amounts Returns ----------- The estimate for theta (a 1D vector) computed using the equation mentioned above. \"\"\" return np.linalg.inv(X.T @ X) @ X.T @ y Y = vehicle_data['mpg'] analytical_thetas = get_analytical_sol(X, Y) analytical_thetas array([39.93586102, -0.15784473])\rgrader.check(\"q1c\") Now, letâ€™s analyze our modelâ€™s performance. Your task will be to interpret the modelâ€™s performance using the two visualizations and one performance metric weâ€™ve implemented below. First, we run sns.lmplot, which will both provide a scatterplot of mpg vs horsepower and display the least-squares line of best fit. (If youâ€™d like to verify the OLS fit you found above is the same line found through Seaborn, change include_OLS to True.) include_OLS = True # change this flag to visualize OLS fit sns.lmplot(x='horsepower', y='mpg', data=vehicle_data); predicted_mpg_hp_only = linear_model(analytical_thetas, X) if include_OLS: # if flag is on, add OLS fit as a dotted red line plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, 'r--') Next, we plot the residuals. While in Simple Linear Regression we have the option to plot residuals vs. the single input feature, in Multiple Linear Regression we often plot residuals vs fitted values $\\hat{\\mathbb{Y}}$. In this lab, we opt for the latter. plt.scatter(predicted_mpg_hp_only, Y - predicted_mpg_hp_only) plt.axhline(0, c='black', linewidth=1) plt.xlabel(r'Fitted Values $\\hat{\\mathbb{Y}}$') plt.ylabel(r'Residuals $\\mathbb{Y} - \\hat{\\mathbb{Y}}$'); Finally, we compute the Multiple $R^2$ metric. As described in Lecture 11 (link), $$R^2 = \\frac{\\text{variance of fitted values}}{\\text{variance of true } y} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$$ $R^2$ can be used in the multiple regression setting, whereas $r$ (the correlation coefficient) is restricted to SLR since it depends on a single input feature. In SLR, $r^{2}$ and Multiple $R^{2}$ are equivalent; the proof is left to you. r2_hp_only = np.var(predicted_mpg_hp_only) / np.var(Y) print('Multiple R^2 using only horsepower: ', r2_hp_only) Multiple R^2 using only horsepower: 0.6059482578894348\r","date":"2024-08-13","objectID":"/datalab6/:2:3","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 1d In the cell below, comment on the above visualization and performance metrics, and whether horsepower and mpg have a good linear fit. poor performance ","date":"2024-08-13","objectID":"/datalab6/:2:4","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 2: Transform a Single Feature The Tukey-Mosteller Bulge Diagram tells us to transform our $\\mathbb{X}$ or $\\mathbb{Y}$ to find a linear fit. Letâ€™s consider the following linear model: $$\\text{predicted mpg} = \\theta_0 + \\theta_1 \\sqrt{\\text{horsepower}}$$ ","date":"2024-08-13","objectID":"/datalab6/:3:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 2a In the cell below, explain why we use the term â€œlinearâ€ to describe the model above, even though it incorporates a square-root of horsepower as a feature. æ³›åŒ–çº¿æ€§æ¨¡å‹ ","date":"2024-08-13","objectID":"/datalab6/:3:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Introduction to sklearn Yet another way to fit a linear regression model is to use scikit learn, an industry standard package for machine learning applications. Because it is application-specific, sklearn is often faster and more robust than the analytical/scipy-based computation methods weâ€™ve used thus far. To use sklearn: Create an sklearn object fit the object to data Analyze fit or call predict. 1. Create object. We first create a LinearRegression object. Hereâ€™s the sklearn documentation. Note that by default, the object will include an intercept term when fitting. Here, model is like a â€œblank slateâ€ for a linear model. # 1. just run this cell from sklearn.linear_model import LinearRegression model = LinearRegression(fit_intercept=True) 2. fit the object to data. Now, we need to tell model to â€œfitâ€ itself to the data. Essentially, this is doing exactly what you did in the previous part of this lab (creating a risk function and finding the parameters that minimize that risk). Note: X needs to be a matrix (or DataFrame), as opposed to a single array (or Series). This is because sklearn.linear_model is robust enough to be used for multiple regression, which we will look at later this lab. # 2. run this cell to add sqrt(hp) column for each car in the dataset vehicle_data['sqrt(hp)'] = np.sqrt(vehicle_data['horsepower']) vehicle_data.head() mpg\rcylinders\rdisplacement\rhorsepower\rweight\racceleration\rmodel_year\rorigin\rname\rsqrt(hp)\r102\r26.0\r4\r97.0\r46.0\r1950\r21.0\r73\reurope\rvolkswagen super beetle\r6.782330\r19\r26.0\r4\r97.0\r46.0\r1835\r20.5\r70\reurope\rvolkswagen 1131 deluxe sedan\r6.782330\r325\r44.3\r4\r90.0\r48.0\r2085\r21.7\r80\reurope\rvw rabbit c (diesel)\r6.928203\r326\r43.4\r4\r90.0\r48.0\r2335\r23.7\r80\reurope\rvw dasher (diesel)\r6.928203\r244\r43.1\r4\r90.0\r48.0\r1985\r21.5\r78\reurope\rvolkswagen rabbit custom diesel\r6.928203\r# 2. run this cell model.fit(X = vehicle_data[['sqrt(hp)']], y= vehicle_data['mpg']) 3. Analyze fit. Now that the model exists, we can look at the $\\hat{\\theta_0}$ and $\\hat{\\theta_1}$ values it found, which are given in the attributes intercept and coef, respectively. model.intercept_ np.float64(58.705172037217466)\rmodel.coef_ array([-3.50352375])\r3 (continued). Call predict. To use the scikit-learn linear regression model to make predictions, you can use the model.predict method. Below, we find the estimated mpg for a single datapoint with a sqrt(hp) of 6.78 (i.e., horsepower 46). Note that unlike the linear algebra approach, we do not need to manually add an intercept term, because our model (which was created with fit_intercept=True) will auto-add one. single_datapoint = [[6.78]] # needs to be a 2D array since the X in step 2 was a 2D array. [[ ]] trick model.predict(single_datapoint) d:\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\rwarnings.warn(\rarray([34.95128104])\r","date":"2024-08-13","objectID":"/datalab6/:3:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 2b Using the model defined above, set predicted_mpg to the predicted mpg for the data below. Running the cell will then compute the multiple $R^2$ value and create a linear regression plot for this new square root feature, overlaid on the original least squares estimate (used in Question 1c). predicted_mpg_hp_sqrt = model.predict(vehicle_data[['sqrt(hp)']]) # do not modify below this line r2_hp_sqrt = np.var(predicted_mpg_hp_sqrt) / np.var(vehicle_data['mpg']) print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt) sns.lmplot(x='horsepower', y='mpg', data=vehicle_data) plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit'); plt.legend(); Multiple R^2 using sqrt(hp): 0.6437035832706468\rThe visualization shows a slight improvement, but note that the underlying pattern is parabolicâ€“suggesting that perhaps we should try a quadratic feature. Next, we use the power of multiple linear regression to add an additional feature. ","date":"2024-08-13","objectID":"/datalab6/:3:3","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Add an Additional Feature For the second part of this lab, we move from SLR to multiple linear regression. Until now, we have established relationships between one independent explanatory variable and one response variable. However, with real-world problems you will often want to use multiple features to model and predict a response variable. Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to the observed data. We can consider including functions of existing features as new features to help improve the predictive power of our model. (This is something we will discuss in further detail in the Feature Engineering lecture.) The cell below adds a column which contains the square of the horsepower for each car in the dataset. # just run this cell vehicle_data['hp^2'] = vehicle_data['horsepower'] ** 2 vehicle_data.head() mpg\rcylinders\rdisplacement\rhorsepower\rweight\racceleration\rmodel_year\rorigin\rname\rsqrt(hp)\rhp^2\r102\r26.0\r4\r97.0\r46.0\r1950\r21.0\r73\reurope\rvolkswagen super beetle\r6.782330\r2116.0\r19\r26.0\r4\r97.0\r46.0\r1835\r20.5\r70\reurope\rvolkswagen 1131 deluxe sedan\r6.782330\r2116.0\r325\r44.3\r4\r90.0\r48.0\r2085\r21.7\r80\reurope\rvw rabbit c (diesel)\r6.928203\r2304.0\r326\r43.4\r4\r90.0\r48.0\r2335\r23.7\r80\reurope\rvw dasher (diesel)\r6.928203\r2304.0\r244\r43.1\r4\r90.0\r48.0\r1985\r21.5\r78\reurope\rvolkswagen rabbit custom diesel\r6.928203\r2304.0\r","date":"2024-08-13","objectID":"/datalab6/:4:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3 ","date":"2024-08-13","objectID":"/datalab6/:5:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3a Using scikit learnâ€™s LinearRegression, create and fit a model that tries to predict mpg from horsepower AND hp^2 using the DataFrame vehicle_data. Name your model model_multi. Hint: We did something very similar in Question 2. model_multi = LinearRegression() # by default, fit_intercept=True model_multi.fit(X = vehicle_data[['horsepower', 'hp^2']], y= vehicle_data['mpg']) grader.check(\"q3a\") After fitting, we can see the coefficients and intercept. Note, there are now two elements in model_multi.coef_, since there are two features. model_multi.intercept_ np.float64(56.90009970211301)\rmodel_multi.coef_ array([-0.46618963, 0.00123054])\r","date":"2024-08-13","objectID":"/datalab6/:5:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3b Using the above values, in LaTeX, write out the function that the model is using to predict mpg from horsepower and hp^2. $$ mpg_{predicted} = 56.90 - 0.47 \\times horsepower + 0.001 \\times hp^2 $$ The plot below shows the prediction of our model. Itâ€™s much better! # just run this cell predicted_mpg_multi = model_multi.predict(vehicle_data[['horsepower', 'hp^2']]) r2_multi = np.var(predicted_mpg_multi) / np.var(vehicle_data['mpg']) print('Multiple R^2 using both horsepower and horsepower squared: ', r2_multi) sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data) plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, label='hp only'); plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit'); plt.plot(vehicle_data['horsepower'], predicted_mpg_multi, color = 'gold', linewidth=2, label='hp and hp^2'); plt.legend(); Multiple R^2 using both horsepower and horsepower squared: 0.6875590305127548\r","date":"2024-08-13","objectID":"/datalab6/:5:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 3c In the cell below, we assign the mean of the mpg column of the vehicle data dataframe to mean_mpg. Given this information, what is the mean of the mean_predicted_mpg_hp_only, predicted_mpg_hp_sqrt, and predicted_mpg_multi arrays? Hint: You should not have to call np.mean in your code. mean_mpg = np.mean(vehicle_data['mpg']) mean_predicted_mpg_hp_only = mean_mpg # æœ€å°äºŒä¹˜æ€§è´¨å†³å®š!y_bar = a + b * x_bar mean_predicted_mpg_hp_sqrt = mean_mpg mean_predicted_mpg_multi = mean_mpg # print(np.mean(predicted_mpg_hp_sqrt)) # print(mean_mpg) # print(np.mean(predicted_mpg_multi)) 23.445918367346934\r23.445918367346938\r23.445918367346938\rgrader.check(\"q3c\") ","date":"2024-08-13","objectID":"/datalab6/:5:3","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Faulty Feature Engineering: Redundant Features Suppose we used the following linear model: \\begin{align} \\text{mpg} \u0026= \\theta_0 + \\theta_1 \\cdot \\text{horsepower} + \\ \u0026\\theta_2 \\cdot \\text{horsepower}^2 + \\theta_3 \\cdot \\text{horsepower} \\end{align} Notice that horsepower appears twice in our model!! We will explore how this redundant feature affects our modeling. ","date":"2024-08-13","objectID":"/datalab6/:6:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 4 ","date":"2024-08-13","objectID":"/datalab6/:7:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 4a: Linear Algebra Construct a matrix X_redundant that uses the vehicle data DataFrame to encode the â€œthreeâ€ features above, as well as a bias feature. Hint: Use the add_intercept term you implemented in Question 1a. X_redundant = add_intercept(vehicle_data[['horsepower', 'hp^2', 'horsepower']]) X_redundant.shape (392, 4)\rgrader.check(\"q4a\") q4a passed! ğŸš€ Now, run the cell below to find the analytical OLS Estimate using the get_analytical_sol function you wrote in Question 1c. Depending on the machine that you run your code on, you should either see a singular matrix error or end up with thetas that are nonsensical (magnitudes greater than 10^15). This is not good! # just run this cell # the try-except block suppresses errors during submission import traceback try: analytical_thetas = get_analytical_sol(X_redundant, vehicle_data['mpg']) analytical_thetas except Exception as e: print(traceback.format_exc()) ","date":"2024-08-13","objectID":"/datalab6/:7:1","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 4b In the cell below, explain why we got the error above when trying to calculate the analytical solution to predict mpg. è§£ææ–¹æ³•ä¸ä¸€å®šæ­£ç¡®ï¼Œæ¯”è¾ƒç†æƒ³ä½†ä¸ç°å®ï¼ˆä½†æ˜¯ä¸Šé¢å¹¶æ²¡æœ‰errorè¯¶ï¼‰ Note: While we encountered errors when using the linear algebra approach, a model fitted with `sklearn` will not encounter matrix singularity errors since it uses numerical methods to find optimums (to be covered in Gradient Descent lecture).\r# just run this cell # sklearn finds optimal parameters despite redundant features model_redundant = LinearRegression(fit_intercept=False) # X_redundant already has an intercept column model_redundant.fit(X = X_redundant, y = vehicle_data['mpg']) model_redundant.coef_ array([ 5.69000997e+01, -2.33094815e-01, 1.23053610e-03, -2.33094815e-01])\r","date":"2024-08-13","objectID":"/datalab6/:7:2","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Overfitting with Too Many Features Letâ€™s take what weâ€™ve learned so far and go one step further: introduce even more features. Again, using scikit learnâ€™s LinearRegression, we fit a model that tries to predict mpg using each of the following as features: horsepower hp^2 model_year acceleration # just run this cell desired_columns = ['horsepower', 'hp^2', 'model_year', 'acceleration'] model_overfit = LinearRegression() model_overfit.fit(X = vehicle_data[desired_columns], y= vehicle_data['mpg']) predicted_mpg_overfit = model_overfit.predict(vehicle_data[['horsepower', 'hp^2', 'model_year', 'acceleration']]) The plot below shows the prediction of our more sophisticated model. Note we arbitrarily plot against horsepower for the ease of keeping our plots 2-dimensional.\r# just run this cell sns.scatterplot(x='horsepower', y='mpg', data=vehicle_data) plt.plot(vehicle_data['horsepower'], predicted_mpg_overfit, color = 'r'); Think about what you see in the above plot. Why is the shape of our prediction curve so jagged? Do you think this is a good model to predict the mpg of some car we donâ€™t already have information on? This idea â€“the bias-variance tradeoff is an idea we will explore in the coming weeks. ","date":"2024-08-13","objectID":"/datalab6/:8:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":"Question 5: Comparing $R^2$ Lastly, set r2_overfit to be the multiple $R^2$ coefficient obtained by using model_overfit. Hint: This is very similar to several pre-computed cells in Questions 1c, 2b, and 3b. r2_overfit = np.var(predicted_mpg_overfit) / np.var(vehicle_data['mpg']) r2_overfit np.float64(0.8163086433998654)\rgrader.check(\"q5\") Comparing this model with previous models: # just run this cell # compares q1, q2, q3, and overfit models (ignores redundant model) print('Multiple R^2 using only horsepower: ', r2_hp_only) print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt) print('Multiple R^2 using both hp and hp^2: ', r2_multi) print('Multiple R^2 using hp, hp^2, model year, and acceleration: ', r2_overfit) Multiple R^2 using only horsepower: 0.6059482578894348\rMultiple R^2 using sqrt(hp): 0.6437035832706468\rMultiple R^2 using both hp and hp^2: 0.6875590305127548\rMultiple R^2 using hp, hp^2, model year, and acceleration: 0.8163086433998654\rIf everything was done correctly, the multiple $R^2$ of our latest model should be substantially higher than that of the previous models. This is because multiple $R^2$ increases with the number of covariates (i.e., features) we add to our model. A Word on Overfitting: We might not always want to use models with large multiple $R^2$ values because these models could be overfitting to our specific sample data, and wonâ€™t generalize well to unseen data from the population. Again, this is an idea we will explore in future lectures and assignments. Congratulations! You finished the lab! ","date":"2024-08-13","objectID":"/datalab6/:9:0","tags":["Scikit-learn","Seaborn"],"title":"DATA100-lab6: Linear Regression","uri":"/datalab6/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab03.ipynb\") Lab 3: Data Cleaning and EDA In this lab you will be working on visualizing a dataset from the City of Berkeley containing data on calls to the Berkeley Police Department. Information about the dataset can be found at this link. ","date":"2024-08-13","objectID":"/datalab3/:0:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Content Warning This lab includes an analysis of crime in Berkeley. If you feel uncomfortable with this topic, please contact your GSI or the instructors. ","date":"2024-08-13","objectID":"/datalab3/:0:1","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Setup In this lab, weâ€™ll perform Exploratory Data Analysis and learn some preliminary tips for working with matplotlib (a Python plotting library). Note that we configure a custom default figure size. Virtually every default aspect of matplotlib can be customized. Collaborators: list names here import pandas as pd import numpy as np import zipfile import matplotlib import matplotlib.pyplot as plt plt.rcParams['figure.figsize'] = (12, 9) fig = plt.figure() plt.show(fig) \u003cFigure size 1200x900 with 0 Axes\u003e\rPart 1: Acquire the Data 1. Obtain data To retrieve the dataset, we will use the ds100_utils.fetch_and_cache utility. # just run this cell import ds100_utils data_dir = 'data' data_url = 'http://www.ds100.org/sp22/resources/assets/datasets/lab03_data_sp22.zip' file_name = 'lab03_data_sp22.zip' dest_path = ds100_utils.fetch_and_cache(data_url=data_url, file=file_name, data_dir=data_dir) print(f'Located at {dest_path}') Using cached version that was downloaded (UTC): Sun Jul 28 01:04:24 2024\rLocated at data\\lab03_data_sp22.zip\r2. Unzip file We will now directly unzip the ZIP archive and start working with the uncompressed files. # just run this cell my_zip = zipfile.ZipFile(dest_path, 'r') my_zip.extractall(data_dir) Note: There is no single right answer regarding whether to work with compressed files in their compressed state or to uncompress them on disk permanently. For example, if you need to work with multiple tools on the same files, or write many notebooks to analyze themâ€”and they are not too largeâ€”it may be more convenient to uncompress them once. But you may also have situations where you find it preferable to work with the compressed data directly. Python gives you tools for both approaches, and you should know how to perform both tasks in order to choose the one that best suits the problem at hand. 3. View files Now, weâ€™ll use the os package to list all files in the data directory. os.walk() recursively traverses the directory, and os.path.join() creates the full pathname of each file. If youâ€™re interested in learning more, check out the Python3 documentation pages for os.walk (link) and os.path (link). We use Python 3 format strings to nicely format the printed variables dpath and fpath. # just run this cell # two for loop in the same time... ? not that funny yet? import os for root, directories, filenames in os.walk(data_dir): # first, print out all directories ---\u003e \"secret\" for directory in directories: dpath = os.path.join(root, directory) print(f\"d {dpath}\") # next, print out all files for filename in filenames: fpath = os.path.join(root,filename) print(f\" {fpath}\") d data\\secret\rdata\\ben_kurtovic.py\rdata\\Berkeley_PD_-_Calls_for_Service.csv\rdata\\dummy.txt\rdata\\hello_world.py\rdata\\lab03_data_sp22.zip\rdata\\secret\\do_not_readme.md\rIn this Lab, weâ€™ll be working with the Berkeley_PD_-_Calls_for_Service.csv file. Feel free to check out the other files, though. Part 2: Clean and Explore the Data Letâ€™s now load the CSV file we have into a pandas.DataFrame object and start exploring the data. # just run this cell calls = pd.read_csv(\"data/Berkeley_PD_-_Calls_for_Service.csv\") calls.head() CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\r0\r21014296\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:58\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\r1\r21014391\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:38\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\r2\r21090494\rTHEFT MISD. (UNDER $950)\r04/19/2021 12:00:00 AM\r12:15\rLARCENY\r1\r06/15/2021 12:00:00 AM\r2100 BLOCK HASTE ST\\nBerkeley, CA\\n(37.864908,...\r2100 BLOCK HASTE ST\rBerkeley\rCA\r3\r21090204\rTHEFT FELONY (OVER $950)\r02/13/2021 12:00:00 AM\r17:00\rLARCENY\r6\r06/15/2021 12:00:00 AM\r2600 BLOCK WARRING ST\\nBerkeley, CA\\n(37.86393...\r2600 BLOCK WARRING ST\rBerkeley\rCA\r4\r21090179\rBURGLARY AUTO\r02/08/2021 12:00:00 AM\r6:20\rBURGLARY - VEHICLE\r1\r06/15/2021 12:00:00","date":"2024-08-13","objectID":"/datalab3/:1:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 1 In the cell below, set answer1 equal to a list of strings corresponding to the possible values (which means unique ğŸ¤” ) for OFFENSE when CVLEGEND is â€œLARCENYâ€. You can type the answer manually, or you can create an expression that automatically extracts the names. answer1 = list(calls[calls['CVLEGEND'] == 'LARCENY']['OFFENSE'].unique()) answer1 ['THEFT MISD. (UNDER $950)', 'THEFT FELONY (OVER $950)', 'THEFT FROM PERSON']\rgrader.check(\"q1\") q1 passed! ğŸŒŸ Part 3: Visualize the Data ","date":"2024-08-13","objectID":"/datalab3/:2:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Matplotlib demo Youâ€™ve seen some matplotlib in this class already, but now we will explain how to work with the object-oriented plotting API mentioned in this matplotlib.pyplot tutorial useful. In matplotlib, plotting occurs on a set of Axes which are associated with a Figure. An analogy is that on a blank canvas (Figure), you choose a location to plot (Axes) and then fill it in (plot). There are two approaches to labeling and manipulating figure contents, which weâ€™ll discuss below. Approach 1 is closest to the plotting paradigm of MATLAB, the namesake of matplotlib; Approach 2 is also common because many matplotlib-based packages (such as Seaborn) explicitly return the current set of axes after plotting data. Both are essentially equivalent, and at the end of this class youâ€™ll be comfortable with both. Approach 1: matplotlib (or Seaborn) will auto-plot onto the current set of Axes or (if none exists) create a new figure/set of default axes. You can plot data using methods from plt, which is shorthand for the matplotlib.pyplot package. Then subsequent plt calls all edit the same set of default-created axes. Approach 2: After creating the initial plot, you can also use plt.gca() to explicitly get the current set of axes, and then edit those specific axes using axes methods. Note the method naming is slightly different! As an example of the built-in plotting functionality of pandas, the following example uses plot method of the Series class to generate a barh plot type to visually display the value counts for CVLEGEND. There are also many other plots that we will explore throughout the lab. Side note: Pandas also offers basic functionality for plotting. For example, the DataFrame and Series classes both have a plot method, which uses matplotlib under the hood. For now weâ€™ll focus on matplotlib itself so you get used to the syntax, but just know that convenient Pandas plotting methods exist for your own future data science exploration. Below, we show both approaches by generating a horizontal bar plot to visually display the value counts for CVLEGEND. See the barhdocumentation for more details. # DEMO CELL: assign demo to 1 or 2. demo = 1 calls_cvlegend = calls['CVLEGEND'].value_counts() if demo == 1: plt.barh(calls_cvlegend.index, calls_cvlegend) # creates figure and axes (y,x) not (x,y)! print(f\"Demo {demo}: Using plt methods to update plot\") plt.ylabel(\"Crime Category\") # uses most recently plotted axes plt.xlabel(\"Number of Calls\") plt.title(\"Number of Calls by Crime Type\") elif demo == 2: print(f\"Demo {demo}: Using axes methods to update plot\") plt.barh(calls_cvlegend.index, calls_cvlegend) # creates figure and axes ax = plt.gca() ax.set_ylabel(\"Crime Category\") ax.set_xlabel(\"Number of Calls\") ax.set_title(\"Axes methods: Number of Calls by Crime Type\") else: print(\"Error: Please assign the demo variable to 1 or 2.\") plt.show() Demo 1: Using plt methods to update plot\r","date":"2024-08-13","objectID":"/datalab3/:2:1","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"An Additional Note on Plotting in Jupyter Notebooks You may have noticed that many of our plotting code cells end with a semicolon ; or plt.show(). The former prevents any extra output from the last line of the cell; the latter explicitly returns (and outputs) the figure. Try adding this to your own code in the following questions! ","date":"2024-08-13","objectID":"/datalab3/:2:2","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2 Now it is your turn to make a plot using matplotlib. Letâ€™s start by transforming the data so that it is easier to work with. The CVDOW field isnâ€™t named helpfully and it is hard to see the meaning from the data alone. According to the website linked at the top of this notebook, CVDOW is actually indicating the day that events happened. 0-\u003eSunday, 1-\u003eMonday â€¦ 6-\u003eSaturday. ","date":"2024-08-13","objectID":"/datalab3/:3:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2a Add a new column Day into the calls dataframe that has the string weekday (eg. â€˜Sundayâ€™) for the corresponding value in CVDOW. For example, if the first 3 values of CVDOW are [3, 6, 0], then the first 3 values of the Day column should be [\"Wednesday\", \"Saturday\", \"Sunday\"]. Hint: Try using the Series.map function on calls[\"CVDOW\"]. Can you assign this to the new column calls[\"Day\"]? days = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"] day_indices = range(7) indices_to_days_dict = dict(zip(day_indices, days)) # Should look like {0:\"Sunday\", 1:\"Monday\", ..., 6:\"Saturday\"} calls[\"Day\"] = calls[\"CVDOW\"].map(indices_to_days_dict) grader.check(\"q2a\") q2a passed! ğŸŒŸ # just run this example cell ax = calls['CVLEGEND'].value_counts().plot(kind='barh') ax.set_ylabel(\"Crime Category\") ax.set_xlabel(\"Number of Calls\") ax.set_title(\"Number of Calls By Crime Type\"); Challenge (OPTIONAL): You could also accomplish this part as a table left join with pd.merge (documentation), instead of using Series.map. You would need to merge calls with a new dataframe that just contains the days of the week. If you have time, try it out in the below cell! mergeæ“ä½œåœ¨æ¥ä¸‹æ¥çš„labæœ‰è¯¦ç»†ä»‹ç»ï¼ğŸ˜‰ # scratch space for optional challenge dow_df = pd.DataFrame(days, columns=[\"Day\"]) ... Ellipsis\r","date":"2024-08-13","objectID":"/datalab3/:4:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2b Now letâ€™s look at the EVENTTM column which indicates the time for events. Since it contains hour and minute information, letâ€™s extract the hour info and create a new column named Hour in the calls dataframe. You should save the hour as an int. Hint: Your code should only require one line. Hint 2: The vectorized Series.str[ind] performs integer indexing on an array entry. calls[\"Hour\"] = calls[\"EVENTTM\"].str.split(\" \").str[0].str.split(\":\").str[0].astype(int) calls[\"Hour\"] 0 10\r1 10\r2 12\r3 17\r4 6\r..\r2627 12\r2628 15\r2629 0\r2630 18\r2631 2\rName: Hour, Length: 2632, dtype: int64\rgrader.check(\"q2b\") ","date":"2024-08-13","objectID":"/datalab3/:5:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2c Using matplotlib, construct a line plot with the count of the number of calls (entries in the table) for each hour of the day ordered by the time (eg. 12:00 AM, 1:00 AM, â€¦). Please use the provided variable hours in your answer. Be sure that your axes are labeled and that your plot is titled. Hint: Check out the plt.plot method in the matplotlib tutorial, as well as our demo above. hours = list(range(24)) calls_index = calls['Hour'].value_counts().sort_index() # calls_index plt.plot(hours, calls_index) plt.xlabel(\"Hour\") plt.ylabel(\"Number of Calls\") plt.title(\"Number of Calls per Hour\") # Leave this for grading purposes ax_3d = plt.gca() grader.check(\"q2c\") q2c passed! ğŸŒŸ To better understand the time of day a report occurs we could stratify the analysis by the day of the week. To do this we will use violin plots (a variation of a box plot), which you will learn in more detail next week. For now, just know that a violin plot shows an estimated distribution of quantitative data (e.g., distribution of calls by hour) over a categorical variable (day of the week). More calls occur in hours corresponding to the fatter part of each violin; the median hour of all calls in a particular day is marked by the white dot in the corresponding violin. # for now, just run this cell. # we will learn the seaborn visualization library next week. import seaborn as sns ax = sns.violinplot(data=calls.sort_values(\"CVDOW\"), x=\"Day\", y=\"Hour\", saturation=0.5, palette=\"Set2\") ax.set_title(\"Stratified Analysis of Phone Calls by Day\"); C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_19968\\4044437892.py:5: FutureWarning: Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\rax = sns.violinplot(data=calls.sort_values(\"CVDOW\"),\r","date":"2024-08-13","objectID":"/datalab3/:6:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 2d Based on your line plot and our violin plot above, what observations can you make about the patterns of calls? Here are some dimensions to consider: Are there more calls in the day or at night? What are the most and least popular times? Do call patterns vary by day of the week? ","date":"2024-08-13","objectID":"/datalab3/:7:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3 In this last part of the lab, letâ€™s extract the GPS coordinates (latitude, longitude) from the Block_Location of each record. # an example block location entry calls.loc[4, 'Block_Location'] '2700 BLOCK GARBER ST\\nBerkeley, CA\\n(37.86066, -122.253407)'\r","date":"2024-08-13","objectID":"/datalab3/:8:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3a: Regular Expressions Use regular expressions to create a dataframe calls_lat_lon that has two columns titled Lat and Lon, containing the respective latitude and longitude of each record in calls. You should use the Block_Location column to extract the latitude and longitude coordinates. Hint: Check out the Series.str.extract documentation. calls_lat_lon = calls['Block_Location'].str.extract(r'(\\d+\\.\\d+),\\s([-]*\\d+\\.\\d+)') # æ³¨æ„æ•è·ç»„æ¶µç›–èŒƒå›´ï¼ calls_lat_lon.columns = ['Lat', 'Lon'] calls_lat_lon.head(10) len(calls_lat_lon) 2632\rgrader.check(\"q3a\") q3a passed! ğŸš€ ","date":"2024-08-13","objectID":"/datalab3/:9:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3b: Join Tables Letâ€™s include the GPS data into our calls data. In the below cell, use calls_lat_lon to add two new columns called Lat and Lon to the calls dataframe. Hint: pd.merge (documentation) could be useful here. Note that the order of records in calls and calls_lat_lon are the same. import pandas as pd # åˆ›å»ºä¸¤ä¸ªé•¿åº¦ç›¸åŒçš„DataFrame df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}) df2 = pd.DataFrame({'C': [7, 8, 9], 'D': [10, 11, 12]}) # åŸºäºç´¢å¼•åˆå¹¶ # ä½¿ç”¨'Key'åˆ—åˆ! merged_df = pd.merge(df1, df2, left_index=True, right_index=True) # æ ¹æ®ä½ çš„éœ€è¦é€‰æ‹©åˆå¹¶ç±»å‹ print(merged_df) A B C D\r0 1 4 7 10\r1 2 5 8 11\r2 3 6 9 12\r# # ç”¨mergeåˆå¹¶ä¸¤ä¸ªdataframeï¼Œæ³¨æ„æ­¤æ—¶æ²¡æœ‰å…±åŒåˆ—ï¼åªèƒ½ç´¢å¼•æ‹¼æ¥ï¼ # print(calls.shape) # print(calls_lat_lon.shape) calls = pd.merge(calls, calls_lat_lon, left_index=True, right_index=True) # print(calls.shape) calls.sample(5) # random rows CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\rDay\rHour\rLat\rLon\r1173\r21024375\rDISTURBANCE\r06/02/2021 12:00:00 AM\r9:00\rDISORDERLY CONDUCT\r3\r06/15/2021 12:00:00 AM\r2020 KITTREDGE ST\\nBerkeley, CA\\n(37.868356, -...\r2020 KITTREDGE ST\rBerkeley\rCA\rWednesday\r9\r37.868356\r-122.268904\r2573\r21005894\rDISTURBANCE\r02/11/2021 12:00:00 AM\r14:12\rDISORDERLY CONDUCT\r4\r06/15/2021 12:00:00 AM\r2400 BLOCK DWIGHT WAY\\nBerkeley, CA\\n(37.86482...\r2400 BLOCK DWIGHT WAY\rBerkeley\rCA\rThursday\r14\r37.864826\r-122.260719\r990\r21022043\rROBBERY\r05/18/2021 12:00:00 AM\r20:15\rROBBERY\r2\r06/15/2021 12:00:00 AM\r2521 TELEGRAPH AVE\\nBerkeley, CA\\n(37.864705, ...\r2521 TELEGRAPH AVE\rBerkeley\rCA\rTuesday\r20\r37.864705\r-122.258463\r908\r21017272\rTHEFT MISD. (UNDER $950)\r04/19/2021 12:00:00 AM\r20:20\rLARCENY\r1\r06/15/2021 12:00:00 AM\r2800 BLOCK ADELINE ST\\nBerkeley, CA\\n(37.85811...\r2800 BLOCK ADELINE ST\rBerkeley\rCA\rMonday\r20\r37.858116\r-122.268002\r597\r21090359\rBURGLARY AUTO\r03/26/2021 12:00:00 AM\r0:00\rBURGLARY - VEHICLE\r5\r06/15/2021 12:00:00 AM\r2100 BLOCK 5TH ST\\nBerkeley, CA\\n(37.86626, -1...\r2100 BLOCK 5TH ST\rBerkeley\rCA\rFriday\r0\r37.86626\r-122.298335\rgrader.check(\"q3b\") q3b passed! ğŸš€ ","date":"2024-08-13","objectID":"/datalab3/:10:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3c: Check for Missing Values It seems like every record has valid GPS coordinates: # just run this cell # fraction of valid lat/lon entries (~calls[[\"Lat\", \"Lon\"]].isna()).mean() Lat 1.0\rLon 1.0\rdtype: float64\rHowever, a closer examination of the data reveals something else. Hereâ€™s the first few records of our data again: calls.head(5) CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\rDay\rHour\rLat\rLon\r0\r21014296\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:58\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r1\r21014391\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:38\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r2\r21090494\rTHEFT MISD. (UNDER $950)\r04/19/2021 12:00:00 AM\r12:15\rLARCENY\r1\r06/15/2021 12:00:00 AM\r2100 BLOCK HASTE ST\\nBerkeley, CA\\n(37.864908,...\r2100 BLOCK HASTE ST\rBerkeley\rCA\rMonday\r12\r37.864908\r-122.267289\r3\r21090204\rTHEFT FELONY (OVER $950)\r02/13/2021 12:00:00 AM\r17:00\rLARCENY\r6\r06/15/2021 12:00:00 AM\r2600 BLOCK WARRING ST\\nBerkeley, CA\\n(37.86393...\r2600 BLOCK WARRING ST\rBerkeley\rCA\rSaturday\r17\r37.863934\r-122.250262\r4\r21090179\rBURGLARY AUTO\r02/08/2021 12:00:00 AM\r6:20\rBURGLARY - VEHICLE\r1\r06/15/2021 12:00:00 AM\r2700 BLOCK GARBER ST\\nBerkeley, CA\\n(37.86066,...\r2700 BLOCK GARBER ST\rBerkeley\rCA\rMonday\r6\r37.86066\r-122.253407\rThere is another field that tells us whether we have a valid Block_Location entry per recordâ€”i.e., with GPS coordinates (latitude, longitude) that match the listed block location. What is it? In the below cell, use the field you found to create a new dataframe, missing_lat_lon, that contains only the rows of calls that have invalid latitude and longitude data. Your new dataframe should have all the same columns of calls. missing_lat_lon = calls[calls['Lat'] == '37.869058'] # ç†è®ºä¸Šåº”è¯¥æ˜¯latå’Œlonï¼Œä½†æ˜¯è¿™é‡Œåªå–äº†latï¼Œç„¶åæ³¨æ„ç±»å‹éšå¼è½¬æ¢ï¼ missing_lat_lon.head() # print(missing_lat_lon.shape) CASENO\rOFFENSE\rEVENTDT\rEVENTTM\rCVLEGEND\rCVDOW\rInDbDate\rBlock_Location\rBLKADDR\rCity\rState\rDay\rHour\rLat\rLon\r0\r21014296\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:58\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r1\r21014391\rTHEFT MISD. (UNDER $950)\r04/01/2021 12:00:00 AM\r10:38\rLARCENY\r4\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rThursday\r10\r37.869058\r-122.270455\r215\r21019124\rBURGLARY RESIDENTIAL\r04/30/2021 12:00:00 AM\r10:00\rBURGLARY - RESIDENTIAL\r5\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rFriday\r10\r37.869058\r-122.270455\r260\r21000289\rVEHICLE STOLEN\r01/01/2021 12:00:00 AM\r12:00\rMOTOR VEHICLE THEFT\r5\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rFriday\r12\r37.869058\r-122.270455\r633\r21013362\rBURGLARY AUTO\r03/27/2021 12:00:00 AM\r4:20\rBURGLARY - VEHICLE\r6\r06/15/2021 12:00:00 AM\rBerkeley, CA\\n(37.869058, -122.270455)\rNaN\rBerkeley\rCA\rSaturday\r4\r37.869058\r-122.270455\rgrader.check(\"q3c\") ","date":"2024-08-13","objectID":"/datalab3/:11:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3d: Check Missing Values Now let us explore if there is a pattern to which types of records have missing latitude and longitude entries. Weâ€™ve implemented the plotting code for you below, but read through it and verify you understand what weâ€™re doing (weâ€™ve thrown in a bonus plt.subplots() call, documentation here). # just run this cell missing_by_time = (pd.to_datetime(missing_lat_lon['EVENTDT']) .value_counts() .sort_index() ) missing_by_crime = (missing_lat_lon['CVLEGEND'] .value_counts() / calls['CVLEGEND'].value_counts() ).dropna().sort_values(ascending=False) fig, ax = plt.subplots(2) ax[0].bar(missing_by_time.index, missing_by_time) ax[0].set_ylabel(\"Calls with Missing Data\") ax[1].barh(missing_by_crime.index, missing_by_crime) ax[1].set_xlabel(\"Fraction of Missing Data per Event Type\") fig.suptitle(\"Characteristics of Missing Lat/Lon Data\") plt.show() C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_19968\\1218153592.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\rmissing_by_time = (pd.to_datetime(missing_lat_lon['EVENTDT'])\rBased on the plots above, are there any patterns among entries that are missing latitude/longitude data? The dataset information linked at the top of this notebook may also give more context. Type your answer here, replacing this text. ","date":"2024-08-13","objectID":"/datalab3/:12:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Question 3d: Explore The below cell plots a map of phonecalls by GPS coordinates (latitude, longitude); we drop missing location data. # just run this cell import folium import folium.plugins SF_COORDINATES = (37.87, -122.28) sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13) locs = calls.drop(missing_lat_lon.index)[['Lat', 'Lon']].astype('float').values heatmap = folium.plugins.HeatMap(locs.tolist(), radius=10) sf_map.add_child(heatmap) Make this Notebook Trusted to load map: File -\u003e Trust Notebook","date":"2024-08-13","objectID":"/datalab3/:13:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Congratulations! Congrats! You are finished with this lab. To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-08-13","objectID":"/datalab3/:14:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export(pdf=False) ","date":"2024-08-13","objectID":"/datalab3/:15:0","tags":["Pandas","Seaborn","Matplotlib"],"title":"DATA100-lab3: Data Cleaning and EDA","uri":"/datalab3/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab04.ipynb\") Lab 4: Visualization, Transformations, and KDEs ","date":"2024-08-13","objectID":"/datalab4/:0:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Objective In this lab you will get some practice plotting, applying data transformations, and working with kernel density estimators (KDEs). We will be working with data from the World Bank containing various statistics for countries and territories around the world. import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import ds100_utils plt.style.use('fivethirtyeight') # Use plt.style.available to see more styles sns.set() sns.set_context(\"talk\") %matplotlib inline ","date":"2024-08-13","objectID":"/datalab4/:0:1","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Loading Data Let us load some World Bank data into a pd.DataFrame object named wb. wb = pd.read_csv(\"data/world_bank_misc.csv\", index_col=0) wb.head() Primary completion rate: Male: % of relevant age group: 2015\rPrimary completion rate: Female: % of relevant age group: 2015\rLower secondary completion rate: Male: % of relevant age group: 2015\rLower secondary completion rate: Female: % of relevant age group: 2015\rYouth literacy rate: Male: % of ages 15-24: 2005-14\rYouth literacy rate: Female: % of ages 15-24: 2005-14\rAdult literacy rate: Male: % ages 15 and older: 2005-14\rAdult literacy rate: Female: % ages 15 and older: 2005-14\rStudents at lowest proficiency on PISA: Mathematics: % of 15 year-olds: 2015\rStudents at lowest proficiency on PISA: Reading: % of 15 year-olds: 2015\r...\rAccess to improved sanitation facilities: % of population: 1990\rAccess to improved sanitation facilities: % of population: 2015\rChild immunization rate: Measles: % of children ages 12-23 months: 2015\rChild immunization rate: DTP3: % of children ages 12-23 months: 2015\rChildren with acute respiratory infection taken to health provider: % of children under age 5 with ARI: 2009-2016\rChildren with diarrhea who received oral rehydration and continuous feeding: % of children under age 5 with diarrhea: 2009-2016\rChildren sleeping under treated bed nets: % of children under age 5: 2009-2016\rChildren with fever receiving antimalarial drugs: % of children under age 5 with fever: 2009-2016\rTuberculosis: Treatment success rate: % of new cases: 2014\rTuberculosis: Cases detection rate: % of new estimated cases: 2015\rAfghanistan\rNaN\rNaN\rNaN\rNaN\r62.0\r32.0\r45.0\r18.0\rNaN\rNaN\r...\r21.0\r32.0\r68.0\r78.0\r62.0\r41.0\r4.6\r11.8\r87.0\r58.0\rAlbania\r108.0\r105.0\r97.0\r97.0\r99.0\r99.0\r98.0\r96.0\r26.0\r7.0\r...\r78.0\r93.0\r98.0\r98.0\r70.0\r63.0\rNaN\rNaN\r88.0\r76.0\rAlgeria\r106.0\r105.0\r68.0\r85.0\r96.0\r92.0\r83.0\r68.0\r51.0\r11.0\r...\r80.0\r88.0\r95.0\r95.0\r66.0\r42.0\rNaN\rNaN\r88.0\r80.0\rAmerican Samoa\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\r61.0\r63.0\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r87.0\rAndorra\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\r100.0\r100.0\r96.0\r97.0\rNaN\rNaN\rNaN\rNaN\r83.0\r87.0\r5 rows Ã— 45 columns This table contains some interesting columns. Take a look: list(wb.columns) ['Primary completion rate: Male: % of relevant age group: 2015',\r'Primary completion rate: Female: % of relevant age group: 2015',\r'Lower secondary completion rate: Male: % of relevant age group: 2015',\r'Lower secondary completion rate: Female: % of relevant age group: 2015',\r'Youth literacy rate: Male: % of ages 15-24: 2005-14',\r'Youth literacy rate: Female: % of ages 15-24: 2005-14',\r'Adult literacy rate: Male: % ages 15 and older: 2005-14',\r'Adult literacy rate: Female: % ages 15 and older: 2005-14',\r'Students at lowest proficiency on PISA: Mathematics: % of 15 year-olds: 2015',\r'Students at lowest proficiency on PISA: Reading: % of 15 year-olds: 2015',\r'Students at lowest proficiency on PISA: Science: % of 15 year-olds: 2015',\r'Population: millions: 2016',\r'Surface area: sq. km thousands: 2016',\r'Population density: people per sq. km: 2016',\r'Gross national income, Atlas method: $ billions: 2016',\r'Gross national income per capita, Atlas method: $: 2016',\r'Purchasing power parity gross national income: $ billions: 2016',\r'per capita: $: 2016',\r'Gross domestic product: % growth : 2016',\r'per capita: % growth: 2016',\r'Prevalence of smoking: Male: % of adults: 2015',\r'Prevalence of smoking: Female: % of adults: 2015',\r'Incidence of tuberculosis: per 100,000 people: 2015',\r'Prevalence of diabetes: % of population ages 20 to 79: 2015',\r'Incidence of HIV: Total: % of uninfected population ages 15-49: 2015',\r'Prevalence of HIV: Total: % of population ages 15-49: 2015',\r\"Prevalence of HIV: Women's share of population ages 15+ living with HIV: %: 2015\",\r'Prevalence of HIV: Youth, Male: % of population ages 15-24: 2015',\r'Prevalence of HIV: Youth, Female: % of population ages 15-24: 2015',\r'Antiretroviral therapy coverage: % of people living w","date":"2024-08-13","objectID":"/datalab4/:1:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1a Suppose we wanted to build a histogram of our data to understand the distribution of literacy rates and income per capita individually. We can use countplot in seaborn to create bar charts from categorical data. sns.countplot(x = \"lit\", data = df) plt.xlabel(\"Combined literacy rate: % ages 15 and older: 2005-14\") plt.title('World Bank Combined Adult Literacy Rate') Text(0.5, 1.0, 'World Bank Combined Adult Literacy Rate')\rsns.countplot(x = \"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rIn the cell below, explain why countplot is NOT the right tool for visualizing the distribution of our data. It is so overwhelming! And ugly! ","date":"2024-08-13","objectID":"/datalab4/:2:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1b In the cell below, create a plot of income per capita (the second plot above) using the histplot function. As above, you should have two subplots, where the left subplot is literacy, and the right subplot is income. Donâ€™t forget to title the plot and label axes! Hint: Copy and paste from above to start. sns.histplot(x = \"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita'); You should see histograms that show the counts of how many data points appear in each bin. distplot uses a heuristic called the Freedman-Diaconis rule to automatically identify the best bin sizes, though it is possible to set the bins yourself (we wonâ€™t). In the cell below, we explore overlaying a rug plot on top of a histogram using rugplot. Note that the rug plot is hard to see. sns.histplot(x=\"inc\", data = df) sns.rugplot(x=\"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rOne way to make it easier to see the difference between the rug plot and the bars is to set a different color, for example: sns.histplot(x=\"inc\", data = df, color = \"lightsteelblue\") sns.rugplot(x=\"inc\", data = df) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rThere is also another function called kdeplot which plots a Kernel Density Estimate as described in class, and covered in more detail later in this lab. Rather than manually calling histplot, rugplot, and kdeplot to plot histograms, rug plots, and KDE plots, respectively, we can instead use displot, which can simultaneously plot histogram bars, a rug plot, and a KDE plot, and adjust all the colors automatically for visbility. Using the documentation for displot (Link), make a plot of the income data that includes a histogram, rug plot, and KDE plot. Hint: Youâ€™ll need to set two parameters to True. sns.displot(x='inc', data=df, kde=True, rug=True) plt.xlabel('Gross national income per capita, Atlas method: $: 2016') # å¤ªé•¿äº†ï¼Œæ˜¾ç¤ºä¸å…¨ plt.title('World Bank Gross National Income Per Capita') Text(0.5, 1.0, 'World Bank Gross National Income Per Capita')\rYou should see roughly the same histogram as before. However, now you should see an overlaid smooth line. This is the kernel density estimate discussed in class. Above, the y-axis is labeled by the counts. We can also label the y-axis by the density. An example is given below, this time using the literacy data from the beginning of this lab. sns.displot(x=\"lit\", data = df, rug = True, kde = True, stat = \"density\") plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older: 2005-14\") plt.title('World Bank Combined Adult Literacy Rate') Text(0.5, 1.0, 'World Bank Combined Adult Literacy Rate')\rObservations: Youâ€™ll also see that the y-axis value is no longer the count. Instead it is a value such that the total area in the histogram is 1. For example, the area of the last bar is approximately 22.22 * 0.028 = 0.62 The KDE is a smooth estimate of the distribution of the given variable. The area under the KDE is also 1. While it is not obvious from the figure, some of the area under the KDE is beyond the 100% literacy. In other words, the KDE is non-zero for values greater than 100%. This, of course, makes no physical sense. Nonetheless, it is a mathematical feature of the KDE. Weâ€™ll talk more about KDEs later in this lab. ","date":"2024-08-13","objectID":"/datalab4/:3:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1c Looking at the income data, it is difficult to see the distribution among low income countries because they are all scrunched up at the left side of the plot. The KDE also has a problem where the density function has a lot of area below 0. Transforming the inc data logarithmically gives us a more symmetric distribution of values. This can make it easier to see patterns. In addition, summary statistics like the mean and standard deviation (square-root of the variance) are more stable with symmetric distributions. In the cell below, make a distribution plot of inc with the data transformed using np.log10 and kde=True. If you want to see the exact counts, just set kde=False. If you donâ€™t specify the kde parameter, it is by default set to True. Hint: Unlike the examples above, you can pass a series to the displot function, i.e. rather than passing an entire DataFrame as data and a column as x, you can instead pass a series. ax = sns.displot(data=np.log10(df['inc']), kde=True, color='blue') plt.title('World Bank Gross National Income Per Capita') plt.ylabel('Density') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016'); When a distribution has a long right tail, a log-transformation often does a good job of symmetrizing the distribution, as it did here. Long right tails are common with variables that have a lower limit on the values. On the other hand, long left tails are common with distributions of variables that have an upper limit, such as percentages (canâ€™t be higher than 100%) and GPAs (canâ€™t be higher than 4). That is the case for the literacy rate. Typically taking a power-transformation such as squaring or cubing the values can help symmetrize the left skew distribution. In the cell below, we will make a distribution plot of lit with the data transformed using a power, i.e., raise lit to the 2nd, 3rd, and 4th power. We plot the transformation with the 4th power below. ax = sns.displot((df['lit']**4), kde = True) # ç»å…¸å‘é‡åŒ–numpy plt.ylabel('Density') plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older: 2005-14\") plt.title('World Bank Combined Adult Literacy Rate (4th power)', pad=30); ","date":"2024-08-13","objectID":"/datalab4/:4:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 1d If we want to examine the relationship between the female adult literacy rate and the gross national income per capita, we need to make a scatter plot. In the cell below, create a scatter plot of untransformed income per capita and literacy rate using the sns.scatterplot function. Make sure to label both axes using plt.xlabel and plt.ylabel. sns.scatterplot(x=df['lit'], y=df['inc']) plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older\") plt.ylabel('Gross national income per capita (non-log scale)') plt.title('World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate'); We can better assess the relationship between two variables when they have been straightened because it is easier for us to recognize linearity. In the cell below, we see a scatter plot of log-transformed income per capita against literacy rate. sns.scatterplot(x = df['lit'], y = np.log10(df['inc'])) plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older\") plt.ylabel('Gross national income per capita (log scale)') plt.title('World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate'); ","date":"2024-08-13","objectID":"/datalab4/:5:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"åŒå˜æ¢ï¼Œæ€è·¯æ‰“å¼€ This scatter plot looks better. The relationship is closer to linear. We can think of the log-linear relationship between x and y, as follows: a constant change in x corresponds to a percent (scaled) change in y. We can also see that the long left tail of literacy is represented in this plot by a lot of the points being bunched up near 100. Try squaring literacy and taking the log of income. Does the plot look better? plt.figure(figsize=(10,5)) sns.scatterplot(x = (df['lit']**2), y = np.log10(df['inc'])) plt.xlabel(\"Adult literacy rate: Combined: % ages 15 and older\") plt.ylabel('Gross national income per capita (log vs. ^2)') plt.title('World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate'); Choosing the best transformation for a relationship is often a balance between keeping the model simple and straightening the scatter plot. Part 2: Kernel Density Estimation In this part of the lab you will develop a deeper understanding of how kernel density estimation works. Explain KDE briefly within the lab ","date":"2024-08-13","objectID":"/datalab4/:6:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Overview Kernel density estimation is used to estimate a probability density function (i.e. a density curve) from a set of data. Just like a histogram, a density functionâ€™s total area must sum to 1. KDE centrally revolves around this idea of a â€œkernelâ€. A kernel is a function whose area sums to 1. The three steps involved in building a kernel density estimate are: Placing a kernel at each observation Normalizing kernels so that the sum of their areas is 1 Summing all kernels together ğŸ˜‹ The end result is a function, that takes in some value x and returns a density estimate at the point x. When constructing a KDE, there are several choices to make regarding the kernel. Specifically, we need to choose the function we want to use as our kernel, as well as a bandwidth parameter, which tells us how wide or narrow each kernel should be. We will explore these ideas now. Suppose we have 3 data points with values 2, 4, and 9. We can compute the (useless) histogram with a KDE as shown below. data3pts = np.array([2, 4, 9]) sns.displot(data3pts, kde = True, stat = \"density\"); To understand how KDEs are computed, we need to see the KDE outside the given range. The easiest way to do this is to use an old function called distplot. During the Spring 2022 offering of this course, distplot was still a working function in Seaborn, but it will be removed at a future date. If you get an error that says that distplot is not a valid function, sorry, you are too far in the future to do this lab exercise. sns.distplot(data3pts, kde = True); C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_9696\\4279347623.py:1: UserWarning: `distplot` is a deprecated function and will be removed in seaborn v0.14.0.\rPlease adapt your code to use either `displot` (a figure-level function with\rsimilar flexibility) or `histplot` (an axes-level function for histograms).\rFor a guide to updating your code to use the new functions, please see\rhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\rsns.distplot(data3pts, kde = True);\rè°ƒæ•´bandwidth One question you might be wondering is how the kernel density estimator decides how â€œwideâ€ each point should be. It turns out this is a parameter you can set called bw, which stands for bandwith. For example, the code below gives a bandwith value of 0.5 to each data point. Youâ€™ll see the resulting KDE is quite different. Try experimenting with different values of bandwidth and see what happens. sns.distplot(data3pts, kde = True, kde_kws = {\"bw\": 0.5}); C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_9696\\942060009.py:1: UserWarning: `distplot` is a deprecated function and will be removed in seaborn v0.14.0.\rPlease adapt your code to use either `displot` (a figure-level function with\rsimilar flexibility) or `histplot` (an axes-level function for histograms).\rFor a guide to updating your code to use the new functions, please see\rhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\rsns.distplot(data3pts, kde = True, kde_kws = {\"bw\": 0.5});\rd:\\miniconda3\\envs\\ds100\\Lib\\site-packages\\seaborn\\distributions.py:2496: UserWarning: The `bw` parameter is deprecated in favor of `bw_method` and `bw_adjust`.\rSetting `bw_method=0.5`, but please see the docs for the new parameters\rand update your code. This will become an error in seaborn v0.14.0.\rkdeplot(**{axis: a}, ax=ax, color=kde_color, **kde_kws)\r","date":"2024-08-13","objectID":"/datalab4/:6:1","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 2a As mentioned above, the kernel density estimate (KDE) is just the sum of a bunch of copies of the kernel, each centered on our data points. The default kernel used by the distplot function (as well as kdeplot) is the Gaussian kernel, given by: $$\\Large K_\\alpha(x, z) = \\frac{1}{\\sqrt{2 \\pi \\alpha^2}} \\exp\\left(-\\frac{(x - z)^2}{2 \\alpha ^2} \\right) $$ Weâ€™ve implemented the Gaussian kernel for you in Python below. Here, alpha is the smoothing or bandwidth parameter $\\alpha$ for the KDE, z is the center of the Gaussian (i.e. a data point or an array of data points), and x is an array of values of the variable whose distribution we are plotting. def gaussian_kernel(alpha, x, z): return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2)) For example, we can plot the Gaussian kernel centered at 9 with $\\alpha$ = 0.5 as below: xs = np.linspace(-2, 12, 200) alpha=0.5 kde_curve = [gaussian_kernel(alpha, x, 9) for x in xs] plt.plot(xs, kde_curve); In the cell below, plot the 3 kernel density functions corresponding to our 3 data points on the same axis. Use an alpha value of 0.5. Recall that our three data points are 2, 4, and 9. Note: Make sure to normalize your kernels! This means that the area under each of your kernels should be $\\frac{1}{3}$ since there are three data points. You donâ€™t have to use the following hints, but they might be helpful in simplifying your code. Hint: The gaussian_kernel function can also take a numpy array as an argument for z. Hint: To plot multiple plots at once, you can use plt.plot(xs, y) with a two dimensional array as y. xs = np.linspace(-2, 12, 200) alpha=0.5 kde_curve = [1/3*gaussian_kernel(alpha, x, data3pts) for x in xs] # æ³¨æ„â€œæ­£åˆ™åŒ–â€ï¼ plt.plot(xs, kde_curve); In the cell below, we see a plot that shows the sum of all three of the kernels above. The plot resembles the kde shown when you called distplot function with bandwidth 0.5 earlier. The area under the final curve will be 1 since the area under each of the three normalized kernels is $\\frac{1}{3}$. xs = np.linspace(-2, 12, 200) alpha=0.5 kde_curve = np.array([1/3 * gaussian_kernel(alpha, x, data3pts) for x in xs]) plt.plot(xs, np.sum(kde_curve, axis = 1)); # å åŠ æ›²çº¿! Recall that earlier we plotted the kernel density estimation for the logarithm of the income data, as shown again below. ax = sns.displot(np.log10(df['inc']), kind = \"kde\", rug = True) plt.title('World Bank Gross National Income Per Capita') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016'); In the cell below, a similar plot is shown using what was done in 2a. Try out different values of alpha in {0.1, 0.2, 0.3, 0.4, 0.5}. You will see that when alpha=0.2, the graph matches the previous graph well, except that the displot function hides the KDE values outside the range of the available data. xs = np.linspace(1, 6, 200) alpha=0.2 kde_curve = np.array([1/len(df['inc']) * gaussian_kernel(alpha, x, np.log10(df['inc'])) for x in xs]) plt.title('World Bank Gross National Income Per Capita') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016') plt.plot(xs, np.sum(kde_curve, axis = 1)); ","date":"2024-08-13","objectID":"/datalab4/:7:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Question 2b In your answers above, you hard-coded a lot of your work. In this problem, youâ€™ll build a more general kernel density estimator function. Implement the KDE function which computes: $$\\Large f_\\alpha(x) = \\frac{1}{n} \\sum_{i=1}^n K_\\alpha(x, z_i) $$ Where $z_i$ are the data, $\\alpha$ is a parameter to control the smoothness, and $K_\\alpha$ is the kernel density function passed as kernel. def kde(kernel, alpha, x, data): \"\"\" Compute the kernel density estimate for the single query point x. Args: kernel: a kernel function with 3 parameters: alpha, x, data alpha: the smoothing parameter to pass to the kernel x: a single query point (in one dimension) data: a numpy array of data points Returns: The smoothed estimate at the query point x \"\"\" return sum(kernel(alpha, x, zi) for zi in data) / len(data) grader.check(\"q2b\") Assuming you implemented kde correctly, the code below should generate the kde of the log of the income data as before. df['trans_inc'] = np.log10(df['inc']) xs = np.linspace(df['trans_inc'].min(), df['trans_inc'].max(), 1000) curve = [kde(gaussian_kernel, alpha, x, df['trans_inc']) for x in xs] plt.hist(df['trans_inc'], density=True, color='orange') plt.title('World Bank Gross National Income Per Capita') plt.xlabel('Log Gross national income per capita, Atlas method: $: 2016'); plt.plot(xs, curve, 'k-'); And the code below should show a 3 x 3 set of plots showing the output of the kde for different alpha values. small to large plt.figure(figsize=(15,15)) alphas = np.arange(0.2, 2.0, 0.2) for i, alpha in enumerate(alphas): plt.subplot(3, 3, i+1) xs = np.linspace(df['trans_inc'].min(), df['trans_inc'].max(), 1000) curve = [kde(gaussian_kernel, alpha, x, df['trans_inc']) for x in xs] plt.hist(df['trans_inc'], density=True, color='orange') plt.plot(xs, curve, 'k-') plt.show() Letâ€™s take a look at another kernel, the Boxcar kernel. def boxcar_kernel(alpha, x, z): return (((x-z)\u003e=-alpha/2)\u0026((x-z)\u003c=alpha/2))/alpha Run the cell below to enable interactive plots. It should give you a green â€˜OKâ€™ when itâ€™s finished. from ipywidgets import interact !jupyter nbextension enable --py widgetsnbextension # è¿™ä¸ªæ˜¯è¦notebooké™çº§å¤„ç†çš„å‘½ä»¤ Enabling notebook extension jupyter-js-widgets/extension...\r- Validating: ok\rNow, we can plot the Boxcar and Gaussian kernel functions to see what they look like. x = np.linspace(-10,10,1000) def f(alpha): plt.plot(x, boxcar_kernel(alpha,x,0), label='Boxcar') plt.plot(x, gaussian_kernel(alpha,x,0), label='Gaussian') plt.legend(title='Kernel Function') plt.show() interact(f, alpha=(1,10,0.1)); Using the interactive plot below compare the the two kernel techniques: (Generating the KDE plot is slow, so you may expect some latency after you move the slider) xs = np.linspace(df['trans_inc'].min(), df['trans_inc'].max(), 1000) def f(alpha_g, alpha_b): plt.hist(df['trans_inc'], density=True, color='orange') g_curve = [kde(gaussian_kernel, alpha_g, x, df['trans_inc']) for x in xs] plt.plot(xs, g_curve, 'k-', label='Gaussian') b_curve = [kde(boxcar_kernel, alpha_b, x, df['trans_inc']) for x in xs] plt.plot(xs, b_curve, 'r-', label='Boxcar') plt.legend(title='Kernel Function') plt.show() interact(f, alpha_g=(0.01,.5,0.01), alpha_b=(0.01,3,0.1)); interactive(children=(FloatSlider(value=0.25, description='alpha_g', max=0.5, min=0.01, step=0.01), FloatSlide\rBriefly compare and contrast the Gaussian and Boxcar kernels in the cell below. How do the two kernels relate with each other for the same alpha value? åœ†æ»‘é—®é¢˜ Congrats! You are finished with this assignment. ","date":"2024-08-13","objectID":"/datalab4/:8:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["DATA100"],"content":"Optionalâ€“pxç”¨æ³•è®²è§£ Below are some examples using plotly. Recall that this is Joshâ€™s preferred plotting library, though it is not officially covered nor required in this class. This is purely for your future reference if you decide to use plotly on your own. import plotly.express as px px.histogram(df, x = \"lit\") In my opinion, distribution plots are the one place where plotly falls short of seaborn. For example, if we want a rug, KDE, and histogram, the code below does this in plotly. Iâ€™m not personally a fan. import plotly.figure_factory as ff ff.create_distplot([df[\"lit\"]], [\"lit\"]) By contrast, I think many of plotlyâ€™s other features are far superior to seaborn. For example, consider the interactive scatterplot below, where one can mouseover each datapoint in order to see the identity of each country. px.scatter(df, x = \"lit\", y = \"inc\", hover_name = df.index, labels={ \"lit\": \"Adult literacy rate: Combined: % ages 15 and older\", \"inc\": \"Gross national income per capita\" }, title=\"World Bank: Gross National Income Per Capita vs\\n Combined Adult Literacy Rate\" ) Naturally there are ways to adjust figure size, text size, marker, etc, but they are not covered here. I just wanted to give you a small taste of plotly. ","date":"2024-08-13","objectID":"/datalab4/:9:0","tags":["Seaborn","Plotly"],"title":"DATA100-lab4: Visualization, Transformations, and KDEs","uri":"/datalab4/"},{"categories":["CS186"],"content":"Index ç«Ÿç„¶æ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Ÿ ","date":"2024-08-11","objectID":"/databasel5/:1:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"search and insertion in ISAM indexed sequential access method æ³¨æ„å»ºç«‹äº†è®¸å¤šç´¢å¼•ï¼Œæ²¿ç”¨BSTçš„æ€æƒ³ï¼Œä½†æ˜¯insertçš„æ—¶å€™ä¼šå‡ºç°overflow pages ï¼ˆIBM in 1960sï¼‰ ","date":"2024-08-11","objectID":"/databasel5/:2:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"B+ Tree å’ŒB Treeçš„åŒºåˆ«åœ¨äºï¼šB+åªæœ‰å¶å­å­˜æ”¾æ•°æ®ï¼Œè€ŒB Treeçš„ä¸­é—´èŠ‚ç‚¹ä¹Ÿå­˜æ”¾æ•°æ®ã€‚ å‡ ä¹å’Œä¸Šé¢ä¸€æ ·ï¼Œä½†æ˜¯å¤šäº† dynamic tree index always balanced support efficient insertions and deletions grows at root not leaves æ³¨æ„: å æœ‰ç‡ï¼šå‡ ä¹åŠæ»¡ï¼Œé™¤äº†root åº•éƒ¨DLL $max\\ fan\\ out = 2d + 1$ å·¥ä¸šå®é™…æƒ…å†µ ğŸ¤” ","date":"2024-08-11","objectID":"/databasel5/:3:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"B+ Tree Operations ","date":"2024-08-11","objectID":"/databasel5/:4:0","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"search, åŒä¸Š ","date":"2024-08-11","objectID":"/databasel5/:4:1","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"insert æ²¡æœ‰overflowï¼Œç›´æ¥æ’å…¥ æ»¡äº†ï¼Œåˆ†è£‚æˆä¸¤ä¸ªèŠ‚ç‚¹ï¼Œä¸­é—´èŠ‚ç‚¹å­˜æ”¾ ä¸­é—´keyï¼ˆè¿‡ç¨‹ä¸­å¯èƒ½æ˜¯å³è¾¹æœ€å°çš„é‚£ä¸ªkeyï¼‰ï¼Œå·¦å³èŠ‚ç‚¹å­˜æ”¾å·¦å³key é€’å½’å‘ä¸Šåˆ†è£‚ï¼Œç›´åˆ°æ ¹èŠ‚ç‚¹ å¶å­ $\\xrightarrow{copy}$ çˆ¶èŠ‚ç‚¹ ï¼Œçˆ¶èŠ‚ç‚¹ $\\xrightarrow{push}$ æ–°çˆ¶èŠ‚ç‚¹ ","date":"2024-08-11","objectID":"/databasel5/:4:2","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"delete ğŸ˜‰ ","date":"2024-08-11","objectID":"/databasel5/:4:3","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"bulk loading æ‰¹é‡åŠ è½½ ç²¾é«“åœ¨äºå…ˆæ’åºï¼Œåæ„å»º time-stamp: 01h09min02s ","date":"2024-08-11","objectID":"/databasel5/:4:4","tags":null,"title":"CS186-L5: B+ Trees","uri":"/databasel5/"},{"categories":["CS186"],"content":"cost model and analysis åŸºæœ¬é‡åŒ–æŒ‡æ ‡å®šä¹‰ åŸºæœ¬å‡è®¾ single record insert and delete equality selection - exactly one match for heap files: insert always appends for sorted files: packed: files compacted after deletions sorted according to search key ä»¥ä¸‹æ˜¯è®¡ç®—æ—¶é—´ï¼Œç±»ä¼¼æ¸è¿›è®°æ³•ï¼Œä½†æ˜¯æœ‰ç»†èŠ‚ ğŸ˜ side note: $\\times D$ æ˜¯ç®€åŒ–äº†æ¯æ¬¡â€œæ“ä½œâ€çš„æ—¶é—´ï¼Œâ€œæ“ä½œâ€æŒ‡çš„æ˜¯â€œè¯»â€ä¸â€œå†™â€ è€ƒè™‘éšæœºå˜é‡ æ“ä½œæ¬¡æ•° $N$ åŠå…¶ $\\mathbb{E}(N)$ Equality Searchå¯¹äºsorted files: $$ \\begin{equation} \\begin{aligned} \\mathbb{E}(N) \u0026= \\sum_{i=1}^{log_2B} i \\times \\frac{2^{i-1}}{B} \\notag\\ \u0026= log_2B - \\frac{B-1}{B}\\ \u0026\\approx log_2B \\end{aligned} \\end{equation} $$ Range Searchå€Ÿé‰´Equality Searchçš„æ€æƒ³ï¼Œå¯¹äºheap filesï¼Œalways go through to find allï¼›å¯¹äºsorted filesï¼ŒäºŒåˆ†æŸ¥æ‰¾ä¸‹ç•Œç„¶å scan right Insertå¯¹äºheap files: å‡è®¾çŸ¥é“free spaceï¼Œä¸€ä¸ªè¯»ï¼Œä¸€ä¸ªå†™ï¼›å¯¹äºsorted files: å‡è®¾ä¸­é—´æ’å…¥ï¼Œè¯»å†™å„å  $B/2$ DeleteåŒæ ·å€Ÿé‰´Equality Searchçš„æ€æƒ³ï¼Œå¯¹äºheap filesï¼Œ $+1$ æ˜¯åˆ é™¤ï¼ˆâ€œå†™â€ï¼‰ï¼›å¯¹äºsorted filesï¼Œ è¿‡ç¨‹å’ŒInsertä¸€æ · time-stamp: 01h13min26s ","date":"2024-08-11","objectID":"/databasel4/:1:0","tags":null,"title":"CS186-L4: Disks, Buffers, Files II","uri":"/databasel4/"},{"categories":["CS186"],"content":"big picture sql client -\u003e DBMS -\u003e database ğŸ¤“ ","date":"2024-08-11","objectID":"/databasel3/:1:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"DBMS parsing \u0026 optimization æ‰§è¡ŒSQLè¯­å¥æ—¶ï¼ŒDBMSéœ€è¦è§£æSQLè¯­å¥ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ‰§è¡Œè®¡åˆ’ã€‚ä¼˜åŒ–å™¨ä¼šæ ¹æ®ç»Ÿè®¡ä¿¡æ¯ã€æŸ¥è¯¢æ¨¡å¼ã€ç´¢å¼•ç­‰å› ç´ ï¼Œé€‰æ‹©æœ€ä¼˜çš„æ‰§è¡Œè®¡åˆ’ã€‚ relational operators å¤„ç†æ•°æ®æµ or å…³ç³»è¿ç®—ç¬¦ï¼Ÿ files and index management buffer management disk space management äº‹å®ä¸Šçºµå‘è¿˜æœ‰ä¸¤ä¸ªæ¨¡å—ï¼šconcurrency controlå’Œrecoveryã€‚ çœæµï¼šä»RAM \u0026 DISKè·å–æ•°æ®éå¸¸æ…¢ï¼Œ ç›¸å¯¹äºCPU ","date":"2024-08-11","objectID":"/databasel3/:1:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"Disk æ³¨æ„sector, disk headï¼Œ å…¶ä¸­åè€…ä¼¼ä¹åªèƒ½å•æ¬¡è¯»å†™ ","date":"2024-08-11","objectID":"/databasel3/:1:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"access time ","date":"2024-08-11","objectID":"/databasel3/:1:3","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"flash SSD æ³¨æ„ï¼š readå¾ˆå¿«ï¼Œéšç€æ•°æ®å˜å¤§ï¼Œå¯ä»¥é¢„æµ‹ writeå¾ˆæ…¢ï¼Œslower for randomï¼Œå†™å…¥æ”¾å¤§ ","date":"2024-08-11","objectID":"/databasel3/:2:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"disk space management ","date":"2024-08-11","objectID":"/databasel3/:3:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"block level storage block: unit of transfer for disk read/write (64~128KB in 2018) page: a common synonym for block, in some contexts, it means in RAM ","date":"2024-08-11","objectID":"/databasel3/:3:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"implementation talk to hardware directly ğŸ¤” use file system (FS) ğŸ˜€ always remember: next is fast ","date":"2024-08-11","objectID":"/databasel3/:3:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"files and representation important! ğŸ˜ tables stored in files consist of pages pages contain a collection of records ","date":"2024-08-11","objectID":"/databasel3/:4:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"DB files unordered heap files DLLæ˜¯ä¸€ç§ç®€å•çš„å®ç°heap fileçš„æ–¹æ¡ˆï¼Œä½†æ˜¯insertæ•ˆç‡ä¸é«˜ ğŸ˜ better: a page directory æ³¨æ„header page is SLL ","date":"2024-08-11","objectID":"/databasel3/:4:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"layout of a page page header nums of records free space maybe a next/last pointer bitmaps, slot table (what is that? ğŸ¤”) æ³¨æ„ï¼Œæ˜¯å¦recordså®šé•¿ä»¥åŠæ˜¯å¦æœ‰free spaceå†³å®šäº†page layout ","date":"2024-08-11","objectID":"/databasel3/:5:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"fixed-length packed records now take a look at a fixed length records, packed page: ","date":"2024-08-11","objectID":"/databasel3/:5:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"fixed-length unpacked records same as before, but with unpacked records: ","date":"2024-08-11","objectID":"/databasel3/:5:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"variable-length records records can have different lengths ğŸ¤¯ header -\u003e footer footer has a slot directory (read from right to left, has a pointer to the start of the free space) slot directory save slot, each slot has a pointer to the start of the record and the length of the record growing slot directory, å‰åå¤¹å‡» ","date":"2024-08-11","objectID":"/databasel3/:5:3","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"layout of records each record has a fixed type system catalog stores the SCHEMA no need to store the type of records catalog just a table æ³¨æ„ï¼Œä»¥ä¸‹è®¨è®ºåœ¨å­—æ®µï¼ˆfieldï¼‰çº§åˆ«ï¼Œä¸æ˜¯recordçº§åˆ«ã€‚ ","date":"2024-08-11","objectID":"/databasel3/:6:0","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"fixed-length records ç±»æ¯”æ•°ç»„ï¼Œæ³¨æ„nullå­˜åœ¨å°±æ˜¯ç©ºç€ï¼Œä¸æ˜¯å¾ˆcompact ","date":"2024-08-11","objectID":"/databasel3/:6:1","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["CS186"],"content":"variable-length records å†—ä½™é…ç½®æŒ‰ç…§å®šé•¿å¤„ç† ï¼ˆpaddingï¼‰ ç±»æ¯”CSVï¼Œæ¯è¡Œè®°å½•ä¸åŒé•¿åº¦ï¼Œç”¨åˆ†éš”ç¬¦åˆ†éš” a record header way ","date":"2024-08-11","objectID":"/databasel3/:6:2","tags":null,"title":"CS186-L3: Disk, Buffers, Files I","uri":"/databasel3/"},{"categories":["TOOLS"],"content":"åŸä½œè€…å¸–å­ï¼Ÿ COSTARæç¤ºè¯æ¡†æ¶æ˜¯ä¸€ä¸ªç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-3/4ï¼‰ç”Ÿæˆç»“æœè´¨é‡çš„å·¥å…·ã€‚è¿™ä¸ªæ¡†æ¶çš„ç›®æ ‡æ˜¯é€šè¿‡æä¾›è¯¦ç»†å’Œå…·ä½“çš„æç¤ºï¼ŒæŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®å’Œæœ‰ç”¨çš„å†…å®¹ã€‚ä»¥ä¸‹æ˜¯å¯¹CoSTARæç¤ºè¯æ¡†æ¶çš„è¯¦ç»†ä»‹ç»ï¼š COSTARæç¤ºè¯æ¡†æ¶æ˜¯ä¸€ç§ç”¨äºä¼˜åŒ–å’Œæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ç”Ÿæˆæ•ˆæœçš„æ–¹æ³•ã€‚é€šè¿‡åº”ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œå¯ä»¥æ›´å¥½åœ°æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆæ›´åŠ ç›¸å…³ã€å‡†ç¡®å’Œæœ‰ç”¨çš„å†…å®¹ã€‚COSTAR æ˜¯ä¸€ä¸ªé¦–å­—æ¯ç¼©ç•¥è¯ï¼Œæ¯ä¸ªå­—æ¯ä»£è¡¨äº†æç¤ºæ¡†æ¶ä¸­çš„ä¸€ä¸ªé‡è¦æ–¹é¢ï¼š Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰ï¼š æä¾›è¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’Œå…·ä½“æƒ…å¢ƒï¼Œå¸®åŠ©æ¨¡å‹ç†è§£é—®é¢˜çš„å…·ä½“éœ€æ±‚ã€‚ ç¤ºä¾‹ï¼šæä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½¿ç”Ÿæˆçš„å†…å®¹æ›´åŠ ç¬¦åˆé¢„æœŸã€‚ Objectiveï¼ˆç›®æ ‡ï¼‰ï¼š æ˜ç¡®ç”Ÿæˆå†…å®¹çš„ç›®æ ‡å’Œé¢„æœŸç»“æœï¼Œå¸®åŠ©æ¨¡å‹èšç„¦äºç‰¹å®šä»»åŠ¡ã€‚ ç¤ºä¾‹ï¼šæ¸…æ™°åœ°é™ˆè¿°å¸Œæœ›æ¨¡å‹ç”Ÿæˆä½•ç§ç±»å‹çš„å†…å®¹ï¼ˆå¦‚è§£é‡Šã€æ€»ç»“ã€åˆ›å»ºæ•…äº‹ç­‰ï¼‰ã€‚ Styleï¼ˆé£æ ¼ï¼‰ï¼š è®¾å®šç”Ÿæˆå†…å®¹çš„è¯­æ°”ã€é£æ ¼å’Œæ ¼å¼ï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆé¢„æœŸçš„é£æ ¼ã€‚ ç¤ºä¾‹ï¼šæŒ‡å®šæ­£å¼ã€éæ­£å¼ã€å¹½é»˜ã€å­¦æœ¯ç­‰ä¸åŒçš„é£æ ¼ã€‚ Toneï¼ˆè¯­æ°”ï¼‰ï¼š æŒ‡å®šç”Ÿæˆå†…å®¹çš„æƒ…æ„ŸåŸºè°ƒï¼Œå¦‚å‹å¥½ã€é¼“åŠ±ã€è­¦å‘Šç­‰ï¼Œä»¥ç¬¦åˆé¢„æœŸçš„äº¤æµæ•ˆæœã€‚ ç¤ºä¾‹ï¼šç¡®å®šéœ€è¦æ¸©å’Œã€ä¸¥è‚ƒã€è½»æ¾ç­‰ä¸åŒçš„è¯­æ°”ã€‚ Audienceï¼ˆå—ä¼—ï¼‰ï¼š æ˜ç¡®ç”Ÿæˆå†…å®¹çš„ç›®æ ‡è¯»è€…æˆ–è§‚ä¼—ï¼Œå¸®åŠ©æ¨¡å‹è°ƒé€‚å†…å®¹çš„å¤æ‚åº¦å’Œé€‚ç”¨æ€§ã€‚ ç¤ºä¾‹ï¼šåŒºåˆ†æ˜¯é¢å‘ä¸“ä¸šäººå£«è¿˜æ˜¯æ™®é€šè¯»è€…ï¼Œä½¿å†…å®¹æ›´å…·é’ˆå¯¹æ€§ã€‚ Relevanceï¼ˆç›¸å…³æ€§ï¼‰ï¼š å¼ºè°ƒç”Ÿæˆå†…å®¹ä¸ä¸»é¢˜çš„ç›¸å…³æ€§ï¼Œé¿å…ä¸å¿…è¦çš„åç¦»å’Œæ— å…³å†…å®¹ã€‚ ç¤ºä¾‹ï¼šç¡®ä¿æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ç›´æ¥å›ç­”é—®é¢˜ï¼Œé¿å…åé¢˜ã€‚ ","date":"2024-07-24","objectID":"/tools/costar/:0:0","tags":null,"title":"COSTARæç¤ºè¯æ¡†æ¶ç¬”è®°","uri":"/tools/costar/"},{"categories":["TOOLS"],"content":"å¦‚ä½•åº”ç”¨COSTARæç¤ºè¯æ¡†æ¶ ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰ï¼š æä¾›è¶³å¤Ÿçš„èƒŒæ™¯ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœéœ€è¦ç”Ÿæˆå…³äºç‰¹å®šå†å²äº‹ä»¶çš„å†…å®¹ï¼Œå¯ä»¥å…ˆæä¾›ä¸€äº›ç›¸å…³çš„å†å²èƒŒæ™¯ã€‚ è¯·æä¾›å…³äº1969å¹´é˜¿æ³¢ç½—11å·ç™»æœˆä»»åŠ¡çš„è¯¦ç»†æè¿°ã€‚é˜¿æ³¢ç½—11å·æ˜¯ç¾å›½å®‡èˆªå±€çš„ä¸€æ¬¡ä»»åŠ¡ï¼Œç›®çš„æ˜¯å°†äººç±»é¦–æ¬¡é€ä¸Šæœˆçƒã€‚ ç›®æ ‡ï¼ˆObjectiveï¼‰ï¼š æ˜ç¡®ç”Ÿæˆå†…å®¹çš„ç›®æ ‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœéœ€è¦æ¨¡å‹ç”Ÿæˆä¸€ç¯‡ä»‹ç»æ–‡ç« ï¼Œå¯ä»¥æ˜ç¡®è¯´æ˜è¿™ä¸€ç‚¹ã€‚ è¯·å†™ä¸€ç¯‡å…³äºå¯æŒç»­å‘å±•çš„ä»‹ç»æ–‡ç« ï¼Œé‡ç‚¹ä»‹ç»å…¶é‡è¦æ€§å’Œä¸»è¦ç­–ç•¥ã€‚ é£æ ¼ï¼ˆStyleï¼‰ï¼š æŒ‡å®šç”Ÿæˆå†…å®¹çš„é£æ ¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœéœ€è¦ç”Ÿæˆå­¦æœ¯é£æ ¼çš„æ–‡ç« ï¼Œå¯ä»¥è¿™æ ·æç¤ºã€‚ è¯·ä»¥å­¦æœ¯é£æ ¼å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„è®ºæ–‡ã€‚ è¯­æ°”ï¼ˆToneï¼‰ï¼š æŒ‡å®šå†…å®¹çš„è¯­æ°”ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¸Œæœ›å†…å®¹å…·æœ‰æ¿€åŠ±æ€§ï¼Œå¯ä»¥æ˜ç¡®è¯´æ˜ã€‚ è¯·ç”¨é¼“åŠ±çš„è¯­æ°”å†™ä¸€ç¯‡å…³äºå¦‚ä½•å…‹æœå›°éš¾çš„æ–‡ç« ã€‚ å—ä¼—ï¼ˆAudienceï¼‰ï¼š æ˜ç¡®å†…å®¹çš„å—ä¼—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå†…å®¹æ˜¯é¢å‘å­¦ç”Ÿçš„ï¼Œå¯ä»¥è¿™æ ·æç¤ºã€‚ è¯·ä¸ºé«˜ä¸­ç”Ÿå†™ä¸€ç¯‡å…³äºæ°”å€™å˜åŒ–çš„ä»‹ç»æ–‡ç« ã€‚ ç›¸å…³æ€§ï¼ˆRelevanceï¼‰ï¼š å¼ºè°ƒå†…å®¹çš„ç›¸å…³æ€§ã€‚ä¾‹å¦‚ï¼Œå¦‚æœéœ€è¦å†…å®¹èšç„¦åœ¨æŸä¸ªä¸»é¢˜ï¼Œå¯ä»¥è¿™æ ·æç¤ºã€‚ è¯·å†™ä¸€ç¯‡å…³äºç”µåŠ¨è½¦ä¼˜åŠ¿çš„æ–‡ç« ï¼Œç‰¹åˆ«å…³æ³¨å…¶å¯¹ç¯å¢ƒçš„ç§¯æå½±å“ã€‚ ","date":"2024-07-24","objectID":"/tools/costar/:0:1","tags":null,"title":"COSTARæç¤ºè¯æ¡†æ¶ç¬”è®°","uri":"/tools/costar/"},{"categories":["CS186"],"content":"æ€ä¹ˆè¯»æ‡‚SQLè¯­å¥ï¼Ÿ FROM WHERE, to eliminate rows SELECT GROUP BY HAVING, to eliminate groups DISTINCT ORDER BY, LIMIT, OFFSETç­‰ç­‰æ ¼å¼åŒ–è¾“å‡º Join Queries ","date":"2024-07-22","objectID":"/databasel2/:0:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"cross product ç”Ÿæˆæ‰€æœ‰çš„ç»„åˆï¼Œç„¶åè¿‡æ»¤æ‰ä¸ç¬¦åˆæ¡ä»¶çš„ç»„åˆï¼Œä½†æ˜¯ä½æ•ˆ è€ƒè™‘ä»¥ä¸‹sqlï¼Œæ›´åŠ ç®€æ´ SELECT S.sid, sname, bid FROM Sailors AS S, Reserves AS R WHERE S.sid = R.sid ","date":"2024-07-22","objectID":"/databasel2/:1:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"self join and more aliases SELECT x.sname, x.age, y.sname AS sname2, y.age AS age2 FROM Sailors AS x, Sailors AS y WHERE x.age \u003e y.age ","date":"2024-07-22","objectID":"/databasel2/:2:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"inner/natural join where clause ğŸ†™ ğŸ¤“ çœ‹ä¸‹é¢ select s.*, r.bid from sailors as s inner join reserves as r on s.sid = r.sid ","date":"2024-07-22","objectID":"/databasel2/:3:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"left outer join returns all matched rows, and preserves all unmatched rows from the left table of the join clause NULLå‡ºç° FULL OUTER JOINç­‰ç­‰åŒç† select r.sid, r.bid, b.bname from reserves as r full outer join boats as b on r.bid = b.bid Arithmetic Expressions æ³¨æ„SELECTå’ŒWHERE SELECT salary * 1.1 AS new_salary FROM Employees WHERE 2*salary \u003e 10000 use sql as calculator ğŸ¤“ SELECT log(1000) as three, exp(ln(2)) as two, cos(pi()) as zero, ln(2*3) = ln(2) + ln(3) as sanity; string functions old way ğŸ¤¨ SELECT s.sname FROM Sailors AS s WHERE s.sname LIKE 'a_%' new way ğŸ˜ use regular expressions! SELECT s.sname FROM Sailors AS s WHERE s.sname ~ 'a.*' bool and combining SELECT r.sid FROM boats as b, reserves as r WHERE b.bid = r.bid AND (b.color = 'blue' OR b.color = 'green') ä»¥ä¸Šä¸¤è€…ç­‰ä»· çœ‹ä¸‹é¢ä¸¤ä¸ª SELECT r.sid FROM boats as b, reserves as r WHERE b.bid = r.bid AND (b.color = 'blue' AND b.color = 'green') return nothing ğŸ¤¯ è¿”å›å³é¢„å®šäº†çº¢èˆ¹åˆé¢„å®šäº†ç»¿èˆ¹çš„äºº ğŸ¤“ Set Operations ","date":"2024-07-22","objectID":"/databasel2/:4:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"only set union, intersect, except, è¿”å›çš„éƒ½æ˜¯é›†åˆï¼Œæ²¡æœ‰é‡å¤ ","date":"2024-07-22","objectID":"/databasel2/:5:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"multiset UNION ALL: è¿”å›æ‰€æœ‰å…ƒç´ ï¼ŒåŒ…æ‹¬é‡å¤å…ƒç´  sum INTERSECT ALL: è¿”å›äº¤é›†ï¼ŒåŒ…æ‹¬é‡å¤å…ƒç´  min EXCEPT ALL: è¿”å›å·®é›†ï¼ŒåŒ…æ‹¬é‡å¤å…ƒç´  minus Nested Queries subquery ğŸ˜€ select s.sname from sailors as s where s.sid in (select r.sid from reserves as r where r.bid = 6767) NOT IN åŒç†å³å¯ è€ƒè™‘EXISTSï¼Œ éç©ºå³å¯è¿”å› å¦ä¸€ä¸ªä¾‹å­ select s.sname from sailors as s where exists (select * from reserves as r where r.sid = s.sid and r.bid = 6767) å…·ä½“æ¥è¯´ï¼ŒæŸ¥è¯¢ä¸­åŒ…å«äº†ä¸€ä¸ªEXISTSæ¡ä»¶ï¼Œè¿™ä¸ªæ¡ä»¶ä¸­çš„å­æŸ¥è¯¢æ˜¯ä¸ä¸»æŸ¥è¯¢ç›¸å…³çš„ã€‚æ¯å½“ä¸»æŸ¥è¯¢å¤„ç†Sailorsè¡¨ä¸­çš„ä¸€è¡Œæ—¶ï¼Œéƒ½ä¼šå°†è¯¥è¡Œä¸­çš„sidå€¼å¸¦å…¥å­æŸ¥è¯¢ä¸­è¿›è¡Œè®¡ç®—ï¼Œä»¥ç¡®å®šè¿™è¡Œæ•°æ®æ˜¯å¦æ»¡è¶³æ¡ä»¶ï¼ˆå³æ˜¯å¦å­˜åœ¨ä¸€æ¡å¯¹åº”çš„Reservesè®°å½•ï¼‰ã€‚è¿™æ ·ï¼Œå­æŸ¥è¯¢çš„è®¡ç®—ä¼šéšç€Sailorsè¡¨ä¸­è¡Œçš„ä¸åŒè€Œå˜åŒ–ï¼Œå› æ­¤éœ€è¦ä¸ºæ¯ä¸€è¡Œé‡æ–°è®¡ç®—ã€‚ è¿™æ„å‘³ç€ï¼Œå¦‚æœSailorsè¡¨ä¸­æœ‰å¾ˆå¤šè¡Œï¼Œå­æŸ¥è¯¢ä¹Ÿä¼šè¢«æ‰§è¡Œå¾ˆå¤šæ¬¡ï¼Œè¿™å¯èƒ½ä¼šå½±å“æŸ¥è¯¢çš„æ€§èƒ½ã€‚ è€ƒè™‘ANYï¼Œ ALL SELECT s.sname FROM sailors AS s WHERE s.age \u003e ANY (SELECT AVG(age) FROM sailors) å…³ç³»é™¤æ³• è¿™ä¸ªPPTå±•ç¤ºäº†ä¸€ä¸ªå…³äºâ€œå…³ç³»é™¤æ³•â€ï¼ˆRelational Divisionï¼‰çš„SQLæŸ¥è¯¢çš„ä¾‹å­ï¼Œç›®çš„æ˜¯å¯»æ‰¾é‚£äº›å·²ç»é¢„è®¢äº†æ‰€æœ‰èˆ¹åªçš„æ°´æ‰‹ã€‚é€šè¿‡è¿™ç§æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°é‚£äº›æ²¡æœ‰æ¼æ‰ä»»ä½•ä¸€è‰˜èˆ¹åªé¢„è®¢çš„æ°´æ‰‹ã€‚ ","date":"2024-07-22","objectID":"/databasel2/:6:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"ç†è§£æ­¥éª¤ï¼š å…³ç³»é™¤æ³•çš„å®šä¹‰ï¼š å…³ç³»é™¤æ³•æ˜¯ä¸€ç§å¤æ‚çš„SQLæŸ¥è¯¢æ“ä½œï¼Œç”¨äºæ‰¾åˆ°é‚£äº›åœ¨ä¸€ä¸ªé›†åˆä¸­å¯¹æ‰€æœ‰å…ƒç´ éƒ½æ»¡è¶³æŸä¸ªæ¡ä»¶çš„è®°å½•ã€‚ åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æƒ³æ‰¾åˆ°é‚£äº›é¢„è®¢äº†æ‰€æœ‰èˆ¹åªçš„æ°´æ‰‹ã€‚ æŸ¥è¯¢çš„é€»è¾‘ï¼š å¤–å±‚æŸ¥è¯¢ï¼šSELECT S.sname FROM Sailors Sï¼šé€‰æ‹©æ‰€æœ‰æ°´æ‰‹çš„åå­—ã€‚ NOT EXISTSå­æŸ¥è¯¢ï¼šè¿™ä¸ªéƒ¨åˆ†æ˜¯å…³é”®ï¼š WHERE NOT EXISTS ( SELECT B.bid FROM Boats B WHERE NOT EXISTS ( SELECT R.bid FROM Reserves R WHERE R.bid = B.bid AND R.sid = S.sid ) ) é€»è¾‘è§£é‡Šï¼š é¦–å…ˆï¼ŒæŸ¥è¯¢äº†æ‰€æœ‰çš„èˆ¹åª (Boats B)ã€‚ å¯¹äºæ¯è‰˜èˆ¹ï¼Œåªè¦å­˜åœ¨ä¸€è‰˜èˆ¹ (B.bid)ï¼Œå½“å‰æ°´æ‰‹ (S.sid) æ²¡æœ‰é¢„è®¢ (R.sid = S.sid AND R.bid = B.bid)ï¼Œé‚£ä¹ˆè¿™ä¸ªæ°´æ‰‹å°±ä¼šè¢«æ’é™¤ã€‚ å¦‚æœå¯¹äºæŸä¸ªæ°´æ‰‹ï¼Œä¸å­˜åœ¨è¿™æ ·ä¸€è‰˜ä»–æ²¡æœ‰é¢„è®¢çš„èˆ¹ï¼ˆå³NOT EXISTSçš„ç»“æœä¸ºçœŸï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ªæ°´æ‰‹å°±æ»¡è¶³é¢„è®¢äº†æ‰€æœ‰èˆ¹çš„æ¡ä»¶ã€‚ ç»“è®ºï¼š æœ€ç»ˆçš„æŸ¥è¯¢å°†ä¼šè¿”å›é‚£äº›åå­—æ˜¯æ°´æ‰‹å¹¶ä¸”é¢„è®¢äº†æ¯ä¸€è‰˜èˆ¹çš„äººã€‚ ARGMAX find the sailor with the highest rating select * from sailors as s where s.rating \u003e= ALL (SELECT sailors.rating FROM sailors) select * from sailors as s where s.rating = (SELECT MAX(sailors.rating) FROM sailors) æ³¨æ„ä¸‹é¢è¿™ä¸ª â˜• select * from sailors as s order by s.rating desc limit 1 ","date":"2024-07-22","objectID":"/databasel2/:6:1","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"ARGMAX GROUP BY æç¤ºï¼šå€ŸåŠ©è§†å›¾ç­›é€‰ Creating Views æœ‰æ—¶å€™ä¸éœ€è¦å»ºç«‹æ˜¾å¼çš„views select b, c from boats as b, (select b.bid, count(*) from reserves as r, boats as b where r.bid = b.bid and b.color = 'blue' group by b.bid) as Reds(bid, c) where b.bid = Reds.bid æœ‰æ—¶å€™CTEï¼ˆcommon table expressionï¼‰è¡¨ç¤ºæ³•æ›´åŠ ç®€æ´ æ³¨æ„WITHä»å¥åé¢èƒ½å»ºç«‹å¤šä¸ªè§†å›¾ï¼Œè®°å¾—åŠ ä¸Šé€—å·ï¼ NULL ","date":"2024-07-22","objectID":"/databasel2/:7:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"NULLçš„æ¯”è¾ƒ ä¸è¦ä½¿ç”¨=ï¼Œè€Œæ˜¯ä½¿ç”¨IS NULLæˆ–IS NOT NULL IS NULL : å·¦è¾¹æ˜¯NULL IS NOT NULL : å·¦è¾¹ä¸æ˜¯NULL ","date":"2024-07-22","objectID":"/databasel2/:8:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"NULL in boolean expressions é¦–å…ˆï¼Œ å½¢å¦‚WHERE NULLæ˜¯ä¸åˆæ³•çš„ï¼ ä¸‰å€¼é€»è¾‘è¡¨å¦‚ä¸‹ ","date":"2024-07-22","objectID":"/databasel2/:9:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"NULL in aggregation functions æ¥ä¸‹æ¥å°±æ˜¯implement! ğŸ˜€ ","date":"2024-07-22","objectID":"/databasel2/:10:0","tags":null,"title":"CS186-L2: SQLâ…¡","uri":"/databasel2/"},{"categories":["CS186"],"content":"å¤§çº²è¿›ç¨‹ï¼š sheet SQL I ","date":"2024-07-22","objectID":"/databasel1/:0:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["CS186"],"content":"pros and cons ","date":"2024-07-22","objectID":"/databasel1/:1:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["CS186"],"content":"relational Terminology and concepts database: set of name relations relation(table): schema: descriptions â€œmetadataâ€ fixed, unique attribute names, atomic types instance: set of data ç¬¦åˆdescription often changed, can duplicate multiset of tuples or â€œrowsâ€ attribute (column,field) tuple (row,record),æ€€ç–‘ä¸€äº›pythonæ¦‚å¿µä¹Ÿæ¥è‡ªäºæ­¤ ","date":"2024-07-22","objectID":"/databasel1/:2:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["CS186"],"content":"DDL (Data Definition Language) CREATE TABLE myTable ( ID INTEGER, myName CHAR(50), Age INTEGER, Salary FLOAT, PRIMARY KEY (ID, myName), FOREIGN KEY (ID) REFERENCES myOtherTable(ID), FOREIGN KEY (myName) REFERENCES myOtherTable(myName) ); SELECT [DISTINCT] \u003ccolumn expression list\u003e FROM \u003csingle_table\u003e [WHERE \u003cpredicate\u003e] ORDER BY Lexicographic order by default å­—å…¸åº LIMIT Aggregation functions AVG: average COUNT: count the number of rows MAX: maximum value MIN: minimum value SUM: sum of values SELECT AVG(Salary) FROM myTable; GROUP BY HAVING SELECT AVG(Salary) FROM myTable GROUP BY Age HAVING AVG(Salary) \u003e 50000; ä¸åŒçš„DISTINCTä½ç½®æ•ˆæœä¸åŒ, å…¶ä¸­ç¬¬äºŒä¸ªå‹æ ¹æ²¡ç”¨ ","date":"2024-07-22","objectID":"/databasel1/:3:0","tags":null,"title":"CS186-L1: Introduction + SQL I","uri":"/databasel1/"},{"categories":["DATA100"],"content":"mapreduce MapReduceæ˜¯ä¸€ç§ç¼–ç¨‹æ¨¡å‹,ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†çš„å¹¶è¡Œè¿ç®—[1][2][3]ã€‚å®ƒå°†å¤æ‚çš„å¹¶è¡Œè®¡ç®—è¿‡ç¨‹æŠ½è±¡ä¸ºä¸¤ä¸ªå‡½æ•°:Mapå’ŒReduce[4]ã€‚ Mapå‡½æ•°å°†è¾“å…¥æ•°æ®é›†æ‹†åˆ†æˆç‹¬ç«‹çš„å—,å¹¶å¯¹æ¯ä¸ªå—åº”ç”¨æ˜ å°„æ“ä½œ,ç”Ÿæˆä¸€ç»„ä¸­é—´é”®å€¼å¯¹[1][2][3]ã€‚Reduceå‡½æ•°ä¼šå¯¹æ‰€æœ‰Mapçš„è¾“å‡ºè¿›è¡Œåˆå¹¶æ“ä½œ,ç”Ÿæˆæœ€ç»ˆç»“æœ[1][2][3]ã€‚ MapReduceçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬[4][5]: æ˜“äºç¼–ç¨‹:ç¨‹åºå‘˜åªéœ€æè¿°åšä»€ä¹ˆ,å…·ä½“æ€ä¹ˆåšç”±ç³»ç»Ÿçš„æ‰§è¡Œæ¡†æ¶å¤„ç† è‰¯å¥½çš„æ‰©å±•æ€§:å¯é€šè¿‡æ·»åŠ èŠ‚ç‚¹æ‰©å±•é›†ç¾¤èƒ½åŠ› é«˜å®¹é”™æ€§:é€šè¿‡è®¡ç®—è¿ç§»æˆ–æ•°æ®è¿ç§»ç­‰ç­–ç•¥æé«˜é›†ç¾¤çš„å¯ç”¨æ€§ä¸å®¹é”™æ€§ MapReduceé‡‡ç”¨\"åˆ†è€Œæ²»ä¹‹\"ç­–ç•¥,å°†å¤§è§„æ¨¡æ•°æ®é›†åˆ‡åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„åˆ†ç‰‡,è¿™äº›åˆ†ç‰‡å¯ä»¥è¢«å¤šä¸ªMapä»»åŠ¡å¹¶è¡Œå¤„ç†[4]ã€‚å®ƒè®¾è®¡çš„ä¸€ä¸ªç†å¿µæ˜¯\"è®¡ç®—å‘æ•°æ®é æ‹¢\",ç§»åŠ¨æ•°æ®éœ€è¦å¤§é‡çš„ç½‘ç»œä¼ è¾“å¼€é”€[4]ã€‚ æ€»ä¹‹,MapReduceæ˜¯ä¸€ç§ç®€å•ã€å¯æ‰©å±•çš„å¹¶è¡Œè®¡ç®—æ¨¡å‹,é€šè¿‡æŠ½è±¡Mapå’ŒReduceå‡½æ•°,ä½¿å¾—ç¨‹åºå‘˜å¯ä»¥è½»æ¾ç¼–å†™å¤§è§„æ¨¡å¹¶è¡Œåº”ç”¨ç¨‹åº,è€Œæ— éœ€å…³æ³¨åº•å±‚çš„åˆ†å¸ƒå¼ç»†èŠ‚[1][2][3][4][5]ã€‚ Citations: [1] https://baike.baidu.com/item/MapReduce/133425 [2] https://zh.wikipedia.org/zh-hans/MapReduce [3] https://www.ibm.com/cn-zh/topics/mapreduce [4] https://cshihong.github.io/2018/05/11/MapReduce%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/ [5] https://cloud.tencent.com/developer/article/1778549 apache spark lazy strategy: å»¶è¿Ÿè®¡ç®—ç­–ç•¥,Sparké»˜è®¤é‡‡ç”¨è¿™ç§ç­–ç•¥,å³åªæœ‰å½“æ•°æ®çœŸæ­£è¢«ä½¿ç”¨æ—¶æ‰ä¼šè®¡ç®—ã€‚ ç¼–è¯‘ä¼˜åŒ–è¯­å¥æ‰§è¡Œé¡ºåºï¼ Conclusion å·¥å…·é“¾ What is next? æœ‰ç”¨çš„data scienceé“¾æ¥ http://kaggle.com https://github.com/awesomedata/awesome-public-datasets http://toolbox.google.com/datasetsearch https://towardsdatascience.com https://www.reddit.com/r/dataisbeautiful/ https://fivethirtyeight.com ","date":"2024-07-19","objectID":"/datal26/:0:0","tags":null,"title":"DATA100-L26: Parallel Data Analytics; Conclusion","uri":"/datal26/"},{"categories":["DATA100"],"content":"introduction to clustering no label at all ğŸ˜¢ K-means clustering ç®—æ³•åŠ¨ç”»æ¼”ç¤º K-Means vs KNN minimizing inertia convex?? æŸå¤±å‡½æ•°ä¸ä¸€å®šå‡¸ï¼Œæ¢¯åº¦ä¸‹é™éš¾é¡¶ how to see which one is better â“ ä½†æ˜¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£éå¸¸å›°éš¾ agglomerative clustering æ¼”ç¤ºè§ä¸Šé¢é“¾æ¥ä»¥åŠlec codeï¼ å’ŒCS61Bçš„minimum spanning treeç±»ä¼¼ï¼Œæ¯æ¬¡åˆå¹¶ä¸¤ä¸ªæœ€è¿‘çš„ç‚¹ï¼Œç›´åˆ°ç»ˆæ­¢æ¡ä»¶ outlier æœ‰æ—¶å¿½ç•¥å¤„ç†æˆ–è€…è‡ªæˆä¸€ç±» picking K Smaxï¼Ÿ can s be negative? ","date":"2024-07-19","objectID":"/datal24/:0:0","tags":null,"title":"DATA100-L24: Clustering","uri":"/datal24/"},{"categories":["DATA100"],"content":"summary ","date":"2024-07-19","objectID":"/datal24/:1:0","tags":null,"title":"DATA100-L24: Clustering","uri":"/datal24/"},{"categories":["DATA100"],"content":"impetus for regulation why â€œyouâ€ should care Because you are gonna to be a data scientist and product owner! regulations: Privacy laws GDPR (General Data Protection Regulation) CCPA (California Consumer Privacy Act) Cyber Security Law in China deletion can be more difficult than you think ğŸ˜ ä¼ è¾“ä¹Ÿè¦ç›‘ç®¡ fully take advantage of the â€œregulationsâ€ take care of gray areas ğŸ¤” work with dear NGO and GO other regulations/ regulatory bodies ","date":"2024-07-19","objectID":"/datal25/:0:0","tags":null,"title":"DATA100-L25: Data Regulations","uri":"/datal25/"},{"categories":["DATA100"],"content":"Multiclass Classification å¤šåˆ†ç±»é—®é¢˜ ä½†æ˜¯æ²¡æœ‰softmax ğŸ˜¢ Decision Trees (conceptually) Decision Tree Demo ","date":"2024-07-19","objectID":"/datal23/:0:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Creating Decision Trees in sklearn å¯è§†åŒ–ä»£ç è§lecture code ","date":"2024-07-19","objectID":"/datal23/:1:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Evaluating Tree Accuracy ","date":"2024-07-19","objectID":"/datal23/:2:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Overfit Decision Tree Example tree is too complex to generalize well to new data too tall and narrow æœ‰ç”¨çš„ç‰¹å¾è¶Šå¤šï¼Œæ ‘çš„ç»“æ„å¯èƒ½æ¯”è¾ƒç®€å•ğŸ¤” The Decision Tree Generation Algorithm ","date":"2024-07-19","objectID":"/datal23/:3:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Intuitively Evaluating Split Quality åˆ†å‰²æ€ä¹ˆæ ·â€œæ›´æ˜æ˜¾â€ï¼Ÿ ","date":"2024-07-19","objectID":"/datal23/:4:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Entropy æ²¿ç€æ ‘å‘ä¸‹ï¼Œä¿¡æ¯ç†µè¶Šå°ï¼Ÿå¯èƒ½å˜å¤§ï¼ ","date":"2024-07-19","objectID":"/datal23/:5:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Generating Trees Using Entropy Weighted entropy can decrease! Traditional decision tree generation algorithm: All of the data starts in the root node. Repeat until every node is either pure or unsplittable: Pick the best feature x and split value Î² such that the Î”WS is maximized, e.g. x = petal_width, Î² = 0.8 has Î”WS = 0.91. Split data into two nodes, one where x \u003c Î², and one where x â‰¥ Î². Notes: A node that has only one samples from one class is called a â€œpureâ€ node. A node that has overlapping data points from different classes and thus that cannot be split is called â€œunsplittableâ€. Avoiding Overfitting æ­£åˆ™åŒ–åœ¨è¿™é‡Œä¸èµ·ä½œç”¨ï¼Ÿ ","date":"2024-07-19","objectID":"/datal23/:6:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Heuristically Restricting Decision Tree Complexityï¼ˆå¯å‘å¼ç®—æ³•ï¼‰ Approach2: allow full growth of the tree, but Prune the tree. ","date":"2024-07-19","objectID":"/datal23/:7:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Embracing Decision Tree Complexity with Random Forests ğŸªµ ğŸ˜‹ Bagging: Short for Bootstrap AGGregatING. Generate bootstrap resamples of training data. Fit one model for each resample. Final model = average predictions of each small model. Invented by Leo Breiman in 1994 (Berkeley Statistics!). ","date":"2024-07-19","objectID":"/datal23/:8:0","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"å¯å‘å¼çš„ç‰¹ç‚¹ These ideas are generally â€œheuristicâ€ Not provably best or mathematically optimal. Instead, they are just ideas that somebody thought sounded good, implemented, then found to work in practice acceptably well. Summary and Context Decision trees provide an alternate non-linear framework for classification and regression. The underlying principle is fundamentally different. Decision boundaries can be more complex. Danger of overfitting is high. Small decision trees are very easy to interpret. Doing regression with a tree is straightforward. See statquest video. Keeping complexity under control is not nearly as mathematically elegant and relies on heuristic rules. Hard constraints. Pruning rules. Random forests: Generate multiple trees using bootstrap. Have the trees vote on the outcome. ","date":"2024-07-19","objectID":"/datal23/:8:1","tags":null,"title":"DATA100-L23: Decision Trees","uri":"/datal23/"},{"categories":["DATA100"],"content":"Regression vs. Classification å…¨æ”»ç•¥ğŸ˜‹ intuition: the coin flip é‡æ–°å®šä¹‰æ¦‚ç‡ï¼Œåªéœ€è¦æ»¡è¶³ä¸€äº›æ€§è´¨å³å¯ã€‚å‚è€ƒ æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ deriving the logistic regression model knnä¸€ç¥ è¿™è¯´æ˜å¯ä»¥ä»æŸäº›å˜åŒ–è½¬æ¢ä¸ºçº¿æ€§æ€§è´¨ è€ƒè™‘ probability $p$ è€ƒè™‘ odds $\\frac{p}{1-p}$ è€ƒè™‘ log odds å¹¿ä¹‰çº¿æ€§ç”±æ­¤å¯è§ ","date":"2024-07-19","objectID":"/datal21/:0:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"Graph of Averages ","date":"2024-07-19","objectID":"/datal21/:1:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"the sigmoid function $$ \\sigma(t)=\\frac{1}{1+e^{-t}} $$ the logistic regression model ","date":"2024-07-19","objectID":"/datal21/:2:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"comparison to linear regression parameter estimation ","date":"2024-07-19","objectID":"/datal21/:3:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"pitfalls of squared loss non-convex bounded, MSE âˆˆ[0ï¼Œ1] conceptually questionable, not matching the â€œProbability and 0/1 labelsâ€ ","date":"2024-07-19","objectID":"/datal21/:4:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"cross-entropy loss $$ -\\frac{1}{N}\\sum_{i=1}^N[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)] $$ Loss function should penalize well! ","date":"2024-07-19","objectID":"/datal21/:5:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"maximum likelihood estimation see extra in L22! ","date":"2024-07-19","objectID":"/datal21/:6:0","tags":null,"title":"DATA100-L21: Classification and Logistic Regression I","uri":"/datal21/"},{"categories":["DATA100"],"content":"logistic regression model continued ","date":"2024-07-19","objectID":"/datal22/:0:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"sklearn demo go to see lec code! ","date":"2024-07-19","objectID":"/datal22/:1:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"MLE: high-level, detailed (recorded) linear separability and regularization çº¿æ€§å¯åˆ†æ€§ï¼šå¦‚æœå­˜åœ¨ä¸€ä¸ª è¶…å¹³é¢ï¼ˆhyperplaneï¼‰ å¯ä»¥å°†æ•°æ®é›†åˆ†å‰²æˆä¸¤éƒ¨åˆ†ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯çº¿æ€§å¯åˆ†çš„ã€‚ è¶…å¹³é¢çš„ç»´åº¦å’Œæ•°æ®é›†çš„ç»´åº¦ç›¸åŒ $$ C $$ æ³¨æ„å¯¹â€œpushâ€çš„ç†è§£ï¼ å¦ä¸€ç§ç†è§£æ­£åˆ™åŒ–çš„è§’åº¦ è¿™é‡Œæ˜¯é¿å…losså‡ºç°æ— é™å¤§çš„æƒ…å†µï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼Ÿï¼‰ï¼Œé¿å…å‡ºç°ä½¿å‰é¢æƒ…å†µå‘ç”Ÿçš„å‚æ•°ï¼ˆinfinite thetaï¼‰å‡ºç°ï¼Œæ‰€ä»¥åœ¨lossé‡Œé¢é¢„å…ˆåŠ å…¥æ­£åˆ™åŒ–é¡¹ã€‚ performance metrics ","date":"2024-07-19","objectID":"/datal22/:2:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"accuracy # using sklearn model.score(X_test, y_test) ","date":"2024-07-19","objectID":"/datal22/:3:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"imbalanced data, precision, recall Acc is not a good metric for imbalanced data, use precision and recall instead!!! $$ acc= \\frac{TP+TN}{n}\\ precision(ç²¾ç¡®ç‡)=\\frac{TP}{TP+FP}\\ recall(å¬å›ç‡)=\\frac{TP}{TP+FN} $$ adjusting the classification threshold(é˜ˆå€¼ç•Œé™) ","date":"2024-07-19","objectID":"/datal22/:4:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"a case study å˜ç•Œé™å¯èƒ½æ˜¯å› ä¸ºimbalanced dataå¯¼è‡´çš„ ","date":"2024-07-19","objectID":"/datal22/:5:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"ROC curves and AUC æ€ä¹ˆé€‰æ‹©é˜ˆå€¼ï¼Ÿ [extra] detailed MLE, gradient descent, PR curves ","date":"2024-07-19","objectID":"/datal22/:6:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"Why cross-entropy? KLæ•£åº¦: https://www.textbook.ds100.org/ch/24/classification_cost_justification.html?highlight=divergence MLE ä»¥ä¸‹è®¨è®ºMLEï¼ŒäºŒåˆ†ç±»çš„è¯ä»¥ ä¼¯åŠªåˆ© ä¸¾ä¾‹ ","date":"2024-07-19","objectID":"/datal22/:7:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"PR curves false positiveåœ¨Tå˜å¤§çš„æ—¶å€™å¢åŠ å¾—æ›´å¿«ï¼Œæ‰€ä»¥å¯èƒ½slightly decrease è€ƒè™‘PR ","date":"2024-07-19","objectID":"/datal22/:8:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"æ’æ›² ä¼¼ä¹è‡ªç„¶ç§‘å­¦æ‰€æœ‰å­¦ç§‘éƒ½å¯ä»¥è¢«è§£æ„ä¸º â€œè§‚æµ‹åˆ°çš„çŸ¥è¯†ç‚¹ï¼ˆcontextï¼‰â€ + ä¿¡æ¯æ•°ç†åŒ–ï¼ˆmath \u0026 computer scienceï¼‰ ï¼Ÿ æ¢è¨€ä¹‹åªéœ€è¦ä¸€æ–¹é¢ä¸æ–­æ‰©å……æ•°æ®/çŸ¥è¯†ç‚¹ï¼Œå¦ä¸€æ–¹é¢æå‡ºé«˜æ˜çš„ä¿¡æ¯æ•°ç†åŒ–åˆ†ææ–¹æ³•ï¼Œå°±å¯ä»¥æ¨åŠ¨ç§‘å­¦çš„è¿›æ­¥ï¼ŸğŸ¤” ğŸ¤” â“ https://docs.google.com/presentation/d/1YsxPERhul760_0TrLhawljbWWqDbtIp5tUm05irfkmw/edit#slide=id.g12444cd4007_0_537 ","date":"2024-07-19","objectID":"/datal22/:9:0","tags":null,"title":"DATA100-L22: Logistic Regression II","uri":"/datal22/"},{"categories":["DATA100"],"content":"SQL II ","date":"2024-07-19","objectID":"/datal19/:0:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"sql and pandas how to connect sql to python import pandas as pd import sqlalchmey engine = sqlalchemy.create_engine('sqlite:///mydatabase.db') connection = engine.connect() pd.read_sql(\"\"\" SELECT * FROM mytable GROUP BY column1, column2 \"\"\", connection) ","date":"2024-07-19","objectID":"/datal19/:1:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"LIKE and CAST LIKE: search for a pattern in a column SELECT * FROM mytable WHERE column1 LIKE '%value%' CAST: convert data type ","date":"2024-07-19","objectID":"/datal19/:2:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"SQL Joins ","date":"2024-07-19","objectID":"/datal19/:3:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"Cross Join SELECT * FROM table1 CROSS JOIN table2 ","date":"2024-07-19","objectID":"/datal19/:3:1","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"Inner Join SELECT * FROM table1 INNER JOIN table2 ON table1.column1 = table2.column1 SELECT * FROM t1, t2 WHERE t1.id = t2.id ","date":"2024-07-19","objectID":"/datal19/:3:2","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"left/right/full outer join ","date":"2024-07-19","objectID":"/datal19/:3:3","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"other join conditions PCA ","date":"2024-07-19","objectID":"/datal19/:3:4","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"dimensionality and rank of data dimensionality \u003c===\u003e rank ","date":"2024-07-19","objectID":"/datal19/:4:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"two interpretations of matrix multiplication matrices as linear operations ~coordinate transformation ","date":"2024-07-19","objectID":"/datal19/:5:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"matrix decomposition and rank å°½å¯èƒ½çš„ä¿ç•™ä¸»æˆåˆ†ï¼Œè€Œèˆå¼ƒæ— å…³çš„æˆåˆ† ===\u003e rank ","date":"2024-07-19","objectID":"/datal19/:6:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"manual matrix decomposition exercise lin algæœ‰çš„æ—¶å€™ä¸èƒ½åˆ†æå‡ºçœŸæ­£çš„â€œrank\",éœ€è¦ ","date":"2024-07-19","objectID":"/datal19/:7:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"singular value decomposition (high level look) ","date":"2024-07-19","objectID":"/datal19/:8:0","tags":null,"title":"DATA100-L19: SQL II and PCA I","uri":"/datal19/"},{"categories":["DATA100"],"content":"recap and Goals ","date":"2024-07-19","objectID":"/datal20/:0:0","tags":null,"title":"DATA100-L20: PCA II","uri":"/datal20/"},{"categories":["DATA100"],"content":"approximate factorization $W\\ L\\rightarrow (W+L)/ 2$ rank ä¸‹é™ä½¿å¾—ä¿¡æ¯ç¼ºå¤±äº† æ‰€ä»¥ $M_{100 \\times 4} = N_{100 \\times P} \\times Q_{P \\times 4}$ Pçš„å€¼å°½é‡ä¸è¦å°äºåŸæ¥çš„\"ç§©\" singular value decomposition (SVD) low rank approximation no bad! seem good! SVD theory éªŒè¯orthonormal set V@V.T = I å½“ç›¸ä¹˜çš„æ—¶å€™æœ¬è´¨ä¸Šæ˜¯æ—‹è½¬ï¼Œä¸ä¼šæ‹‰ä¼¸ Principal Components é›¶ä¸­å¿ƒåŒ–å†æ¥çœ‹PCA Principal Components and Variance PCA example Why is useful? ğŸ¤” ","date":"2024-07-19","objectID":"/datal20/:1:0","tags":null,"title":"DATA100-L20: PCA II","uri":"/datal20/"},{"categories":["DATA100"],"content":"sample statistics (from last time) å‚è€ƒ æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ prediction vs. inference ","date":"2024-07-19","objectID":"/datal17/:0:0","tags":null,"title":"DATA100-L17: Estimators, Bias, and Variance","uri":"/datal17/"},{"categories":["DATA100"],"content":"modeling: assumptions of randomness the bias-variance tradeoff $$ model\\ risk = observation\\ variance + (model\\ bias)^2+model\\ variance $$ $$ \\mathbb{E}[(Y-\\hat{Y}(x))^2] = \\sigma^2+(\\mathbb{E}[\\hat{Y}(x)]-g(x))^2+Var(\\hat{Y}(x)) $$ interpreting slopes slope == 0? å‡è®¾æ£€éªŒè¯æ˜æ˜¯å¦æ— å…³ [Extra]review of the Bootstrap [Extra]derivation of Bias-Variance decomposition https://docs.google.com/presentation/d/1gzgxGO_nbCDajYs7qIpjzjQfJqKadliBOat7Es10Ll8/edit#slide=id.g11df3da7bd7_0_467 ","date":"2024-07-19","objectID":"/datal17/:1:0","tags":null,"title":"DATA100-L17: Estimators, Bias, and Variance","uri":"/datal17/"},{"categories":["DATA100"],"content":"why databases structured query language (SQL) ğŸ˜‹ DBMS: database management system sql example type INT for integer REAL for decimal TEXT for string BLOB for ARBITRARY data DATETIME for date and time different implementations of sql support different types sql table use singular, CamelCase for SQL tables! basic sql queries é€šé… SELECT * FROM table_name; é€‰å®šå­é›† SELECT column1, column2 FROM table_name; AS rename columns SELECT cute AS cuteness, smart AS intelligence FROM table_name; WHERE filter rows SELECT * FROM table_name WHERE column1 = 'value1' AND column2 = 'value2'; ORDER BY sort rows DESC for descending order, ASC for ascending order SELECT * FROM table_name ORDER BY column1 DESC; LIMIT restrict number of rows returned SELECT * FROM table_name LIMIT 10; OFFSET 5; basic GROUP BY Operations SELECT column1 FROM table_name GROUP BY column1; SELECT column1, SUM(column2), MAX(column3), MIN(column4) FROM table_name GROUP BY column1; SUM, AVG, COUNT, MAX, MIN, etc. SELECT column1, COUNT(*) FROM table_name GROUP BY column1; COUNT(*) counts the number of rows in each group.(even null values) SELECT column1, column2 FROM table_name GROUP BY column1, column2; HAVING COUNT(*) \u003e 5; Generate a group for each unique combination of column1 and column2 values, but only include groups with more than 5 rows. To filter groups, HAVING, to filter rows, WHERE (before HAVING). trickier GROUP BY Operations DISTINCT see in lecture 18 SELECT type, AVG(DISTINCT cost) FROM Dish GROUP BY type; SELECT DISTINCT type, cost FROM Dish WHERE cost \u003c 9; ","date":"2024-07-19","objectID":"/datal18/:0:0","tags":null,"title":"DATA100-L18: SQL I","uri":"/datal18/"},{"categories":["DATA100"],"content":"Cross Validation ","date":"2024-07-19","objectID":"/datal15/:0:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"the holdout method from sklearn.utils import shuffle training_set, dev_set = np.split(shuffle(data), [int(.8*len(data))]) æ¯”è¾ƒvalidation errorå’Œtraining errorï¼Œé€‰æ‹©æœ€ä¼˜çš„æ¨¡å‹ã€‚ ","date":"2024-07-19","objectID":"/datal15/:1:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"K-fold cross validation K=1 is equivalent to holdout method. ","date":"2024-07-19","objectID":"/datal15/:2:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"Test sets provide an unbiased estimate of the modelâ€™s performance on new, unseen data. Regularization ","date":"2024-07-19","objectID":"/datal15/:3:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"L2 regularization (Ridge) the small the ball, the simpler the model æ‹‰æ ¼æœ—æ—¥æ€æƒ³ï¼Œ$\\alpha$ è¶Šå¤§ï¼Œçº¦æŸè¶Šå¼ºï¼Œæ¨¡å‹è¶Šç®€å•ã€‚ å²­å›å½’ ","date":"2024-07-19","objectID":"/datal15/:4:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"scaling data for regularization æ ‡å‡†åŒ–æ•°æ®ï¼Œbe on the same scale ","date":"2024-07-19","objectID":"/datal15/:5:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"L1 regularization (Lasso) ","date":"2024-07-19","objectID":"/datal15/:6:0","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"summary ","date":"2024-07-19","objectID":"/datal15/:6:1","tags":null,"title":"DATA100-L15: Cross Validation, Regularization","uri":"/datal15/"},{"categories":["DATA100"],"content":"éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ å‚è€ƒ æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ è¯¾ç¨‹å³å¯ æœŸæœ›ä¸æ–¹å·® $$ \\mathbb{E}[X] å’Œ Var(X) $$ å‚è€ƒ æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ è¯¾ç¨‹å³å¯ éšæœºå˜é‡çš„å’Œ ","date":"2024-07-19","objectID":"/datal16/:0:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"equality vs Identically distributed vs IID æœ‰æ„æ€çš„åˆ†æ ğŸ˜ ","date":"2024-07-19","objectID":"/datal16/:1:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"æœŸæœ›å’Œæ–¹å·®çš„æ€§è´¨ ","date":"2024-07-19","objectID":"/datal16/:2:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"åæ–¹å·®ä¸ç›¸å…³ç³»æ•° ä¼¯åŠªåˆ©ï¼ˆ0-1ï¼‰å’ŒBinomialåˆ†å¸ƒ Sample Statistics ","date":"2024-07-19","objectID":"/datal16/:3:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"sample mean ","date":"2024-07-19","objectID":"/datal16/:4:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":"central limit theorem ","date":"2024-07-19","objectID":"/datal16/:5:0","tags":null,"title":"DATA100-L16: Probability I: Random Variables","uri":"/datal16/"},{"categories":["DATA100"],"content":" $\\downarrow{shuffle}$ SGD: Stochastic Gradient Descent(but size == 1) ","date":"2024-07-19","objectID":"/datal13/:0:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"convexity (å‡¹å‡¸æ€§) feature engineering åœ¨äºæ€ä¹ˆä½¿ç”¨transforming ","date":"2024-07-19","objectID":"/datal13/:1:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"feature function see website code ","date":"2024-07-19","objectID":"/datal13/:2:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"non-numeric features one-hot encoding ","date":"2024-07-19","objectID":"/datal13/:2:1","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"concat ","date":"2024-07-19","objectID":"/datal13/:2:2","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"high order polynomials ","date":"2024-07-19","objectID":"/datal13/:3:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":"detect overfitting collect more data more see next lecture ","date":"2024-07-19","objectID":"/datal13/:4:0","tags":null,"title":"DATA100-L13: Gradient Descent, Feature Engineering","uri":"/datal13/"},{"categories":["DATA100"],"content":" Accuracy is a necessary, but not sufficient, condition for fair system. Fairness and transparency are context-dependent. Learn to work with contexts, and consider how your data analysis will reshape them. Keep in mind the power, and limits, of data analysis. ","date":"2024-07-19","objectID":"/datal14/:0:0","tags":null,"title":"DATA100-L14: Case Study (HCE): Fairness in Housing Appraisal","uri":"/datal14/"},{"categories":["DATA100"],"content":"å¼€å§‹è°ƒåŒ…ï¼ğŸ˜ from sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(df[[\"total_bill\"]], df[\"tip\"]) df[\"predicted_tip\"] = model.predict(df[[\"total_bill\"]]) æ‰€æœ‰çš„æœºå™¨å­¦ä¹ ä¼¼ä¹éƒ½åœ¨æœ€å°åŒ–loss functionï¼Œè€Œæ¢¯åº¦ä¸‹é™å°±æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œå®ƒé€šè¿‡è¿­ä»£çš„æ–¹å¼ä¸æ–­æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä½¿å¾—loss functionçš„å€¼ä¸æ–­å‡å°ã€‚ è¯¦æƒ…è§NNDLæ ç›® ","date":"2024-07-19","objectID":"/datal12/:0:0","tags":null,"title":"DATA100-L12: Gradient Descent, sklearn","uri":"/datal12/"},{"categories":["DATA100"],"content":"linear in theta linear combination of parameters $\\theta$ ","date":"2024-07-19","objectID":"/datal11/:1:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"define multiple linear regression OLS problem formulation ordinary least squares (OLS) ç”¨çº¿æ€§ä»£æ•°é‡å†™ä¹‹ $$ \\mathbb{\\hat{Y}} = \\mathbb{X}\\theta $$ ","date":"2024-07-19","objectID":"/datal11/:2:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"multiple linear regression model ","date":"2024-07-19","objectID":"/datal11/:3:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"MSE $$ R(\\theta) = \\frac{1}{n}||\\mathbb{Y}-\\hat{\\mathbb{Y}}||_2^2 $$ geometric derivation ","date":"2024-07-19","objectID":"/datal11/:4:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"lin alg review: orthogonality, span $$ span(\\mathbb{A})æ˜¯ä¸€ä¸ªç”±åˆ—å‘é‡ç»„æˆçš„space $$ æ­£äº¤ ","date":"2024-07-19","objectID":"/datal11/:5:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"least squares estimate proof performance: residuals, multiple R-squared lec11.ipynb $$ R^2âˆˆ[0,1] $$ è¶Šå¤§æ‹Ÿåˆæ•ˆæœè¶Šå¥½ OLS properties ","date":"2024-07-19","objectID":"/datal11/:6:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"residuals ","date":"2024-07-19","objectID":"/datal11/:7:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"the bias/intercept term ","date":"2024-07-19","objectID":"/datal11/:8:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"existence of a unique solution ","date":"2024-07-19","objectID":"/datal11/:9:0","tags":null,"title":"DATA100-L11: Ordinary Least Squares","uri":"/datal11/"},{"categories":["DATA100"],"content":"constant model + MSE å¾®ç§¯åˆ†æ˜¯æ±‚æœ€ä¼˜åŒ–çš„ä¸€ç§æ–¹æ³• ä¸¤ç§è®°æ³• constant model + MAE ç»å¯¹å€¼æ±‚å¯¼æ–°è§†è§’ $$ \\sum_{\\theta \u003cy_i} 1=\\sum_{\\theta \u003ey_i} 1 $$ æ˜¯è®¡æ•°ï¼==\u003eä¸­ä½æ•° lossçš„æ•æ„Ÿæ€§é—®é¢˜ revisiting SLR evaluation ç”»å›¾before modelingï¼ï¼ï¼ transformations to fit linear model ç»éªŒä¹‹è°ˆ introducing notation for multiple linear regression ","date":"2024-07-18","objectID":"/datal10/:0:0","tags":null,"title":"DATA100-L10: Constant Model, Loss, and Transformations","uri":"/datal10/"},{"categories":["DATA100"],"content":"regression line, correlation é«˜ä¸­æœ€å°äºŒä¹˜æ³•(least squares regression)ï¼Œçº¿æ€§å›å½’ model $â€œall\\ models\\ are\\ wrong,\\ but\\ some\\ are\\ usefulâ€$ trade between interpretability and accuracy ç‰©ç†orç»Ÿè®¡æ¨¡å‹ the modeling process: definitions SLR: Simple Linear Regression æ˜ç¡®inputå’Œparameterçš„åŒºåˆ« æœ‰äº›ç»Ÿè®¡æ¨¡å‹å¯ä»¥æ²¡æœ‰å‚æ•°ï¼ loss functions metric for good or bad minimizing average loss (Empirical Risk æœŸæœ›é£é™©ï¼Ÿ) æœ€ä¼˜åŒ–ï¼ interpreting SLR: slope, Anscombeâ€™s quartet è§£é‡Šå‚æ•°æ„ä¹‰ é¢„æµ‹æœªçŸ¥æ•°æ® evaluating the model: RMSE, Residual Plot ","date":"2024-07-18","objectID":"/datal9/:0:0","tags":null,"title":"DATA100-L9: Introduction to Modeling, Simple Linear Regression","uri":"/datal9/"},{"categories":["NNDL"],"content":"ç‚¹äº‘çš„æ•°æ®æ ¼å¼ ä¸€ä¸ªç‚¹äº‘æ˜¯ä¸€ä¸ªæ— åºçš„å‘é‡é›†ï¼Œæ¯ä¸ªç‚¹ç”¨ä¸‰ç»´åæ ‡æ„æˆå‘é‡ï¼Œå…¶ä»–ä¿¡æ¯å¯ä»¥æ‰©å±• æœ‰.xyzç­‰æ ¼å¼ æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªçŸ©é˜µ $Num \\times Dim$ PointNetå’Œ++çš„ç®—æ³•å…·ä½“æ“ä½œ ","date":"2024-07-18","objectID":"/papers/pointnet/:0:0","tags":null,"title":"Pointnet","uri":"/papers/pointnet/"},{"categories":["NNDL"],"content":"PointNetç®—æ³•çš„å…·ä½“æ“ä½œ å¿…é¡»è€ƒè™‘çš„å‡ ä¸ªç‰¹ç‚¹ æ—‹è½¬ä¸å˜æ€§ï¼Œåœ¨ä»¿å°„å˜æ¢ä¸‹ç‚¹çš„æ’åˆ—ä¸å˜ ç½®æ¢ä¸å˜æ€§(æ— åºçš„)ï¼šåº”è¯¥æ˜¯å¯¹ç§°å‡½æ•°ï¼Œä¸è€ƒè™‘é¡ºåº å±€éƒ¨ç»“æ„éœ€è¦è€ƒè™‘(åˆ†å‰²æ—¶)ï¼Œå¹³è¡¡globalå’Œlocal è¾“å…¥ï¼šç‚¹äº‘æ•°æ® MLPå‡ç»´ï¼ŒæŠ½å–ç‰¹å¾ Maxå¯¹ç§°å‡½æ•°æ“ä½œï¼Œé™ç»´ï¼Œå¾—åˆ°å…¨å±€ç‰¹å¾ $N \\times 1$ åç»­åˆ†ç±»oråˆ†å‰² è¾“å…¥ï¼šç‚¹äº‘æ•°æ® T-Netä»¿å°„å˜æ¢ï¼ˆæ—‹è½¬ï¼‰æ¥ä¿è¯æ—‹è½¬ä¸å˜æ€§ MLPå‡ç»´ï¼ŒæŠ½å–ç‰¹å¾ maxpoolingæ“ä½œï¼Œé™ç»´ï¼Œå¾—åˆ°globalç‰¹å¾ $N \\times 1$ åç»­åˆ†ç±»èµ°mlpï¼ˆfcnnï¼‰+softmax èµ°åˆ†å‰²çš„è¯æ‹¼æ¥åˆ°feature $N\\times 64$ å¾—åˆ° $N\\times 1088$ï¼Œç„¶åmlpç»™æ¯ä¸ªç‚¹å±äºå“ªå‡ ä¸ªç±»åˆ«æ‰“åˆ† ä¸è¶³ä¹‹å¤„ maxpoolingè¿™ä¸€æ­¥ç›´æ¥æŠŠæ‰€æœ‰ç‚¹æ± åŒ–ä¸ºä»¥ä¸€ä¸ªå…¨å±€ç‰¹å¾ï¼Œå±€éƒ¨ç‚¹ä¸ç‚¹çš„è”ç³»æ²¡æœ‰å¯Ÿè§‰ ","date":"2024-07-18","objectID":"/papers/pointnet/:1:0","tags":null,"title":"Pointnet","uri":"/papers/pointnet/"},{"categories":["NNDL"],"content":"PointNet++ç®—æ³•çš„å…·ä½“æ“ä½œ å€Ÿé‰´äº†å·ç§¯ç¥ç»ç½‘ç»œçš„æ€æƒ³ï¼Œå¯¹ç©ºé—´åŒ–æ•´ä¸ºé›¶ï¼Œåˆ†è€Œæ²»ä¹‹ è¿›è¡Œå¤šæ¬¡set abstraction = sampling + grouping + PointNet èµ°åˆ†ç±»ï¼ŒåŒä¸ŠPointNetå¤„ç† èµ°åˆ†å‰²ï¼Œinterpolateæ’å€¼ï¼Œç„¶åskip link concatenationæ‹¼æ¥åˆ°ä½å±‚æ¬¡ç‚¹ç‰¹å¾åé¢ï¼Œunit pointnetå¤„ç†,é‡å¤å‡ æ¬¡ï¼Œå›åˆ°åŸå§‹ç‚¹äº‘ï¼Œå¾—åˆ°åˆ†å‰²ç»“æœï¼ˆæ¯ä¸ªç‚¹çš„è¯„åˆ†ï¼‰ ä»¥ä¸‹é€æ­¥åˆ†æ set abstraction sampling FPSå–ç‚¹ grouping èšç±»ball queryï¼Œ ball queryé‡‡ç”¨metric distanceå’ŒCNNæ›¼å“ˆé¡¿è·ç¦»ä¸åŒï¼Œä½¿å¾—å‰è€…èƒ½æ³›åŒ–èƒ½åŠ›æ›´å¥½ ç„¶åé‡‡ç”¨pointnetå¯¹æ¯ä¸ªgroupè¿›è¡Œç‰¹å¾æå– æå‡ºé’ˆå¯¹ ç‚¹ å¯†åº¦åˆ†å¸ƒä¸å‡åŒ€çš„ density adaptive PointNet MSGï¼Œå¯¹ä¸åŒå¤§å°çš„åŒºåŸŸè¿›è¡Œç‰¹å¾æŠ½å–å¹¶ä¸”æ‹¼åœ¨ä¸€èµ· random input dropout MRG, â€œå †å â€å¤šä¸ªPointNeté«˜ç»´æŠ½å–ï¼Œæ”¹è¿›ï¼šä¸€ä¸ªå‘é‡æ˜¯æŠ½å–ä½ä¸€å±‚çš„ç‰¹å¾ï¼Œå¦ä¸€ä¸ªå‘é‡ç›´æ¥æŠ½å–åº•å±‚ç‰¹å¾ï¼Œç„¶åç”±ç‚¹å¯†åº¦åˆ†å¸ƒæ¥å†³å®šæŒ‰æƒé‡æ‹¼æ¥ interpolate \u0026 skip link concatenation ï¼ˆç‰¹å¾ä¼ æ’­feature propagationï¼‰ åœ¨ä½ä¸€å±‚çš„åæ ‡æ’å…¥ä¸´è¿‘å‡ ä¸ªç‚¹çš„ç‰¹å¾åŠ æƒå’Œï¼Œæƒé‡é‡‡ç”¨è·ç¦»å€’æ•° å’Œskip link çš„ç‚¹ç‰¹å¾è¿›è¡Œæ‹¼æ¥ è¿‡ä¸€å±‚ç±»ä¼¼1 $\\times$ 1çš„å·ç§¯unit pointnet ä¸¤ç¯‡çš„åŒºåˆ« ä½œç”¨åŸŸï¼špointnetå…³æ³¨å…¨å±€ï¼Œ++ä¾§é‡å…³æ³¨å±€éƒ¨ç„¶åæ‰æ˜¯å…¨å±€ ç‚¹å¯†åº¦ï¼špointnetå…³æ³¨ç‚¹å¾®å°æ‰°åŠ¨ã€å¼‚å¸¸å€¼çš„å½±å“ï¼Œ++ä¾§é‡ç‚¹å¯†åº¦åˆ†å¸ƒä¸å‡åŒ€çš„å½±å“ å±‚æ¬¡ï¼š++æä¾›çš„æ˜¯ä¸€ä¸ªæ¶æ„ï¼Œç›´è§‰ä¸Šæ¥è®²æ¯”pointnetå±‚æ¬¡é«˜ä¸€çº§ï¼Œpointnetå¯ä»¥åµŒå…¥pointnet++ï¼Œä½†ä¹Ÿæœ‰å…¶ä»–ç¼–ç æ–¹æ¡ˆå¯ä»¥åµŒå…¥++ï¼Ÿ $By\\ HZ$ ","date":"2024-07-18","objectID":"/papers/pointnet/:2:0","tags":null,"title":"Pointnet","uri":"/papers/pointnet/"},{"categories":["DATA100"],"content":"Kernel Density Functions ","date":"2024-07-16","objectID":"/datal8/:0:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"KDE Mechanics ","date":"2024-07-16","objectID":"/datal8/:1:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"smoothing in 1Dï¼ˆhistogramsï¼‰ rug â€”\u003e histogram ","date":"2024-07-16","objectID":"/datal8/:1:1","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"smoothing in 2Dï¼ˆheatmaps/Hex Plotï¼‰ ","date":"2024-07-16","objectID":"/datal8/:1:2","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"KDEs ä»£ç å®ç°ï¼š sns.distplot(data, kde=True) ","date":"2024-07-16","objectID":"/datal8/:1:3","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"Kernel Functions and Bandwidth $\\alpha$ è¶Šå¤§ï¼Œæ›²çº¿è¶Šå¹³æ»‘ å½“ç„¶ä¹Ÿæœ‰å…¶ä»–çš„kernelå‡½æ•°ï¼Œæ¯”å¦‚ï¼š triangular kernel epanechnikov kernel boxcar kernel Visualization Theory æ³¨æ„å¯è§†åŒ–çš„ç›®çš„ï¼ ä»…ä»…é ç»Ÿè®¡æ–¹æ³•ä¸å¤Ÿç›´è§‚å¹¶ä¸”ä¸å¤Ÿå‡†ç¡®ï¼ ","date":"2024-07-16","objectID":"/datal8/:2:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"Information Channels color, shape, size, position (coordinate), and orientation ","date":"2024-07-16","objectID":"/datal8/:3:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing X/Y do not use different scales for x and y in the same visualization! æ¯”ä¾‹é€‚ä¸­ ","date":"2024-07-16","objectID":"/datal8/:4:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Color é€‰é¢œè‰²ï¼Œjet, viridisä¸»é¢˜ç­‰ç­‰ æœ€å¥½é€‰æ‹©perceptually uniformçš„é¢œè‰²ï¼è€Œjetä¸æ˜¯ï¼Infernoï¼Œ Turboå¯ä»¥ ","date":"2024-07-16","objectID":"/datal8/:5:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Markings äººæ›´å€¾å‘äºæ¯”è¾ƒæ•´é½çš„ç›´æ–¹å›¾ï¼ˆä¸€ç»´é•¿åº¦ï¼‰ é¿å…ç§»åŠ¨è°ƒæ•´åŸºçº¿ï¼ å–å†³äºè®²ä»€ä¹ˆæ•…äº‹ ","date":"2024-07-16","objectID":"/datal8/:6:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Conditioning ","date":"2024-07-16","objectID":"/datal8/:7:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":"Harnessing Context Transformations linearizeçº¿æ€§åŒ–å¤„ç† log transformå¯¹æ•°å˜æ¢ æ›´å¤šçš„ä»£ç å‚è€ƒjupyter notebook ","date":"2024-07-16","objectID":"/datal8/:8:0","tags":null,"title":"DATA100-L8: Visualizations â…¡","uri":"/datal8/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab02.ipynb\") Pandas is one of the most widely used Python libraries in data science. In this lab, you will review commonly used data wrangling operations/tools in Pandas. We aim to give you familiarity with: Creating DataFrames Slicing DataFrames (i.e. selecting rows and columns) Filtering data (using boolean arrays and groupby.filter) Aggregating (using groupby.agg) In this lab you are going to use several pandas methods. Reminder from lecture that you may press shift+tab on method parameters to see the documentation for that method. For example, if you were using the drop method in pandas, you couold press shift+tab to see what drop is expecting. Pandas is very similar to the datascience library that you saw in Data 8. This conversion notebook may serve as a useful guide! This lab expects that you have watched the pandas lectures. If you have not, this lab will probably take a very long time. Note: The Pandas interface is notoriously confusing for beginners, and the documentation is not consistently great. Throughout the semester, you will have to search through Pandas documentation and experiment, but remember it is part of the learning experience and will help shape you as a data scientist! import numpy as np import matplotlib.pyplot as plt import pandas as pd import plotly.express as px %matplotlib inline ","date":"2024-07-15","objectID":"/datalab2/:0:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Creating DataFrames \u0026 Basic Manipulations Recall that a DataFrame is a table in which each column has a specific data type; there is an index over the columns (typically string labels) and an index over the rows (typically ordinal numbers). Usually youâ€™ll create DataFrames by using a function like pd.read_csv. However, in this section, weâ€™ll discuss how to create them from scratch. The documentation for the pandas DataFrame class provides several constructors for the DataFrame class. Syntax 1: You can create a DataFrame by specifying the columns and values using a dictionary as shown below. The keys of the dictionary are the column names, and the values of the dictionary are lists containing the row entries. fruit_info = pd.DataFrame( data = {'fruit': ['apple', 'orange', 'banana', 'raspberry'], 'color': ['red', 'orange', 'yellow', 'pink'], 'price': [1.0, 0.75, 0.35, 0.05] }) fruit_info fruit\rcolor\rprice\r0\rapple\rred\r1.00\r1\rorange\rorange\r0.75\r2\rbanana\ryellow\r0.35\r3\rraspberry\rpink\r0.05\rSyntax 2: You can also define a DataFrame by specifying the rows as shown below. Each row corresponds to a distinct tuple, and the columns are specified separately. è¿™é‡Œå¯ä»¥çœ‹å‡ºcolumnsæ˜¯ä¸ªå‚æ•° fruit_info2 = pd.DataFrame( [(\"red\", \"apple\", 1.0), (\"orange\", \"orange\", 0.75), (\"yellow\", \"banana\", 0.35), (\"pink\", \"raspberry\", 0.05)], columns = [\"color\", \"fruit\", \"price\"]) fruit_info2 color\rfruit\rprice\r0\rred\rapple\r1.00\r1\rorange\rorange\r0.75\r2\ryellow\rbanana\r0.35\r3\rpink\rraspberry\r0.05\rYou can obtain the dimensions of a DataFrame by using the shape attribute DataFrame.shape. fruit_info.shape (4, 3)\rYou can also convert the entire DataFrame into a two-dimensional NumPy array. fruit_info.values array([['apple', 'red', 1.0],\r['orange', 'orange', 0.75],\r['banana', 'yellow', 0.35],\r['raspberry', 'pink', 0.05]], dtype=object)\rThere are other constructors but we do not discuss them here. ","date":"2024-07-15","objectID":"/datalab2/:1:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"REVIEW: Selecting Rows and Columns in Pandas As youâ€™ve seen in lecture and discussion, there are two verbose operators in Python for selecting rows: loc and iloc. Letâ€™s review them briefly. Approach 1: loc The first of the two verbose operators is loc, which takes two arguments. The first is one or more row labels, the second is one or more column labels. The desired rows or columns can be provided individually, in slice notation, or as a list. Some examples are given below. Note that slicing in loc is inclusive on the provided labels. #get rows 0 through 2 and columns fruit through price fruit_info.loc[0:2, 'fruit':'price'] # é—­åŒºé—´ fruit\rcolor\rprice\r0\rapple\rred\r1.00\r1\rorange\rorange\r0.75\r2\rbanana\ryellow\r0.35\r# get rows 0 through 2 and columns fruit and price. # Note the difference in notation and result from the previous example. fruit_info.loc[0:2, ['fruit', 'price']] # ç¦»æ•£ fruit\rprice\r0\rapple\r1.00\r1\rorange\r0.75\r2\rbanana\r0.35\r# get rows 0 and 2 and columns fruit and price. fruit_info.loc[[0, 2], ['fruit', 'price']] # æ›´åŠ ç¦»æ•£ fruit\rprice\r0\rapple\r1.00\r2\rbanana\r0.35\r# get rows 0 and 2 and column fruit fruit_info.loc[[0, 2], ['fruit']] fruit\r0\rapple\r2\rbanana\rNote that if we request a single column but donâ€™t enclose it in a list, the return type of the loc operator is a Series rather than a DataFrame. æ³¨æ„[ ]åŒ…è£¹é—®é¢˜ # get rows 0 and 2 and column fruit, returning the result as a Series fruit_info.loc[[0, 2], 'fruit'] 0 apple\r2 banana\rName: fruit, dtype: object\rIf we provide only one argument to loc, it uses the provided argument to select rows, and returns all columns.å¯ä»¥åªç»™è¡Œï¼Œ ä¸å¯ä»¥åªç»™åˆ— fruit_info.loc[0:1] fruit\rcolor\rprice\r0\rapple\rred\r1.00\r1\rorange\rorange\r0.75\rNote that if you try to access columns without providing rows, loc will crash. # uncomment, this code will crash # fruit_info.loc[[\"fruit\", \"price\"]] # uncomment, this code works fine: fruit_info.loc[:, [\"fruit\", \"price\"]] fruit\rprice\r0\rapple\r1.00\r1\rorange\r0.75\r2\rbanana\r0.35\r3\rraspberry\r0.05\rApproach 2: iloc iloc is very similar to loc except that its arguments are row numbers and column numbers, rather than row labels and labels names. A usueful mnemonic is that the i stands for â€œintegerâ€. In addition, slicing for iloc is exclusive on the provided integer indices. Some examples are given below: è€ƒè™‘æ­¤æ—¶å˜æˆpythonç»å…¸ç´¢å¼• # get rows 0 through 3 (exclusive) and columns 0 through 2 (exclusive) fruit_info.iloc[0:3, 0:3] fruit\rcolor\rprice\r0\rapple\rred\r1.00\r1\rorange\rorange\r0.75\r2\rbanana\ryellow\r0.35\r# get rows 0 through 3 (exclusive) and columns 0 and 2. fruit_info.iloc[0:3, [0, 2]] fruit\rprice\r0\rapple\r1.00\r1\rorange\r0.75\r2\rbanana\r0.35\r# get rows 0 and 2 and columns 0 and 2. fruit_info.iloc[[0, 2], [0, 2]] fruit\rprice\r0\rapple\r1.00\r2\rbanana\r0.35\r#get rows 0 and 2 and column fruit fruit_info.iloc[[0, 2], [0]] fruit\r0\rapple\r2\rbanana\r# get rows 0 and 2 and column fruit fruit_info.iloc[[0, 2], 0] # return a Series! 0 apple\r2 banana\rName: fruit, dtype: object\rNote that in these loc and iloc examples above, the row label and row number were always the same. Letâ€™s see an example where they are different. If we sort our fruits by price, we get: fruit_info_sorted = fruit_info.sort_values(\"price\") fruit_info_sorted fruit\rcolor\rprice\r3\rraspberry\rpink\r0.05\r2\rbanana\ryellow\r0.35\r1\rorange\rorange\r0.75\r0\rapple\rred\r1.00\rObserve that the row number 0 now has index 3, row number 1 now has index 2, etc. These indices are the arbitrary numerical index generated when we created the DataFrame. For example, banana was originally in row 2, and so it has row label 2. If we request the rows in positions 0 and 2 using iloc, weâ€™re indexing using the row NUMBERS, not labels. è¿™é‡Œä¼¼ä¹å¹¶ä¸æ˜¯æŒ‰ç…§labæ‰€è¯´çš„é‚£æ ·? fruit_info_sorted.iloc[[0, 2], 0] # åˆ«å’Œæ•°å­¦è¡¨è¾¾æ··æ·†! 3 raspberry\r1 orange\rName: fruit, dtype: object\rLastly, similar to with loc, the second argument to iloc is optional. That is, if you provide only one argument to iloc, it treats the argument you provide as a set of desired row numbers, not column numbers. å¯ä»¥åªç»™è¡Œï¼Œä¸å¯ç»™åˆ— fruit_info.iloc[[0, 2]] fruit\rcolor\rpr","date":"2024-07-15","objectID":"/datalab2/:1:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 1(a) For a DataFrame d, you can add a column with d['new column name'] = ... and assign a list or array of values to the column. Add a column of integers containing 1, 2, 3, and 4 called rank1 to the fruit_info table which expresses your personal preference about the taste ordering for each fruit (1 is tastiest; 4 is least tasty). fruit_info['rank1'] = [1,2,3,4] fruit_info fruit\rcolor\rprice\rrank1\r0\rapple\rred\r1.00\r1\r1\rorange\rorange\r0.75\r2\r2\rbanana\ryellow\r0.35\r3\r3\rraspberry\rpink\r0.05\r4\rgrader.check(\"q1a\") q1a passed! ğŸ’¯ ","date":"2024-07-15","objectID":"/datalab2/:1:2","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 1(b) You can also add a column to d with d.loc[:, 'new column name'] = .... As above, the first parameter is for the rows and second is for columns. The : means change all rows and the 'new column name' indicates the name of the column you are modifying (or in this case, adding). Add a column called rank2 to the fruit_info table which contains the same values in the same order as the rank1 column. fruit_info.loc[:, 'rank2'] = [1,2,3,4] fruit_info fruit\rcolor\rprice\rrank1\rrank2\r0\rapple\rred\r1.00\r1\r1\r1\rorange\rorange\r0.75\r2\r2\r2\rbanana\ryellow\r0.35\r3\r3\r3\rraspberry\rpink\r0.05\r4\r4\rgrader.check(\"q1b\") q1b passed! ğŸ€ ","date":"2024-07-15","objectID":"/datalab2/:1:3","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 2 Use the .drop() method to drop both the rank1 and rank2 columns you created. Make sure to use the axis parameter correctly. Note that drop does not change a table, but instead returns a new table with fewer columns or rows unless you set the optional inplace parameter. Hint: Look through the documentation to see how you can drop multiple columns of a Pandas DataFrame at once using a list of column names. fruit_info_original = fruit_info.drop(labels=['rank1','rank2'],axis=1) fruit_info_original fruit\rcolor\rprice\r0\rapple\rred\r1.00\r1\rorange\rorange\r0.75\r2\rbanana\ryellow\r0.35\r3\rraspberry\rpink\r0.05\rgrader.check(\"q2\") q2 passed! ğŸ’¯ ","date":"2024-07-15","objectID":"/datalab2/:1:4","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 3 Use the .rename() method to rename the columns of fruit_info_original so they begin with capital letters. Set this new DataFrame to fruit_info_caps. For an example of how to use rename, see the linked documentation above. fruit_info_caps = fruit_info_original.rename(columns={'fruit':'Fruit', 'color':'Color', 'price':'Price'}) fruit_info_caps Fruit\rColor\rPrice\r0\rapple\rred\r1.00\r1\rorange\rorange\r0.75\r2\rbanana\ryellow\r0.35\r3\rraspberry\rpink\r0.05\rgrader.check(\"q3\") q3 passed! ğŸ‰ ","date":"2024-07-15","objectID":"/datalab2/:1:5","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Babynames Dataset For the new few questions of this lab, letâ€™s move on to a real world dataset. Weâ€™ll be using the babynames dataset from Lecture 1. The babynames dataset contains a record of the given names of babies born in the United States each year. First letâ€™s run the following cells to build the DataFrame baby_names. The cells below download the data from the web and extract the data into a DataFrame. There should be a total of 6215834 records. ","date":"2024-07-15","objectID":"/datalab2/:1:6","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"fetch_and_cache Helper The following function downloads and caches data in the data/ directory and returns the Path to the downloaded file. The cell below the function describes how it works. You are not expected to understand this code, but you may find it useful as a reference as a practitioner of data science after the course. import requests from pathlib import Path def fetch_and_cache(data_url, file, data_dir=\"data\", force=False): \"\"\" Download and cache a url and return the file object. data_url: the web address to download file: the file in which to save the results. data_dir: (default=\"data\") the location to save the data force: if true the file is always re-downloaded return: The pathlib.Path to the file. \"\"\" data_dir = Path(data_dir) data_dir.mkdir(exist_ok=True) file_path = data_dir/Path(file) if force and file_path.exists(): file_path.unlink() if force or not file_path.exists(): print('Downloading...', end=' ') resp = requests.get(data_url) with file_path.open('wb') as f: f.write(resp.content) print('Done!') else: import time created = time.ctime(file_path.stat().st_ctime) print(\"Using cached version downloaded at\", created) return file_path In Python, a Path object represents the filesystem paths to files (and other resources). The pathlib module is effective for writing code that works on different operating systems and filesystems. To check if a file exists at a path, use .exists(). To create a directory for a path, use .mkdir(). To remove a file that might be a symbolic link, use .unlink(). This function creates a path to a directory that will contain data files. It ensures that the directory exists (which is required to write files in that directory), then proceeds to download the file based on its URL. The benefit of this function is that not only can you force when you want a new file to be downloaded using the force parameter, but in cases when you donâ€™t need the file to be re-downloaded, you can use the cached version and save download time. Below we use fetch_and_cache to download the namesbystate.zip zip file, which is a compressed directory of CSV files. This might take a little while! Consider stretching. data_url = 'https://www.ssa.gov/oact/babynames/state/namesbystate.zip' namesbystate_path = fetch_and_cache(data_url, 'namesbystate.zip') Using cached version downloaded at Fri Jul 12 20:04:41 2024\rThe following cell builds the final full baby_names DataFrame. It first builds one DataFrame per state, because thatâ€™s how the data are stored in the zip file. Here is documentation for pd.concat if you want to know more about its functionality. As before, you are not expected to understand this code. import zipfile zf = zipfile.ZipFile(namesbystate_path, 'r') column_labels = ['State', 'Sex', 'Year', 'Name', 'Count'] def load_dataframe_from_zip(zf, f): with zf.open(f) as fh: return pd.read_csv(fh, header=None, names=column_labels) states = [ load_dataframe_from_zip(zf, f) for f in sorted(zf.filelist, key=lambda x:x.filename) if f.filename.endswith('.TXT') ] baby_names = states[0] for state_df in states[1:]: baby_names = pd.concat([baby_names, state_df]) baby_names = baby_names.reset_index().iloc[:, 1:] len(baby_names) 6215834\rbaby_names.head() State\rSex\rYear\rName\rCount\r0\rAK\rF\r1910\rMary\r14\r1\rAK\rF\r1910\rAnnie\r12\r2\rAK\rF\r1910\rAnna\r10\r3\rAK\rF\r1910\rMargaret\r8\r4\rAK\rF\r1910\rHelen\r7\r","date":"2024-07-15","objectID":"/datalab2/:1:7","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Selection Examples on Baby Names As with our synthetic fruit dataset, we can use loc and iloc to select rows and columns of interest from our dataset. baby_names.loc[2:5, 'Name']# Series 2 Anna\r3 Margaret\r4 Helen\r5 Elsie\rName: Name, dtype: object\rNotice the difference between the following cell and the previous one, just passing in 'Name' returns a Series while ['Name'] returns a DataFrame. baby_names.loc[2:5, ['Name']] #df Name\r2\rAnna\r3\rMargaret\r4\rHelen\r5\rElsie\rThe code below collects the rows in positions 1 through 3, and the column in position 3 (â€œNameâ€). baby_names.iloc[1:4, [3]] Name\r1\rAnnie\r2\rAnna\r3\rMargaret\r","date":"2024-07-15","objectID":"/datalab2/:1:8","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 4 Use .loc to select Name and Year in that order from the baby_names table. name_and_year = baby_names.loc[:, ['Name', 'Year']] name_and_year[:5] # ç‰ˆæœ¬é—®é¢˜ Name\rYear\r0\rMary\r1910\r1\rAnnie\r1910\r2\rAnna\r1910\r3\rMargaret\r1910\r4\rHelen\r1910\rgrader.check(\"q4\") Now repeat the same selection using the plain [] notation. æ¥å—ä¸€ä¸ªlist of columns name_and_year = baby_names[['Name','Year']] name_and_year[:5] Name\rYear\r0\rMary\r1910\r1\rAnnie\r1910\r2\rAnna\r1910\r3\rMargaret\r1910\r4\rHelen\r1910\r","date":"2024-07-15","objectID":"/datalab2/:1:9","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Filtering Data ","date":"2024-07-15","objectID":"/datalab2/:2:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Review: Filtering with boolean arrays Filtering is the process of removing unwanted material. In your quest for cleaner data, you will undoubtedly filter your data at some point: whether it be for clearing up cases with missing values, for culling out fishy outliers, or for analyzing subgroups of your data set. Example usage looks like df[df['column name'] \u003c 5]. For your reference, some commonly used comparison operators are given below. Symbol Usage Meaning == a == b Does a equal b? \u003c= a \u003c= b Is a less than or equal to b? \u003e= a \u003e= b Is a greater than or equal to b? \u003c a \u003c b Is a less than b? \u003e a \u003e b Is a greater than b? ~ ~p Returns negation of p | p | q p OR q \u0026 p \u0026 q p AND q ^ p ^ q p XOR q (exclusive or) In the following we construct the DataFrame containing only names registered in Californiaæ³¨æ„è¿™é‡Œååˆ†é‡è¦! ca = baby_names[baby_names['State'] == 'CA'] ca.head(5) State\rSex\rYear\rName\rCount\r390635\rCA\rF\r1910\rMary\r295\r390636\rCA\rF\r1910\rHelen\r239\r390637\rCA\rF\r1910\rDorothy\r220\r390638\rCA\rF\r1910\rMargaret\r163\r390639\rCA\rF\r1910\rFrances\r134\r","date":"2024-07-15","objectID":"/datalab2/:2:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 5 Using a boolean array, select the names in Year 2000 (from baby_names) that have larger than 3000 counts. Keep all columns from the original baby_names DataFrame. Note: Note that compound expressions have to be grouped with parentheses. That is, any time you use p \u0026 q to filter the DataFrame, make sure to use df[(df[p]) \u0026 (df[q])] or df.loc[(df[p]) \u0026 (df[q])]. You may use either [] or loc. Both will achieve the same result. For more on [] vs. loc see the stack overflow links from the intro portion of this lab. result = baby_names[(baby_names['Year'] == 2000) \u0026 (baby_names['Count'] \u003e 3000)] result.head() State\rSex\rYear\rName\rCount\r725638\rCA\rM\r2000\rDaniel\r4342\r725639\rCA\rM\r2000\rAnthony\r3839\r725640\rCA\rM\r2000\rJose\r3804\r725641\rCA\rM\r2000\rAndrew\r3600\r725642\rCA\rM\r2000\rMichael\r3572\rgrader.check(\"q5\") Query Review Recall that pandas also has a query command. For example, we can get California baby names with the code below. ca = baby_names.query('State == \"CA\"') ca.head(5) State\rSex\rYear\rName\rCount\r390635\rCA\rF\r1910\rMary\r295\r390636\rCA\rF\r1910\rHelen\r239\r390637\rCA\rF\r1910\rDorothy\r220\r390638\rCA\rF\r1910\rMargaret\r163\r390639\rCA\rF\r1910\rFrances\r134\rUsing the query command, select the names in Year 2000 (from baby_names) that have larger than 3000 counts. result_using_query = baby_names.query(\"Count \u003e 3000 and Year == 2000\") result_using_query.head(5) State\rSex\rYear\rName\rCount\r725638\rCA\rM\r2000\rDaniel\r4342\r725639\rCA\rM\r2000\rAnthony\r3839\r725640\rCA\rM\r2000\rJose\r3804\r725641\rCA\rM\r2000\rAndrew\r3600\r725642\rCA\rM\r2000\rMichael\r3572\r","date":"2024-07-15","objectID":"/datalab2/:2:2","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Groupby Letâ€™s now turn to using groupby from lecture 4. Note: This slide provides a visual picture of how groupby.agg works if youâ€™d like a reference. ","date":"2024-07-15","objectID":"/datalab2/:3:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6: Elections Review: Letâ€™s start by reading in the election dataset from the pandas lectures. # run this cell elections = pd.read_csv(\"data/elections.csv\") elections.head(5) Year\rCandidate\rParty\rPopular vote\rResult\r%\r0\r1824\rAndrew Jackson\rDemocratic-Republican\r151271\rloss\r57.210122\r1\r1824\rJohn Quincy Adams\rDemocratic-Republican\r113142\rwin\r42.789878\r2\r1828\rAndrew Jackson\rDemocratic\r642806\rwin\r56.203927\r3\r1828\rJohn Quincy Adams\rNational Republican\r500897\rloss\r43.796073\r4\r1832\rAndrew Jackson\rDemocratic\r702735\rwin\r54.574789\rAs we saw, we can groupby a specific column, e.g. â€œPartyâ€. It turns out that using some syntax we didnâ€™t cover in lecture, we can print out the subframes that result. This isnâ€™t something youâ€™ll do for any practical purpose. However, it may help you get an understanding of what groupby is actually doing. An example is given below for elections since 1980. # run this cell for n, g in elections.query(\"Year \u003e= 1980\").groupby(\"Party\"): print(f\"Name: {n}\") # by the way this is an \"f string\", a relatively new and great feature of Python display(g) Name: Citizens\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r127\r1980\rBarry Commoner\rCitizens\r233052\rloss\r0.270182\rName: Constitution\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r160\r2004\rMichael Peroutka\rConstitution\r143630\rloss\r0.117542\r164\r2008\rChuck Baldwin\rConstitution\r199750\rloss\r0.152398\r172\r2016\rDarrell Castle\rConstitution\r203091\rloss\r0.149640\rName: Democratic\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r129\r1980\rJimmy Carter\rDemocratic\r35480115\rloss\r41.132848\r134\r1984\rWalter Mondale\rDemocratic\r37577352\rloss\r40.729429\r137\r1988\rMichael Dukakis\rDemocratic\r41809074\rloss\r45.770691\r140\r1992\rBill Clinton\rDemocratic\r44909806\rwin\r43.118485\r144\r1996\rBill Clinton\rDemocratic\r47400125\rwin\r49.296938\r151\r2000\rAl Gore\rDemocratic\r50999897\rloss\r48.491813\r158\r2004\rJohn Kerry\rDemocratic\r59028444\rloss\r48.306775\r162\r2008\rBarack Obama\rDemocratic\r69498516\rwin\r53.023510\r168\r2012\rBarack Obama\rDemocratic\r65915795\rwin\r51.258484\r176\r2016\rHillary Clinton\rDemocratic\r65853514\rloss\r48.521539\r178\r2020\rJoseph Biden\rDemocratic\r81268924\rwin\r51.311515\rName: Green\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r149\r1996\rRalph Nader\rGreen\r685297\rloss\r0.712721\r155\r2000\rRalph Nader\rGreen\r2882955\rloss\r2.741176\r156\r2004\rDavid Cobb\rGreen\r119859\rloss\r0.098088\r165\r2008\rCynthia McKinney\rGreen\r161797\rloss\r0.123442\r170\r2012\rJill Stein\rGreen\r469627\rloss\r0.365199\r177\r2016\rJill Stein\rGreen\r1457226\rloss\r1.073699\r181\r2020\rHoward Hawkins\rGreen\r405035\rloss\r0.255731\rName: Independent\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r130\r1980\rJohn B. Anderson\rIndependent\r5719850\rloss\r6.631143\r143\r1992\rRoss Perot\rIndependent\r19743821\rloss\r18.956298\r161\r2004\rRalph Nader\rIndependent\r465151\rloss\r0.380663\r167\r2008\rRalph Nader\rIndependent\r739034\rloss\r0.563842\r174\r2016\rEvan McMullin\rIndependent\r732273\rloss\r0.539546\rName: Libertarian\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r128\r1980\rEd Clark\rLibertarian\r921128\rloss\r1.067883\r132\r1984\rDavid Bergland\rLibertarian\r228111\rloss\r0.247245\r138\r1988\rRon Paul\rLibertarian\r431750\rloss\r0.472660\r139\r1992\rAndre Marrou\rLibertarian\r290087\rloss\r0.278516\r146\r1996\rHarry Browne\rLibertarian\r485759\rloss\r0.505198\r153\r2000\rHarry Browne\rLibertarian\r384431\rloss\r0.365525\r159\r2004\rMichael Badnarik\rLibertarian\r397265\rloss\r0.325108\r163\r2008\rBob Barr\rLibertarian\r523715\rloss\r0.399565\r169\r2012\rGary Johnson\rLibertarian\r1275971\rloss\r0.992241\r175\r2016\rGary Johnson\rLibertarian\r4489235\rloss\r3.307714\r180\r2020\rJo Jorgensen\rLibertarian\r1865724\rloss\r1.177979\rName: Natural Law\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r148\r1996\rJohn Hagelin\rNatural Law\r113670\rloss\r0.118219\rName: New Alliance\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r136\r1988\rLenora Fulani\rNew Alliance\r217221\rloss\r0.237804\rName: Populist\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r141\r1992\rBo Gritz\rPopulist\r106152\rloss\r0.101918\rName: Reform\rYear\rCandidate\rParty\rPopular vote\rResult\r%\r150\r1996\rRoss Perot\rReform\r8085294\rloss\r8.408844\r154\r2000\rPat Buchanan\rReform\r448895\rl","date":"2024-07-15","objectID":"/datalab2/:3:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6a Using groupby.agg or one of the shorthand methods (groupby.min, groupby.first, etc.), create a Series best_result_percentage_only that returns a Series showing the entire best result for every party, sorted in decreasing order. Your Series should include only parties which have earned at least 10% of the vote in some election. Your result should look like this: Party Democratic 61.344703 Republican 60.907806 Democratic-Republican 57.210122 National Union 54.951512 Whig 53.051213 Liberal Republican 44.071406 National Republican 43.796073 Northern Democratic 29.522311 Progressive 27.457433 American 21.554001 Independent 18.956298 Southern Democratic 18.138998 American Independent 13.571218 Constitutional Union 12.639283 Free Soil 10.138474 Name: %, dtype: float64 A list of named groupby.agg shorthand methods is here (youâ€™ll have to scroll down about one page). best_result_percentage_only = elections[elections['%']\u003e=10].groupby('Party')['%'].agg(max).sort_values(ascending=False) # put your code above this line best_result_percentage_only C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_61736\\687541662.py:1: FutureWarning: The provided callable \u003cbuilt-in function max\u003e is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\rbest_result_percentage_only = elections[elections['%']\u003e=10].groupby('Party')['%'].agg(max).sort_values(ascending=False)\rParty\rDemocratic 61.344703\rRepublican 60.907806\rDemocratic-Republican 57.210122\rNational Union 54.951512\rWhig 53.051213\rLiberal Republican 44.071406\rNational Republican 43.796073\rNorthern Democratic 29.522311\rProgressive 27.457433\rAmerican 21.554001\rIndependent 18.956298\rSouthern Democratic 18.138998\rAmerican Independent 13.571218\rConstitutional Union 12.639283\rFree Soil 10.138474\rName: %, dtype: float64\rgrader.check(\"q6a\") ","date":"2024-07-15","objectID":"/datalab2/:3:2","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6b Repeat Question 6a. However, this time, your result should be a DataFrame showing all available information rather than only the percentage as a series. This question is trickier than Question 6a. Make sure to check the Lecture 4 slides if youâ€™re stuck! Itâ€™s very easy to make a subtle mistake that shows Woodrow Wilson and Howard Taft both winning the 2020 election. For example, the first 3 rows of your table should be: Party Year Candidate Popular Vote Result % Democratic 1964 Lyndon Johnson 43127041 win 61.344703 Republican 1972 Richard Nixon 47168710 win 60.907806 Democratic-Republican 1824 Andrew Jackson 151271 loss 57.210122 Note that the index is Party. In other words, donâ€™t use reset_index. best_result = elections[elections['%']\u003e=10].sort_values(by='%',ascending=False).groupby(['Party']).agg(lambda x: x.iloc[0]).sort_values(by='%',ascending=False) # @ 52:03 in the video of Lecture 4 # put your code above this line best_result Year\rCandidate\rPopular vote\rResult\r%\rParty\rDemocratic\r1964\rLyndon Johnson\r43127041\rwin\r61.344703\rRepublican\r1972\rRichard Nixon\r47168710\rwin\r60.907806\rDemocratic-Republican\r1824\rAndrew Jackson\r151271\rloss\r57.210122\rNational Union\r1864\rAbraham Lincoln\r2211317\rwin\r54.951512\rWhig\r1840\rWilliam Henry Harrison\r1275583\rwin\r53.051213\rLiberal Republican\r1872\rHorace Greeley\r2834761\rloss\r44.071406\rNational Republican\r1828\rJohn Quincy Adams\r500897\rloss\r43.796073\rNorthern Democratic\r1860\rStephen A. Douglas\r1380202\rloss\r29.522311\rProgressive\r1912\rTheodore Roosevelt\r4122721\rloss\r27.457433\rAmerican\r1856\rMillard Fillmore\r873053\rloss\r21.554001\rIndependent\r1992\rRoss Perot\r19743821\rloss\r18.956298\rSouthern Democratic\r1860\rJohn C. Breckinridge\r848019\rloss\r18.138998\rAmerican Independent\r1968\rGeorge Wallace\r9901118\rloss\r13.571218\rConstitutional Union\r1860\rJohn Bell\r590901\rloss\r12.639283\rFree Soil\r1848\rMartin Van Buren\r291501\rloss\r10.138474\rgrader.check(\"q6b\") ","date":"2024-07-15","objectID":"/datalab2/:3:3","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 6c Our DataFrame contains a number of parties which have never had a successful presidential run. For example, the 2020 elections included candiates from the Libertarian and Green parties, neither of which have elected a president. # just run this cell elections.tail(5) Year\rCandidate\rParty\rPopular vote\rResult\r%\r177\r2016\rJill Stein\rGreen\r1457226\rloss\r1.073699\r178\r2020\rJoseph Biden\rDemocratic\r81268924\rwin\r51.311515\r179\r2020\rDonald Trump\rRepublican\r74216154\rloss\r46.858542\r180\r2020\rJo Jorgensen\rLibertarian\r1865724\rloss\r1.177979\r181\r2020\rHoward Hawkins\rGreen\r405035\rloss\r0.255731\rSuppose we were conducting an analysis trying to focus our attention on parties that had elected a president. The most natural approach is to use groupby.filter. This is an incredibly powerful but subtle tool for filtering data. As a reminder of how filter works, see this slide. The code below accomplishes the task at hand. It does this by creating a function that returns True if and only if a sub-dataframe (a.k.a. group) contains at least one winner. This function in turn uses the Pandas function â€œanyâ€. # just run this cell def at_least_one_candidate_in_the_frame_has_won(frame): \"\"\"Returns df with rows only kept for parties that have won at least one election \"\"\" return (frame[\"Result\"] == 'win').any() winners_only = ( elections .groupby(\"Party\") .filter(at_least_one_candidate_in_the_frame_has_won) ) winners_only.tail(5) Year\rCandidate\rParty\rPopular vote\rResult\r%\r171\r2012\rMitt Romney\rRepublican\r60933504\rloss\r47.384076\r173\r2016\rDonald Trump\rRepublican\r62984828\rwin\r46.407862\r176\r2016\rHillary Clinton\rDemocratic\r65853514\rloss\r48.521539\r178\r2020\rJoseph Biden\rDemocratic\r81268924\rwin\r51.311515\r179\r2020\rDonald Trump\rRepublican\r74216154\rloss\r46.858542\rAlternately we could have used a lambda function instead of explicitly defining a named function using def. # just run this cell (alternative) winners_only = ( elections .groupby(\"Party\") .filter(lambda x : (x[\"Result\"] == \"win\").any()) ) winners_only.tail(5) Year\rCandidate\rParty\rPopular vote\rResult\r%\r171\r2012\rMitt Romney\rRepublican\r60933504\rloss\r47.384076\r173\r2016\rDonald Trump\rRepublican\r62984828\rwin\r46.407862\r176\r2016\rHillary Clinton\rDemocratic\r65853514\rloss\r48.521539\r178\r2020\rJoseph Biden\rDemocratic\r81268924\rwin\r51.311515\r179\r2020\rDonald Trump\rRepublican\r74216154\rloss\r46.858542\rFor your exercise, youâ€™ll do a less restrictive filtering of the elections data. Exercise: Using filter, create a DataFrame major_party_results_since_1988 that includes all election results starting in 1988, but only show a row if the Party it belongs to has earned at least 1% of the popular vote in ANY election since 1988. For example, in 1988, you should not include the New Alliance candidate, since this party has not earned 1% of the vote since 1988. However, you should include the Libertarian candidate from 1988 despite only having 0.47 percent of the vote in 1988, because in 2016 and 2020, the Libertarian candidates Gary Johnson and Jo Jorgensen exceeded 1% of the vote. For example, the first three rows of the table you generate should look like: Year Candidate Party Popular vote Result % 135 1988 George H. W. Bush Republican 48886597 win 53.5188 137 1988 Michael Dukakis Democratic 41809074 loss 45.7707 138 1988 Ron Paul Libertarian 431750 loss 0.47266 major_party_results_since_1988 = elections[(elections['Year']\u003e=1988)].groupby('Party').filter(lambda x: (x['%'] \u003e= 1).any()) major_party_results_since_1988.head() Year\rCandidate\rParty\rPopular vote\rResult\r%\r135\r1988\rGeorge H. W. Bush\rRepublican\r48886597\rwin\r53.518845\r137\r1988\rMichael Dukakis\rDemocratic\r41809074\rloss\r45.770691\r138\r1988\rRon Paul\rLibertarian\r431750\rloss\r0.472660\r139\r1992\rAndre Marrou\rLibertarian\r290087\rloss\r0.278516\r140\r1992\rBill Clinton\rDemocratic\r44909806\rwin\r43.118485\rgrader.check(\"q6c\") ","date":"2024-07-15","objectID":"/datalab2/:3:4","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 7 Pandas provides special purpose functions for working with specific common data types such as strings and dates. For example, the code below provides the length of every Candidateâ€™s name from our elections dataset. elections[\"Candidate\"].str.len() 0 14\r1 17\r2 14\r3 17\r4 14\r..\r177 10\r178 12\r179 12\r180 12\r181 14\rName: Candidate, Length: 182, dtype: int64\rExercise: Using .str.split. Create a new DataFrame called elections_with_first_name with a new column First Name that is equal to the Candidateâ€™s first name. See the Pandas str documentation for documentation on using str.split. Hint: Use [0] somewhere in your code. elections_with_first_name = elections.copy() # your code here elections_with_first_name['First Name'] = elections_with_first_name['Candidate'].str.split(' ').str[0].to_frame() # end your code elections_with_first_name Year\rCandidate\rParty\rPopular vote\rResult\r%\rFirst Name\r0\r1824\rAndrew Jackson\rDemocratic-Republican\r151271\rloss\r57.210122\rAndrew\r1\r1824\rJohn Quincy Adams\rDemocratic-Republican\r113142\rwin\r42.789878\rJohn\r2\r1828\rAndrew Jackson\rDemocratic\r642806\rwin\r56.203927\rAndrew\r3\r1828\rJohn Quincy Adams\rNational Republican\r500897\rloss\r43.796073\rJohn\r4\r1832\rAndrew Jackson\rDemocratic\r702735\rwin\r54.574789\rAndrew\r...\r...\r...\r...\r...\r...\r...\r...\r177\r2016\rJill Stein\rGreen\r1457226\rloss\r1.073699\rJill\r178\r2020\rJoseph Biden\rDemocratic\r81268924\rwin\r51.311515\rJoseph\r179\r2020\rDonald Trump\rRepublican\r74216154\rloss\r46.858542\rDonald\r180\r2020\rJo Jorgensen\rLibertarian\r1865724\rloss\r1.177979\rJo\r181\r2020\rHoward Hawkins\rGreen\r405035\rloss\r0.255731\rHoward\r182 rows Ã— 7 columns grader.check(\"q7\") q7 passed! ğŸ™Œ ","date":"2024-07-15","objectID":"/datalab2/:3:5","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Question 8 The code below creates a table with the frequency of all names from 2020. # just run this cell baby_names_2020 = ( baby_names.query('Year == 2020') .groupby(\"Name\") .sum()[[\"Count\"]] .reset_index() ) baby_names_2020 Name\rCount\r0\rAaden\r15\r1\rAadhira\r6\r2\rAadhvik\r5\r3\rAadhya\r186\r4\rAadi\r14\r...\r...\r...\r8697\rZymere\r6\r8698\rZymir\r74\r8699\rZyon\r130\r8700\rZyra\r33\r8701\rZyrah\r5\r8702 rows Ã— 2 columns Exercise: Using the pd.merge function described in lecture, combine the baby_names_2020 table with the elections_with_first_name table you created earlier to form presidential_candidates_and_name_popularity. presidential_candidates_and_name_popularity = pd.merge(elections_with_first_name,baby_names_2020, left_on='First Name', right_on='Name') presidential_candidates_and_name_popularity Year\rCandidate\rParty\rPopular vote\rResult\r%\rFirst Name\rName\rCount\r0\r1824\rAndrew Jackson\rDemocratic-Republican\r151271\rloss\r57.210122\rAndrew\rAndrew\r5991\r1\r1824\rJohn Quincy Adams\rDemocratic-Republican\r113142\rwin\r42.789878\rJohn\rJohn\r8180\r2\r1828\rAndrew Jackson\rDemocratic\r642806\rwin\r56.203927\rAndrew\rAndrew\r5991\r3\r1828\rJohn Quincy Adams\rNational Republican\r500897\rloss\r43.796073\rJohn\rJohn\r8180\r4\r1832\rAndrew Jackson\rDemocratic\r702735\rwin\r54.574789\rAndrew\rAndrew\r5991\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r148\r2016\rHillary Clinton\rDemocratic\r65853514\rloss\r48.521539\rHillary\rHillary\r20\r149\r2020\rJoseph Biden\rDemocratic\r81268924\rwin\r51.311515\rJoseph\rJoseph\r8349\r150\r2020\rDonald Trump\rRepublican\r74216154\rloss\r46.858542\rDonald\rDonald\r407\r151\r2020\rJo Jorgensen\rLibertarian\r1865724\rloss\r1.177979\rJo\rJo\r6\r152\r2020\rHoward Hawkins\rGreen\r405035\rloss\r0.255731\rHoward\rHoward\r131\r153 rows Ã— 9 columns grader.check(\"q8\") ","date":"2024-07-15","objectID":"/datalab2/:3:6","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Bonus Exercises The following exercises are optional and use the ca_baby_names dataset defined below. perhaps next time! ğŸ˜ # just run this cell ca_baby_names = baby_names.query('State == \"CA\"') ca_baby_names State\rSex\rYear\rName\rCount\r390635\rCA\rF\r1910\rMary\r295\r390636\rCA\rF\r1910\rHelen\r239\r390637\rCA\rF\r1910\rDorothy\r220\r390638\rCA\rF\r1910\rMargaret\r163\r390639\rCA\rF\r1910\rFrances\r134\r...\r...\r...\r...\r...\r...\r784809\rCA\rM\r2020\rZiaan\r5\r784810\rCA\rM\r2020\rZiad\r5\r784811\rCA\rM\r2020\rZiaire\r5\r784812\rCA\rM\r2020\rZidan\r5\r784813\rCA\rM\r2020\rZymir\r5\r394179 rows Ã— 5 columns Sorted Female Name Counts Create a Series female_name_since_2000_count which gives the total number of occurrences of each name for female babies born in California from the year 2000 or later. The index should be the name, and the value should be the total number of births. Your Series should be ordered in decreasing order of count. For example, your first row should have index â€œEmilyâ€ and value 52334, because 52334 Emilys have been born since the year 2000 in California. female_name_since_2000_count = female_name_since_2000_count Counts for All Names Using groupby, create a Series count_for_names_2020 listing all baby names from 2020 in California, in decreasing order of popularity. The result should not be broken down by sex! If a name is used by both male and female babies, the number you provide should be the total. Note: In this question we are now computing the number of registered babies with a given name. For example, count_for_names_2020[\"Noah\"] should be the number 2631 because in 2018 there were 2631 Noahs born (23 female and 2608 male). ... count_for_names_2020 ","date":"2024-07-15","objectID":"/datalab2/:4:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Extra: Explore the Data Set The popularity of some baby names may be influenced by cultural phenomena, such as a political figure coming to power. Below, we plot the popularity of name Hillary for female babies in Calfiornia over time. What do you notice about this plot? What real-world events in the U.S. occurred when there was a steep drop in babies named Hillary? hillary_baby_name = baby_names[(baby_names['Name'] == 'Hillary') \u0026 (baby_names['State'] == 'CA') \u0026 (baby_names['Sex'] == 'F')] plt.plot(hillary_baby_name['Year'], hillary_baby_name['Count']) plt.title(\"Hillary Popularity Over Time\") plt.xlabel('Year') plt.ylabel('Count'); The code above is hard coded to generate a dataframe representing the popularity of the female name Hillary in the state of California. While this approach works, itâ€™s inelegant. Here weâ€™ll use a more elegant approach that builds a dataframe such that: It contains ALL names. The counts are summed across all 50 states, not just California. To do this, we use groupby, though here weâ€™re grouping on two columns (â€œNameâ€ and â€œYearâ€) instead of just one. After grouping, we use the sum aggregation function. # just run this cell counts_aggregated_by_name_and_year = baby_names.groupby([\"Name\", \"Year\"]).sum() counts_aggregated_by_name_and_year Note that the resulting DataFrame is multi-indexed, i.e. it has two indices. The outer index is the Name, and the inner index is the Year. In order to visualize this data, weâ€™ll use reset_index in order to set the index back to an integer and transform the Name and Year back into columnar data. # just run this cell counts_aggregated_by_name_and_year = counts_aggregated_by_name_and_year.reset_index() counts_aggregated_by_name_and_year Similar to before, we can plot the popularity of a given name by selecting the name we want to visualize. The code below is very similar to the plotting code above, except that we use query to get the name of interest instead of using a boolean array. Note: Here we use a special syntax @name_of_interest to tell the query command to use the python variable name_of_interest. Try out some other names and see what trends you observe. Note that since this is the American social security database, international names are not well represented. # just run this cell name_of_interest = 'Hillary' chosen_baby_name = counts_aggregated_by_name_and_year.query(\"Name == @name_of_interest\") plt.plot(chosen_baby_name['Year'], chosen_baby_name['Count']) plt.title(f\"Popularity Of {name_of_interest} Over Time\") plt.xlabel('Year') plt.ylabel('Count'); To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-07-15","objectID":"/datalab2/:4:1","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export(pdf=False) ","date":"2024-07-15","objectID":"/datalab2/:5:0","tags":["Pandas"],"title":"DATA100-lab2","uri":"/datalab2/"},{"categories":["DATA100"],"content":"distribution å®šä¹‰ ","date":"2024-07-15","objectID":"/datal7/:1:0","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"bar plots for distribution ","date":"2024-07-15","objectID":"/datal7/:2:0","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"data8 example ","date":"2024-07-15","objectID":"/datal7/:2:1","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"compound way ","date":"2024-07-15","objectID":"/datal7/:2:2","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"seaborn example import seaborn as sns sns.countplot(x='variable', data=df) # rug plot sns.rugplot(x='variable', data=df, color='black') ","date":"2024-07-15","objectID":"/datal7/:2:3","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"plotly example ","date":"2024-07-15","objectID":"/datal7/:2:4","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"å¤„ç†å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰å’Œå³°å€¼ï¼ˆmodeï¼‰ ","date":"2024-07-15","objectID":"/datal7/:3:0","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"density curve å¯†åº¦æ›²çº¿çœ‹å³° ç®±å‹å›¾ import seaborn as sns sns.boxplot(x='variable', data=df) violin plot å’Œç®±å‹å›¾å¯¹æ¯”æ¥çœ‹ï¼Œviolin plotå®½åº¦æœ‰æ„ä¹‰ import seaborn as sns sns.violinplot(x='variable', data=df) å¤„ç†overplotting random jitter ","date":"2024-07-15","objectID":"/datal7/:3:1","tags":null,"title":"DATA100-L7: Visualization â… ","uri":"/datal7/"},{"categories":["DATA100"],"content":"text data ","date":"2024-07-15","objectID":"/datal6/:1:0","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"python string methods .replace(str1, str2) .split('/') ","date":"2024-07-15","objectID":"/datal6/:1:1","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"regex å‚è€ƒToolsæ­£åˆ™è¡¨è¾¾å¼ç¬”è®° | ä¼˜å…ˆçº§è¾ƒä½ æ„Ÿå…´è¶£çš„ç»ƒä¹ â†“ https://alf.nu/RegexGolf ","date":"2024-07-15","objectID":"/datal6/:2:0","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"python re .sub() pattern(r\"......\") is a raw string, which means that backslashes are not interpreted as escape characters. eg: â€œ\\\\sectionâ€ in regular str, â€œ\\sectionâ€ in raw str. .findall(pattern, string) import re pattern = r'\\b\\w{3}\\b' string = 'The quick brown fox jumps over the lazy dog' matches = re.findall(pattern, string) print(matches) Output: ['The', 'fox', 'dog'] extract() extractall() æœ‰æ—¶ä¼šæœ‰ser.str.extract()å½¢å¼ï¼ ","date":"2024-07-15","objectID":"/datal6/:2:1","tags":null,"title":"DATA100-L6: Regex","uri":"/datal6/"},{"categories":["DATA100"],"content":"Discussion 2: Pandas Practice We will begin our discussion of Pandas. You will practice: Selecting columns Filtering with boolean conditions Counting with value_counts import pandas as pd import numpy as np ","date":"2024-07-15","objectID":"/datadisc02/:0:0","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Pandas Practise In the first Pandas question, we will be working with the elections dataset from lecture. elections = pd.read_csv(\"elections.csv\") # read in the elections data into a pandas dataframe! elections.head(5) Year Candidate Party Popular vote Result % 0 1824 Andrew Jackson Democratic-Republican 151271 loss 57.210122 1 1824 John Quincy Adams Democratic-Republican 113142 win 42.789878 2 1828 Andrew Jackson Democratic 642806 win 56.203927 3 1828 John Quincy Adams National Republican 500897 loss 43.796073 4 1832 Andrew Jackson Democratic 702735 win 54.574789 ","date":"2024-07-15","objectID":"/datadisc02/:1:0","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 5 We want to select the â€œPopular voteâ€ column as a pd.Series. Which of the following lines of code will error? elections['Popular vote'] elections.iloc['Popular vote'] elections.loc['Popular vote'] elections.loc[:, 'Popular vote'] elections.iloc[:, 'Popular vote'] Run each line in the cell below and see for yourself! # elections.iloc['Popular vote'] # wrong # elections.iloc[:, 'popular votes'] # wrong # elections['Popular vote'] # right # elections.loc['Popular vote'] # ket error # elections.loc[:,'Popular vote'] # right 0 151271 1 113142 2 642806 3 500897 4 702735 ... 173 62984828 174 732273 175 4489235 176 65853514 177 1457226 Name: Popular vote, Length: 178, dtype: int64 ","date":"2024-07-15","objectID":"/datadisc02/:1:1","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 6 Write one line of Pandas code that returns a pd.DataFrame that only contains election results from the 1900s. elections[(elections['Year'] \u003e= 1900) \u0026 (elections['Year'] \u003c 2000)] # æ³¨æ„æ˜¯ \u0026 Year Candidate Party Popular vote Result % 54 1900 John G. Woolley Prohibition 210864 loss 1.526821 55 1900 William Jennings Bryan Democratic 6370932 loss 46.130540 56 1900 William McKinley Republican 7228864 win 52.342640 57 1904 Alton B. Parker Democratic 5083880 loss 37.685116 58 1904 Eugene V. Debs Socialist 402810 loss 2.985897 ... ... ... ... ... ... ... 146 1996 Harry Browne Libertarian 485759 loss 0.505198 147 1996 Howard Phillips Taxpayers 184656 loss 0.192045 148 1996 John Hagelin Natural Law 113670 loss 0.118219 149 1996 Ralph Nader Green 685297 loss 0.712721 150 1996 Ross Perot Reform 8085294 loss 8.408844 97 rows Ã— 6 columns ","date":"2024-07-15","objectID":"/datadisc02/:1:2","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 7 Write one line of Pandas code that returns a pd.Series, where the index is the Party, and the values are how many times that party won an election. Hint: use value_counts. # Your answer here elections['Party'].value_counts() Party Democratic 46 Republican 40 Prohibition 11 Libertarian 11 Socialist 10 Independent 6 Whig 6 Green 6 Progressive 4 Populist 3 Constitution 3 American Independent 3 American 2 National Republican 2 Democratic-Republican 2 Reform 2 Free Soil 2 Anti-Masonic 1 National Union 1 Constitutional Union 1 National Democratic 1 Union Labor 1 Greenback 1 Anti-Monopoly 1 Liberal Republican 1 Southern Democratic 1 Northern Democratic 1 Farmerâ€“Labor 1 Dixiecrat 1 States' Rights 1 Communist 1 Union 1 Taxpayers 1 New Alliance 1 Citizens 1 Natural Law 1 Name: count, dtype: int64 ","date":"2024-07-15","objectID":"/datadisc02/:1:3","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Grading Assistance (Bonus) Fernando is writing a grading script to compute grades for students in Data 101. Recall that many factors go into computing a studentâ€™s final grade, including homework, discussion, exams, and labs. In this question, we will help Fernando compute the homework grades for all students using a DataFrame, hw_grades, provided by Gradescope. The Pandas DataFrame hw_grades contains homework grades for all students for all homework assignments, with one row for each combination of student and homework assignment. Any assignments that are incomplete are denoted by NaN (missing) values, and any late assignments are denoted by a True boolean value in the Late column. You may assume that the names of students are unique. Below is a sample of hw_grades. hw_grades = pd.read_csv(\"hw_grades.csv\") hw_grades.sample(5, random_state = 0) Name Assignment Grade Late 28 Sid Homework 9 82.517998 True 11 Ash Homework 2 78.264844 True 10 Ash Homework 1 98.421049 False 41 Emily Homework 2 62.900313 False 2 Meg Homework 3 89.785619 False ","date":"2024-07-15","objectID":"/datadisc02/:2:0","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8a Assuming there is a late penalty that causes a 10% grade reduction to the studentâ€™s current score (i.e. a 65% score would become a 65% - 6.5% = 58.5%), write a line of Pandas code to calculate all the homework grades, including the late penalty if applicable, and store it in a column named â€™LPGradeâ€™. # Your answer here hw_grades['LPGrade'] = hw_grades['Grade'] * (1 - hw_grades['Late'] * 0.1) # ç”¨ä¸ªéšå¼è½¬æ¢ hw_grades.head() Name Assignment Grade Late LPGrade 0 Meg Homework 1 NaN False NaN 1 Meg Homework 2 64.191844 False 64.191844 2 Meg Homework 3 89.785619 False 89.785619 3 Meg Homework 4 74.420033 False 74.420033 4 Meg Homework 5 74.372434 True 66.935190 ","date":"2024-07-15","objectID":"/datadisc02/:2:1","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8b Which of the following expressions outputs the studentsâ€™ names and number of late assignments, from least to greatest number of late assignments? hw_grades.groupby([â€™Nameâ€™]).sum().sort_values() hw_grades.groupby([â€™Nameâ€™, â€™Lateâ€™]).sum().sort_values() hw_grades.groupby([â€™Nameâ€™]).sum()[â€™Lateâ€™].sort_values() hw_grades.groupby([â€™Nameâ€™]).sum().sort_values()[â€™Lateâ€™] # Your answer here # hw_grades.groupby(['Name']).sum().sort_values() # \u003c---- Try to sort on df, but have to give 'by=...' into sort_values() hw_grades.groupby(['Name']).sum()['Late'].sort_values() Name Sid 1 Emily 2 Meg 2 Ash 3 Smith 3 Name: Late, dtype: int64 ","date":"2024-07-15","objectID":"/datadisc02/:2:2","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8c If each assignment is weighted equally, fill in the blanks below to calculate each studentâ€™s overall homework grade, including late penalties for any applicable assignments. Hint: Recall that incomplete assignments have NaN values. How can we use fillna to replace these null values? hw_grades._________(_______) \\ .groupby(___________)[____________] \\ .agg(____________) # Your answer here hw_grades.fillna(0)\\ .groupby(['Name'])['LPGrade']\\ .agg('mean') # Pythonä¸­ï¼Œåæ–œæ  \\ ç”¨ä½œè¡Œç»­å­—ç¬¦ï¼Œå®ƒå…è®¸ä½ å°†ä¸€è¡Œä»£ç åˆ†å‰²æˆå¤šè¡Œï¼Œä»¥æé«˜ä»£ç çš„å¯è¯»æ€§ã€‚è¿™åœ¨ç¼–å†™è¾ƒé•¿çš„ä¸€è¡Œä»£ç æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå¯ä»¥é¿å…ä»£ç è¿‡äºæ‹¥æŒ¤ï¼Œä½¿å¾—ä»£ç æ›´æ˜“äºé˜…è¯»å’Œç»´æŠ¤ã€‚ Name Ash 80.830657 Emily 84.297725 Meg 69.218137 Sid 63.020729 Smith 58.332233 Name: LPGrade, dtype: float64 ","date":"2024-07-15","objectID":"/datadisc02/:2:3","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"Question 8d Of all the homework assignments, which are the most difficult in terms of the median grade? Order by the median grade, from lowest to greatest. Do not consider incomplete assignments or late penalties in this calculation. Fill in the blanks below to answer this question. Hint: Recall that incomplete assignments have NaN values. How can we use dropna to remove these null values? hw_grades._________() \\ .groupby(___________)[____________] \\ .agg(____________) \\ .sort_values() # Your answer here hw_grades.dropna()\\ .groupby('Assignment')['Grade']\\ .agg('median')\\ .sort_values() Assignment Homework 2 64.160918 Homework 10 66.366211 Homework 5 74.372434 Homework 8 76.362904 Homework 4 78.207572 Homework 3 78.348163 Homework 9 82.517998 Homework 6 84.369535 Homework 1 85.473281 Homework 7 92.200688 Name: Grade, dtype: float64 ","date":"2024-07-15","objectID":"/datadisc02/:2:4","tags":null,"title":"DATA100-disc02","uri":"/datadisc02/"},{"categories":["DATA100"],"content":"EDA: Exploratory Data Analysis ","date":"2024-07-14","objectID":"/datal5/:0:0","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"Infinite loop DW \u0026 EDAâ€¦â€¦ DW: raw data -\u003e clean data -\u003e usable data ","date":"2024-07-14","objectID":"/datal5/:1:0","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"key data properties to consider in EDA ","date":"2024-07-14","objectID":"/datal5/:2:0","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"structure file format rectangular data: tables and matrices CSV, TabSV/TSV, json(is a dict) txt, XML pd.read_csv('filename.tsv',delimiter='\\t') turn to lec jupyter notebook to see more details å˜é‡ç§ç±» æ³¨æ„ï¼šä¸å”¯ä¸€ï¼Œä¸å…¨é¢ multiple files ä¸»é”®ï¼Ÿ ","date":"2024-07-14","objectID":"/datal5/:2:1","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"granularity(é¢—ç²’åº¦ï¼Ÿ) scope and temporality é¢—ç²’åº¦ scope: sampling frame temporality: time-series data unix time posix time ","date":"2024-07-14","objectID":"/datal5/:2:2","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["DATA100"],"content":"faithfulness missing data? ","date":"2024-07-14","objectID":"/datal5/:2:3","tags":null,"title":"DATA100-L5: Data Wrangling and EDA","uri":"/datal5/"},{"categories":["UCB-CS61B"],"content":"Exceptions throw statement: throws an exception public V get(K key) { int location = findKey(key); if (location \u003c 0) { throw new IllegalArgumentException(\"Key \" + key + \" does not exist in map.\"); } return values[findKey(key)]; } æ˜¾å¼æŠ›å‡ºå¼‚å¸¸ public static void main(String[] args) { System.out.println(\"ayyy lmao\"); throw new RuntimeException(\"For no reason.\"); } ","date":"2024-07-14","objectID":"/61b-14/:0:0","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"What has been Thrown, can be Caught Dog d = new Dog(\"Lucy\", \"Retriever\", 80); d.becomeAngry(); try { d.receivePat(); } catch (Exception e) { System.out.println( \"Tried to pat: \" + e); } System.out.println(d); ç”±callstacké¡ºåº exceptionæ˜¯ä¸€ç§å¯¹è±¡ï¼Œæœ‰æ—¶å¯èƒ½è§åˆ°çš„é”™è¯¯â†“ æœ€å¥½æ˜ç¡®æ€ä¹ˆå¤„ç†exception public static void gulgate() throws IOException { ... throw new IOException(\"hi\"); ... } æœ‰æ—¶éœ€è¦è€ƒè™‘mainæƒ…å†µ ä¸Šé¢æ²¡æœ‰æ˜ç¡®å¤„ç† checkedä¸å¦è§ç§ç±» Iteration åˆ›å»ºèƒ½æ”¯æŒfor (Item i : someIterable)çš„æƒ…å†µ ","date":"2024-07-14","objectID":"/61b-14/:1:0","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"The Iterable Interface public interface Iterable\u003cT\u003e { Iterator\u003cT\u003e iterator(); } package java.util; public interface Iterator\u003cT\u003e { boolean hasNext(); T next(); } ","date":"2024-07-14","objectID":"/61b-14/:2:0","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"å‡å¦‚è¦éå†ArrayMapï¼Œéœ€è¦å¯¹keyè¿›è¡Œæ“ä½œ åˆ°æ­¤å®Œæˆï¼Œå¯ä»¥æ‰§è¡Œéå† ","date":"2024-07-14","objectID":"/61b-14/:2:1","tags":null,"title":"61B-14: Exceptions, Iterators, Iterables","uri":"/61b-14/"},{"categories":["UCB-CS61B"],"content":"Packages and JAR Files ","date":"2024-07-14","objectID":"/61b-15/:0:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"åˆ›å»ºåŒ… At the top of every file in the package, put the package name. Make sure that the file is stored in a folder with the appropriate folder name. For a package with name ug.joshh.animal, use folder ug/joshh/animal. è¦ç”¨çš„æ—¶å€™importå³å¯ ","date":"2024-07-14","objectID":"/61b-15/:1:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"default package ","date":"2024-07-14","objectID":"/61b-15/:2:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"JAR Files Access Control Object Methods: Equals and toString( ) ","date":"2024-07-14","objectID":"/61b-15/:3:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"toString( ) ","date":"2024-07-14","objectID":"/61b-15/:4:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"== vs equals( ) == compares references equals( ) compares values, but pay attention to the type! public class Date { private final int month; private final int day; private final int year; public Date(int m, int d, int y) { month = m; day = d; year = y; } public boolean equals(Object x) { if (this == x) return true; if (x == null) return false; if (this.getClass() != x.getClass()) { return false; } Date that = (Date) x; if (this.day != that.day) { return false; } if (this.month != that.month) { return false; } if (this.year != that.year) { return false; } return true; } } ","date":"2024-07-14","objectID":"/61b-15/:5:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"Rules for Equals in Java åèº«æ€§ï¼šx.equals(x) == true å¯¹ç§°æ€§ ä¼ é€’æ€§ æ³¨æ„å®ç°equalsæ–¹æ³•æ—¶ï¼Œä¸è¦è¿èƒŒè¿™äº›æ€§è´¨ï¼ ","date":"2024-07-14","objectID":"/61b-15/:6:0","tags":null,"title":"61B-15: Packages, Access Control, Objects","uri":"/61b-15/"},{"categories":["UCB-CS61B"],"content":"61B: Writing Efficient Programs Programming cost. How long does it take to develop your programs? How easy is it to read, modify, and maintain your code? More important than you might think! Majority of cost is in maintenance, not development! è‡ªé¡¶å‘ä¸‹ï¼Œé€å±‚æŠ½è±¡ï¼Œåˆ†è€Œæ²»ä¹‹ï¼ŒåŒ–æ•´ä¸ºé›¶ ADT Implementations Designing ADTs è™½ç„¶extensionç®€å•ï¼Œä½†æ˜¯å§”æ‰˜delegationæ›´åŠ çµæ´» Views è§†å›¾ åœ¨Javaä¸­ï¼Œâ€œview\"é€šå¸¸æŒ‡çš„æ˜¯ä¸€ç§æ•°æ®ç»“æ„çš„è§†å›¾ï¼Œå®ƒæä¾›äº†ä¸€ç§è®¿é—®å’Œæ“ä½œåº•å±‚æ•°æ®çš„æ–¹å¼ï¼Œè€Œä¸éœ€è¦å¤åˆ¶æ•´ä¸ªæ•°æ®é›†ã€‚è§†å›¾çš„ä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒä»¬æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–¹å¼æ¥æ“ä½œæ•°æ®å­é›†ï¼Œè€Œä¸éœ€è¦å¤åˆ¶æ•°æ®ï¼Œä»è€ŒèŠ‚çœå†…å­˜å’Œæé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè§†å›¾ä¹Ÿæœ‰ä¸€äº›é™åˆ¶ï¼Œä¾‹å¦‚å›ºå®šå¤§å°çš„è§†å›¾ä¸èƒ½æ·»åŠ æˆ–åˆ é™¤å…ƒç´ ã€‚ Occasionally, implementation details may allow for views that are too difficult to implement for an abstract type. ","date":"2024-07-14","objectID":"/61b-16/:1:0","tags":null,"title":"61B-16: Encapsulation, Lists, Delegation vs. Extension","uri":"/61b-16/"},{"categories":["UCB-CS61B"],"content":"Programming in the Real World å¯¹æŠ€æœ¯è¦æ•¬ç• midterm review Comparing strings for equality using == vs .equals â€”\u003e see in autoboxing lecture åœ¨Javaä¸­ï¼Œthis æ˜¯ä¸€ä¸ªæŒ‡å‘å½“å‰å¯¹è±¡å®ä¾‹çš„å¼•ç”¨ã€‚å®ƒé€šå¸¸ç”¨äºå¼•ç”¨å½“å‰ç±»çš„å®ä¾‹æˆå‘˜ï¼Œæˆ–è€…åœ¨æ–¹æ³•ä¸­åŒºåˆ†æˆå‘˜å˜é‡å’Œå±€éƒ¨å˜é‡ã€‚ç„¶è€Œï¼Œä½ ä¸èƒ½å°† this é‡æ–°èµ‹å€¼ä¸ºå¦ä¸€ä¸ªå¯¹è±¡çš„å¼•ç”¨ï¼Œå› ä¸º this æ˜¯ä¸€ä¸ªå›ºå®šçš„æ¦‚å¿µï¼Œå®ƒä»£è¡¨å½“å‰å¯¹è±¡æœ¬èº«ã€‚ ä½ æä¾›çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œå°è¯•å°† this èµ‹å€¼ä¸ºä¸€ä¸ªæ–°çš„ Dog å¯¹è±¡ï¼Œè¿™æ˜¯ä¸å…è®¸çš„ã€‚Java ç¼–è¯‘å™¨ä¼šæŠ¥é”™ï¼Œå› ä¸ºå®ƒè¿åäº† this çš„ä½¿ç”¨è§„åˆ™ã€‚ public class Dog { public void f() { this = new Dog(); // è¿™è¡Œä»£ç ä¼šå¯¼è‡´ç¼–è¯‘é”™è¯¯ï¼Œå› ä¸ºä¸èƒ½é‡æ–°èµ‹å€¼this } } Dog d = new Dog(); d.f(); // è°ƒç”¨f()æ–¹æ³•ï¼Œä½†ç”±äºä¸Šé¢çš„é”™è¯¯ï¼Œè¿™è¡Œä»£ç å®é™…ä¸Šæ— æ³•æ‰§è¡Œ å¦‚æœä½ æƒ³è¦åˆ›å»ºä¸€ä¸ªæ–°çš„ Dog å¯¹è±¡å¹¶å°†å…¶å¼•ç”¨èµ‹ç»™ thisï¼Œä½ éœ€è¦ä½¿ç”¨å¦ä¸€ä¸ªå˜é‡ï¼Œæ¯”å¦‚ anotherDogã€‚ä¸‹é¢æ˜¯ä¿®æ”¹åçš„ä»£ç ç¤ºä¾‹ï¼š public class Dog { public void f() { Dog anotherDog = new Dog(); // åˆ›å»ºä¸€ä¸ªæ–°çš„Dogå¯¹è±¡ // è¿™é‡Œä½ å¯ä»¥ä½¿ç”¨anotherDogæ¥å¼•ç”¨æ–°åˆ›å»ºçš„Dogå¯¹è±¡ } } Dog d = new Dog(); d.f(); // ç°åœ¨f()æ–¹æ³•å¯ä»¥æ­£å¸¸æ‰§è¡Œï¼Œæ²¡æœ‰ç¼–è¯‘é”™è¯¯ åœ¨è¿™æ®µä¿®æ”¹åçš„ä»£ç ä¸­ï¼ŒanotherDog å˜é‡ç”¨äºå­˜å‚¨æ–°åˆ›å»ºçš„ Dog å¯¹è±¡çš„å¼•ç”¨ï¼Œè€Œ this ä»ç„¶ä¿æŒå…¶åŸå§‹å«ä¹‰ï¼Œå³æŒ‡å‘å½“å‰çš„ Dog å¯¹è±¡å®ä¾‹ã€‚ ","date":"2024-07-14","objectID":"/61b-12/:0:0","tags":null,"title":"61B-12:  Coding in the Real World, Review","uri":"/61b-12/"},{"categories":["UCB-CS61B"],"content":"13-16å‡ ä¹æ˜¯javaè¯­æ³•è®²è§£ğŸ˜„ ","date":"2024-07-14","objectID":"/61b-13/:0:0","tags":null,"title":"61B-13: Generics, Autoboxing","uri":"/61b-13/"},{"categories":["UCB-CS61B"],"content":"Primitives Cannot Be Used as Actual Type Arguments ","date":"2024-07-14","objectID":"/61b-13/:1:0","tags":null,"title":"61B-13: Generics, Autoboxing","uri":"/61b-13/"},{"categories":["UCB-CS61B"],"content":"Autoboxing Wrapper Types Are (Mostly) Just Like Any Class 8ç§åŸºæœ¬ç±»å‹ä¹‹é—´è½¬æ¢ä¹Ÿå­˜åœ¨widening Immutability ç±»ä¼¼äºconståœ¨cpp public class Date { public final int month; public final int day; public final int year; private boolean contrived = true; public Date(int m, int d, int y) { month = m; day = d; year = y; } } Warning: Declaring a reference as Final does not make object immutable. Example: public final ArrayDeque d = new ArrayDeque(); The d variable can never change, but the referenced deque can! è§æŒ‡é’ˆå¸¸é‡ä¸å¸¸é‡æŒ‡é’ˆçš„åŒºåˆ«in C++ Defining Generic Classes Goal 1: Create a class ArrayMap with the following methods: put(key, value): Associate key with value. If -1, adds k and v to the last position of the arrays. containsKey(key): Checks to see if arraymap contains the key. get(key): Returns value, assuming key exists.. keys(): Returns a list of all keys. size(): Returns number of keys. public class ArrayMap\u003cK, V\u003e { private K[] keys; private V[] values; private int size; public ArrayMap() { keys = (K[]) new Object[100]; values = (V[]) new Object[100]; size = 0; } private int findKey(K key) { for (int i = 0; i \u003c size; i++) { if (keys[i].equals(key)) { return i; } } return -1; } private int getKeyIndex(K key) { for (int i = 0; i \u003c size; i++) { if (keys[i].equals(key)) { return i; } return -1; } } public void put(K key, V value) { int i = getKeyIndex(key); if (i \u003e -1) { values[i] = value; return; } keys[size] = key; values[size] = value; size += 1; } public V get(K key) { return values[findKey(key)]; } public boolean containsKey(K key) { int i = findKey(key); return (i \u003e -1); } public List\u003cK\u003e keys() { List\u003cK\u003e list = new ArrayList\u003cK\u003e(); for (int i = 0; i \u003c size; i++) { list.add(keys[i]); } return list; } public int size() { return size; } } Generic Methods Goal: Create a class MapHelper with two methods: get(Map61B, key): Returns the value corresponding to the given key in the map if it exists, otherwise null. Unlike the ArrayMapâ€™s get method, which crashes if the key doesnâ€™t exist. maxKey(Map61B): Returns the maximum of all keys in the given ArrayMap. Works only if keys can be compared. public class MapHelper { public static \u003cX,Z\u003e Z get(ArrayMap\u003cX,Z\u003e map, X key) { if (map.containsKey(key)) { return map.get(key); } else { return null; } } public static \u003cX extends Comparable\u003cX\u003e,Z\u003e X maxKey(ArrayMap\u003cX,Z\u003e map) { X max = null; for (X key : map.keys()) { if (max == null || key.compareTo(max) \u003e 0) { max = key; } } return max; } } ","date":"2024-07-14","objectID":"/61b-13/:2:0","tags":null,"title":"61B-13: Generics, Autoboxing","uri":"/61b-13/"},{"categories":["UCB-CS61B"],"content":"Subtype Polymorphism æŒ‡çš„æ˜¯å¯ä»¥ä½¿ç”¨çˆ¶ç±»ç±»å‹çš„å¯¹è±¡æ¥å¼•ç”¨å­ç±»ç±»å‹çš„å®ä¾‹ã€‚ DIY Comparison æ¯”è¾ƒObjectç±»å¯¹è±¡æ—¶äº§ç”Ÿé—®é¢˜ï¼Œå¦‚ä½•æ¯”è¾ƒï¼Ÿï¼Ÿï¼Ÿ è€ƒè™‘å†™ä¸€ä¸ªæ¯”è¾ƒå™¨ï¼Œæ¯”è¾ƒä¸¤ä¸ªObjectå¯¹è±¡ åŠ æ·±ç¼–è¯‘ç†è§£ Comparable Interface public interface Comparable\u003cT\u003e { public int compareTo(T obj); } Comparator Interface public interface Comparator\u003cT\u003e { public int compare(T obj1, T obj2); } ","date":"2024-07-14","objectID":"/61b-10/:0:0","tags":null,"title":"61B-10: Subtype Polymorphism vs. HoFs","uri":"/61b-10/"},{"categories":["UCB-CS61B"],"content":"ä¸¤è€…çš„å…³ç³»â†“ æ€»ç»“ ","date":"2024-07-14","objectID":"/61b-10/:0:1","tags":null,"title":"61B-10: Subtype Polymorphism vs. HoFs","uri":"/61b-10/"},{"categories":["UCB-CS61B"],"content":"Java Libraries ","date":"2024-07-14","objectID":"/61b-11/:0:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"Collections Collections is a package in Java that provides various utility classes for working with collections. ","date":"2024-07-14","objectID":"/61b-11/:1:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"Taskså¼•å…¥ 3 tasks, given the text of a book: Create a list of all words in the book. Count the number of unique words. Keep track of the number of times that specific words are mentioned. #1 way set public static int countUniqueWords(List\u003cString\u003e words) { Set\u003cString\u003e ss = new HashSet\u003c\u003e(); for (String s : words) { ss.add(s); } return ss.size(); } public static int countUniqueWords(List\u003cString\u003e words) { Set\u003cString\u003e ss = new HashSet\u003c\u003e(); ss.addAll(words); return ss; } #2 way map public static Map\u003cString, Integer\u003e collectWordCount(List\u003cString\u003e words, List\u003cString\u003e targets) { Map\u003cString, Integer\u003e wordCounts = new HashMap\u003c\u003e(); for (String s : targets) { wordCounts.put(s, 0); } for (String s : words) { if (wordCounts.containsKey(s)) { int oldCount = wordCounts.get(s); wordCounts.put(s, oldCount + 1); } } return wordCounts; } Pythoné‡Œé¢åˆ™æ˜¯dictå®ç°ï¼Œå’‹ä¸€çœ‹ä¼¼ä¹æ›´å¥½ï¼Ÿä½†æ˜¯ï¼Ÿ Java9æ–°ç‰¹ç‚¹ï¼š Interfaces and Abstract Classes More interface details: Can provide variables, but they are public static final. final means the value can never change. A class can implement multiple interfaces. ","date":"2024-07-14","objectID":"/61b-11/:2:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"interface summary ","date":"2024-07-14","objectID":"/61b-11/:3:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"abstract class intro ","date":"2024-07-14","objectID":"/61b-11/:4:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"ä¸¤è€…å¯¹æ¯” Packages A package is a namespace that organizes classes and interfaces. ","date":"2024-07-14","objectID":"/61b-11/:5:0","tags":null,"title":"61B-11: Libraries, Abstract Classes, Packages","uri":"/61b-11/"},{"categories":["UCB-CS61B"],"content":"Implementation Inheritance: Extends extends Because of extends, RotatingSLList inherits all members of SLList: All instance and static variables. ï¼ˆæ³¨æ„public, private, protectedçš„åŒºåˆ«ï¼‰ All methods. All nested classes. Constructors are not inherited. super public class VengefulSLList\u003cItem\u003e extends SLList\u003cItem\u003e { private SLList\u003cItem\u003e deletedItems; public VengefulSLList() { deletedItems = new SLList\u003cItem\u003e(); } @Override public Item removeLast() { Item oldBack = super.removeLast(); deletedItems.addLast(oldBack); return oldBack; } public void printLostItems() { deletedItems.print(); } } æ³¨æ„æ²¡æœ‰super.superçš„æƒ…å†µ ","date":"2024-07-14","objectID":"/61b-9/:0:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"constructor ","date":"2024-07-14","objectID":"/61b-9/:1:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"Object class Encapsulation ","date":"2024-07-14","objectID":"/61b-9/:2:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"Module Module: A set of methods that work together as a whole to perform some task or set of related tasks. Implementation Inheritance Breaks Encapsulation æ³¨æ„private è¿˜æœ‰ åå¤è‡ªæˆ‘è°ƒç”¨â†“ Type Checking and Casting å­ç±»èµ‹å€¼ç»™åŸºç±»å¯ä»¥ï¼Œåä¹‹ä¸è¡Œ Dynamic Method Selection and Casting Puzzle Higher Order Functions (A First Look) æ‰§è¡Œç±»ä¼¼f(f(x))çš„æ“ä½œ Java7åŠä¹‹å‰ä¸èƒ½ä½¿ç”¨å‡½æ•°æŒ‡é’ˆï¼Œè€ƒè™‘å®ä½“åŒ–ä¸€ä¸ªå‡½æ•°å¯¹è±¡ Java8åŠå…¶ä»¥åï¼š ä¸€å¼ å›¾æ€»ç»“ç»§æ‰¿ ","date":"2024-07-14","objectID":"/61b-9/:3:0","tags":null,"title":"61B-9: Extends, Casting, Higher Order Functions","uri":"/61b-9/"},{"categories":["UCB-CS61B"],"content":"Ad Hoc Testing vs. JUnit public class TestSort { /** Tests the sort method of the Sort class. */ public static testSort() { String[] input = {\"cows\", \"dwell\", \"above\", \"clouds\"}; String[] expected = {\"above\", \"cows\", \"clouds\", \"dwell\"}; Sort.sort(input); org.junit.Assert.assertArrayEquals(expected, input); } public static void main(String[] args) { testSort(); } } Selection Sort ç®€å•ä»‹ç»ä¸€ä¸‹äº†ï¼Œå…³æ³¨ç‚¹åœ¨junit Simpler JUnit Tests ADD, TDD, Integration Testing More On JUnit (Extra) ","date":"2024-07-14","objectID":"/61b-7/:0:0","tags":null,"title":"61B-7: Testing","uri":"/61b-7/"},{"categories":["UCB-CS61B"],"content":" but hard to maintain! Hypernyms, Hyponyms, and Interface Inheritance ","date":"2024-07-14","objectID":"/61b-8/:0:0","tags":null,"title":"61B-8: Inheritance, Implements","uri":"/61b-8/"},{"categories":["UCB-CS61B"],"content":"interface public interface List61B\u003cItem\u003e { public void addFirst(Item x); public void addLast(Item y); public Item getFirst(); public Item getLast(); public Item removeLast(); public Item get(int i); public void insert(Item x, int position); public int size(); } Overriding vs. Overloading overrideæ³¨æ„åŠ ä¸Š@Override!!! Interface Inheritance åŸºç±»å­˜æ”¾æŒ‡é’ˆé—®é¢˜ Answer: If X is a superclass of Y, then memory boxes for X may contain Y. An AList is-a List. Therefore List variables can hold ALList addresses. Implementation Inheritance: Default Methods public interface List61B\u003cItem\u003e { public void addFirst(Item x); public void addLast(Item y); public Item getFirst(); public Item getLast(); public Item removeLast(); public Item get(int i); public void insert(Item x, int position); public int size(); default public void print() { for (int i = 0; i \u003c size(); i += 1) { System.out.print(get(i) + \" \"); } System.out.println(); } } Static and Dynamic Type, Dynamic Method Selection âš ï¸ âš ï¸ âš ï¸ More Dynamic Method Selection, Overloading vs. Overriding âš ï¸ âš ï¸ âš ï¸ ç´§ç´§ç›¯ä½ç»†èŠ‚ï¼ Interface vs. Implementation Inheritance ","date":"2024-07-14","objectID":"/61b-8/:1:0","tags":null,"title":"61B-8: Inheritance, Implements","uri":"/61b-8/"},{"categories":["UCB-CS61B"],"content":"From IntList to SLList äº‹å®æ˜¯åœ¨SLListé‡Œé¢æ·»åŠ ä¸€ä¸ªIntlistæ•°æ®æˆå‘˜ Public vs. Private or Nested Classes ä»‹ç»äº†private ","date":"2024-07-14","objectID":"/61b-4/:0:0","tags":null,"title":"61B-4: SLLists, Nested Classes, Sentinel Nodes","uri":"/61b-4/"},{"categories":["UCB-CS61B"],"content":"Nested Classes public class SLList { public class IntNode { public int item; public IntNode next; public IntNode(int i, IntNode n) { item = i; next = n; } } private IntNode first; public SLList(int x) { first = new IntNode(x, null); } ... } addLast() and size() è®¨è®ºrecursionå’Œiterationä¸¤ç§æ€è·¯ï¼Œç•¥è¿‡ ç©ºé—´æ¢æ—¶é—´åˆè§ Sentinel Nodes å¯¹è¾¹ç•Œç°è±¡ï¼ˆç©ºæŒ‡é’ˆï¼‰çš„è®¨è®ºå’Œä¿æŠ¤ ç®€åŒ–ä»£ç .addLast(x) ","date":"2024-07-14","objectID":"/61b-4/:1:0","tags":null,"title":"61B-4: SLLists, Nested Classes, Sentinel Nodes","uri":"/61b-4/"},{"categories":["UCB-CS61B"],"content":"Doubly Linked Lists æ³¨æ„åªæœ‰sentinelæ—¶è¦è®¨è®ºä¸€äº›ç‰¹æ®Šæƒ…å†µï¼Œç‰¹åˆ«æ˜¯ç¯çŠ¶é“¾è¡¨ã€‚ Generic Lists æ³›å‹åˆ—è¡¨ public class SLList\u003cBleepBlorp\u003e { private IntNode sentinel; private int size; public class IntNode { public BleepBlorp item; public IntNode next; ... } ... } SLList\u003cInteger\u003e s1 = new SLList\u003c\u003e(5); s1.insertFront(10); SLList\u003cString\u003e s2 = new SLList\u003c\u003e(\"hi\"); s2.insertFront(\"apple\"); Arrays, AList ä»‹ç»äº†System.arraycopy()ç”¨æ¥resize 2D Arrays Arrays vs. Classes arrayçš„runtimeåŠ¨æ€ç´¢å¼•ï¼ˆå’Œcppä¸ä¸€æ ·ï¼‰ class runtime ","date":"2024-07-14","objectID":"/61b-5/:0:0","tags":null,"title":"61B-5: DLLists, Arrays","uri":"/61b-5/"},{"categories":["UCB-CS61B"],"content":"A Last Look at Linked Lists Naive Array Lists public class AList { private int[] items; private int size; public AList() { items = new int[100]; size = 0; } public void addLast(int x) { items[size] = x; size += 1; } public int getLast() { return items[size - 1]; } public int get(int i) { return items[i]; } public int size() { return size; } } Resizing Arrays public void addLast(int x) { if (size == items.length) { int[] a = new int[size + 1]; System.arraycopy(items, 0, a, 0, size); items = a; } items[size] = x; size += 1; } private void resize(int capacity) { int[] a = new int[capacity]; System.arraycopy(items, 0, a, 0, size); items = a; } public void addLast(int x) { if (size == items.length) { resize(size + 1); } items[size] = x; size += 1; } å‡ ä½•resize ","date":"2024-07-14","objectID":"/61b-6/:0:0","tags":null,"title":"61B-6: ALists, Resizing, vs. SLists","uri":"/61b-6/"},{"categories":["UCB-CS61B"],"content":"memory usage ä»”ç»†è€ƒé‡ Generic ALists public class AList\u003cGlorp\u003e { private Glorp[] items; private int size; public AList() { items = (Glorp []) new Object[8]; size = 0; } private void resize(int cap) { Glorp[] a = (Glorp []) new Object[cap]; // reinterprets as Glorp[] System.arraycopy(items, 0, a, 0, size); items = a; } public Glorp get(int i) { return items[i]; } public Glorp deleteBack() { Glorp returnItem = getBack(); items[size - 1] = null; // to help garbage collection size -= 1; return returnItem; } ... ","date":"2024-07-14","objectID":"/61b-6/:1:0","tags":null,"title":"61B-6: ALists, Resizing, vs. SLists","uri":"/61b-6/"},{"categories":["UCB-CS61B"],"content":"java oop Java is an object oriented language with strict requirements: Every Java file must contain a class declaration*. All code lives inside a class*, even helper functions, global constants, etc. To run a Java program, you typically define a main method using public static void main(String[] args) *: This is not completely true, e.g. we can also declare â€œinterfacesâ€ in .Java files that may contain code. Weâ€™ll cover these later. ","date":"2024-07-14","objectID":"/61b-1/:1:0","tags":null,"title":"61B-1: Intro, Hello World Java","uri":"/61b-1/"},{"categories":["UCB-CS61B"],"content":"Java and Static Typing The compiler checks that all the types in your program are compatible before the program ever runs! This is unlike a language like Python, where type checks are performed DURING execution. ","date":"2024-07-14","objectID":"/61b-1/:2:0","tags":null,"title":"61B-1: Intro, Hello World Java","uri":"/61b-1/"},{"categories":["UCB-CS61B"],"content":"ç¼–è¯‘åˆçœ‹ ","date":"2024-07-14","objectID":"/61b-2/:1:0","tags":null,"title":"61B-2: Defining and Using Classes","uri":"/61b-2/"},{"categories":["UCB-CS61B"],"content":"Defining and Instantiating Classes Static vs. Instance Members ","date":"2024-07-14","objectID":"/61b-2/:2:0","tags":null,"title":"61B-2: Defining and Using Classes","uri":"/61b-2/"},{"categories":["UCB-CS61B"],"content":"Static vs. Non-static public static void main(String[] args) Using Libraries ","date":"2024-07-14","objectID":"/61b-2/:3:0","tags":null,"title":"61B-2: Defining and Using Classes","uri":"/61b-2/"},{"categories":["UCB-CS61B"],"content":"Primitive Types 8 primitive types in Java: byte, short, int, long, float, double, boolean, char Everything else, including arrays, is a reference type. ","date":"2024-07-14","objectID":"/61b-3/:0:0","tags":null,"title":"61B-3: References, Recursion, and Lists","uri":"/61b-3/"},{"categories":["UCB-CS61B"],"content":"The Golden Rule of Equals (GRoE) Given variables y and x: y = x copies all the bits from x into y. Reference Types Everything else, including arrays, is a reference type. å’Œcppçš„åŒºåˆ«ä¹‹ä¸€ä¸æ˜¾å¼ä½¿ç”¨æŒ‡é’ˆ Parameter Passing pass by value ğŸ˜‹ pass by referenceï¼ˆæŸç§æ„ä¹‰ä¸Šjavaçº¯çº¯pass by value ğŸ˜ï¼‰ Instantiation of Arrays IntList and Linked Data Structures å•é“¾è¡¨ï¼Œä¸èµ˜è¿° ","date":"2024-07-14","objectID":"/61b-3/:1:0","tags":null,"title":"61B-3: References, Recursion, and Lists","uri":"/61b-3/"},{"categories":null,"content":"CS61ABC DATA100(DS100) è§¦åŠ¨çš„ç¬é—´ğŸ˜‹ ","date":"2024-07-14","objectID":"/beyondcode/thinking1/:0:0","tags":null,"title":"Thinking1","uri":"/beyondcode/thinking1/"},{"categories":null,"content":"è®°å½•2024å¯’å‡æ—¶å€™çš„æ€è€ƒ ","date":"2024-07-14","objectID":"/beyondcode/thinking0/:0:0","tags":null,"title":"Thinking0","uri":"/beyondcode/thinking0/"},{"categories":["DATA100"],"content":"lambda function lambda x: x**2 éæ˜¾å¼å®šä¹‰å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨lambdaè¡¨è¾¾å¼æ¥å®šä¹‰ä¸€ä¸ªå‡½æ•°ã€‚ This is a lambda function that takes in one argument x and returns the square of x. å†çœ‹sort_values() æ³¨æ„ä¼ é€’key df.sort_values(by='column_name', keys=lambda x: x.str.lower(), ascending=True) ","date":"2024-07-14","objectID":"/datal4/:1:0","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":"add move modify and so on sort by length of string approach1: create a new column and add to original df newdf = df[\"Name\"].str.len() # create a new column with length of each string df['length'] = newdf df.sort_values(by='length', ascending=True) drop column ","date":"2024-07-14","objectID":"/datal4/:2:0","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":"groupby.agg ","date":"2024-07-14","objectID":"/datal4/:3:0","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":"never use loops in this class! df.groupby('column_name').agg(f) f is a dictionary of functions to apply to each column. The function can be a lambda function or a named function. ","date":"2024-07-14","objectID":"/datal4/:3:1","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":"groupby type: pandas.core.groupby.generic.DataFrameGroupBy ","date":"2024-07-14","objectID":"/datal4/:4:0","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":"filter df.groupby('column_name').filter(lambda x: x['column_name'].mean() \u003e 10) This will return a new DataFrame with only the groups that have a mean value greater than 10. ","date":"2024-07-14","objectID":"/datal4/:4:1","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":"multi index å¤šç»´ç´¢å¼• using pivot_table() df.pivot_table(index=['column1', 'column2'], columns='column3', values='column4', aggfunc='mean') This will create a multi-index pivot table with the specified index and columns, and the mean of column4 for each group. using groupby() df.groupby(['column1', 'column2']).agg({'column3': 'mean', 'column4':'sum'}) This will group the DataFrame by column1 and column2, and calculate the mean and sum of column3 and column4 for each group. ","date":"2024-07-14","objectID":"/datal4/:4:2","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":"joining tables left.merge(right, on='column_name', how='inner', lefton='column_name_left', righton='column_name_right') ","date":"2024-07-14","objectID":"/datal4/:5:0","tags":["pandas"],"title":"DATA100-L4: Pandas â…¡","uri":"/datal4/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"hw01.ipynb\") HW 1: Math Review and Plotting é‡ç‚¹åœ¨codingï¼Œç†è®ºä¾›å‚è€ƒ ","date":"2024-07-14","objectID":"/datahw01/:0:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Due Date: Thursday Jan 27, 11:59 PM ","date":"2024-07-14","objectID":"/datahw01/:1:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Collaboration Policy Data science is a collaborative activity. While you may talk with others about the homework, we ask that you write your solutions individually. If you do discuss the assignments with others please include their names at the top of your notebook. Collaborators: list collaborators here ","date":"2024-07-14","objectID":"/datahw01/:2:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"This Assignment The purpose of this assignment is for you to combine Python, math, and the ideas in Data 8 to draw some interesting conclusions. The methods and results will help build the foundation of Data 100. ","date":"2024-07-14","objectID":"/datahw01/:3:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Score Breakdown Question Points 1a 1 1b 2 2a 1 2b 1 2c 2 2d 2 2e 1 3a 2 3b 2 3c 1 3d 2 3e 2 4a 1 4b 1 4c 1 4d 1 5a 1 5b 1 5d 3 6a 2 6b(i) 2 6b(ii) 2 6c 2 Total 36 ","date":"2024-07-14","objectID":"/datahw01/:4:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Before You Start For each question in the assignment, please write down your answer in the answer cell(s) right below the question. We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, NEVER add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file. Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder. Please be sure to check your results carefully. ","date":"2024-07-14","objectID":"/datahw01/:5:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Initialize your environment This cell should run without error if youâ€™re using the course Jupyter Hub or you have set up your personal computer correctly. import numpy as np import matplotlib import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') ","date":"2024-07-14","objectID":"/datahw01/:5:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: Jupyter Shortcuts Here are some useful Jupyter notebook keyboard shortcuts. To learn more keyboard shortcuts, go to Help -\u003e Keyboard Shortcuts in the menu above. Here are a few we like: ctrl+return : Evaluate the current cell shift+return: Evaluate the current cell and move to the next esc : command mode (may need to press before using any of the commands below) a : create a cell above b : create a cell below dd : delete a cell m : convert a cell to markdown y : convert a cell to code ","date":"2024-07-14","objectID":"/datahw01/:5:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: NumPy You should be able to understand the code in the following cells. If not, review the following: The Data 8 Textbook Chapter on NumPy DS100 NumPy Review Condensed NumPy Review The Official NumPy Tutorial Jupyter pro-tip: Pull up the docs for any function in Jupyter by running a cell with the function name and a ? at the end: np.arange? \u001b[1;31mDocstring:\u001b[0m arange([start,] stop[, step,], dtype=None, *, device=None, like=None) Return evenly spaced values within a given interval. ``arange`` can be called with a varying number of positional arguments: * ``arange(stop)``: Values are generated within the half-open interval ``[0, stop)`` (in other words, the interval including `start` but excluding `stop`). * ``arange(start, stop)``: Values are generated within the half-open interval ``[start, stop)``. * ``arange(start, stop, step)`` Values are generated within the half-open interval ``[start, stop)``, with spacing between values given by ``step``. For integer arguments the function is roughly equivalent to the Python built-in :py:class:`range`, but returns an ndarray rather than a ``range`` instance. When using a non-integer step, such as 0.1, it is often better to use `numpy.linspace`. See the Warning sections below for more information. Parameters ---------- start : integer or real, optional Start of interval. The interval includes this value. The default start value is 0. stop : integer or real End of interval. The interval does not include this value, except in some cases where `step` is not an integer and floating point round-off affects the length of `out`. step : integer or real, optional Spacing between values. For any output `out`, this is the distance between two adjacent values, ``out[i+1] - out[i]``. The default step size is 1. If `step` is specified as a position argument, `start` must also be given. dtype : dtype, optional The type of the output array. If `dtype` is not given, infer the data type from the other input arguments. device : str, optional The device on which to place the created array. Default: None. For Array-API interoperability only, so must be ``\"cpu\"`` if passed. .. versionadded:: 2.0.0 like : array_like, optional Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as ``like`` supports the ``__array_function__`` protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument. .. versionadded:: 1.20.0 Returns ------- arange : ndarray Array of evenly spaced values. For floating point arguments, the length of the result is ``ceil((stop - start)/step)``. Because of floating point overflow, this rule may result in the last element of `out` being greater than `stop`. Warnings -------- The length of the output might not be numerically stable. Another stability issue is due to the internal implementation of `numpy.arange`. The actual step value used to populate the array is ``dtype(start + step) - dtype(start)`` and not `step`. Precision loss can occur here, due to casting or due to using floating points when `start` is much larger than `step`. This can lead to unexpected behaviour. For example:: \u003e\u003e\u003e np.arange(0, 5, 0.5, dtype=int) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \u003e\u003e\u003e np.arange(-3, 3, 0.5, dtype=int) array([-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8]) In such cases, the use of `numpy.linspace` should be preferred. The built-in :py:class:`range` generates :std:doc:`Python built-in integers that have arbitrary size \u003cpython:c-api/long\u003e`, while `numpy.arange` produces `numpy.int32` or `numpy.int64` numbers. This may result in incorrect results for large integer values:: \u003e\u003e\u003e power = 40 \u003e\u003e\u003e modulo = 10000 \u003e\u003e\u003e x1 = [(n ** power) % modulo for n in range(8)] \u003e\u003e\u003e x2 = [(n ** power) % modulo for n in np.arange(8)] \u003e\u003e\u003e print(x1) [0, 1, 7776, 8801, 6176, 625, 6576, 4001] # correct \u003e\u003e\u003e print(x2) [0, 1, 7776, 7185, 0, 5969, 4816, 3361] # incorrect See Also -------- numpy","date":"2024-07-14","objectID":"/datahw01/:5:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: LaTeX You should use LaTeX to format math in your answers. If you arenâ€™t familiar with LaTeX, not to worry. Itâ€™s not hard to use in a Jupyter notebook. Just place your math in between dollar signs within Markdown cells: $ f(x) = 2x $ becomes $ f(x) = 2x $. If you have a longer equation, use double dollar signs to place it on a line by itself: $$ \\sum_{i=0}^n i^2 $$ becomes: $$ \\sum_{i=0}^n i^2$$ You can align multiple lines using the \u0026 anchor, \\\\ newline, in an align block as follows: \\begin{align} f(x) \u0026= (x - 1)^2 \\\\ \u0026= x^2 - 2x + 1 \\end{align} becomes \\begin{align} f(x) \u0026= (x - 1)^2 \\ \u0026= x^2 - 2x + 1 \\end{align} This PDF has some handy LaTeX. For more about basic LaTeX formatting, you can read this article. ","date":"2024-07-14","objectID":"/datahw01/:5:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Preliminary: Sums Hereâ€™s a recap of some basic algebra written in sigma notation. The facts are all just applications of the ordinary associative and distributive properties of addition and multiplication, written compactly and without the possibly ambiguous â€œâ€¦â€. But if you are ever unsure of whether youâ€™re working correctly with a sum, you can always try writing $\\sum_{i=1}^n a_i$ as $a_1 + a_2 + \\cdots + a_n$ and see if that helps. You can use any reasonable notation for the index over which you are summing, just as in Python you can use any reasonable name in for name in list. Thus $\\sum_{i=1}^n a_i = \\sum_{k=1}^n a_k$. $\\sum_{i=1}^n (a_i + b_i) = \\sum_{i=1}^n a_i + \\sum_{i=1}^n b_i$ $\\sum_{i=1}^n d = nd$ $\\sum_{i=1}^n (ca_i + d) = c\\sum_{i=1}^n a_i + nd$ These properties may be useful in the Least Squares Predictor question. To see the LaTeX we used, double-click this cell. Evaluate the cell to exit. ","date":"2024-07-14","objectID":"/datahw01/:5:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 1: Calculus In this question we will review some fundamental properties of the sigmoid function, which will be discussed when we talk more about logistic regression in the latter half of the class. The sigmoid function is defined to be $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ ","date":"2024-07-14","objectID":"/datahw01/:6:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 1a Show that $\\sigma(-x) = 1 - \\sigma(x)$. Note, again: In this class, you must always put your answer in the cell that immediately follows the question. DO NOT create any cells between this one and the one that says Type your answer here, replacing this text. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:6:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 1b Show that the derivative of the sigmoid function can be written as: $$\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$ This PDF has some handy LaTeX. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:6:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2: Probabilities and Proportions Much of data analysis involves interpreting proportions â€“ lots and lots of related proportions. So letâ€™s recall the basics. It might help to start by reviewing the main rules from Data 8, with particular attention to whatâ€™s being multiplied in the multiplication rule. ","date":"2024-07-14","objectID":"/datahw01/:7:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2a The Pew Research Foundation publishes the results of numerous surveys, one of which is about the trust that Americans have in groups such as the military, scientists, and elected officials to act in the public interest. A table in the article summarizes the results. Pick one of the options (i) and (ii) to answer the question below; if you pick (i), fill in the blank with the percent. Then, explain your choice. The percent of surveyed U.S. adults who had a great deal of confidence in both scientists and religious leaders (i) is equal to ______________________. (ii) cannot be found with the information in the article. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:7:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2b In a famous (or infamous) survey, members of the Harvard medical school were asked to consider a scenario in which â€œa test to detect a disease whose prevalence is 1/1,000 has a false positive rate of 5 percentâ€. The terminology, the specific question asked in the survey, and the answer, are discussed in detail in a Stat 88 textbook section that you are strongly encouraged to read. As Stat 88 is a Data 8 connector course, the section is another look at the same ideas as in the corresponding Data 8 textbook section. The corresponding tree diagram is copied below for your reference. The survey did not provide the true positive rate. The respondents and Stat 88 were allowed to assume that the true positive rate is 1, but we will not do so here. Let the true positive rate be some unknown proportion $p$. Suppose a person is picked at random from the population. Let $N$ be the event that the person doesnâ€™t have the disease and let $T_N$ be the event that the personâ€™s test result is negative. Fill in Blanks 1 and 2 with options chosen from (1)-(9). The proportion $P(N \\mid T_N)$ is the number of people who $\\underline{1}$ relative to the total number of people who $\\underline{2}$. (1) are in the population (2) have the disease (3) donâ€™t have the disease (4) test positive (5) test negative (6) have the disease and test positive (7) have the disease and test negative (8) donâ€™t have the disease and test positive (9) donâ€™t have the disease and test negative Assign the variable q4bi to your answer to the first blank and q4bii to your answer to the second blank. q4bi = ... q4bii = ... q4bi, q4bii grader.check(\"q2b\") ","date":"2024-07-14","objectID":"/datahw01/:7:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2c (This is a continuation of the previous part.) Define a function no_disease_given_negative that takes $p$ as its argument and returns $P(N \\mid T_N)$. def no_disease_given_negative(p): ... grader.check(\"q4c\") ","date":"2024-07-14","objectID":"/datahw01/:7:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2d (This part is a continuation of the previous two.) Pick all of the options (i)-(iv) that are true for all values of $p$. Explain by algebraic or probailistic reasoning; you are welcome to use your function no_disease_given_negative to try a few cases numerically. Your explanation should include the reasons why you didnâ€™t choose some options. $P(N \\mid T_N)$ is (i) equal to $0.95$. (ii) equal to $0.999 \\times 0.95$. (iii) greater than $0.999 \\times 0.95$. (iv) greater than $0.95$. Type your answer here, replacing this text. # Use this cell for experimenting if you wish, but your answer should be written in the cell above. ","date":"2024-07-14","objectID":"/datahw01/:7:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 2e Suzuki is one of most commonly owned makes of cars in our county (Alameda). A car heading from Berkeley to San Francisco is pulled over on the freeway for speeding. Suppose I tell you that the car is either a Suzuki or a Lamborghini, and you have to guess which of the two is more likely. What would you guess, and why? Make some reasonable assumptions and explain them (data scientists often have to do this), justify your answer, and say how itâ€™s connected to the previous parts. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:7:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3: Distributions Visualizing distributions, both categorical and numerical, helps us understand variability. In Data 8 you visualized numerical distributions by drawing histograms, which look like bar charts but represent proportions by the areas of the bars instead of the heights or lengths. In this exercise you will use the hist function in matplotlib instead of the corresponding Table method to draw histograms. To start off, suppose we want to plot the probability distribution of the number of spots on a single roll of a die. That should be a flat histogram since the chance of each of the values 1 through 6 is 1/6. Here is a first attempt at drawing the histogram. faces = range(1, 7) plt.hist(faces) (array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1.]), array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]), \u003cBarContainer object of 10 artists\u003e) This default plot is not helpful. We have to choose some arguments to get a visualization that we can interpret. Note that the second printed line shows the left ends of the default bins, as well as the right end of the last bin. The first line shows the counts in the bins. If you donâ€™t want the printed lines you can add a semi-colon at the end of the call to plt.hist, but weâ€™ll keep the lines for now. Letâ€™s redraw the histogram with bins of unit length centered at the possible values. By the end of the exercise youâ€™ll see a reason for centering. Notice that the argument for specifying bins is the same as the one for the Table method hist. unit_bins = np.arange(0.5, 6.6) plt.hist(faces, bins = unit_bins) (array([1., 1., 1., 1., 1., 1.]), array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]), \u003cBarContainer object of 6 artists\u003e) We need to see the edges of the bars! Letâ€™s specify the edge color ec to be white. Here are all the colors you could use, but do try to drag yourself away from the poetic names. plt.hist(faces, bins = unit_bins, ec='white') (array([1., 1., 1., 1., 1., 1.]), array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]), \u003cBarContainer object of 6 artists\u003e) Thatâ€™s much better, but look at the vertical axis. It is not drawn to the density scale defined in Data 8. We want a histogram of a probability distribution, so the total area should be 1. We just have to ask for that. plt.hist(faces, bins = unit_bins, ec='white', density=True) (array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667]), array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]), \u003cBarContainer object of 6 artists\u003e) Thatâ€™s the probability histogram of the number of spots on one roll of a die. The proportion is $1/6$ in each of the bins. Note: You may notice that running the above cells also displayed the return value of the last function call of each cell. This was intentional on our part to show you how plt.hist() (documentation) returned different values per plot. Note 2: Going forward, you can use a semicolon ; on the last line to suppress additional display, as below. plt.hist(faces, bins = unit_bins, ec='white', density=True); ","date":"2024-07-14","objectID":"/datahw01/:8:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3a Define a function integer_distribution that takes an array of integers and draws the histogram of the distribution using unit bins centered at the integers and white edges for the bars. The histogram should be drawn to the density scale. The left-most bar should be centered at the smallest integer in the array, and the right-most bar at the largest. Your function does not have to check that the input is an array consisting only of integers. The display does not need to include the printed proportions and bins. If you have trouble defining the function, go back and carefully read all the lines of code that resulted in the probability histogram of the number of spots on one roll of a die. Pay special attention to the bins. def integer_distribution(x): bins = np.arange(min(x) - 0.5, max(x) + 1.5) plt.hist(x, bins=bins, ec='white',density=True) integer_distribution(faces) ","date":"2024-07-14","objectID":"/datahw01/:8:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3b (Note: You can complete this part with just prerequisite knowledge for Data 100. That being said, Lecture 2 provides additional historical context and definitions for probability sample, sampling bias, and chance error). One way to use probability samples is to quantify sampling bias and chance error. Put briefly, if we assume that a sample distribution was selected at random from a known population, then we can quantify how likely that sample is to have arisen due to random chance (chance error). If the difference in sample and population distributions is too great, then we suspect that the given sample has bias in how it was selected from the population. Letâ€™s see this process in a post-analysis of pre-election polling of the 1936 U.S. Presidential Election. Through the U.S. electoral college process (weâ€™ll ignore it in this question, but read more here), Franklin D. Roosevelt won the election by an overwhelming margin. The popular vote results were approximately 61% Roosevelt (Democrat, incumbent), 37% Alf Landon (Republican), and 2% other candidates. For this problem, this is our population distribution. You can use np.random.multinomial to simulate drawing at random with replacement from a categorical distribution. The arguments are the sample size n and an array pvals of the proportions in all the categories. The function simulates n independent random draws from the distribution and returns the observed counts in all the categories. Read the documentation to see how this is described formally; we will use the formal terminology and notation in future assignments after we have discussed them in class. You will see that the function also takes a third argument size, which for our purposes will be an integer that specifies the number of times to run the entire simulation. All the runs are independent of each other. Write one line of code that uses np.random.multinomial to run 10 independent simulations of drawing 100 times at random with replacement from a population in which 61% of the people vote for Roosevelt, 37% for Landon, and 2% for other candidatdes. The output should be an array containing the counts in the Roosevelt category in the 10 simulations. It will help to recall how to slice NumPy arrays. Assign your answer to the variable sample. sample = np.random.multinomial(100, [0.61, 0.37, 0.02], size=10)[:, 0] sample array([54, 65, 58, 53, 66, 58, 61, 56, 60, 57], dtype=int32) grader.check(\"q3b\") q3b passed! ğŸ™Œ ","date":"2024-07-14","objectID":"/datahw01/:8:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3c Replace the â€œâ€¦â€ in the code cell below with a Python expression so that the output of the cell is an empirical histogram of 500,000 simulated counts of voters for Roosevelt in 100 draws made at random with replacement from the voting population. After you have drawn the histogram, you might want to take a moment to recall the conclusion reached by the Literary Digest, a magazine thatâ€”while having successfully predicted the outcome of many previous presidential electionsâ€”failed to correctly predict the winner of the 1936 presidential election. In their survey of 10 million individuals, they predicted the popular vote as just 43% for Roosevelt and 57% for Landon. Based on our simulation, there was most definitely sampling bias in the Digestâ€™s sampling process. simulated_counts = np.random.multinomial(100, [0.61, 0.37, 0.02], size=500000)[:, 0] integer_distribution(simulated_counts) ","date":"2024-07-14","objectID":"/datahw01/:8:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3d As you know, the count of Roosevelt voters in a sample of 100 people drawn at random from the eligible population is expected to be 61. Just by looking at the histogram in Part c, and no other calculation, pick the correct option and explain your choice. You might want to refer to the Data 8 textbook again. The SD of the distribution of the number of Roosevelt voters in a random sample of 100 people drawn from the eligible population is closest to (i) 1.9 (ii) 4.9 (iii) 10.9 (iv) 15.9 Type your answer here. å‡è®¾ä¸Šé¢å›¾ç‰‡æ˜¯å¯¹çš„ï¼Œç”¨3ÏƒåŸåˆ™ç²—ç•¥ä¼°è®¡5å·¦å³ã€‚ ","date":"2024-07-14","objectID":"/datahw01/:8:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 3e The normal curve with mean $\\mu$ and SD $\\sigma$ is defined by $$ f(x) ~ = ~ \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}}, ~~~ -\\infty \u003c x \u003c \\infty $$ Redraw your histogram from Part c and overlay the normal curve with $\\mu = 61$ and $\\sigma$ equal to the choice you made in Part d. You just have to call plt.plot after integer_distribution. Use np.e for $e$. For the curve, use 2 as the line width, and any color that is easy to see over the blue histogram. Itâ€™s fine to just let Python use its default color. Now you can see why centering the histogram bars over the integers was a good idea. The normal curve peaks at 26, which is the center of the corresponding bar. mu = 61 sigma = 4.9 x = np.linspace(40, 80, 200) f_x = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp(-(x - mu)**2 / (2 * sigma**2)) plt.plot(x, f_x) integer_distribution(simulated_counts) ","date":"2024-07-14","objectID":"/datahw01/:8:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4: Linear Algebra A common representation of data uses matrices and vectors, so it is helpful to familiarize ourselves with linear algebra notation, as well as some simple operations. Define a vector $\\vec{v}$ to be a column vector. Then, the following properties hold: $c\\vec{v}$ with $c$ some constant $c \\in \\mathbb{R}$, is equal to a new vector where every element in $c\\vec{v}$ is equal to the corresponding element in $\\vec{v}$ multiplied by $c$. For example, $2 \\begin{bmatrix} 1 \\ 2 \\ \\end{bmatrix} = \\begin{bmatrix} 2 \\ 4 \\ \\end{bmatrix}$ $\\vec{v}_1 + \\vec{v}_2$ is equal to a new vector with elements equal to the elementwise addition of $\\vec{v}_1$ and $\\vec{v}_2$. For example, $\\begin{bmatrix} 1 \\ 2 \\ \\end{bmatrix} + \\begin{bmatrix} -3 \\ 4 \\ \\end{bmatrix} = \\begin{bmatrix} -2 \\ 6 \\ \\end{bmatrix}$. The above properties form our definition for a linear combination of vectors. $\\vec{v}_3$ is a linear combination of $\\vec{v}_1$ and $\\vec{v}_2$ if $\\vec{v}_3 = a\\vec{v}_1 + b\\vec{v}_2$, where $a$ and $b$ are some constants. Oftentimes, we stack column vectors to form a matrix. Define the rank of a matrix $A$ to be equal to the maximal number of linearly independent columns in $A$. A set of columns is linearly independent if no column can be written as a linear combination of any other column(s) within the set. For example, let $A$ be a matrix with 4 columns. If three of these columns are linearly independent, but the fourth can be written as a linear combination of the other three, then $\\text{rank}(A) = 3$. For each part below, you will be presented with a set of vectors, and a matrix consisting of those vectors stacked in columns. State the rank of the matrix, and whether or not the matrix is full rank. If the matrix is not full rank, state a linear relationship among the vectorsâ€”for example: $\\vec{v}_1 = 2\\vec{v}_2$. ","date":"2024-07-14","objectID":"/datahw01/:9:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4a $$ \\vec{v}_1 = \\begin{bmatrix} 1 \\ 0 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} 1 \\ 1 \\ \\end{bmatrix} , A = \\begin{bmatrix} \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \\ \\vert \u0026 \\vert \\end{bmatrix}$$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4b $$ \\vec{v}_1 = \\begin{bmatrix} 3 \\ -4 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} 0 \\ 0 \\ \\end{bmatrix} , B = \\begin{bmatrix} \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \\ \\vert \u0026 \\vert \\end{bmatrix} $$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4c $$ \\vec{v}_1 = \\begin{bmatrix} 0 \\ 1 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} 5 \\ 0 \\ \\end{bmatrix} , \\vec{v}_3 = \\begin{bmatrix} 10 \\ 10 \\ \\end{bmatrix} , C = \\begin{bmatrix} \\vert \u0026 \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \u0026 \\vec{v}_3 \\ \\vert \u0026 \\vert \u0026 \\vert \\end{bmatrix} $$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 4d $$ \\vec{v}_1 = \\begin{bmatrix} 0 \\ 2 \\ 3 \\ \\end{bmatrix} , \\vec{v}_2 = \\begin{bmatrix} -2 \\ -2 \\ 5 \\ \\end{bmatrix} , \\vec{v}_3 = \\begin{bmatrix} 2 \\ 4 \\ -2 \\ \\end{bmatrix} , D = \\begin{bmatrix} \\vert \u0026 \\vert \u0026 \\vert \\ \\vec{v}_1 \u0026 \\vec{v}_2 \u0026 \\vec{v}_3 \\ \\vert \u0026 \\vert \u0026 \\vert \\end{bmatrix} $$ Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:9:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5: A Least Squares Predictor Let the list of numbers $(x_1, x_2, \\ldots, x_n)$ be data. You can think of each index $i$ as the label of a household, and the entry $x_i$ as the annual income of Household $i$. Define the mean or average $\\mu$ of the list to be $$\\mu ~ = ~ \\frac{1}{n}\\sum_{i=1}^n x_i.$$ ","date":"2024-07-14","objectID":"/datahw01/:10:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5a The $i$ th deviation from average is the difference $x_i - \\mu$. In Data 8 you saw in numerical examples that the sum of all these deviations is 0. Now prove that fact. That is, show that $\\sum_{i=1}^n (x_i - \\mu) = 0$. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:10:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5b Recall that the variance of a list is defined as the mean squared deviation from average, and that the standard deviation (SD) of the list is the square root of the variance. The SD is in the same units as the data and measures the rough size of the deviations from average. Denote the variance of the list by $\\sigma^2$. Write a math expression for $\\sigma^2$ in terms of the data ($x_{1} \\dots x_{n}$) and $\\mu$. We recommend building your expression by reading the definition of variance from right to left. That is, start by writing the notation for â€œaverageâ€, then â€œdeviation from averageâ€, and so on. Type your answer here, replacing this text. ","date":"2024-07-14","objectID":"/datahw01/:10:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Mean Squared Error Suppose you have to predict the value of $x_i$ for some $i$, but you donâ€™t get to see $i$ and you certainly donâ€™t get to see $x_i$. You decide that whatever $x_i$ is, youâ€™re just going to use some number $c$ as your predictor. The error in your prediction is $x_i - c$. Thus the mean squared error (MSE) of your predictor $c$ over the entire list of $n$ data points can be written as: $$MSE(c) = \\frac{1}{n}\\sum_{i=1}^n (x_i - c)^2.$$ You may already see some similarities to your definition of variance from above! You then start to wonderâ€”if you picked your favorite number $c = \\mu$ as the predictor, would it be â€œbetterâ€ than other choices $c \\neq \\mu$? ","date":"2024-07-14","objectID":"/datahw01/:10:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 5c One common approach to defining a â€œbestâ€ predictor is as predictor that minimizes the MSE on the data $(x_1, \\dots, x_n)$. In this course, we commonly use calculus to find the predictor $c$ as follows: Define $MSE$ to be a function of $c$, i.e., $MSE(c)$ as above. Assume that the data points $x_1, x_2, â€¦, x_n$ are fixed, and that $c$ is the only variable. Determine the value of $c$ that minimizes $MSE(c)$. Justify that this is indeed a minimum, not a maximum. Step 1 is done for you in the problem statement; follow steps 2 and 3 to show that $\\mu$ is the value of $c$ that minimizes $MSE(c)$. You must do both steps. Type your answer here, replacing this text. Your proof above shows that $\\mu$ is the least squares constant predictor. ","date":"2024-07-14","objectID":"/datahw01/:10:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6: A More Familiar Least Squares Predictor In Data 8 you found (numerically) the least squares linear predictor of a variable $y$ based on a related variable $x$. In this course, we will prove your findings using a generalization of your calculation in the previous question. When we get to this proof later in this course, you will need to be comfortable with vector operations. For now, you will get familiar with this notation by rewriting your least squares findings from Data 8 (and the previous question) using vector notation. This question wonâ€™t require you to write LaTeX, so just focus on the mathematical notation weâ€™re presenting. ","date":"2024-07-14","objectID":"/datahw01/:11:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"The Dot Product (1) We start by defining the dot product of two real vectors $x = \\begin{bmatrix} x_1 \\ x_2 \\ \\dots \\ x_n \\end{bmatrix}$ and $y = \\begin{bmatrix} y_1 \\ y_2 \\ \\dots \\ y_n \\end{bmatrix}$ as follows: $$x^T y = \\sum_{i=1}^n x_i y_i $$ Given the above definition, the dot product is (1) a scalar, not another vector; and (2) only defined for two vectors of the same length. Note: In this course we often opt for $x$ instead of $\\vec{x}$ to simplify notation; $x$ as a vector is inferred from its use in the dot product. Then $x_i$ is the $i$-th element of the vector $x$. Detail: In this course, we prefer the notation $x^Ty$ to illustrate a dot product, defined as matrix multiplication of $x^T$ and $y$. In the literature you may also see $x \\cdot y$, but we avoid this notation since the dot ($\\cdot$) notation is occasionally used for scalar values. Detail: The dot product is a special case of an inner product, where $x, y \\in \\mathbb{R}^n$. (2) We introduce a special vector, $\\mathbb{1}$, to write the mean $\\bar{x}$ of data $(x_1, x_2, \\dots, x_n)$ as a dot product: \\begin{align} \\bar{x} \u0026= \\frac{1}{n}\\sum_{i=1}^n x_i = \\frac{1}{n}\\sum_{i=1}^n 1x_i \\ \u0026= \\frac{1}{n}(x^T\\mathbb{1}). \\end{align} The data $(x_1, \\dots, x_n)$ have been defined as an $n$-dimensional column vector $x$, where $x = \\begin{bmatrix} x_1 \\ x_2 \\ \\dots \\ x_n \\end{bmatrix}$. The special vector $\\mathbb{1}$ is a vector of ones, whose length is defined by the vector operation in which it is used. So with $n$-dimensional column vector $x$, the dot product $x^T\\mathbb{1}$ implies that $\\mathbb{1}$ is an $n$-dimensional column vector where every element is $1$. Because dot products produce scalars, the multiplication of two scalars $\\frac{1}{n}$ and $x^T\\mathbb{1}$ produces another scalar, $\\bar{x}$. Note: We use bar notation for the mean ($\\bar{x}$ instead of $\\mu$) in this problem to differentiate $\\bar{x}$ from $\\bar{y}$, the latter of which is the mean of data $(y_1, \\dots, y_n)$. (3) We can further use this definition of $\\bar{x}$ to additionally write the variance $\\sigma_x^2$ of the data $(x_1, \\dots, x_n)$ as a dot product. Verify for yourself that the below operation defines $\\sigma_x^2$ as a scalar: \\begin{align} \\sigma_x^2 \u0026= \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2 \\ \u0026= \\frac{1}{n}(x - \\bar{x})^T(x - \\bar{x}). \\end{align} ","date":"2024-07-14","objectID":"/datahw01/:11:1","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6a To verify your understanding of the dot product as defined above, suppose you are working with $n$ datapoints ${(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)}$. Define the $x$ data as $(x_1, \\dots, x_n)$ and the $y$ data as $(y_1, \\dots, y_n)$, and define $x$ and $y$ as two $n$-dimensional column vectors, where the $i$-th elements of $x$ and $y$ are $x_i$ and $y_i$, respectively. Define $\\bar{x}$ and $\\bar{y}$ as the means of the $x$ data and $y$ data, respectively. Define $\\sigma_x^2$ and $\\sigma_y^2$ as the variances of the $x$ data and $y$ data, respectively. Therefore $\\sigma_x = \\sqrt{\\sigma_x^2}$ and $\\sigma_y = \\sqrt{\\sigma_y^2}$ are the standard deviations of the $x$ data and $y$ data, respectively. Suppose $n = 32$. What is the dimension of each of the following expressions? Expression (i). Note there are two ways it is written in the literature. $$\\dfrac{1}{\\sigma_x} (x - \\bar{x}) = \\dfrac{x - \\bar{x}}{\\sigma_x} $$ Expression (ii). $$\\dfrac{1}{n} \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)^T \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)$$ Assign the variables q6a_i and q6a_ii to an integer representing the dimension of the above expressions (i) and (ii), respectively. q6a_i = ... q6a_ii = ... # do not modify these lines print(f\"Q6a(i) is {q6a_i}-dimensional\") print(f\"Q6a(ii) is {q6a_ii}-dimensional\") grader.check(\"q6a\") ","date":"2024-07-14","objectID":"/datahw01/:11:2","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Dot Products in NumPy Next, weâ€™ll use NumPyâ€™s matrix multiplication operators to compute expressions for the regression line, which you learned in Data 8 was the unique line that minimizes the mean squared error of estimation among all straight lines. At this time, it may be helpful to review the Data 8 section. Before we continue, letâ€™s contextualize our computation by loading in a dataset you saw in Data 8: the relation between weight lifted and shot put distance among surveyed female collegiate athletes. Weâ€™ve plotted the point using matplotlibâ€™s scatter function, which you will see in more detail in two weeks. # Run this cell to plot the data. weight_lifted = np.array([ 37.5, 51.5, 61.3, 61.3, 63.6, 66.1, 70. , 92.7, 90.5, 90.5, 94.8, 97. , 97. , 97. , 102. , 102. , 103.6, 100.4, 108.4, 114. , 115.3, 114.9, 114.7, 123.6, 125.8, 119.1, 118.9, 141.1]) shot_put_distance = np.array([ 6.4, 10.2, 12.4, 13. , 13.2, 13. , 12.7, 13.9, 15.5, 15.8, 15.8, 16.8, 17.1, 17.8, 14.8, 15.5, 16.1, 16.2, 17.9, 15.9, 15.8, 16.7, 17.6, 16.8, 17. , 18.2, 19.2, 18.6]) plt.scatter(weight_lifted, shot_put_distance) plt.xlabel(\"Weight Lifted\") plt.ylabel(\"Shot Put Distance\") Looks pretty linear! Letâ€™s try to fit a regression line to this data. Define the vectors $x$ as the weight lifted data vector and $y$ as the shot put distance data vector, respectively, of the college athletes. Then the regression line uses the weight lifted $x$ to predict $\\hat{y}$, which is the linear estimate of the actual value shot put distance $y$ as follows: \\begin{align} \\hat{y} \u0026= \\hat{a} + \\hat{b}{x}\\text{, where} \\ \\hat{a} \u0026= \\bar{y} - \\hat{b}\\bar{x} \\ \\hat{b} \u0026= r \\dfrac{\\sigma_y}{\\sigma_x} \\end{align} $\\bar{x}, \\bar{y}$ and $\\sigma_x, \\sigma_y$ are the means and standard deviations, respectively of the data $x$ and $y$, respectively. Here, $r$ is the correlation coefficient as defined in Data 8! Note: We use the hat $\\hat{}$ notation to indicate values we are estimating: $\\hat{y}$, the predicted shot put distance, as well as $\\hat{a}$ and $\\hat{b}$, the respective estimated intercept and slope parameters we are using to model the â€œbestâ€ linear predictor of $y$ from $x$. Weâ€™ll dive into this later in the course. Note: Remember how we dropped the $\\vec{}$ vector notation? These linear regression equations therefore represent both the scalar case (predict a single value $\\hat{y}$ from a single $x$) and the vector case (predict a vector $\\hat{y}$ element-wise from a vector $x$). How convenient!! In this part, instead of using NumPyâ€™s built-in statistical functions like np.mean() and np.std(), you are going to use NumPyâ€™s matrix operations to create the components of the regression line from first principles. The @ operator multiplies NumPy matrices or arrays together (documentation). We can use this operator to write functions to compute statistics on data, using the expressions that we defined in part (a). Check it out: # Just run this cell. def dot_mean(arr): n = len(arr) all_ones = np.ones(n) # creates n-dimensional vector of ones return (arr.T @ all_ones)/n def dot_var(arr): n = len(arr) mean = dot_mean(arr) zero_mean_arr = arr - mean return (zero_mean_arr.T @ zero_mean_arr)/n def dot_std(arr): return np.sqrt(dot_var(arr)) print(\"np.mean(weight_lifted) =\", np.mean(weight_lifted), \"\\tdot_mean(weight_lifted) =\", dot_mean(weight_lifted)) print(\"np.var(weight_lifted) =\", np.std(weight_lifted), \"\\tdot_var(weight_lifted =\", dot_var(weight_lifted)) print(\"np.std(weight_lifted) =\", np.std(weight_lifted), \"\\tdot_std(weight_lifted =\", dot_std(weight_lifted)) Now, you will write code to define the expressions you explored in part (a) of this question. ","date":"2024-07-14","objectID":"/datahw01/:11:3","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6b (i) Use the NumPy @ operator to compute expression (i) from part (a). For convenience, weâ€™ve rewritten the expression below. Note that this expression is also referred to as $x$ in standard units (Data 8 textbook section). $$\\dfrac{x - \\bar{x}}{\\sigma_x} $$ Write the body of the function dot_su which takes in a 1-D NumPy array arr and returns arr in standard units. Do not use np.mean(), np.std(), np.var(), np.sum() nor any Python loops. You should only use a subset of @, /, +, -, len(), the dot_mean(), dot_var(), and dot_std() functions defined above. def dot_su(arr): ... # do not edit below this line q6bi_su = dot_su(weight_lifted) q6bi_su grader.check(\"q6bi\") ","date":"2024-07-14","objectID":"/datahw01/:11:4","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6b (ii) Next use the NumPy @ operator to compute the correlation coefficient $r$, which is expression (ii) from part (a). For convenience, weâ€™ve rewritten the expression below. $$r = \\dfrac{1}{n} \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)^T \\left( \\dfrac{x - \\bar{x}}{\\sigma^x}\\right)$$ Write the body of the function dot_corr_coeff which takes in two 1-D NumPy arrays x and y and returns the correlation coefficient of x and y. As before, Do not use np.mean(), np.std(), np.var(), np.sum() nor any Python loops. As before, you should only use a subset of @, /, +, -, len(), the dot_mean(), dot_var(), and dot_std() functions defined above. You may also use the dot_su() function that you defined in the previous part. def dot_corr_coeff(x, y): ... # do not edit below this line q6bii_r = dot_corr_coeff(weight_lifted, shot_put_distance) q6bii_r grader.check(\"q6bii\") ","date":"2024-07-14","objectID":"/datahw01/:11:5","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Question 6c Weâ€™re ready to put everything together! Finally, use the dot_-prefixed functions in this question to compute the regression line. For convenience, weâ€™ve rewritten the expressions below. $\\hat{y}$ is the linear estimate of the value $y$ based on $x$. \\begin{align} \\hat{y} \u0026= \\hat{a} + \\hat{b}{x}\\text{, where} \\ \\hat{a} \u0026= \\bar{y} - \\hat{b}\\bar{x} \\ \\hat{b} \u0026= r \\dfrac{\\sigma_y}{\\sigma_x} \\end{align} Define the functions compute_a_hat and compute_b_hat which return the intercept and slope, respectively, of the regression line defind above for a linear estimator of y using x. Verify how the functions are used to plot the linear regression line (implemented for you). As before, Do not use np.mean(), np.std(), np.var(), np.sum(), or any for loops. You may use a subset of @, /, +, -, len(), dot_mean(), dot_var(), dot_std(), dot_su(), dot_corr_coeff(). Hint: You may want to define a_hat in terms of b_hat. def compute_a_hat(x, y): ... def compute_b_hat(x, y): ... # do not edit below this line a_hat = compute_a_hat(weight_lifted, shot_put_distance) b_hat = compute_b_hat(weight_lifted, shot_put_distance) shot_put_hats = a_hat + b_hat * weight_lifted plt.scatter(weight_lifted, shot_put_distance) # the actual data plt.plot(weight_lifted, shot_put_hats, color='g', alpha=0.5) # the prediction line, transparent green plt.xlabel(\"Weight Lifted\") plt.ylabel(\"Shot Put Distance\") display(compute_a_hat(weight_lifted, shot_put_distance)) display(compute_b_hat(weight_lifted, shot_put_distance)) grader.check(\"q6c\") To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-07-14","objectID":"/datahw01/:11:6","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export() ","date":"2024-07-14","objectID":"/datahw01/:12:0","tags":["math","plotting"],"title":"DATA100-hw01","uri":"/datahw01/"},{"categories":["DATA100"],"content":"DataFrames: a data structure for tabular data ","date":"2024-07-13","objectID":"/datal3/:1:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"API always remember to turn to GPT/google/doc ","date":"2024-07-13","objectID":"/datal3/:1:1","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"indexing and loc/iloc generate subsets: loc: an operator select items by labels df.loc[row_indexer, column_indexer] row_indexer: can be a single label, a list of labels, sliceï¼ˆé—­åŒºé—´ï¼‰, single value column_indexer: same as row_indexer returns a DataFrame or Series iloc: an operator select items by positions df.iloc[row_indexer, column_indexer] row_indexer: numeric index or a list of numeric indicesï¼Œæ­¤æ—¶å›åˆ°pythonç»å…¸ç´¢å¼• å·¦é—­å³å¼€ column_indexer: same as row_indexer returns a DataFrame or Series é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ loc è¿›è¡Œç´¢å¼• .head(6) and .tail() to get the first or last few rows of a DataFrame(syntactic sugar) ","date":"2024-07-13","objectID":"/datal3/:2:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"[ ]: context sensitive operator ","date":"2024-07-13","objectID":"/datal3/:3:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"series: a data structure for 1D labeled data ","date":"2024-07-13","objectID":"/datal3/:4:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"index: a array-like object that labels the rows and columns of a DataFrame columns: usually do not have same name. è½¬æ¢ï¼š ","date":"2024-07-13","objectID":"/datal3/:5:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"conditional selection ","date":"2024-07-13","objectID":"/datal3/:6:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"ç±»å‹æ„è¯†! ä¸‰ç§æ•°æ®ç±»å‹ä¸­çš„å“ªä¸€ä¸ªï¼Ÿï¼Ÿï¼Ÿ ","date":"2024-07-13","objectID":"/datal3/:7:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"describe() ","date":"2024-07-13","objectID":"/datal3/:7:1","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"sample() df.sample(n=5, replace=True) # randomly select 5 rows ","date":"2024-07-13","objectID":"/datal3/:7:2","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"value_counts() df['column_name'].value_counts() # count the frequency of each value in a column return a Series with the count of each value in the column. ","date":"2024-07-13","objectID":"/datal3/:7:3","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"unique() df['column_name'].unique() # get all unique values in a column return a numpy array with the unique values in the column. ","date":"2024-07-13","objectID":"/datal3/:7:4","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"sort_values() df.sort_values(by='column_name', ascending=False) # sort the DataFrame by values in a column return a new DataFrame with the rows sorted by values in a column. ","date":"2024-07-13","objectID":"/datal3/:7:5","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["DATA100"],"content":"reference https://www.textbook.ds100.org/ch/a04/ref_pandas.html https://pandas.pydata.org/pandas-docs/stable/reference/index.html ","date":"2024-07-13","objectID":"/datal3/:8:0","tags":["pandas"],"title":"DATA100-L3: Pandas â… ","uri":"/datal3/"},{"categories":["UCB-CS61B"],"content":"definitions A sort is a permutation (re-arrangement) of a sequence of elements that brings them into order according to some total order. A total order â‰¼ is: Total: x â‰¼ y or y â‰¼ x for all x, y Reflexive: x â‰¼ x Antisymmetric: x â‰¼ y and y â‰¼ x iff x = y (x and y are equivalent). Transitive: x â‰¼ y and y â‰¼ z implies x â‰¼ z. In Java, total order is typically specified by compareTo or compare methods. May be inconsistent with equals! For example sorting an array of Strings by length has items that are equivalent, but not equal, e.g. â€œcatâ€ and â€œdogâ€. Goal of sorting: Given a sequence of elements with Z inversions. Perform a sequence of operations that reduces inversions to 0. ","date":"2024-07-13","objectID":"/61b-32/:1:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"Performance definition Characterizations of the runtime efficiency are sometimes called the time complexity of an algorithm. Examples: DFS has time complexity Î˜(V+E). Characterizations of the â€œextraâ€ memory usage of an algorithm is sometimes called the space complexity of an algorithm. DFS has space complexity Î˜(V). Note that the graph takes up space Î˜(V+E), but we donâ€™t count this as part of the runtime of DFS, since weâ€™re only accounting for the extra space that DFS uses. Selection Sort and Heapsort selection Sort Properties: ç›´æ¥é€‰æ‹©æ³• in Chinese $Î˜(N^2)$ time if we use an array (or similar data structure). ","date":"2024-07-13","objectID":"/61b-32/:2:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"Naive Heapsort: Leveraging a Max-Oriented Heap åˆ†æ ","date":"2024-07-13","objectID":"/61b-32/:3:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"in-place Heapsort demo ","date":"2024-07-13","objectID":"/61b-32/:4:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"performance analysis Merge Sort demo Time complexity, analysis from previous lecture: Î˜(N log N runtime) Space complexity with aux array: Costs Î˜(N) memory. Also possible to do in-place merge sort, but algorithm is very complicated, and runtime performance suffers by a significant constant factor. Insertion Sort General strategy: Starting with an empty output sequence. Add each item from input, inserting into output at right point. Naive approach, build entirely new output: Demo ","date":"2024-07-13","objectID":"/61b-32/:4:1","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"in-place Insertion Sort Demo æœ‰ç‚¹åƒå†’æ³¡ï¼Ÿ çœ‹ä¸Šå»æ²¡ä»€ä¹ˆå¥½çš„è¡¨ç°ï¼Ÿ Shellâ€™s Sort (Extra) (Not on Exam) so far summary ","date":"2024-07-13","objectID":"/61b-32/:5:0","tags":null,"title":"61B-32: Basic Sorting Algorithms","uri":"/61b-32/"},{"categories":["UCB-CS61B"],"content":"Backstory, Partitioning Quick Sort Partition Sort, a.k.a. Quicksort Quicksort Runtime Theoretical analysis: Best case: Î˜(N log N) Worst case: Î˜(N2) Compare this to Mergesort. Best case: Î˜(N log N) Worst case: Î˜(N log N) Recall that Î˜(N log N) vs. Î˜(N2) is a really big deal. So how can Quicksort be the fastest sort empirically? Because on average it is Î˜(N log N). Rigorous proof requires probability theory + calculus, but intuition + empirical analysis will hopefully convince you. Argument #2: Quicksort is BST Sort ğŸ¤” ","date":"2024-07-13","objectID":"/61b-33/:0:0","tags":null,"title":"61B-33: Quick Sort","uri":"/61b-33/"},{"categories":["UCB-CS61B"],"content":"so far summary Avoiding the Quicksort Worst Case ","date":"2024-07-13","objectID":"/61b-33/:0:1","tags":null,"title":"61B-33: Quick Sort","uri":"/61b-33/"},{"categories":["UCB-CS61B"],"content":"summary so far ","date":"2024-07-13","objectID":"/61b-33/:1:0","tags":null,"title":"61B-33: Quick Sort","uri":"/61b-33/"},{"categories":["UCB-CS61B"],"content":"Shortest Paths in a DAG (Directed Acyclic Graphs) Dynamic Programming ","date":"2024-07-13","objectID":"/61b-31/:0:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"the DAG SPT algorithm The DAG SPT algorithm can be thought of as solving increasingly large subproblems: Distance from source to source is very easy, and is just zero. We then tackle distances to vertices that are a bit farther to the right. We repeat this until we get all the way to the end of the graph. Problems grow larger and larger. By â€œlargeâ€ we informally mean depending on more and more of the earlier subproblems. This approach of solving increasingly large subproblems is sometimes called dynamic programming. a simple and powerful idea for solving â€œbig problemsâ€: Identify a collection of subproblems. Solve subproblems one by one, working from smallest to largest. Use the answers to the smaller problems to help solve the larger ones. Identification of the â€œrightâ€ subproblems is often quite tricky. Largely beyond scope of CS61B. Youâ€™ll study this in much more detail in CS170. Longest Increasing Subsequence The Longest Increasing Subsequence (LIS) problem is a classic dynamic programming problem. ","date":"2024-07-13","objectID":"/61b-31/:1:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"LLIS Related problem: Find the length of the longest increasing subsequence (LLIS). åŠ¨ç”¨è´Ÿæ•°å¤§å˜å° LIS Using Dynamic Programming è¿˜æ˜¯å¯¹LLISè¿›è¡Œè€ƒé‡ Can think of the Q values as memoized answers to shorter subproblems. Q(K) is the length of the longest subsequence ending at K. Thus, length of the longest subsequence is just the maximum of all Q. no DAG version ","date":"2024-07-13","objectID":"/61b-31/:2:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"summary of LLIS solutions ","date":"2024-07-13","objectID":"/61b-31/:3:0","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"#1 ","date":"2024-07-13","objectID":"/61b-31/:3:1","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"#2a ","date":"2024-07-13","objectID":"/61b-31/:3:2","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"#2b Runtime for Approach 1 (Extra) https://docs.google.com/presentation/d/1RlGUoB0bvKlHkxZXogmWGEzsI9ZXRWlg3ngf1FJkpac/edit#slide=id.g1298bbb99e_0_1920 å–å†³äºæ€ä¹ˆå»ºç«‹DAG ","date":"2024-07-13","objectID":"/61b-31/:3:3","tags":null,"title":"61B-31: Dynamic Programming","uri":"/61b-31/"},{"categories":["UCB-CS61B"],"content":"pros and cons ","date":"2024-07-13","objectID":"/61b-29/:0:1","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"Graph Problem so far Punchline: DFS and BFS both traverse entire graphs, just in a different order (like preorder, inorder, postorder, and level order for trees). Solving graph problems is often a question of identifying the right traversal. Many traversals may work. Example: DFS for topological sort. BFS for shortest paths. Example: DFS or BFS about equally good for checking existence of path. Dijkstraâ€™s Algorithm problem restatement: Find the shortest paths from source vertex s to some target vertex t. another problem restatement: Find the shortest path from source vertex s to all other vertices. æ·±å±‚è§£é‡Š ","date":"2024-07-13","objectID":"/61b-29/:1:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"SPTï¼ˆShortest Path Treeï¼‰ Edge Count If G is a connected edge-weighted graph with V vertices and E edges, how many edges are in the Shortest Paths Tree of G? [assume every vertex is reachable] Always V-1: For each vertex, there is exactly one input edge (except source). Insert all vertices into fringe PQ, storing vertices in order of distance from source. Repeat: Remove (closest) vertex v from PQ, and relax all edges pointing from v. Dijkstraâ€™s Algorithm Demo Link. è´ªå¿ƒæ€æƒ³è¯æ˜ ","date":"2024-07-13","objectID":"/61b-29/:2:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"ä¼ªä»£ç  http://algs4.cs.princeton.edu/44sp/DijkstraSP.java.html ","date":"2024-07-13","objectID":"/61b-29/:3:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"runtime analysis ","date":"2024-07-13","objectID":"/61b-29/:4:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"so far graph problems ","date":"2024-07-13","objectID":"/61b-29/:5:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"The Problem with Dijkstraâ€™s A* (CS188 Preview) We have only a single target in mind, so we need a different algorithm. How can we do better? A* Demo Link How do we get our estimate? Estimate is an arbitrary heuristic h(v). heuristic: â€œusing experience to learn and improveâ€ Doesnâ€™t have to be perfect! /** h(v) DOES NOT CHANGE as algorithm runs. */ public method h(v) { return computeLineDistance(v.latLong, NYC.latLong); } å¯è§†åŒ–ä¸¤è€…å¯¹æ¯” http://qiao.github.io/PathFinding.js/visual/ ","date":"2024-07-13","objectID":"/61b-29/:5:1","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"Summary ","date":"2024-07-13","objectID":"/61b-29/:6:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"graph problems so so far A* Tree Search vs. A* Graph Search Admissibility vs. Consistency (Extra: See CS188 for more) see Iterative DFS (Extra) see ","date":"2024-07-13","objectID":"/61b-29/:7:0","tags":null,"title":"61-29: Shortest Paths","uri":"/61b-29/"},{"categories":["UCB-CS61B"],"content":"warm up MST, Cut Property, Generic MST Algorithm ","date":"2024-07-13","objectID":"/61b-30/:0:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"MST vs SPT A shortest paths tree depends on the start vertex: Because it tells you how to get from a source to EVERYTHING. There is no source for a MST. Nonetheless, the MST sometimes happens to be an SPT for a specific vertex. ä¸¤è€…å…³ç³»ä¸å¤§ï¼Ÿ ","date":"2024-07-13","objectID":"/61b-30/:1:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Cut Property ç®€å•è¯æ˜ cross bridge ä¸€å®šåœ¨ MST ä¸­ã€‚ ","date":"2024-07-13","objectID":"/61b-30/:2:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Generic MST Algorithm Start with no edges in the MST. Find a cut that has no crossing edges in the MST. Add smallest crossing edge to the MST. Repeat until V-1 edges. This should work, but we need some way of finding a cut with no crossing edges! Random isnâ€™t a very good idea. Primâ€™s Algorithm https://docs.google.com/presentation/d/1NFLbVeCuhhaZAM1z3s9zIYGGnhT4M4PWwAc-TLmCJjc/edit#slide=id.g9a60b2f52_0_0 https://docs.google.com/presentation/d/1GPizbySYMsUhnXSXKvbqV4UhPCvrt750MiqPPgU-eCY/edit#slide=id.g9a60b2f52_0_0 ","date":"2024-07-13","objectID":"/61b-30/:3:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Primâ€™s vs. Dijkstraâ€™s Primâ€™s and Dijkstraâ€™s algorithms are exactly the same, except Dijkstraâ€™s considers â€œdistance from the sourceâ€, and Primâ€™s considers â€œdistance from the tree.â€ Visit order: Dijkstraâ€™s algorithm visits vertices in order of distance from the source. Primâ€™s algorithm visits vertices in order of distance from the MST under construction. Relaxation: Relaxation in Dijkstraâ€™s considers an edge better based on distance to source. Relaxation in Primâ€™s considers an edge better based on distance to tree. ","date":"2024-07-13","objectID":"/61b-30/:4:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"pseudocode ","date":"2024-07-13","objectID":"/61b-30/:5:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"runtime Kruskalâ€™s Algorithm conceptual real ","date":"2024-07-13","objectID":"/61b-30/:6:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Kruskalâ€™s Algorithm Pseudocode ","date":"2024-07-13","objectID":"/61b-30/:7:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"runtime ","date":"2024-07-13","objectID":"/61b-30/:8:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Summary https://docs.google.com/presentation/d/1I8GSEL0CgT09_JjSUF7MfoRMJkyzPjo8lKRd8XdOaRA/edit#slide=id.g772f8a8e2_0_117 SOTA of compare-based MST algorithms ğŸ†™ ","date":"2024-07-13","objectID":"/61b-30/:9:0","tags":null,"title":"61B-30: Minimum Spanning Trees","uri":"/61b-30/"},{"categories":["UCB-CS61B"],"content":"Depth First Paths â€œDepth First Searchâ€ is a more general term for any graph search algorithm that traverses a graph as deep as possible before backtracking. The term is used for several slightly different algorithms. For example: DFS may refer to the version from the previous lecture that doesnâ€™t use any marks (and thus can get caught in a loop). DFS may refer to the version where vertices are marked. DFS may refer to a version where vertices are marked and source edges recorded (as in Depth First Paths). DFS may refer to other algorithms like the â€œtopological sort algorithmâ€ well discuss later in lecture. And more! space åˆ†æå¯èƒ½æ˜¯è¦è€ƒè™‘runtimeçš„å†…å­˜å ç”¨ï¼Œå› ä¸ºDFSç®—æ³•éœ€è¦é€’å½’è°ƒç”¨ï¼Œæ¯ä¸€æ¬¡é€’å½’è°ƒç”¨éƒ½ä¼šæ¶ˆè€—ä¸€å®šçš„å†…å­˜ã€‚ Graph Traversals åº”è¯¥è®²çš„æ˜¯BFS Topological Sorting DFS, then reverse the order of the vertices. public class DepthFirstOrder { private boolean[] marked; private Stack\u003cInteger\u003e reversePostorder; public DepthFirstOrder(Digraph G) { reversePostorder = new Stack\u003cInteger\u003e(); marked = new boolean[G.V()]; for (int v = 0; v \u003c G.V(); v++) { if (!marked[v]) { dfs(G, v); } /** * Perform DFS of all unmarked vertices. * Note: Our algorithm earlier started at vertices with indegree zero. It turns out this * algorithm works no matter where you start! * */ } private void dfs(Digraph G, int v) { marked[v] = true; for (int w : G.adj(v)) { if (!marked[w]) { dfs(G, w); } } reversePostorder.push(v); // After each DFS is done, â€˜visitâ€™ vertex by putting on a stack. } public Iterable\u003cInteger\u003e reversePostorder() { return reversePostorder; } } ","date":"2024-07-13","objectID":"/61b-28/:0:0","tags":null,"title":"61B-28:  Graph Traversals","uri":"/61b-28/"},{"categories":["UCB-CS61B"],"content":"now graph problem summary æœ¬è´¨ä¸Šè¿˜æ˜¯ DFS Breadth First Search â€œfringeâ€ï¼ˆè¾¹ç¼˜ï¼‰é€šå¸¸æŒ‡çš„æ˜¯ä¸€ä¸ªæ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨ç­‰å¾…å¤„ç†çš„èŠ‚ç‚¹ã€‚see public class BreadthFirstPaths { private boolean[] marked; private int[] edgeTo; ... private void bfs(Graph G, int s) { Queue\u003cInteger\u003e fringe = new Queue\u003cInteger\u003e(); fringe.enqueue(s); // set up starting vertex marked[s] = true; while (!fringe.isEmpty()) { int v = fringe.dequeue(); /** * for freshly dequeued vertex v, for each neighbor that is unmarked: * Enqueue that neighbor to the fringe. * Mark it. * Set its edgeTo to v. */ for (int w : G.adj(v)) { if (!marked[w]) { fringe.enqueue(w); marked[w] = true; edgeTo[w] = v; } } } } ","date":"2024-07-13","objectID":"/61b-28/:1:0","tags":null,"title":"61B-28:  Graph Traversals","uri":"/61b-28/"},{"categories":["UCB-CS61B"],"content":"For BSTs, the most inefficient way to add is in to put it in order. åœ¨äºŒå‰æœç´¢æ ‘ï¼ˆBSTï¼‰ä¸­ï¼Œæœ€ä½æ•ˆçš„æ–¹å¼æ˜¯æŒ‰å‡åºæˆ–é™åºæ’å…¥èŠ‚ç‚¹ï¼Œå› ä¸ºè¿™ç§æ–¹å¼ä¼šå¯¼è‡´æ ‘å˜æˆä¸€æ¡é“¾è¡¨ï¼Œä»è€Œä½¿å…¶æ€§èƒ½é€€åŒ–åˆ°O(n)çš„æ—¶é—´å¤æ‚åº¦ã€‚ è¿™é‡Œæ˜¯ä¸€ä¸ªç”¨Javaå®ç°çš„ä¾‹å­ï¼Œå±•ç¤ºäº†æŒ‰å‡åºæ’å…¥èŠ‚ç‚¹çš„äºŒå‰æœç´¢æ ‘ï¼ˆBSTï¼‰ä¼šå˜æˆé“¾è¡¨çš„æƒ…å†µï¼š // å®šä¹‰æ ‘çš„èŠ‚ç‚¹ class TreeNode { int value; TreeNode left; TreeNode right; TreeNode(int value) { this.value = value; this.left = null; this.right = null; } } // å®šä¹‰äºŒå‰æœç´¢æ ‘ class BinarySearchTree { private TreeNode root; // æ’å…¥èŠ‚ç‚¹ public void insert(int value) { root = insertRec(root, value); } private TreeNode insertRec(TreeNode root, int value) { if (root == null) { root = new TreeNode(value); return root; } if (value \u003c root.value) { root.left = insertRec(root.left, value); } else { root.right = insertRec(root.right, value); } return root; } // æ‰“å°æ ‘çš„ä¸­åºéå† public void inorderTraversal() { inorderRec(root); System.out.println(); } private void inorderRec(TreeNode root) { if (root != null) { inorderRec(root.left); System.out.print(root.value + \" \"); inorderRec(root.right); } } // æ‰“å°æ ‘çš„ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰ public void printTree() { printTreeRec(root, \"\", true); } private void printTreeRec(TreeNode root, String indent, boolean last) { if (root != null) { System.out.print(indent); if (last) { System.out.print(\"R----\"); indent += \" \"; } else { System.out.print(\"L----\"); indent += \"| \"; } System.out.println(root.value); printTreeRec(root.left, indent, false); printTreeRec(root.right, indent, true); } } } // æµ‹è¯•ç±» public class Main { public static void main(String[] args) { BinarySearchTree bst = new BinarySearchTree(); // æŒ‰å‡åºæ’å…¥èŠ‚ç‚¹ï¼ˆ1, 2, 3, 4, 5ï¼‰ bst.insert(1); bst.insert(2); bst.insert(3); bst.insert(4); bst.insert(5); System.out.println(\"Inorder Traversal of BST:\"); bst.inorderTraversal(); System.out.println(\"Tree Structure:\"); bst.printTree(); } } ","date":"2024-07-13","objectID":"/61b-26/:0:0","tags":null,"title":"61B-26: Midterm 2 Review","uri":"/61b-26/"},{"categories":["UCB-CS61B"],"content":"ä»£ç è§£æ TreeNodeç±»ï¼šå®šä¹‰äº†æ ‘çš„èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹çš„å€¼ä»¥åŠå·¦å­èŠ‚ç‚¹å’Œå³å­èŠ‚ç‚¹ã€‚ BinarySearchTreeç±»ï¼šå®šä¹‰äº†äºŒå‰æœç´¢æ ‘ï¼ŒåŒ…æ‹¬æ’å…¥èŠ‚ç‚¹çš„é€»è¾‘å’Œä¸­åºéå†æ‰“å°æ ‘çš„ç»“æ„ã€‚ insertæ–¹æ³•ï¼šå°†èŠ‚ç‚¹æŒ‰å‡åºæ’å…¥ã€‚ç”±äºæ¯ä¸ªæ–°æ’å…¥çš„èŠ‚ç‚¹éƒ½æ˜¯æ¯”å½“å‰èŠ‚ç‚¹å¤§çš„ï¼Œå› æ­¤æ‰€æœ‰æ–°èŠ‚ç‚¹éƒ½ä¼šè¢«æ’å…¥åˆ°å³å­æ ‘ä¸Šï¼Œå¯¼è‡´æ ‘ç»“æ„åƒé“¾è¡¨ã€‚ printTreeæ–¹æ³•ï¼šç”¨äºä»¥ç»“æ„åŒ–æ–¹å¼æ‰“å°æ ‘ï¼Œç”¨äºè°ƒè¯•æ ‘çš„ç»“æ„ã€‚ ","date":"2024-07-13","objectID":"/61b-26/:0:1","tags":null,"title":"61B-26: Midterm 2 Review","uri":"/61b-26/"},{"categories":["UCB-CS61B"],"content":"è¿è¡Œç»“æœ æ’å…¥å‡åºèŠ‚ç‚¹åï¼Œæ ‘çš„ç»“æ„å°†å˜æˆé“¾è¡¨æ ·å¼ï¼Œå³æ¯ä¸ªèŠ‚ç‚¹åªæœ‰å³å­èŠ‚ç‚¹ï¼Œæ²¡æœ‰å·¦å­èŠ‚ç‚¹ã€‚ Inorder Traversal of BST: 1 2 3 4 5 Tree Structure: R----1 R----2 R----3 R----4 R----5 å¯ä»¥çœ‹åˆ°ï¼Œæ ‘çš„ç»“æ„å˜æˆäº†ä¸€ä¸ªå‘å³å€¾æ–œçš„é“¾è¡¨ï¼Œè¯´æ˜æ¯ä¸ªèŠ‚ç‚¹åªæœ‰ä¸€ä¸ªå³å­èŠ‚ç‚¹ï¼Œæ²¡æœ‰å·¦å­èŠ‚ç‚¹ã€‚è¿™ç§ç»“æ„ä½¿å¾—æ ‘çš„æ€§èƒ½é€€åŒ–ä¸ºO(n)çš„æ—¶é—´å¤æ‚åº¦ã€‚ more to see https://docs.google.com/presentation/d/1TA-xr-z7df4vnJz6oo4s7OkpGLoVy1EYYTwR_8AVhxA/edit#slide=id.g35a9240b53_0_150 ","date":"2024-07-13","objectID":"/61b-26/:0:2","tags":null,"title":"61B-26: Midterm 2 Review","uri":"/61b-26/"},{"categories":["UCB-CS61B"],"content":"intro ","date":"2024-07-13","objectID":"/61b-27/:0:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"types of graph ","date":"2024-07-13","objectID":"/61b-27/:1:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"terminology ","date":"2024-07-13","objectID":"/61b-27/:2:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"å›¾è®ºé—®é¢˜ s-t Path. Is there a path between vertices s and t? Shortest s-t Path. What is the shortest path between vertices s and t? Cycle. Does the graph contain any cycles? Euler Tour. Is there a cycle that uses every edge exactly once? Hamilton Tour. Is there a cycle that uses every vertex exactly once? Connectivity. Is the graph connected, i.e. is there a path between all vertex pairs? Biconnectivity. Is there a vertex whose removal disconnects the graph? Planarity. Can you draw the graph on a piece of paper with no crossing edges? Isomorphism. Are two graphs isomorphic (the same graph in disguise)? Graph problems: Unobvious which are easy, hard, or computationally intractable. Graph Representations Common Simplification: Integer Vertices ","date":"2024-07-13","objectID":"/61b-27/:3:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"API public class Graph { public Graph(int V): // Create empty graph with v vertices public void addEdge(int v, int w): // add an edge v-w Iterable\u003cInteger\u003e adj(int v): // vertices adjacent to v int V(): // number of vertices int E(): // number of edges ... ","date":"2024-07-13","objectID":"/61b-27/:4:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"#1 Adjacency Matrix ","date":"2024-07-13","objectID":"/61b-27/:5:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"#2 edge set ","date":"2024-07-13","objectID":"/61b-27/:6:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"#3 Adjacency List $\\Theta(V) \\rightarrow \\Theta(V^2) $ âˆš $$ \\Theta(V+E) $$ æ‰æ˜¯æœ‰æ„æ€çš„ç­”æ¡ˆ ","date":"2024-07-13","objectID":"/61b-27/:7:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"æ€»ç»“ public class Graph { private final int V; private List\u003cInteger\u003e[] adj; public Graph(int V) { this.V = V; adj = (List\u003cInteger\u003e[]) new ArrayList[V]; // cast! for (int v = 0; v \u003c V; v++) { adj[v] = new ArrayList\u003cInteger\u003e(); } } public void addEdge(int v, int w) { adj[v].add(w); adj[w].add(v); } public Iterable\u003cInteger\u003e adj(int v) { return adj[v]; } } Depth First Traversal https://docs.google.com/presentation/d/1IJyC4cAogU2x3erW7E3hDz8jWDTLoaaot8u2ebHtpto/pub?start=false\u0026loop=false\u0026delayms=3000 ","date":"2024-07-13","objectID":"/61b-27/:8:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"implementation public class Paths { public Paths(Graph G, int s): Find all paths from G boolean hasPathTo(int v): is there a path from s to v? Iterable\u003cInteger\u003e pathTo(int v): path from s to v (if any) } ","date":"2024-07-13","objectID":"/61b-27/:9:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"summary ","date":"2024-07-13","objectID":"/61b-27/:10:0","tags":null,"title":"61B-27: Graphs","uri":"/61b-27/"},{"categories":["UCB-CS61B"],"content":"interface /** (Min) Priority Queue: Allowing tracking and removal of the * smallest item in a priority queue. */ public interface MinPQ\u003cItem\u003e { /** Adds the item to the priority queue. */ public void add(Item x); /** Returns the smallest item in the priority queue. */ public Item getSmallest(); /** Removes the smallest item from the priority queue. */ public Item removeSmallest(); /** Returns the size of the priority queue. */ public int size(); } å…¨è®°å½•ä¸‹æ¥ç„¶åæ’åºï¼Ÿnaive way! better way heaps ","date":"2024-07-13","objectID":"/61b-24/:0:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"bst way see https://goo.gl/wBKdFQ Given a heap, how do we implement PQ operations? getSmallest() - return the item in the root node. add(x) - place the new employee in the last position, and promote as high as possible. removeSmallest() - assassinate the president (of the company), promote the rightmost person in the company to president. Then demote repeatedly, always taking the â€˜betterâ€™ successor. Tree Representations ","date":"2024-07-13","objectID":"/61b-24/:1:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"How do we Represent a Tree in Java? ","date":"2024-07-13","objectID":"/61b-24/:2:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"1a ","date":"2024-07-13","objectID":"/61b-24/:2:1","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"1b ","date":"2024-07-13","objectID":"/61b-24/:2:2","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"1c ","date":"2024-07-13","objectID":"/61b-24/:2:3","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"2 Store keys in an array. Store parentIDs in an array. Similar to what we did with disjointSets. ","date":"2024-07-13","objectID":"/61b-24/:2:4","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"3 Store keys in an array. Donâ€™t store structure anywhere. ä»å·¦åˆ°å³åˆ†å±‚çº§ç¼–ç è¿›å…¥æ•°ç»„ public class Tree3\u003cKey\u003e { Key[] keys; ... public void swim(int k) { if (keys[parent(k)] â‰» keys[k]) { swap(k, parent(k)); swim(parent(k)); } } public int parent(int k) { return (k - 1) / 2; } // è§‚å¯Ÿæ³• 3b å¯¹æ¯” Data Structures Summary ","date":"2024-07-13","objectID":"/61b-24/:2:5","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"Search Data Structures (The particularly abstract ones) ","date":"2024-07-13","objectID":"/61b-24/:3:0","tags":null,"title":"61B-24: Priority Queues and Heaps","uri":"/61b-24/"},{"categories":["UCB-CS61B"],"content":"Traversals Level Order Traverse top-to-bottom, left-to-right (like reading in English): We say that the nodes are â€˜visitedâ€™ in the given order. Depth First Traversals Preorder, Inorder, Postorder Basic (rough) idea: Traverse â€œdeep nodesâ€ (e.g. A) before shallow ones (e.g. F). ","date":"2024-07-13","objectID":"/61b-25/:0:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"preorder preOrder(BSTNode x) { if (x == null) return; print(x.key) preOrder(x.left) preOrder(x.right) } D B A C F E G ","date":"2024-07-13","objectID":"/61b-25/:0:1","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"inorder inOrder(BSTNode x) { if (x == null) return; inOrder(x.left) print(x.key) inOrder(x.right) } A B C D E F G ","date":"2024-07-13","objectID":"/61b-25/:0:2","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"postorder postOrder(BSTNode x) { if (x == null) return; postOrder(x.left) postOrder(x.right) print(x.key) } A C B E G F D ","date":"2024-07-13","objectID":"/61b-25/:0:3","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"trick to think about ","date":"2024-07-13","objectID":"/61b-25/:1:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"visitor pattern interface Action\u003cLabel\u003e { void visit(Tree\u003cLabel\u003e T); } class FindPig implements Action\u003cString\u003e { boolean found = false; @Override void visit(Tree\u003cString\u003e T) { if (\"pig\".equals(T.label)) { found = true; } } } void preorderTraverse(Tree\u003cLabel\u003e T, Action\u003cLabel\u003e whatToDo) { if (T == null) { return; } whatToDo.visit(T); /* before we hard coded a print */ preorderTraverse(T.left, whatToDo); preorderTraverse(T.right, whatToDo); } preorderTraverse(someTree, new FindPig()); What is the runtime of a preorder traversal in terms of N, the number of nodes? (in code below, assume the visit action takes constant time) Î˜(N) : Every node visited exactly once. Constant work per visit. Level Order Traversal ","date":"2024-07-13","objectID":"/61b-25/:2:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"Iterative Deepening public void levelOrder(Tree T, Action toDo) { for (int i = 0; i \u003c T.height(); i += 1) { visitLevel(T, i, toDo); } } // Run visitLevel H times, one for each level. public void visitLevel(Tree T, int level, Action toDo) { if (T == null) { return; } if (lev == 0) { toDo.visit(T.key); } else { visitLevel(T.left(), lev - 1, toDo); visitLevel(T.right(), lev - 1, toDo); } } For algorithms whose runtime depends on height, difference between bushy tree and spindly tree can be huge! Range Finding é—®é¢˜æè¿° Easy approach, just do a traversal of the whole tree, and use visitor pattern to collect matching items. $\\Theta(N)$ better way: Pruning, Restricting our search to only nodes that might contain the answers we seek. $\\Theta(log N+R)$ ï¼Œå…¶ä¸­Ræ˜¯åŒ¹é…çš„çš„ä¸ªæ•°ã€‚ Spatial Trees 2D Range Finding, Building Trees of Two Dimensional Data Optional: Tree Iterators https://docs.google.com/presentation/d/14pqGRZAN_Q60xqYGR3c8XTqHdQT8BVWGPJPC_G_INJQ/edit#slide=id.g75c09ac94_0284 ","date":"2024-07-13","objectID":"/61b-25/:3:0","tags":null,"title":"61B-25: Advanced Trees, incl. Geometric","uri":"/61b-25/"},{"categories":["UCB-CS61B"],"content":"ordered linked listâ€”\u003e binary search tree or skip list (out of course) BST Definitions A tree consists of: A set of nodes. A set of edges that connect those nodes. Constraint: There is exactly one path between any two nodes. In a rooted tree, we call one node the root. Every node N except the root has exactly one parent, defined as the first node on the path from N to the root. Unlike (most) real trees, the root is usually depicted at the top of the tree. A node with no child is called a leaf. In a rooted binary tree, every node has either 0, 1, or 2 children (subtrees). ","date":"2024-07-13","objectID":"/61b-21/:0:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Properties of BSTs A binary search tree is a rooted binary tree with the BST property. BST Property. For every node X in the tree: Every key in the left subtree is less than Xâ€™s key. Every key in the right subtree is greater than Xâ€™s key. One consequence of these rules: No duplicate keys allowed! BST Operations ","date":"2024-07-13","objectID":"/61b-21/:1:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Finding a searchKey in a BST static BST find(BST T, Key sk) { if (T == null) return null; if (sk.keyequals(T.label())) return T; else if (sk â‰º T.label()) return find(T.left, sk); else return find(T.right, sk); } runtime: $O(h)$, where $h$ is the height of the tree, i.e., $O(log (N))$, where $N$ is the number of nodes in the tree. ","date":"2024-07-13","objectID":"/61b-21/:2:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Inserting a new key into a BST static BST insert(BST T, Key ik) { if (T == null) return new BST(ik); if (ik â‰º T.label()) T.left = insert(T.left, ik); else if (ik â‰» T.label()) T.right = insert(T.right, ik); return T; } ","date":"2024-07-13","objectID":"/61b-21/:3:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Deleting a key from a BST no child: simply remove the node. one child: replace the node with its child. two children: find the inorder successor (smallest in the right subtree) and replace the node with it. Then recursively delete the inorder successor from the right subtree. BST Performance ","date":"2024-07-13","objectID":"/61b-21/:4:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Tree Height Performance of spindly trees can be just as bad as a linked list! usually, the height of a BST is $O(log(N))$, where $N$ is the number of nodes in the tree. ","date":"2024-07-13","objectID":"/61b-21/:5:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Insertion demo Random inserts take on average only Î˜(log N) each. ","date":"2024-07-13","objectID":"/61b-21/:6:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"Deletion demo https://docs.google.com/presentation/d/1rEHpAx8Xu2LnJBWsRPWy8blL20qb96Q5UhdZtQYFkBI/edit#slide=id.g75707c75c_0224 ","date":"2024-07-13","objectID":"/61b-21/:7:0","tags":null,"title":"61B-21: Trees, BSTs","uri":"/61b-21/"},{"categories":["UCB-CS61B"],"content":"To avoid the worst-case time complexity of O(n), we need to use a balanced binary search tree. Tree Rotation Non-obvious fact: Rotation allows balancing of any BST. One way to achieve balance: Rotate after each insertion and deletion to maintain balance. â€¦ the mystery is to know which rotations. Weâ€™ll come back to this. B-trees / 2-3 trees / 2-3-4 trees ","date":"2024-07-13","objectID":"/61b-22/:0:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"search tree ","date":"2024-07-13","objectID":"/61b-22/:1:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"weird solution to achieve balance add leaf overstuff leaf, but can not be too juicy! ","date":"2024-07-13","objectID":"/61b-22/:2:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"Performance of B-trees Splitting tree is a better name, but I didnâ€™t invent them, so weâ€™re stuck with their real name: B-trees. A B-tree of order M=4 (like we used today) is also called a 2-3-4 tree or a 2-4 tree. The name refers to the number of children that a node can have, e.g. a 2-3-4 tree node may have 2, 3, or 4 children. A B-tree of order M=3 (like in the textbook) is also called a 2-3 tree. Red-Black Trees There are many types of search trees: Binary search trees: Require rotations to maintain balance. There are many strategies for rotation. Coming up with a strategy is hard. 2-3 trees: No rotations required. Clever (and strange idea): Build a BST that is isometric (structurally identical) to a 2-3 tree. Use rotations to ensure the isometry. Since 2-3 trees are balanced, rotations on BST will ensure balance. Maintaining Isometry Through Rotations (Optional) Violations for LLRBs: Two red children. Two consecutive red links. Right red child. Operations for Fixing LLRB Tree Violations: Tree rotations and Color Flips! when insert , use red link if right-insert happens, rotateLeft two red children? two reds in a row? Left-Red-Right-Red? ","date":"2024-07-13","objectID":"/61b-22/:3:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"summary ","date":"2024-07-13","objectID":"/61b-22/:4:0","tags":null,"title":"61B-22: Balanced BSTs","uri":"/61b-22/"},{"categories":["UCB-CS61B"],"content":"èµ·å› ï¼Œæ— åºarray ","date":"2024-07-13","objectID":"/61b-23/:0:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"Using data as an Index One extreme approach: All data is really just bits. Use data itself as an array index. Store true and false in the array. Extremely wasteful of memory. To support checking presence of all positive integers, we need 2 billion booleans. Need some way to generalize beyond integers. public class DataIndexedIntegerSet { boolean[] present; public DataIndexedIntegerSet() { present = new boolean[16]; } public insert(int i) { present[i] = true; } public contains(int i) { return present[i]; } } Binary Representations DataIndexedSet insert a cat ä½†æ˜¯æœ‰å¼±ç‚¹â†“ collision handling \u0026 computing a hashCodeï¼ Handling Collisions æŠ½å±‰åŸç†å‘Šè¯‰æˆ‘ä»¬ï¼Œä¸å¯ä»¥åªé æ‰©å±•æ•°ç»„å®¹é‡æ¥å¤„ç†ä¹‹ã€‚ Suppose N items have the same hashcode h: Instead of storing true in position h, store a list of these N items at position h. ","date":"2024-07-13","objectID":"/61b-23/:1:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"external chaining Depends on the number of items in the â€˜bucketâ€™. If N items are distributed across M buckets, average time grows with N/M = L, also known as the load factor. Average runtime is Î˜(L). Whenever L=N/M exceeds some number, increase M by resizing. è´Ÿæ•°ç´¢å¼• Hash Functions ","date":"2024-07-13","objectID":"/61b-23/:2:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"str example @Override public int hashCode() { int hashCode = 1; for (Object o : this) { hashCode = hashCode * 31; hashCode = hashCode + o.hashCode(); } return hashCode; } ","date":"2024-07-13","objectID":"/61b-23/:3:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"recursive example ","date":"2024-07-13","objectID":"/61b-23/:4:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"default hashCode() All Objects have hashCode() function. Default: returns this (i.e. address of object). Can have strange consequences: â€œhelloâ€.hashCode() is not the same as (â€œhâ€ + â€œelloâ€).hashCode() Can override for your type. Hash tables (HashSet, HashMap, etc.) are so important that Java requires that all objects implement hashCode(). ","date":"2024-07-13","objectID":"/61b-23/:5:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"HashSets and HashMaps Java provides a hash table based implementation of sets and maps. Idea is very similar to what weâ€™ve done in lecture. Warning: Never store mutable objects in a HashSet or HashMap! Warning #2: Never override equals without also overriding hashCode. extra ","date":"2024-07-13","objectID":"/61b-23/:6:0","tags":null,"title":"61B-23: Hashing","uri":"/61b-23/"},{"categories":["UCB-CS61B"],"content":"Big-O Notation ","date":"2024-07-13","objectID":"/61b-19/:1:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"ç»†èŠ‚åˆ†æ ","date":"2024-07-13","objectID":"/61b-19/:2:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"å±€é™æ€§ ","date":"2024-07-13","objectID":"/61b-19/:3:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"å¯¹æ¯” Important: Big O does not mean â€œworst caseâ€! Often abused to mean this. ","date":"2024-07-13","objectID":"/61b-19/:4:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"å¤§Oè®°å·çš„ç”¨å¤„ ","date":"2024-07-13","objectID":"/61b-19/:5:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"Big-Omega Notation $\\Omega(f(n))$ ","date":"2024-07-13","objectID":"/61b-19/:6:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"å¤§Î©è®°å·çš„ç”¨å¤„ ","date":"2024-07-13","objectID":"/61b-19/:7:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"éä¸¥æ ¼è¯æ˜å’Œä¸¥æ ¼è¯æ˜ ","date":"2024-07-13","objectID":"/61b-19/:8:0","tags":null,"title":"61B-19: Asymptotics III","uri":"/61b-19/"},{"categories":["UCB-CS61B"],"content":"ä¸ç›¸äº¤é›†é—®é¢˜ public interface DisjointSets { /** Connects two items P and Q. */ void connect(int p, int q); /** Checks to see if two items are connected. */ boolean isConnected(int p, int q); } ","date":"2024-07-13","objectID":"/61b-20/:0:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"naive implementation çœŸçš„é“¾æ¥ä¸¤ä¸ªå…ƒç´ ï¼Œç„¶åè€ƒè™‘éå†æ•´ä¸ªé›†åˆï¼Œåˆ¤æ–­æ˜¯å¦æœ‰ä¸¤ä¸ªå…ƒç´ æ˜¯è¿é€šçš„ã€‚ ","date":"2024-07-13","objectID":"/61b-20/:0:1","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"better implementation Better approach: Model connectedness in terms of sets. How things are connected isnâ€™t something we need to know.ğŸ˜‰ ","date":"2024-07-13","objectID":"/61b-20/:0:2","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"quick-find public class QuickFindDS implements DisjointSets { private int[] id; // really fast public boolean isConnected(int p, int q) { return id[p] == id[q]; } public void connect(int p, int q) { int pid = id[p]; int qid = id[q]; for (int i = 0; i \u003c id.length; i++) { if (id[i] == pid) { id[i] = qid; } }... } // constructor public QuickFindDS(int N) { id = new int[N]; for (int i = 0; i \u003c N; i++) id[i] = i; } } ","date":"2024-07-13","objectID":"/61b-20/:1:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"quick-union è€ƒè™‘ä¸ç”¨æ•°ç»„ï¼Œç”¨ ğŸŒ³ğŸ˜‹ tree can not be too tall: æ ‘ä¸èƒ½å¤ªé«˜ï¼Œå¦åˆ™ä¼šé€€åŒ–æˆé“¾è¡¨âš ï¸ public class QuickUnionDS implements DisjointSets { private int[] parent; public QuickUnionDS(int N) { parent = new int[N]; for (int i = 0; i \u003c N; i++) parent[i] = i; } // linear time to create N trees private int find(int p) { while (p != parent[p]) p = parent[p]; // p[i] and i å¾ˆé‡è¦ï¼ return p; } public boolean isConnected(int p, int q) { return find(p) == find(q); } public void connect(int p, int q) { int i = find(p); int j = find(q); parent[i] = j; // åˆå¹¶ä¸¤ä¸ªæ ‘ } } ","date":"2024-07-13","objectID":"/61b-20/:2:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"weighted quick-union å¸Œæœ›å¹³è¡¡æƒé‡ æƒé‡å¯ä»¥æ˜¯æ ‘çš„å¤§å°ï¼Œä¹Ÿå¯ä»¥æ˜¯æ ‘çš„æ·±åº¦ã€‚ ä»¥ä¸‹è€ƒè™‘å…ƒç´ ä¸ªæ•°ï¼ˆæ ‘çš„å¤§å°ï¼‰ New ruleï¼ˆç›®å‰æ˜¯ä¸åŠ è¯æ˜çš„ç»éªŒå…¬å¼ï¼‰: Always link root of smaller tree to larger tree. public class WeightedQuickUnionDS implements DisjointSets { private int[] parent; private int[] size; // size of each tree public WeightedQuickUnionDS(int N) { parent = new int[N]; size = new int[N]; // å¢åŠ äº†size arrayè®°å½• for (int i = 0; i \u003c N; i++) { parent[i] = i; size[i] = 1; // each tree is of size 1 } } // find and isConnected are the same as before! private int find(int p) { while (p != parent[p]) p = parent[p]; // p[i] and i å¾ˆé‡è¦ï¼ return p; } public boolean isConnected(int p, int q) { return find(p) == find(q); } public void connect(int p, int q) { int i = find(p); int j = find(q); if (size[i] \u003c size[j]) { parent[i] = j; size[j] += size[i]; // add size of i to j } else { parent[j] = i; size[i] += size[j]; // add size of j to i } } } ","date":"2024-07-13","objectID":"/61b-20/:3:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"path compressionï¼ˆUCB-CS170ğŸ˜‹ï¼‰ è·¯å¾„å‹ç¼©ï¼šå°†æ ‘çš„æ ¹èŠ‚ç‚¹æŒ‡å‘æ ‘çš„æ ¹èŠ‚ç‚¹ï¼Œå‡å°‘æ ‘çš„é«˜åº¦ã€‚ è·¯å¾„å‹ç¼©çš„å¥½å¤„ï¼š å‡å°‘æ ‘çš„é«˜åº¦ï¼Œä½¿å¾—findå’ŒisConnectedçš„æ•ˆç‡æ›´é«˜ã€‚ å‡å°‘å†…å­˜æ¶ˆè€—ã€‚ log*(n) is the iterated log - itâ€™s the number of times you need to apply log to n to go below 1. Note that 2^65536 is higher than the number of atoms in the universe. ","date":"2024-07-13","objectID":"/61b-20/:4:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"ä¸åŠ è¯æ˜ç»™å‡ºç›®å‰æœ€æé™çš„æƒ…å†µ $\\alpha(N)$ public class WeightedQuickUnionDSWithPathCompression implements DisjointSets { private int[] parent; private int[] size; public WeightedQuickUnionDSWithPathCompression(int N) { parent = new int[N]; size = new int[N]; for (int i = 0; i \u003c N; i++) { parent[i] = i; size[i] = 1; } } // find å¹¶ä¸ä¼šå¤ªéš¾ ä¹ private int find(int p) { if (p == parent[p]) { return p; } else { parent[p] = find(parent[p]); return parent[p]; } } public boolean isConnected(int p, int q) { return find(p) == find(q); } public void connect(int p, int q) { int i = find(p); int j = find(q); if (i == j) return; if (size[i] \u003c size[j]) { parent[i] = j; size[j] += size[i]; } else { parent[j] = i; size[i] += size[j]; } } } ","date":"2024-07-13","objectID":"/61b-20/:4:1","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"references Nazca Lines: http://redicecreations.com/ul_img/24592nazca_bird.jpg Implementation code adapted from Algorithms, 4th edition and Professor Jonathan Shewchukâ€™s lecture notes on disjoint sets, where he presents a faster one-array solution. I would recommend taking a look. (http://www.cs.berkeley.edu/~jrs/61b/lec/33) The proof of the inverse ackermann runtime for disjoint sets is given here: http://www.uni-trier.de/fileadmin/fb4/prof/INF/DEA/Uebungen_LVA-Ankuendigungen/ws07/KAuD/effi.pdf as originally proved by Tarjan here at UC Berkeley in 1975. ","date":"2024-07-13","objectID":"/61b-20/:5:0","tags":null,"title":"61B-20: Disjoint Sets","uri":"/61b-20/"},{"categories":["UCB-CS61B"],"content":"æ²¡æœ‰æ·å¾„ï¼Œå…¨é ä»”ç»† for loops ","date":"2024-07-13","objectID":"/61b-18/:1:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"recursion å½¢å¦‚$\\Theta(n^k)$, $k$ æ˜¯é€’å½’çš„æ·±åº¦ ","date":"2024-07-13","objectID":"/61b-18/:2:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"binary search å½¢å¦‚$\\Theta(\\log n)$ C(N) = âŒŠlog2(N)âŒ‹+1 ","date":"2024-07-13","objectID":"/61b-18/:3:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"merge sort å½¢å¦‚$\\Theta(n\\log n)$ ","date":"2024-07-13","objectID":"/61b-18/:4:0","tags":null,"title":"61B-18: Asymptotics II","uri":"/61b-18/"},{"categories":["UCB-CS61B"],"content":"Runtime Characterizations In most cases, we care only about asymptotic behavior, i.e. what happens for very large N. ","date":"2024-07-13","objectID":"/61b-17/:1:0","tags":null,"title":"61B-17: Asymptotics I","uri":"/61b-17/"},{"categories":["UCB-CS61B"],"content":"Intuitive Simplification Consider only the worst case. Restrict Attention to One Operation. æ‰¾å¾—å¥½å¹¶ä¸”å·§çš„è¯å¯å¾ˆå¿«çœ‹å‡ºï¼Œé€€è€Œæ±‚å…¶æ¬¡çš„è¯å¯ä»¥è€ƒè™‘ç”»å›¾åˆ†æ Eliminate low order terms. Eliminate multiplicative constants. ","date":"2024-07-13","objectID":"/61b-17/:2:0","tags":null,"title":"61B-17: Asymptotics I","uri":"/61b-17/"},{"categories":["UCB-CS61B"],"content":"Big-Theta Notation $\\Theta(f(n))$ The only difference is that we use the Î˜ symbol anywhere we would have said â€œorder of growthâ€. ","date":"2024-07-13","objectID":"/61b-17/:3:0","tags":null,"title":"61B-17: Asymptotics I","uri":"/61b-17/"},{"categories":["DATA100"],"content":" # Initialize Otter import otter grader = otter.Notebook(\"lab01.ipynb\") Lab 01 Welcome to the first lab of Data 100! This lab is meant to help you familiarize yourself with JupyterHub, review Python and numpy, and introduce you to matplotlib, a Python visualization library. ","date":"2024-07-13","objectID":"/datalab1/:0:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Part 1: Jupyter Tips ","date":"2024-07-13","objectID":"/datalab1/:1:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Viewing Documentation To output the documentation for a function, use the help function. # help(print) ?print You can also use Jupyter to view function documentation inside your notebook. The function must already be defined in the kernel for this to work. Below, click your mouse anywhere on the print block below and use Shift + Tab to view the functionâ€™s documentation. ","date":"2024-07-13","objectID":"/datalab1/:1:1","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Importing Libraries and Magic Commands import pandas as pd import numpy as np import matplotlib.pyplot as plt plt.style.use('fivethirtyeight') %matplotlib inline %matplotlib inline is a Jupyter magic command that configures the notebook so that Matplotlib displays any plots that you draw directly in the notebook rather than to a file, allowing you to view the plots upon executing your code. (Note: In practice, this is no longer necessary, but weâ€™re showing it to you now anyway.) Another useful magic command is %%time, which times the execution of that cell. You can use this by writing it as the first line of a cell. (Note that %% is used for cell magic commands that apply to the entire cell, whereas % is used for line magic commands that only apply to a single line.) %%time lst = [] for i in range(100): lst.append(i) CPU times: total: 0 ns\rWall time: 0 ns\r","date":"2024-07-13","objectID":"/datalab1/:1:2","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Keyboard Shortcuts To learn about keyboard shortcuts, go to Help â€“\u003e Keyboard Shortcuts in the menu above. Here are a few that we like: Ctrl + Return (or Cmd + Return on Mac): Evaluate the current cell Shift + Return: Evaluate the current cell and move to the next ESC : command mode (may need to press before using any of the commands below) a : create a cell above b : create a cell below dd : delete a cell z : undo the last cell operation m : convert a cell to markdown y : convert a cell to code ","date":"2024-07-13","objectID":"/datalab1/:1:3","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Part 2: Prerequisites ","date":"2024-07-13","objectID":"/datalab1/:2:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Python Python is the main programming language weâ€™ll use in the course. We expect that youâ€™ve taken CS 61A, Data 8, or an equivalent class, so we will not be covering general Python syntax. If any of the following exercises are challenging (or if you would like to refresh your Python knowledge), please review one or more of the following materials. Python Tutorial: Introduction to Python from the creators of Python. Composing Programs Chapter 1: This is more of a introduction to programming with Python. Advanced Crash Course: A fast crash course which assumes some programming background. ","date":"2024-07-13","objectID":"/datalab1/:2:1","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"NumPy NumPy is the numerical computing module introduced in Data 8, which is a prerequisite for this course. Hereâ€™s a quick recap of NumPy. For more review, read the following materials. NumPy Quick Start Tutorial DS100 NumPy Review Stanford CS231n NumPy Tutorial The Data 8 Textbook Chapter on NumPy ","date":"2024-07-13","objectID":"/datalab1/:2:2","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 1 The core of NumPy is the array. Like Python lists, arrays store data; however, they store data in a more efficient manner. In many cases, this allows for faster computation and data manipulation. In Data 8, we used make_array from the datascience module, but thatâ€™s not the most typical way. Instead, use np.array to create an array. It takes a sequence, such as a list or range. Below, create an array arr containing the values 1, 2, 3, 4, and 5 (in that order). arr = np.array([1,2,3,4,5]) grader.check(\"q1\") q1 passed! ğŸŒŸ In addition to values in the array, we can access attributes such as shape and data type. A full list of attributes can be found here. arr[3] np.int64(4)\rarr[2:4] # å·¦é—­å³å¼€ array([3, 4])\rarr.shape # ä¸€ç»´é•¿åº¦ä¸º5 (5,)\rarr.dtype dtype('int64')\rArrays, unlike Python lists, cannot store items of different data types. # A regular Python list can store items of different data types [1, '3'] [1, '3']\r# Arrays will convert everything to the same data type np.array([1, '3']) array(['1', '3'], dtype='\u003cU21')\r# Another example of array type conversion np.array([5, 8.3]) array([5. , 8.3])\rArrays are also useful in performing vectorized operations. Given two or more arrays of equal length, arithmetic will perform element-wise computations across the arrays. For example, observe the following: # Python list addition will concatenate the two lists [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6]\r# NumPy array addition will add them element-wise np.array([1, 2, 3]) + np.array([4, 5, 6]) array([5, 7, 9])\r","date":"2024-07-13","objectID":"/datalab1/:2:3","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 2 Question 2a Write a function summation that evaluates the following summation for $n \\geq 1$: $$\\sum_{i=1}^{n} i^3 + 3 i^2$$ Note: You should not use for loops in your solution. Check the NumPy documentation. If youâ€™re stuck, try a search engine! Searching the web for examples of how to use modules is very common in data science. # ç”¨å¥½npå‘é‡åŒ– def summation(n): \"\"\"Compute the summation i^3 + 3 * i^2 for 1 \u003c= i \u003c= n.\"\"\" arr = np.arange(1, n+1) newArr = arr**3 + 3 * arr**2 return int(np.sum(newArr)) grader.check(\"q2a\") q2a passed! ğŸ’¯ Question 2b Write a function elementwise_array_sum that computes the square of each value in list_1, the cube of each value in list_2, then returns a list containing the element-wise sum of these results. Assume that list_1 and list_2 have the same number of elements, do not use for loops. The input parameters will both be python lists, so you may need to convert the lists into arrays before performing your operations. The output should be a numpy array. def elementwise_array_sum(list_1, list_2): \"\"\"Compute x^2 + y^3 for each x, y in list_1, list_2. Assume list_1 and list_2 have the same length. Return a NumPy array. \"\"\" assert len(list_1) == len(list_2), \"both args must have the same number of elements\" # create a NumPy array from the two lists arr1 = np.array(list_1) arr2 = np.array(list_2) arr_sum = arr1 ** 2 + arr2 ** 3 return arr_sum grader.check(\"q2b\") q2b passed! ğŸŒŸ You might have been told that Python is slow, but array arithmetic is carried out very fast, even for large arrays. Below is an implementation of the above code that does not use NumPy arrays. def elementwise_list_sum(list_1, list_2): \"\"\"Compute x^2 + y^3 for each x, y in list_1, list_2. Assume list_1 and list_2 have the same length. \"\"\" return [x ** 2 + y ** 3 for x, y in zip(list_1, list_2)] For ten numbers, elementwise_list_sum and elementwise_array_sum both take a similar amount of time. sample_list_1 = list(range(10)) sample_array_1 = np.arange(10) %%time elementwise_list_sum(sample_list_1, sample_list_1) CPU times: total: 0 ns\rWall time: 0 ns\r[0, 2, 12, 36, 80, 150, 252, 392, 576, 810]\r%%time elementwise_array_sum(sample_array_1, sample_array_1) CPU times: total: 0 ns\rWall time: 0 ns\rarray([ 0, 2, 12, 36, 80, 150, 252, 392, 576, 810])\rThe time difference seems negligible for a list/array of size 10; depending on your setup, you may even observe that elementwise_list_sum executes faster than elementwise_array_sum! However, we will commonly be working with much larger datasets: sample_list_2 = list(range(100000)) sample_array_2 = np.arange(100000) %%time elementwise_list_sum(sample_list_2, sample_list_2) ; # The semicolon hides the output CPU times: total: 15.6 ms\rWall time: 33.2 ms\r%%time elementwise_array_sum(sample_array_2, sample_array_2) CPU times: total: 0 ns\rWall time: 557 Î¼s\rarray([ 0, 2, 12, ...,\r999920002099982, 999950000799996, 999980000100000])\rWith the larger dataset, we see that using NumPy results in code that executes over 50 times faster! Throughout this course (and in the real world), you will find that writing efficient code will be important; arrays and vectorized operations are the most common way of making Python programs run quickly. Question 2c Recall the formula for population variance below: $$\\sigma^2 = \\frac{\\sum_{i=1}^N (x_i - \\mu)^2}{N}$$ Complete the functions below to compute the population variance of population, an array of numbers. For this question, do not use built in NumPy functions, such as np.var. Again, avoid using for loops! def mean(population): \"\"\" Returns the mean of population (mu) Keyword arguments: population -- a numpy array of numbers \"\"\" # Calculate the mean of a population return sum(population)/float(len(population)) def variance(population): \"\"\" Returns the variance of population (sigma squared) Keyword arguments: population -- a numpy array of numbers \"\"\" # Calculate the variance of a population mu = mean(population) return sum((population-mu) ** 2)/float(len(populati","date":"2024-07-13","objectID":"/datalab1/:2:4","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Part 3: Plotting Here we explore plotting using matplotlib and numpy. ","date":"2024-07-13","objectID":"/datalab1/:3:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 3 Consider the function $f(x) = x^2$ for $-\\infty \u003c x \u003c \\infty$. Question 3a Find the equation of the tangent line to $f$ at $x = 0$. Use LaTeX to type your solution, such that it looks like the serif font used to display the math expressions in the sentences above. HINT: You can click any text cell to see the raw Markdown syntax. $tangent line: åˆ‡çº¿$ $y = 0$ Question 3b Find the equation of the tangent line to $f$ at $x = 8$. Please use LaTeX to type your solution. $y = 16x$ Question 3c Write code to plot the function $f$, the tangent line at $x=8$, and the tangent line at $x=0$. Set the range of the x-axis to (-15, 15) and the range of the y-axis to (-100, 300) and the figure size to (4,4). Your resulting plot should look like this (itâ€™s okay if the colors in your plot donâ€™t match with ours, as long as theyâ€™re all different colors): You should use the plt.plot function to plot lines. You may find the following functions useful: plt.plot(..) plt.figure(figsize=..) plt.ylim(..) plt.axhline(..) def f(x): return x ** 2 def df(x): return 2*x def plot(f, df): plt.figure(figsize=(4, 4)) x = np.array([-15, 15]) plt.plot(x,f(x),color='blue') plt.plot(x,df(x),color='green') plt.axhline(0,color='red') plt.ylim(-100, 300) plot(f, df) ","date":"2024-07-13","objectID":"/datalab1/:3:1","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 4 (Ungraded) Data science is a rapidly expanding field and no degree program can hope to teach you everything that will be helpful to you as a data scientist. So itâ€™s important that you become familiar with looking up documentation and learning how to read it. Below is a section of code that plots a three-dimensional â€œwireframeâ€ plot. Youâ€™ll see what that means when you draw it. Replace each # Your answer here with a description of what the line above does, what the arguments being passed in are, and how the arguments are used in the function. For example, np.arange(2, 5, 0.2) # This returns an array of numbers from 2 to 5 with an interval size of 0.2 Hint: The Shift + Tab tip from earlier in the notebook may help here. Remember that objects must be defined in order for the documentation shortcut to work; for example, all of the documentation will show for method calls from np since weâ€™ve already executed import numpy as np. However, since z is not yet defined in the kernel, z.reshape(x.shape) will not show documentation until you run the line z = np.cos(squared). from mpl_toolkits.mplot3d import axes3d u = np.linspace(1.5 * np.pi, -1.5 * np.pi, 100) # Your answer here [x, y] = np.meshgrid(u, u) # Your answer here squared = np.sqrt(x.flatten() ** 2 + y.flatten() ** 2) z = np.cos(squared) # Your answer here z = z.reshape(x.shape) # Your answer here fig = plt.figure(figsize = (6, 6)) ax = fig.add_subplot(111, projection = '3d') # Your answer here ax.plot_wireframe(x, y, z, rstride = 5, cstride = 5, lw = 2) # Your answer here ax.view_init(elev = 60, azim = 25) # Your answer here plt.savefig(\"figure1.png\") # Your answer here ","date":"2024-07-13","objectID":"/datalab1/:3:2","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Question 5 (Ungraded) Do you think that eating french fries with mayonnaise is a crime? Tell us what you think in the following Markdown cell. :) To double-check your work, the cell below will rerun all of the autograder tests. grader.check_all() ","date":"2024-07-13","objectID":"/datalab1/:3:3","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"Submission Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting! # Save your notebook first, then run this cell to export your submission. grader.export(pdf=False) ","date":"2024-07-13","objectID":"/datalab1/:4:0","tags":["Jupyter","numpy","matplotlib"],"title":"DATA100-lab1","uri":"/datalab1/"},{"categories":["DATA100"],"content":"two common errors chance error: randomness can vary bias error: systematic error in one direction ","date":"2024-07-13","objectID":"/datal2/:1:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"bias ","date":"2024-07-13","objectID":"/datal2/:2:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"common non-random samples convenience samples: samples that are easy to obtain but may not be representative of the population quota samples: samples that are drawn from a limited number of individuals or groups ","date":"2024-07-13","objectID":"/datal2/:3:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"random samples random can produce biases but we can estimate the bias and chance error properties of random samples: æ˜ç¡®æ¦‚ç‡ no need to be same chance ğŸ˜‹ scheme of random sampling: â€œRandom sample with replacementâ€ æ˜¯ç»Ÿè®¡å­¦ä¸­çš„ä¸€ä¸ªæœ¯è¯­ï¼ŒæŒ‡çš„æ˜¯åœ¨è¿›è¡ŒæŠ½æ ·æ—¶ï¼Œæ¯æ¬¡æŠ½å–çš„æ ·æœ¬åœ¨æ”¾å›åŸæ€»ä½“ä¹‹åï¼Œå†è¿›è¡Œä¸‹ä¸€æ¬¡æŠ½å–ã€‚è¿™æ„å‘³ç€åŒä¸€ä¸ªä¸ªä½“æˆ–å…ƒç´ æœ‰å¯èƒ½è¢«å¤šæ¬¡æŠ½å–ã€‚ è¿™ç§æŠ½æ ·æ–¹æ³•çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š æ¯æ¬¡æŠ½å–éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå³å‰ä¸€æ¬¡çš„æŠ½å–ç»“æœä¸ä¼šå½±å“åä¸€æ¬¡çš„æŠ½å–ã€‚ æ€»ä½“ä¸­çš„æ¯ä¸ªå…ƒç´ åœ¨æ¯æ¬¡æŠ½å–ä¸­è¢«é€‰ä¸­çš„æ¦‚ç‡æ˜¯ç›¸åŒçš„ã€‚ ç”±äºæ ·æœ¬è¢«æ”¾å›ï¼Œæ ·æœ¬çš„å¤§å°å¯ä»¥ç­‰äºæˆ–å°äºæ€»ä½“çš„å¤§å°ã€‚ ä¸ä¹‹ç›¸å¯¹çš„æ˜¯ â€œRandom sample without replacementâ€ï¼Œå³ä¸æ”¾å›æŠ½æ ·ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä¸€æ—¦ä¸€ä¸ªå…ƒç´ è¢«æŠ½å–ï¼Œå®ƒå°±ä¸ä¼šå†æ¬¡è¢«æŠ½å–ï¼Œå› æ­¤æŠ½å–çš„æ ·æœ¬é‡æ€»æ˜¯å°äºæ€»ä½“çš„å¤§å°ã€‚ğŸ˜‰ SRSï¼šğŸ¤”æ³¨æ„æ˜¯æ¯ä¸ª\"pair\"ï¼ ","date":"2024-07-13","objectID":"/datal2/:4:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"å¤šé¡¹å¼å’ŒäºŒé¡¹å¼åˆ†å¸ƒé‡‡æ · ","date":"2024-07-13","objectID":"/datal2/:5:0","tags":null,"title":"DATA100-L2: Data Sampling and Probability","uri":"/datal2/"},{"categories":["DATA100"],"content":"cycle of data science ","date":"2024-07-12","objectID":"/datal1/:1:0","tags":null,"title":"DATA100-L1: course overview","uri":"/datal1/"},{"categories":["UCB-CS61B"],"content":"math problems $$ N! âˆˆ \\Omega (N^{N}) ? $$ âˆš $$ log(N!) âˆˆ \\Omega (NlogN) ? $$ âˆš $$ NlogNâˆˆ \\Omega (log(N!)) ? $$ âˆš æ‰€ä»¥å¯ä»¥æ¨å‡ºï¼š $$ NlogN âˆˆ \\Theta (logN!) $$ $$ log(N!) âˆˆ \\Theta (NlogN) $$ ","date":"2024-07-11","objectID":"/61b-35/:1:0","tags":null,"title":"61B-35: Sorting and Algorithmic Bounds","uri":"/61b-35/"},{"categories":["UCB-CS61B"],"content":"TUCSç”¨æ—¶ ä¸Šä¸‹ç•Œï¼Ÿ the ultimate comparison sort run time $$ \\Omega(NlogN) $$ $$ O(NlogN) $$ ä¸‹é¢å¼€å§‹è¯æ˜ï¼š è€ƒè™‘ä¸‹ç•Œï¼Œå¯¹nä¸ªç‰©ä½“è¿›è¡Œæ’åºï¼Œæœ‰Nï¼ç§å¯èƒ½ï¼Œç”¨ä¸¤ä¸¤æ¯”å¤§å°ï¼Œè€ƒè™‘å†³ç­–æ ‘çš„é«˜åº¦$$ H = \\log_2 N! $$ å› æ­¤ä¸‹ç•Œä¸º $$ \\Omega (log(N!)) $$ æˆ–è€… $$ \\Omega (NlogN) $$ ä¸Šç•Œé€šè¿‡TUCSçš„æ€§è´¨å¯ä»¥é€šè¿‡å…·ä½“ç¤ºä¾‹åè¯å¾—åˆ°ï¼Œæ¯”å¦‚ç”¨merge sort ","date":"2024-07-11","objectID":"/61b-35/:2:0","tags":null,"title":"61B-35: Sorting and Algorithmic Bounds","uri":"/61b-35/"},{"categories":["UCB-CS61B"],"content":"More quick sort, Stability, Shuffling ","date":"2024-07-11","objectID":"/61b-34/:0:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"quick sort VS merge sort QuicksortL3S = left + 3-scan + shuffle Quicksort_LTHS: Tony Hoare partition scheme: L ptr ä»…ä»…æŒ‡å‘å°çš„ G ptr ä»…ä»…æŒ‡å‘å¤§çš„ ptr walk towards to each other, stopping on a hated item ä¸¤ä¸ªéƒ½åœä¸‹æ¥çš„è¯ï¼Œ äº¤æ¢ä¸€ä¸‹ï¼Œ ç„¶åç§»åŠ¨å…¶ä¸­ä¸€ä¸ª when ptrs cross, done. å’ŒGäº¤æ¢pivot ","date":"2024-07-11","objectID":"/61b-34/:1:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"Not random smarter pivot selection: median Quicksort_PickTH è€ƒè™‘äº†å¦‚ä½•è®¡ç®—æ•°ç»„åœ°å€çš„å¤æ‚åº¦ï¼Œ ä»¥åŠå¦‚ä½•é€‰æ‹©pivotçš„å¤æ‚åº¦ã€‚ worst case: $$ \\Theta(NlogN) $$ ä½†å®é™…ä¸Šå¹¶æ²¡æœ‰é‚£ä¹ˆå¥½ï¼Œå› ä¸ºè®¡ç®—ä¸­ä½æ•°çš„å¤æ‚åº¦æ˜¯$$\\Theta(N)$$ã€‚è€—è´¹äº†æ›´å¤šæ—¶é—´ã€‚ ","date":"2024-07-11","objectID":"/61b-34/:2:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"quick selectâ€“using partitioning worst case: a sorted array $$ \\Theta(N^2) $$ on average: $$ N + N/2 + N/4 +â€¦ + 1 = \\Theta(N) $$ ","date":"2024-07-11","objectID":"/61b-34/:3:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"stability for stable sort, we need to keep the relative order of equal elements Is insertion sort stable? yes, it is stable. Is quick sort stable? depends on the partitioning scheme. ","date":"2024-07-11","objectID":"/61b-34/:4:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"adaptive array.sort() is adaptive æŸ¥çœ‹javaå®˜æ–¹æ–‡æ¡£ ","date":"2024-07-11","objectID":"/61b-34/:5:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"shuffling random number and then sort ","date":"2024-07-11","objectID":"/61b-34/:6:0","tags":null,"title":"61B-34: More Quick Sort, Stability, Shuffling","uri":"/61b-34/"},{"categories":["UCB-CS61B"],"content":"ä¿¡æ¯æ— æŸæ€§ æ¨¡ç³Šæ€§ ","date":"2024-07-11","objectID":"/61b-38/:0:0","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"prefix-free codes ","date":"2024-07-11","objectID":"/61b-38/:1:0","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"Huffman codes ","date":"2024-07-11","objectID":"/61b-38/:1:1","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"shannon-fano codes using tries to convert compressed data into a original data longest prefix matching ","date":"2024-07-11","objectID":"/61b-38/:1:2","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"self-extracting bits ","date":"2024-07-11","objectID":"/61b-38/:2:0","tags":null,"title":"61B-38: Compression","uri":"/61b-38/"},{"categories":["UCB-CS61B"],"content":"Overview ","date":"2024-07-11","objectID":"/61b-37/:1:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"Triesâ€”â€”å‰ç¼€æ ‘/å­—å…¸æ ‘ usages: prefix matching approximate matching keysWithPrefix(String prefix) // returns all keys in the trie that start with the given prefix longestPrefixOf(String query) // returns the longest key in the trie that is a prefix of the query ","date":"2024-07-11","objectID":"/61b-37/:2:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"implementation private class Node{ boolean exists; Map\u003cCharacter, Node\u003e links; public Node(){ links = new TreeMap\u003c\u003e(); exists = false; } } ","date":"2024-07-11","objectID":"/61b-37/:3:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"T9 keyboard ","date":"2024-07-11","objectID":"/61b-37/:4:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"Ternary search Tries public class TSTSet\u003cValue\u003e{ private Node\u003cValue\u003e root; private static class Node\u003cValue\u003e{ private char c; private Node\u003cValue\u003e lo, mid, hi; } } ä½†æ˜¯è¿™ç§å®ç°æ–¹å¼è¡¨ç°ä¸ä½³ ","date":"2024-07-11","objectID":"/61b-37/:5:0","tags":null,"title":"61B-37:overview, Tries","uri":"/61b-37/"},{"categories":["UCB-CS61B"],"content":"radix sort ä¸ç”¨comparisonsçš„æ’åºç®—æ³•ï¼Œæ—¶é—´å¤æ‚åº¦O(dn)ï¼Œdä¸ºæœ€å¤§æ•°çš„ä½æ•°ï¼Œnä¸ºå¾…æ’åºæ•°çš„ä¸ªæ•°ã€‚ ç©ºé—´æ¢æ—¶é—´ bucket sort counting sort: æ‰¾å‡ºå¾…æ’åºæ•°çš„æœ€å¤§å€¼maxï¼Œç¡®å®šè®¡æ•°æ•°ç»„çš„é•¿åº¦ä¸ºmax+1ã€‚ éå†å¾…æ’åºæ•°ï¼Œå°†æ¯ä¸ªæ•°çš„ä¸ªä½æ•°å€¼ä½œä¸ºç´¢å¼•ï¼Œå°†è¯¥ç´¢å¼•å¯¹åº”çš„è®¡æ•°æ•°ç»„å…ƒç´ åŠ 1ã€‚ éå†è®¡æ•°æ•°ç»„ï¼Œå°†æ¯ä¸ªå…ƒç´ çš„å€¼ä½œä¸ºç´¢å¼•ï¼Œå°†è¯¥ç´¢å¼•å¯¹åº”çš„å…ƒç´ å€¼è¾“å‡ºåˆ°ç»“æœæ•°ç»„ä¸­ã€‚ runtime: O(n+k) LSD radix sort: least significant digit radix sort æ‰¾å‡ºå¾…æ’åºæ•°çš„æœ€å¤§å€¼maxï¼Œç¡®å®šè®¡æ•°æ•°ç»„çš„é•¿åº¦ä¸º10ã€‚ éå†å¾…æ’åºæ•°ï¼Œå°†æ¯ä¸ªæ•°çš„ä¸ªä½æ•°å€¼ä½œä¸ºç´¢å¼•ï¼Œå°†è¯¥ç´¢å¼•å¯¹åº”çš„è®¡æ•°æ•°ç»„å…ƒç´ åŠ 1ã€‚ LSD sort vs merge sort: similar strings:LSD sort is better dissimilar strings:merge sort is better MSD radix sort: most significant digit radix sort æ‰¾å‡ºå¾…æ’åºæ•°çš„æœ€å¤§å€¼maxï¼Œç¡®å®šè®¡æ•°æ•°ç»„çš„é•¿åº¦ä¸º10ã€‚ éå†å¾…æ’åºæ•°ï¼Œå°†æ¯ä¸ªæ•°çš„ä¸ªä½æ•°å€¼ä½œä¸ºç´¢å¼•ï¼Œå°†è¯¥ç´¢å¼•å¯¹åº”çš„è®¡æ•°æ•°ç»„å…ƒç´ åŠ 1ã€‚ éå†è®¡æ•°æ•°ç»„ï¼Œå°†æ¯ä¸ªå…ƒç´ çš„å€¼ä½œä¸ºç´¢å¼•ï¼Œå°†è¯¥ç´¢å¼•å¯¹åº”çš„å…ƒç´ å€¼è¾“å‡ºåˆ°ç»“æœæ•°ç»„ä¸­ã€‚ runtime: O(n+k) ","date":"2024-07-10","objectID":"/61b-36/:0:0","tags":null,"title":"61B-36: Radix Sorts","uri":"/61b-36/"},{"categories":["TOOLS"],"content":"æ–‡ä»¶çŠ¶æ€ æœªè·Ÿè¸ª-æœªä¿®æ”¹-å·²ä¿®æ”¹-æš‚å­˜ git add \u003cname\u003e - *-\u003eæš‚å­˜ git commit -m \"message\" - æš‚å­˜-\u003eæœªä¿®æ”¹ git rm \u003cname\u003e - æœªä¿®æ”¹-\u003eæœªè·Ÿè¸ª ","date":"2024-06-29","objectID":"/tools/git/git/:1:0","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"æŸ¥çœ‹çŠ¶æ€ git status æ›´åŠ ç»†è‡´å‡ è¡Œå‡ åˆ— git diff æŸ¥çœ‹å†å²æ—¥å¿— git log --pretty=oneline git log --graph --oneline --decorate ","date":"2024-06-29","objectID":"/tools/git/git/:1:1","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"åŸºæœ¬æ“ä½œ ","date":"2024-06-29","objectID":"/tools/git/git/:2:0","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"åŸºç¡€é…ç½® git config --global user.name \"your name\" git config --global user.email \"your email\" ","date":"2024-06-29","objectID":"/tools/git/git/:2:1","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"åˆ›å»ºç‰ˆæœ¬åº“ mkdir myproject cd myproject git init ","date":"2024-06-29","objectID":"/tools/git/git/:2:2","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"å…‹éš†ç‰ˆæœ¬åº“ git clone https://github.com/username/repository.git ","date":"2024-06-29","objectID":"/tools/git/git/:2:3","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"è·Ÿè¸ªæ–‡ä»¶oræ–‡ä»¶å¤¹ git add \u003cfilename\u003e git rm \u003cfilename\u003e git rm --cache \u003cfilename\u003e ","date":"2024-06-29","objectID":"/tools/git/git/:2:4","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"è®¾ç½®ç¼“å­˜çŠ¶æ€ git add git reset HEAD \u003cfilename\u003e ","date":"2024-06-29","objectID":"/tools/git/git/:2:5","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"æäº¤ä¿®æ”¹ git commit -m \"commit message str\" æ’¤é”€éé¦–æ¬¡ä¿®æ”¹ git reset head~ --soft ","date":"2024-06-29","objectID":"/tools/git/git/:2:6","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"å’Œgithubè”ç³» git remote add origin https://github.com/username/repository.git git remote git remote rename origin old_name æ¨åˆ°è¿œç¨‹ä»“åº“ git push origin master sshè¿æ¥ï¼Ÿ ","date":"2024-06-29","objectID":"/tools/git/git/:2:7","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"åˆ†æ”¯ç®¡ç† åˆ›å»ºåˆ†æ”¯ git branch --list git branch hhzz git checkout hhzz git checkout -b hhzz åˆå¹¶åˆ†æ”¯ git merge hhzz åˆ é™¤åˆ†æ”¯ git branch -d hhzz ","date":"2024-06-29","objectID":"/tools/git/git/:2:8","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"è´®è—åŠŸèƒ½ stash å¾…æ–½å·¥ ","date":"2024-06-29","objectID":"/tools/git/git/:2:9","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"é‡ç½®ã€æ¢åŸºåŠŸèƒ½ å¾…æ–½å·¥ ","date":"2024-06-29","objectID":"/tools/git/git/:2:10","tags":null,"title":"Git","uri":"/tools/git/git/"},{"categories":["TOOLS"],"content":"Regular Expressions ","date":"2024-06-29","objectID":"/tools/reg/:0:0","tags":null,"title":"æ­£åˆ™è¡¨è¾¾å¼ç¬”è®°","uri":"/tools/reg/"},{"categories":["TOOLS"],"content":"æ³¨æ„ç‰ˆæœ¬å’Œæ–‡æ¡£ï¼ ","date":"2024-06-29","objectID":"/tools/reg/:1:0","tags":null,"title":"æ­£åˆ™è¡¨è¾¾å¼ç¬”è®°","uri":"/tools/reg/"},{"categories":["TOOLS"],"content":"å¸¸ç”¨å·¥å…· https://regex101.com/ https://regexr.com/ python reæ¨¡å— å­—ç¬¦ . åŒ¹é…ä»»æ„ä¸€ä¸ªå­—ç¬¦ [] åŒ¹é…æ‹¬å·ä¸­çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦,å¦‚ [a-zA-Z1-3] åŒ¹é…å¤§å†™å­—æ¯æˆ–å°å†™å­—æ¯æˆ–æ•°å­—1-3, [^] åŒ¹é…é™¤äº†æ‹¬å·ä¸­çš„å­—ç¬¦ é¢„å®šå­—ç¬¦ç±» \\d åŒ¹é…æ•°å­— \\D åŒ¹é…éæ•°å­— \\w åŒ¹é…å­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿ \\W åŒ¹é…éå­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿ \\s åŒ¹é…ç©ºç™½å­—ç¬¦æˆ–è€…tab \\S åŒ¹é…éç©ºç™½å­—ç¬¦ è¾¹ç•ŒåŒ¹é… ^ åŒ¹é…å­—ç¬¦ä¸²çš„å¼€å¤´ $ åŒ¹é…å­—ç¬¦ä¸²çš„ç»“å°¾ \\b åŒ¹é…å•è¯çš„è¾¹ç•Œ, å¦‚ \\bthe\\b åŒ¹é…the \\B åŒ¹é…éå•è¯è¾¹ç•Œ æ•°é‡è¯ * åŒ¹é…å‰é¢çš„å­—ç¬¦0æ¬¡æˆ–å¤šæ¬¡ + åŒ¹é…å‰é¢çš„å­—ç¬¦1æ¬¡æˆ–å¤šæ¬¡ ? åŒ¹é…å‰é¢çš„å­—ç¬¦0æ¬¡æˆ–1æ¬¡ {n} åŒ¹é…å‰é¢çš„å­—ç¬¦næ¬¡ {n,} åŒ¹é…å‰é¢çš„å­—ç¬¦è‡³å°‘næ¬¡ {n,m} åŒ¹é…å‰é¢çš„å­—ç¬¦è‡³å°‘næ¬¡, è‡³å¤šmæ¬¡ éè´ªå©ªåŒ¹é… é‡è¯é»˜è®¤æ˜¯è´ªå©ªåŒ¹é…, å³å°½å¯èƒ½å¤šçš„åŒ¹é…å­—ç¬¦, å¦‚ a.*b ä¼šåŒ¹é…åˆ°æœ€é•¿çš„ä»¥aå¼€å¤´çš„b åé¢çš„é‡è¯åŠ ä¸Š? åˆ™ä¸ºéè´ªå©ªåŒ¹é…, å³å°½å¯èƒ½å°‘çš„åŒ¹é…å­—ç¬¦, å¦‚ a.*?b ä¼šåŒ¹é…åˆ°æœ€çŸ­çš„ä»¥aå¼€å¤´çš„b åˆ†ç»„ä¸æ•è· () ç”¨æ¥åˆ›å»ºåˆ†ç»„, æ•è·æ‹¬å·ä¸­çš„å­—ç¬¦, å¹¶åœ¨åŒ¹é…æ—¶è¿”å›åŒ¹é…åˆ°çš„å†…å®¹ [] ç”¨æ¥åˆ›å»ºå­—ç¬¦ç±», å¦‚ [Pp] åŒ¹é…Pæˆ–p | ç”¨æ¥åˆ›å»ºæˆ–å…³ç³», å¦‚ a(bc|de) åŒ¹é…aåé¢æ˜¯bcæˆ–de \\n å¼•ç”¨åˆ†ç»„, å¦‚ \\1 å¼•ç”¨ç¬¬ä¸€ä¸ªåˆ†ç»„ $n å¼•ç”¨ç¬¬nä¸ªåˆ†ç»„ ?: éæ•è·åˆ†ç»„, å¦‚ (?:abc) åŒ¹é…abc, ä¸æ•è·åŒ¹é…åˆ°çš„å†…å®¹ å‰ç»å’Œåé¡¾ æ­£å‘å‰ç» (?=abc) åŒ¹é…abcå‰é¢çš„å­—ç¬¦ åå‘å‰ç» (?!abc) åŒ¹é…abcå‰é¢çš„å­—ç¬¦ æ­£å‘åé¡¾ (?\u003c=abc) åŒ¹é…abcåé¢çš„å­—ç¬¦ åå‘åé¡¾ (?\u003c!abc) åŒ¹é…abcåé¢çš„å­—ç¬¦ ","date":"2024-06-29","objectID":"/tools/reg/:2:0","tags":null,"title":"æ­£åˆ™è¡¨è¾¾å¼ç¬”è®°","uri":"/tools/reg/"},{"categories":["Programming Language Crash Course"],"content":"learning pointer(advanced version) ä¸ºäº†é˜²æ­¢ææ··è€Œå†™ ","date":"2024-05-05","objectID":"/crash/cpp1/:0:0","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["Programming Language Crash Course"],"content":"ä¸‹æ ‡ä¸º0?é¦–åœ°å€? void test0(){ int arr[] = {1, 2, 3}; cout \u003c\u003c \u0026arr[0] \u003c\u003c endl; cout \u003c\u003c \u0026arr \u003c\u003c endl; cout \u003c\u003c arr \u003c\u003c endl; } arr \u0026arr \u0026arr[0] å­˜å‚¨çš„éƒ½æ˜¯ç›¸åŒçš„åœ°å€ arr å¸¸é‡æŒ‡é’ˆä¸èƒ½è¢«æ”¹å˜ ","date":"2024-05-05","objectID":"/crash/cpp1/:0:1","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["Programming Language Crash Course"],"content":"æŒ‡å‘æ•°ç»„å…ƒç´ çš„æŒ‡é’ˆ(ä¸ä¸€å®šæ˜¯é¦–å…ƒç´ )ä»¥ç”¨[]æ¥è®¿é—®æ•°ç»„å…ƒç´  void test2() { int a[3] = {1,2,3}; int *p = a; p++; cout \u003c\u003c p[0] \u003c\u003c endl; // 2 } ","date":"2024-05-05","objectID":"/crash/cpp1/:0:2","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["Programming Language Crash Course"],"content":"æ•°ç»„ç±»å‹çš„æŒ‡é’ˆ void test2(){ int arr[] = {1, 2, 3}; int (*p)[] = \u0026arr; // å®šä¹‰ä¸€ä¸ªæŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆ cout \u003c\u003c (*p)[0] \u003c\u003c endl; // è¾“å‡ºæ•°ç»„é¦–åœ°å€ cout \u003c\u003c p[0] \u003c\u003c endl; // è¾“å‡ºæ•°ç»„é¦–åœ°å€ cout \u003c\u003c p[0][0] \u003c\u003c endl; // è¾“å‡ºæ•°ç»„é¦–å…ƒç´  } int *p[] = \u0026arr vs int (*p)[] = \u0026arr???? [ ]ä¼˜å…ˆçº§é«˜äº* int (*p)[] = \u0026arr; *p --\u003e ä¸€ä¸ªæŒ‡é’ˆ ï¼ˆ*pï¼‰[] --\u003e æŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆ int (*p)[] --\u003e æŒ‡å‘çš„æ•°ç»„çš„å…ƒç´ æ˜¯intç±»å‹ p = \u0026arr å®šä¹‰äº†ä¸€ä¸ªæŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆï¼Œ(*p) = arr è§£å¼•ç”¨æŒ‡é’ˆå¾—åˆ°æ•°ç»„é¦–åœ°å€ï¼Œ(*p)[0] = arr[0] è®¿é—®æ•°ç»„é¦–å…ƒç´  p[0] = arr è®¿é—®æ•°ç»„é¦–åœ°å€ï¼Œp[0][0] = arr[0] è®¿é—®æ•°ç»„é¦–å…ƒç´  ","date":"2024-05-05","objectID":"/crash/cpp1/:0:3","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["Programming Language Crash Course"],"content":"é‚£ä¹ˆint *(*p)[] = { };æ°´åˆ°æ¸ æˆäº† ","date":"2024-05-05","objectID":"/crash/cpp1/:0:4","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["Programming Language Crash Course"],"content":"å†…å­˜æ˜ åƒå›¾ å†…å­˜æ˜ åƒå›¾ 1 2 â€¦ å†…å­˜åœ°å€ä»ä¸Šå¾€ä¸‹é€’å¢ å’ŒCSAPPé‡Œé¢çš„æ ˆç”»æ³•æœ‰ç‚¹ä¸ä¸€æ · ","date":"2024-05-05","objectID":"/crash/cpp1/:0:5","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["Programming Language Crash Course"],"content":"delete ç”³è¯·ä¸€ä¸ªè¿ç»­çš„å†…å­˜å—ï¼Œç„¶åå°†å…¶è§†ä¸ºäºŒç»´æ•°ç»„ï¼š int** arr = new int*[rows]; for (int i = 0; i \u003c rows; ++i) { arr[i] = new int[cols]; } é‡Šæ”¾æ—¶ï¼Œä½ éœ€è¦å…ˆé‡Šæ”¾æ¯ä¸€è¡Œçš„å†…å­˜ï¼Œç„¶åé‡Šæ”¾è¡ŒæŒ‡é’ˆæ•°ç»„ï¼š for (int i = 0; i \u003c rows; ++i) { delete[] arr[i]; } delete[] arr; ç”³è¯·ä¸€ä¸ªè¶³å¤Ÿå¤§çš„è¿ç»­å†…å­˜å—ï¼Œç„¶åå°†å…¶è§†ä¸ºäºŒç»´æ•°ç»„ï¼š int* arr = new int[rows * cols]; åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ åªéœ€è¦é‡Šæ”¾ä¸€æ¬¡ï¼š delete[] arr; æ³¨æ„ï¼Œè¿™ç§æ–¹å¼ä¸‹ï¼Œarrå®é™…ä¸Šæ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œä½†æ˜¯ä½ å¯ä»¥åƒè®¿é—®äºŒç»´æ•°ç»„ä¸€æ ·ä½¿ç”¨å®ƒï¼ˆä¾‹å¦‚ï¼Œarr[i][j]å®é™…ä¸Šæ˜¯arr[i * cols + j]ï¼‰ã€‚ ç¡®ä¿åœ¨é‡Šæ”¾å†…å­˜åå°†æŒ‡é’ˆè®¾ç½®ä¸ºnullptrï¼Œä»¥é¿å…æ‚¬å‚æŒ‡é’ˆé—®é¢˜ï¼š delete[] arr; arr = nullptr; // æˆ–è€…ä½¿ç”¨æ™ºèƒ½æŒ‡é’ˆè‡ªåŠ¨ç®¡ç†å†…å­˜ ","date":"2024-05-05","objectID":"/crash/cpp1/:0:6","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"},{"categories":["Programming Language Crash Course"],"content":"char? int main() { char **p, *city[] = {\"aaa\",\"bbb\"}; for (p = city; p \u003c city + 2; ++p) { cout \u003c\u003c *p \u003c\u003c endl; } return 0; } ç»“æœä¸ºï¼š aaa bbb ","date":"2024-05-05","objectID":"/crash/cpp1/:0:7","tags":["C++"],"title":"C++ ptr","uri":"/crash/cpp1/"}]