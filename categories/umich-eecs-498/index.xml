<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>UMich-EECS-498 - Category - HHZZ`s space</title>
        <link>http://example.org/categories/umich-eecs-498/</link>
        <description>UMich-EECS-498 - Category - HHZZ`s space</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 19 Apr 2025 08:27:18 &#43;0800</lastBuildDate><atom:link href="http://example.org/categories/umich-eecs-498/" rel="self" type="application/rss+xml" /><item>
    <title>L13-Attention</title>
    <link>http://example.org/l13-attention/</link>
    <pubDate>Sat, 19 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l13-attention/</guid>
    <description><![CDATA[Attention Mechanisms in Neural Networks introduction What if Seq to Seq models processed long long sequences?
Attention Mechanisms the core idea is that using weighted sum, and the coefficient can be learned from the model itself
In math, we do not actually care that wether input is a sequence or not.
given hidden states $h_i$ and the context vector $c$, we can calculate the attention weights as follows:
$$ e_{t, i, j} = f_{att}(s_{t-1}, h_{i,j}) \ a_{t, :, :} = softmax(e_{t, :, :}) \ c_{t} = \sum_{i,j} a_{t, i, j} h_{i,j} $$]]></description>
</item>
<item>
    <title>L17-3D Vision</title>
    <link>http://example.org/l17-3d-vision/</link>
    <pubDate>Sat, 19 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l17-3d-vision/</guid>
    <description><![CDATA[3D Vision back to the start &#x1f606;
shape prediction and ingest 3D information 5 data representation
depth map
RGB + Depth image = RGB-D image 2.5D raw 3D sensor can easily capture depth information another type of depth map is surface normal map, which is a 2D image that represents the surface normal (using a normal vector) at each pixel location all the mentioned maps can be learned with Fully Convolutional Networks (FCN) &#x1f914; Voxel grid]]></description>
</item>
<item>
    <title>L21-Reinforcement Learning</title>
    <link>http://example.org/l21-reinforcement-learning/</link>
    <pubDate>Fri, 18 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l21-reinforcement-learning/</guid>
    <description><![CDATA[Reinforcement Learning So far, we have discussed supervised learning, and a little bit unsupervised learning (in UCB-data100 &#x1f609;)
What is Reinforcement Learning at time $t$, env $\rightarrow^{state}$ agent $\rightarrow^{action}$ env $\rightarrow^{reward}$ agent, then env changed, agent learned, then repeated.
state can be partial -&gt; noisy reward can be delayed, implicit and sparse -&gt; noisy AND Nondifferentiable &#x1f632; Nonstationary environment, change over time &#x1f60e; Generative Adversarial Networks (GANs) somehow is a part of Reinforcement Learning.]]></description>
</item>
<item>
    <title>L14-Visualizing and Understanding</title>
    <link>http://example.org/l14-visualizing-and-understanding/</link>
    <pubDate>Wed, 06 Nov 2024 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l14-visualizing-and-understanding/</guid>
    <description><![CDATA[Visualizing and Understanding Visualizing 对第一层、第二层以及最后进入FC层的特征图进行可视化
PCA√
t-SNE√ 非线性降维
最大激活
Understanding 细节]]></description>
</item>
<item>
    <title>L10-Training I</title>
    <link>http://example.org/l10-training-i/</link>
    <pubDate>Mon, 04 Nov 2024 09:46:08 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l10-training-i/</guid>
    <description><![CDATA[Training I Activation Functions Sigmoid function: $\sigma(x) = \frac{1}{1 + e^{-x}}$ 不是0中心 两端饱和 always all positive or negative :( exp() 计算复杂，但是对于GPU不是问题 tanh function: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ sigmoid变体 ReLU function: $f(x) = max(0, x)$ 不会饱和 计算快 非0中心 dead relu ==&gt; leaky relu Leaky ReLU function: $f(x) = max(0.01x, x)$ 解决了dead relu问题 ==&gt; PRelu function：把0.01改成可学习的参数 ELU function: $f(x) = \begin{cases} x &amp; x \geq 0 \ \alpha(e^x - 1) &amp; x &lt; 0 \end{cases}$ Data Preprocessing 参见DATA-100相关课程]]></description>
</item>
<item>
    <title>L9-Hard and Software</title>
    <link>http://example.org/l9-hard-and-software/</link>
    <pubDate>Sat, 02 Nov 2024 09:59:30 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l9-hard-and-software/</guid>
    <description><![CDATA[Hard and Soft ware Hardware eecs 598.009 GPU programming!
其实我很想了解一下cuda编程
tensorflow支持TPU，pytorch呢？
计算图存储在GPU内存里面
Software the point of deep learning frameworks
allow rapid prototyping automatically compute gradients run it all efficiently on GPUs or else PyTorch sigmoid0减少计算图节点的设计，因为反向传播重写了
1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Sigmoid(torch.autograd.Function): @staticmethod def forward(ctx, input): y = 1 / (1 + torch.exp(-input)) ctx.save_for_backward(input) return y @staticmethod def backward(ctx, grad_output): input, = ctx.]]></description>
</item>
<item>
    <title>L8-CNN Arch</title>
    <link>http://example.org/l8-cnn-arch/</link>
    <pubDate>Sat, 02 Nov 2024 09:58:45 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l8-cnn-arch/</guid>
    <description><![CDATA[CNN Architectures 今日首绷 计算题 4 bytes per elem
右边三列体现了一个规律 2013的ImageNet winner仍然是AlexNet变体(ZFNet, ECCV)，只是trial and error的结果
2014的ImageNet winner是VGGNet ICLR，提出了规则化 3x3卷积核? 两个3x3卷积核 比 一个5x5卷积核 Params和FLOPs更少，但是感受野一样，并且可以插入更多的relu channel翻倍，每次卷积计算cost same amount of floating points computation 2014的ImageNet有GoogLeNet CVPR:
初期快速下采样 Inception模块: 1x1, 3x3, 5x5卷积核(使得kernel size不再是一个超参数) 1x1适配器的引入 resnet雏形 Global Average Pooling: 替换掉一层fcnn 其次还有auxiliary classifier取中间层输出，作为loss加入到loss function中 2015年首先是BN被发现了，auxiliary classifier被弃用 接着ResNet CVPR:
引入残差结构，提升准确率 引入bottleneck结构，层数增加，但是flops减少 ECCV有一篇进一步讨论了残差块的结构 CVPR2017有一篇文章提出了ResNeXt 1 torch.nn.Conv2d(groups=) # groups参数控制了分组卷积的数量 2017年的ImageNet结束
DenseNet: fancier 趋势 MobileNet: 轻量化趋势 ICLR 2017自动化设计神经网络结构 Neural Architecture Search ]]></description>
</item>
<item>
    <title>L7-CNN</title>
    <link>http://example.org/l7-cnn/</link>
    <pubDate>Tue, 29 Oct 2024 20:20:37 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l7-cnn/</guid>
    <description><![CDATA[Convolutional Neural Networks Components of a CNN Convolutional layers Pooling layers Normalization layers Convolutional Layers 注意到一个通道的卷积核也是全通道数 3 x5x5
偏置是一个向量
(b, c, h, w)表示batch size, channel, height, width!
注意四个维度的意义
卷积本质上也是一种linear layer，所以要relu等
高维全局，低维局部 1x1 Convolutions 一种适配器，调整通道数
other types of convolutions PyTorch Implementation Pooling Layers another way to downsample data, no learnable parameters
局部最大值微小移动不变性
Normalization Layers 主要讨论的是batch normalization
层与层之间数据分布更加稳定 此时
1 model.eval() 此时bn可以作为线形层被fuse进入fcnn or conv
layer norm也有，主要是rnn和transformer用到了 Example: LeNet-5 ]]></description>
</item>
<item>
    <title>L6-BP</title>
    <link>http://example.org/l6-bp/</link>
    <pubDate>Thu, 24 Oct 2024 23:01:36 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l6-bp/</guid>
    <description><![CDATA[Backpropagation 参见cmu 10-414 &#x1f600;
RNN 初见 Computation Graph &#x1f60f; A2要hardcode直接的反向传播了 555
真正的代码 &#x1f60b;
1 2 3 4 5 6 7 8 9 class Multiply(torch.autograd.Function): @staticmethod def forward(ctx, x, y): ctx.save_for_backward(x, y) return x * y @staticmethod def backward(ctx, grad_output): x, y = ctx.saved_tensors return grad_output * y, grad_output * x # 解析计算 PyTorch operators in deep engine &#x1f914;
BP rules BP with vector-valued functions 假装标量求导，然后匹配矩阵形状即可（典中典）
element-wise functions in BP 不用使用矩阵求导，直接一一对应去想梯度的传递即可]]></description>
</item>
<item>
    <title>L5-NN</title>
    <link>http://example.org/l5-nn/</link>
    <pubDate>Thu, 24 Oct 2024 15:13:23 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l5-nn/</guid>
    <description><![CDATA[Neural Networks 线性不可分怎么处理？ 怎么encode不同的信息？
王朝落幕 Neural Networks Architecture data -&gt; input layer -&gt; hidden layer -&gt; output layer
data driven, non-linear &#x1f60b;
&#x1f604; ?
ReLU, 通用近似定理 详细证明看一看官方给出的课本
凸优化 牢NN当然是进行非凸优化 &#x1f600;]]></description>
</item>
</channel>
</rss>
