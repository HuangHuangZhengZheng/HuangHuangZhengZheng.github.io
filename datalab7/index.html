<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>DATA100-lab7: Gradient Descent and Feature Engineering - HHZZ`s space</title><meta name="Description" content="This is my cool site"><meta property="og:url" content="http://example.org/datalab7/">
  <meta property="og:site_name" content="HHZZ`s space">
  <meta property="og:title" content="DATA100-lab7: Gradient Descent and Feature Engineering">
  <meta property="og:description" content="1 2 3 # Initialize Otter import otter grader = otter.Notebook(&amp;#34;lab07.ipynb&amp;#34;) Lab 7: Gradient Descent and Feature Engineering In this lab, we will work through the process of:
Defining loss functions Feature engineering Minimizing loss functions using numeric methods and analytical methods Understanding what happens if we use the analytical solution for OLS on a matrix with redundant features Computing a gradient for a nonlinear model Using gradient descent to optimize the nonline model This lab will continue using the toy tips calculation dataset used in Labs 5 and 6.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-13T15:19:34+08:00">
    <meta property="article:modified_time" content="2024-08-13T15:19:34+08:00">
    <meta property="article:tag" content="Numpy">
    <meta property="article:tag" content="Pandas">
    <meta property="article:tag" content="Scikit-Learn">
    <meta property="article:tag" content="SciPy">
    <meta property="article:tag" content="Plotly">
    <meta property="article:tag" content="Seaborn">
    <meta property="og:image" content="http://example.org/logo.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.org/logo.png"><meta name="twitter:title" content="DATA100-lab7: Gradient Descent and Feature Engineering">
<meta name="twitter:description" content="1 2 3 # Initialize Otter import otter grader = otter.Notebook(&#34;lab07.ipynb&#34;) Lab 7: Gradient Descent and Feature Engineering In this lab, we will work through the process of:
Defining loss functions Feature engineering Minimizing loss functions using numeric methods and analytical methods Understanding what happens if we use the analytical solution for OLS on a matrix with redundant features Computing a gradient for a nonlinear model Using gradient descent to optimize the nonline model This lab will continue using the toy tips calculation dataset used in Labs 5 and 6.">
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://example.org/datalab7/" /><link rel="prev" href="http://example.org/datalab8/" /><link rel="next" href="http://example.org/datalab9/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "DATA100-lab7: Gradient Descent and Feature Engineering",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/example.org\/datalab7\/"
        },"genre": "posts","keywords": "Numpy, Pandas, Scikit-Learn, SciPy, Plotly, Seaborn","wordcount":  3959 ,
        "url": "http:\/\/example.org\/datalab7\/","datePublished": "2024-08-13T15:19:34+08:00","dateModified": "2024-08-13T15:19:34+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "HHZZ"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="HHZZ`s space">Code and BeyondCodeüòã</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tools/"> Tools </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/beyondcode/"> BeyondCodeüòã </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="HHZZ`s space">Code and BeyondCodeüòã</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tools/" title="">Tools</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/beyondcode/" title="">BeyondCodeüòã</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">DATA100-lab7: Gradient Descent and Feature Engineering</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>HHZZ</a></span>&nbsp;<span class="post-category">included in <a href="/categories/data100/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>DATA100</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-13">2024-08-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;3959 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;19 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#intro-to-feature-engineering">Intro to Feature Engineering</a>
      <ul>
        <li><a href="#example-feature-functions-ÁºñÁ†Å‰∏ÄÁõ¥ÊòØ‰∏Ä‰∏™ÂÖàÈ™åÂ∑•Á®ãÈóÆÈ¢ò-vs-autoencoders">Example feature functions ÁºñÁ†Å‰∏ÄÁõ¥ÊòØ‰∏Ä‰∏™ÂÖàÈ™åÂ∑•Á®ãÈóÆÈ¢òÔºü vs AutoEncoders</a></li>
      </ul>
    </li>
    <li><a href="#question-1-defining-the-model-and-feature-engineering">Question 1: Defining the Model and Feature Engineering</a>
      <ul>
        <li><a href="#question-1a-feature-engineering">Question 1a: Feature Engineering</a></li>
        <li><a href="#question-1b-defining-the-model">Question 1b: Defining the Model</a></li>
      </ul>
    </li>
    <li><a href="#question-2-fitting-a-linear-model-using-scipyoptimizeminimize-methods">Question 2: Fitting a Linear Model using scipy.optimize.minimize Methods</a></li>
    <li><a href="#question-3-fitting-the-model-using-analytic-methods">Question 3: Fitting the Model using Analytic Methods</a>
      <ul>
        <li><a href="#question-3a-analytic-solution-using-explicit-inverses">Question 3a: Analytic Solution Using Explicit Inverses</a></li>
        <li><a href="#question-3b-fixing-our-one-hot-encoding">Question 3b: Fixing our One-Hot Encoding</a></li>
        <li><a href="#question-3c-analyzing-our-new-one-hot-encoding">Question 3c: Analyzing our new One-Hot Encoding</a></li>
      </ul>
    </li>
    <li><a href="#question-4-gradient-descent">Question 4: Gradient Descent</a>
      <ul>
        <li><a href="#question-4a-computing-the-gradient-of-the-mse-with-respect-to-theta-on-the-sin-model">Question 4a: Computing the Gradient of the MSE With Respect to Theta on the Sin Model</a></li>
        <li><a href="#question-4b-implementing-gradient-descent-and-using-it-to-optimize-the-sin-model">Question 4b: Implementing Gradient Descent and Using It to Optimize the Sin Model</a></li>
      </ul>
    </li>
    <li><a href="#visualizing-loss-extra">Visualizing Loss (Extra)</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Initialize Otter</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">otter</span>
</span></span><span class="line"><span class="cl"><span class="n">grader</span> <span class="o">=</span> <span class="n">otter</span><span class="o">.</span><span class="n">Notebook</span><span class="p">(</span><span class="s2">&#34;lab07.ipynb&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="lab-7-gradient-descent-and-feature-engineering">Lab 7: Gradient Descent and Feature Engineering</h1>
<p>In this lab, we will work through the process of:</p>
<ol>
<li>Defining loss functions</li>
<li>Feature engineering</li>
<li>Minimizing loss functions using numeric methods and analytical methods</li>
<li>Understanding what happens if we use the analytical solution for OLS on a matrix with redundant features</li>
<li>Computing a gradient for a nonlinear model</li>
<li>Using gradient descent to optimize the nonline model</li>
</ol>
<p>This lab will continue using the toy <code>tips</code> calculation dataset used in Labs 5 and 6.</p>
<p><br/><br/></p>
<hr style="border: 5px solid #003262;" />
<hr style="border: 1px solid #fdb515;" />
<h1 id="loading-the-tips-dataset">Loading the Tips Dataset</h1>
<p>To begin, let&rsquo;s load the tips dataset from the <code>seaborn</code> library.  This dataset contains records of tips, total bill, and information about the person who paid the bill. As earlier, we&rsquo;ll be trying to predict tips from the other data.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&#34;talk&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&#34;tips&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Number of Records:&#34;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>Number of Records: 244
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>total_bill</th>
      <th>tip</th>
      <th>sex</th>
      <th>smoker</th>
      <th>day</th>
      <th>time</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.99</td>
      <td>1.01</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10.34</td>
      <td>1.66</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>21.01</td>
      <td>3.50</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.68</td>
      <td>3.31</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.59</td>
      <td>3.61</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
<hr>
<h2 id="intro-to-feature-engineering">Intro to Feature Engineering</h2>
<p>So far, we&rsquo;ve only considered models of the form $\hat{y} = f_{\theta}(x) = \sum_{j=0}^d x_j\theta_j$, where $\hat{y}$ is quantitative continuous.</p>
<p>We call this a linear model because it is a linear combination of the features (the $x_j$). However, our features don&rsquo;t need to be numbers: we could have categorical values such as names. Additionally, the true relationship doesn&rsquo;t have to be linear, as we could have a relationship that is quadratic, such as the relationship between the height of a projectile and time.</p>
<p>In these cases, we often apply <strong>feature functions</strong>, functions that take in some value and output another value. This might look like converting a string into a number, combining multiple numeric values, or creating a boolean value from some filter.</p>
<p>Then, if we call $\phi$ (&ldquo;phi&rdquo;) our &ldquo;phi&rdquo;-ture function, our model takes the form $\hat{y} = f_{\theta}(x) = \sum_{j=0}^d \phi(x)_j\theta_j$.</p>
<h3 id="example-feature-functions-ÁºñÁ†Å‰∏ÄÁõ¥ÊòØ‰∏Ä‰∏™ÂÖàÈ™åÂ∑•Á®ãÈóÆÈ¢ò-vs-autoencoders">Example feature functions ÁºñÁ†Å‰∏ÄÁõ¥ÊòØ‰∏Ä‰∏™ÂÖàÈ™åÂ∑•Á®ãÈóÆÈ¢òÔºü vs AutoEncoders</h3>
<ol>
<li>One-hot encoding
<ul>
<li>converts a single categorical feature into many binary features, each of which represents one of the possible values in the original column</li>
<li>each of the binary feature columns produced contains a 1 for rows that had that column&rsquo;s label in the original column, and 0 elsewhere</li>
</ul>
</li>
<li>Polynomial features
<ul>
<li>create polynomial combinations of features</li>
</ul>
</li>
</ol>
<br/>
<hr style="border: 1px solid #fdb515;" />
<h2 id="question-1-defining-the-model-and-feature-engineering">Question 1: Defining the Model and Feature Engineering</h2>
<p>In Lab 6 we used the constant model. Now let&rsquo;s make a more complicated model that utilizes other features in our dataset. You can imagine that we might want to use the features with an equation that looks as shown below:</p>
<p>$$ \text{Tip} = \theta_1 \cdot \text{total}_\text{bill} + \theta_2 \cdot \text{sex} + \theta_3 \cdot \text{smoker} + \theta_4 \cdot \text{day} + \theta_5 \cdot \text{time} + \theta_6 \cdot \text{size} $$</p>
<p>Unfortunately, that&rsquo;s not possible because some of these features like &ldquo;day&rdquo; are not numbers, so it doesn&rsquo;t make sense to multiply by a numerical parameter.</p>
<p>Let&rsquo;s start by converting some of these non-numerical values into numerical values. Before we do this, let&rsquo;s separate out the tips and the features into two separate variables.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">tips</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;tip&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;tip&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h3 id="question-1a-feature-engineering">Question 1a: Feature Engineering</h3>
<p>First, let&rsquo;s convert our features to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones.</p>
<p>For example, we can treat the day as a value from 1-7. However, one of the disadvantages in directly translating to a numeric value is that we unintentionally assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7, and Monday to the numeric value of 1. In our linear model, Sunday will have 7 times the influence of Monday, which can lower the accuracy of our model.</p>
<p>Instead, let&rsquo;s use one-hot encoding to better represent these features!</p>
<p>As you will learn in lecture, one-hot encoding is a way that we can produce a binary vector to indicate non-numeric features.</p>
<p>In the <code>tips</code> dataset for example, we encode Sunday as the vector <code>[0 0 0 1]</code> because our dataset only contains bills from Thursday through Sunday. This assigns a more even weight across each category in non-numeric features. Complete the code below to one-hot encode our dataset. This dataframe holds our &ldquo;featurized&rdquo; data, which is also often denoted by $\phi$.</p>
<p><strong>Hint:</strong> You may find the <a href="https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html" target="_blank" rel="noopener noreffer ">pd.get_dummies method</a> or the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html" target="_blank" rel="noopener noreffer ">DictVectorizer class</a> useful when doing your one-hot encoding.</p>
<!--
BEGIN QUESTION
name: q1a
points: 2
-->
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">one_hot_encode</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Return the one-hot encoded dataframe of our input data.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters
</span></span></span><span class="line"><span class="cl"><span class="s2">    -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">    data: a dataframe that may include non-numerical features
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns
</span></span></span><span class="line"><span class="cl"><span class="s2">    -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">    A one-hot encoded dataframe that only contains numeric features
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">one_hot_X</span> <span class="o">=</span> <span class="n">one_hot_encode</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">one_hot_X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>total_bill</th>
      <th>size</th>
      <th>sex_Male</th>
      <th>sex_Female</th>
      <th>smoker_Yes</th>
      <th>smoker_No</th>
      <th>day_Thur</th>
      <th>day_Fri</th>
      <th>day_Sat</th>
      <th>day_Sun</th>
      <th>time_Lunch</th>
      <th>time_Dinner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.99</td>
      <td>2</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10.34</td>
      <td>3</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>21.01</td>
      <td>3</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.68</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.59</td>
      <td>4</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">grader</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&#34;q1a&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h3 id="question-1b-defining-the-model">Question 1b: Defining the Model</h3>
<p>Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 12 features instead of 6. Therefore, our linear model now looks like:</p>
<p>$$ \text{Tip} = \theta_1 \cdot \text{size} + \theta_2 \cdot \text{total}_\text{bill} + \theta_3 \cdot \text{day}_\text{Thur} + \theta_4 \cdot \text{day}_\text{Fri} + &hellip; + \theta_{11} \cdot \text{time}_\text{Lunch} + \theta_{12} \cdot \text{time}_\text{Dinner} $$</p>
<p>We can represent the linear combination above as a matrix-vector product. Implement the <code>linear_model</code> function to evaluate this product.</p>
<p>Below, we create a <code>MyLinearModel</code> class with two methods, <code>predict</code> and <code>fit</code>. When fitted, this model fails to do anything useful, setting all of its 12 parameters to zero.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyLinearModel</span><span class="p">():</span>    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thetas</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">number_of_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">number_of_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">MyLinearModel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">one_hot_X</span><span class="p">,</span> <span class="n">tips</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">_thetas</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>array([[0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.],
       [0.]])
</code></pre>
<br/>
<hr style="border: 1px solid #fdb515;" />
<h2 id="question-2-fitting-a-linear-model-using-scipyoptimizeminimize-methods">Question 2: Fitting a Linear Model using scipy.optimize.minimize Methods</h2>
<p>Recall in Lab 5 and in lecture 12 we defined multiple loss functions and found the optimal theta using the <code>scipy.optimize.minimize</code> function. Adapt the code below to implement the fit method of the linear model.</p>
<p>Note that we&rsquo;ve added a <code>loss_function</code> parameter where the model is fit using the desired loss function, i.e. not necssarily the L2 loss. Example loss function are given as <code>l1</code> and <code>l2</code>.</p>
<!--
BEGIN QUESTION
name: q2
points: 2
-->
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">l1</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">l2</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyLinearModel</span><span class="p">():</span>    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thetas</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Produce the estimated optimal _thetas for the given loss function, 
</span></span></span><span class="line"><span class="cl"><span class="s2">        feature matrix X, and observations y.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Parameters
</span></span></span><span class="line"><span class="cl"><span class="s2">        -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">        loss_function: either the squared or absolute loss functions defined above
</span></span></span><span class="line"><span class="cl"><span class="s2">        X: a 2D dataframe (or numpy array) of numeric features (one-hot encoded)
</span></span></span><span class="line"><span class="cl"><span class="s2">        y: a 1D vector of tip amounts
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns
</span></span></span><span class="line"><span class="cl"><span class="s2">        -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">        The estimate for the optimal theta vector that minimizes our loss
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">number_of_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">## Notes on the following function call which you need to finish:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 0. The starting guess should be some arbitrary array of the correct length.</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#    Note the &#34;number of features&#34; variable above.&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. The ... in &#34;lambda theta: ...&#34; should be replaced by the average loss if we</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#    compute X @ theta. The loss is measured using the given loss function,</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#    relative to the observations in the variable y.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">starting_guess</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">number_of_features</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_thetas</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">loss_function</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span> <span class="o">@</span> <span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                                <span class="p">,</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">starting_guess</span><span class="p">)[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Notice above that we extract the &#39;x&#39; entry in the dictionary returned by `minimize`. </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This entry corresponds to the optimal theta estimated by the function. Sorry</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># we know it&#39;s a little confusing, but &#39;x&#39; is hard coded into the minimize function</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># because of the fact that in the optimization universe &#34;x&#34; is what you optimize over.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># It&#39;d be less confusing for DS100 students if they used &#34;theta&#34;.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="c1"># When you run the code below, you should get back some non zero thetas.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">MyLinearModel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="n">one_hot_X</span><span class="p">,</span> <span class="n">tips</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">_thetas</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>array([ 0.09448702,  0.17599315,  0.31373886,  0.34618029, -0.22256393,
       -0.13615575,  0.30569628,  0.46797868,  0.34653012,  0.44250887,
        0.1939605 ,  0.1258012 ])
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">grader</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&#34;q2&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong><pre style='display: inline;'>q2</pre></strong> passed! üíØ</p>
<p>The MSE for your model above should be just slightly larger than 1:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">one_hot_X</span><span class="p">),</span> <span class="n">tips</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>np.float64(1.0103535612506567)
</code></pre>
<br/>
<hr style="border: 1px solid #fdb515;" />
<h2 id="question-3-fitting-the-model-using-analytic-methods">Question 3: Fitting the Model using Analytic Methods</h2>
<p>Let&rsquo;s also fit our model analytically for the L2 loss function. Recall from lecture that with a linear model, we are solving the following optimization problem for least squares:</p>
<p>$$\min_{\theta} ||\Bbb{X}\theta - \Bbb{y}||^2$$</p>
<p>We showed in <a href="https://docs.google.com/presentation/d/15eEbroVt2r36TXh28C2wm6wgUHlCBCsODR09kLHhDJ8/edit#slide=id.g113dfce000f_0_2682" target="_blank" rel="noopener noreffer ">Lecture 11</a> that the optimal $\hat{\theta}$ when $X^TX$ is invertible is given by the equation: $(X^TX)^{-1}X^TY$</p>
<hr>
<h3 id="question-3a-analytic-solution-using-explicit-inverses">Question 3a: Analytic Solution Using Explicit Inverses</h3>
<p>For this problem, implement the analytic solution above using <code>np.linalg.inv</code> to compute the inverse of $X^TX$.</p>
<p>Reminder: To compute the transpose of a matrix, you can use <code>X.T</code> or <code>X.transpose()</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyAnalyticallyFitOLSModel</span><span class="p">():</span>    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thetas</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Sets _thetas using the analytical solution to the ordinary least squares problem
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Parameters
</span></span></span><span class="line"><span class="cl"><span class="s2">        -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">        X: a 2D dataframe (or numpy array) of numeric features (one-hot encoded)
</span></span></span><span class="line"><span class="cl"><span class="s2">        y: a 1D vector of tip amounts
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns
</span></span></span><span class="line"><span class="cl"><span class="s2">        -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">        The estimate for theta computed using the equation mentioned above
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">xTx</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
</span></span><span class="line"><span class="cl">        <span class="n">xTy</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">xTx</span><span class="p">)</span> <span class="o">@</span> <span class="n">xTy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now, run the cell below to find the analytical solution for the <code>tips</code> dataset. Depending on the machine that you run your code on, you should either see a singular matrix error or end up with thetas that are nonsensical (magnitudes greater than 10^15). This is not good!</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># When you run the code below, you should get back some non zero thetas.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">MyAnalyticallyFitOLSModel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">one_hot_X</span><span class="p">,</span> <span class="n">tips</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">analytical_thetas</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_thetas</span>
</span></span><span class="line"><span class="cl"><span class="n">analytical_thetas</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>array([ 9.66544413e+00, -1.89677732e+02, -8.30149679e+17, -8.30149679e+17,
        8.30149679e+17,  8.30149679e+17, -2.56000000e+02,  0.00000000e+00,
       -3.20000000e+01,  3.20000000e+01, -8.00000000e+00,  0.00000000e+00])
</code></pre>
<p>In the cell below, explain why we got the errorÔºàÊåáÂèÇÊï∞‰∏çÂØπÔºüÔºâ above when trying to calculate the analytical solution for our one-hot encoded <code>tips</code> dataset.</p>
<!--
BEGIN QUESTION
name: q3a
-->
<p><em>Êú¨Ë¥®‰∏äÊòØÂõ†‰∏∫Áü©Èòµ <strong>‰∏çÂèØÈÄÜ</strong>ÔºåÁã¨ÁÉ≠ÁºñÁ†ÅÊüê‰∫õÁ∫øÊÄßÁªÑÂêà‰πãÂêéÂèØ‰ª•ËΩªÊòìÁúãÂá∫Áü©Èòµ $X^TX$ ‰∏çÊòØÊª°Áß©ÁöÑ</em></p>
<hr>
<h3 id="question-3b-fixing-our-one-hot-encoding">Question 3b: Fixing our One-Hot Encoding</h3>
<p>Now, let&rsquo;s fix our one-hot encoding approach from question 1 so we don&rsquo;t get the error we saw in question 3a. Complete the code below to one-hot-encode our dataset such that <code>one_hot_X_revised</code> has no redundant features.</p>
<!--
BEGIN QUESTION
name: q3b
-->
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">one_hot_encode_revised</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Return the one-hot encoded dataframe of our input data, removing redundancies.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Parameters
</span></span></span><span class="line"><span class="cl"><span class="s2">    -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">    data: a dataframe that may include non-numerical features
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns
</span></span></span><span class="line"><span class="cl"><span class="s2">    -----------
</span></span></span><span class="line"><span class="cl"><span class="s2">    A one-hot encoded dataframe that only contains numeric features without any redundancies.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span> <span class="s1">&#39;smoker&#39;</span><span class="p">,</span> <span class="s1">&#39;day&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">values</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="c1"># ËøôÊòØÁî®[]ÂàáÁâáÁöÑÊäÄÂ∑ßÔºå‰ªévalues‰∏≠ÂèñÈô§‰∫ÜÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑÊâÄÊúâÂÖÉÁ¥†</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span><span class="p">[</span><span class="n">column</span> <span class="o">+</span> <span class="s1">&#39;=&#39;</span> <span class="o">+</span> <span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">==</span> <span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Âà†Èô§ÂéüÂßãÁöÑÂàó</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">one_hot_X_revised</span> <span class="o">=</span> <span class="n">one_hot_encode_revised</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>    
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">numerical_model</span> <span class="o">=</span> <span class="n">MyLinearModel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">numerical_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="n">one_hot_X_revised</span><span class="p">,</span> <span class="n">tips</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">analytical_model</span> <span class="o">=</span> <span class="n">MyAnalyticallyFitOLSModel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">analytical_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">one_hot_X_revised</span><span class="p">,</span> <span class="n">tips</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Our numerical model&#39;s loss is: &#34;</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">numerical_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">one_hot_X_revised</span><span class="p">),</span> <span class="n">tips</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Our analytical model&#39;s loss is: &#34;</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">analytical_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">one_hot_X_revised</span><span class="p">),</span> <span class="n">tips</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>Our numerical model's loss is:  1.0255082437778105
Our analytical model's loss is:  1.0255082436053506
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">grader</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&#34;q3b&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h3 id="question-3c-analyzing-our-new-one-hot-encoding">Question 3c: Analyzing our new One-Hot Encoding</h3>
<p>Why did removing redundancies in our one-hot encoding fix the problem we had in 3a?</p>
<!--
BEGIN QUESTION
name: q3c
-->
<p><em>‰∏çÊòØÂÖ®ÈÉ®ËøõË°åÁã¨ÁÉ≠ÁºñÁ†ÅÊìç‰ΩúÔºåÈÅøÂÖçÁ∫øÊÄßÁõ∏ÂÖ≥ÊÄß</em></p>
<hr>
<p>Note: An alternate approach is to use <code>np.linalg.solve</code> instead of <code>np.linalg.inv</code>. For the example above, even with the redundant features, <code>np.linalg.solve</code> will work well. Though in general, it&rsquo;s best to drop redundant features anyway.</p>
<p>In case you want to learn more, here is a relevant Stack Overflow post: <a href="https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li" target="_blank" rel="noopener noreffer ">https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li</a></p>
<br/>
<hr style="border: 1px solid #fdb515;" />
<h2 id="question-4-gradient-descent">Question 4: Gradient Descent</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Run this cell to load the data for this problem</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;lab7_data.csv&#34;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-5.000000</td>
      <td>-7.672309</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-4.966555</td>
      <td>-7.779735</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-4.933110</td>
      <td>-7.995938</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-4.899666</td>
      <td>-8.197059</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-4.866221</td>
      <td>-8.183883</td>
    </tr>
  </tbody>
</table>
</div>
<p>If we plot this data, we see that there is a clear sinusoidal relationship between x and y.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
</span></span><span class="line"><span class="cl"><span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s2">&#34;x&#34;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&#34;y&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>In this exercise, we&rsquo;ll show gradient descent is so powerful it can even optimize a nonlinear model. Specifically, we&rsquo;re going to model the relationship of our data by:</p>
<p>$$\Large{
f_{\boldsymbol{\theta(x)}} = \theta_1x + sin(\theta_2x)
}$$</p>
<p>Our model is parameterized by both $\theta_1$ and $\theta_2$, which we can represent in the vector, $\boldsymbol{\theta}$.</p>
<p>Note that a general sine function $a\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$.</p>
<p>Here, we&rsquo;re assuming the amplitude $a$ is around 1, and the phase shifting parameter $c$ is around zero. We do not attempt to justify this assumption and you&rsquo;re welcome to see what happens if you ignore this assumption at the end of this lab.</p>
<p>You might ask why we don&rsquo;t just create a linear model like we did earlier with a sinusoidal feature. The issue is that the theta is INSIDE the sin function. In other words, linear models use their parameters to adjust the scale of each feature, but $\theta_2$ in this model adjusts the frequency of the feature. There are tricks we could play to use our linear model framework here, but we won&rsquo;t attempt this in our lab.</p>
<p>We define the <code>sin_model</code> function below that predicts $\textbf{y}$ (the $y$-values) using $\textbf{x}$ (the $x$-values) based on our new equation.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sin_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Predict the estimate of y given x, theta_1, theta_2
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Keyword arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    x -- the vector of values x
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta -- a vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta_1</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta_2</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">theta_1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta_2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h3 id="question-4a-computing-the-gradient-of-the-mse-with-respect-to-theta-on-the-sin-model">Question 4a: Computing the Gradient of the MSE With Respect to Theta on the Sin Model</h3>
<p>Recall $\hat{\theta}$ is the value of $\theta$ that minimizes our loss function. One way of solving for $\hat{\theta}$ is by computing the gradient of our loss function with respect to $\theta$, like we did in lecture: <a href="https://docs.google.com/presentation/d/1j9ESgjn-aeZSOX5ON1wjkF5WBZHc4IN7XvTpYnX1pFs/edit#slide=id.gfc76b62ec3_0_27" target="_blank" rel="noopener noreffer ">https://docs.google.com/presentation/d/1j9ESgjn-aeZSOX5ON1wjkF5WBZHc4IN7XvTpYnX1pFs/edit#slide=id.gfc76b62ec3_0_27</a>. Recall that the gradient is a column vector of two partial derivatives.</p>
<p>Write/derive the expressions for following values and use them to fill in the functions below.</p>
<ul>
<li>$L(\textbf{x}, \textbf{y}, \theta_1, \theta_2)$: our loss function, the mean squared error</li>
<li>$\frac{\partial L }{\partial \theta_1}$: the partial derivative of $L$ with respect to $\theta_1$</li>
<li>$\frac{\partial L }{\partial \theta_2}$: the partial derivative of $L$ with respect to $\theta_2$</li>
</ul>
<p>Recall that $L(\textbf{x}, \textbf{y}, \theta_1, \theta_2) = \frac{1}{n} \sum_{i=1}^{n} (\textbf{y}_i - \hat{\textbf{y}}_i)^2$</p>
<p>Specifically, the functions <code>sin_MSE</code>, <code>sin_MSE_dt1</code> and <code>sin_MSE_dt2</code> should compute $R$, $\frac{\partial R }{\partial \theta_1}$ and $\frac{\partial R }{\partial \theta_2}$ respectively. Use the expressions you wrote for $\frac{\partial R }{\partial \theta_1}$ and $\frac{\partial R }{\partial \theta_2}$ to implement these functions. In the functions below, the parameter <code>theta</code> is a vector that looks like $\begin{bmatrix} \theta_1 \ \theta_2 \end{bmatrix}$. We have completed <code>sin_MSE_gradient</code>, which calls <code>dt1</code> and <code>dt2</code> and returns the gradient <code>dt</code> for you.</p>
<p>Notes:</p>
<ul>
<li>Keep in mind that we are still working with our original set of data, <code>df</code>.</li>
<li>To keep your code a bit more concise, be aware that <code>np.mean</code> does the same thing as <code>np.sum</code> divided by the length of the numpy array. *Ê≥®ÊÑèmeanÁöÑÂ±ÇÁ∫ßÂà´</li>
<li>Another way to keep your code more concise is to use the function <code>sin_model</code> we defined which computes the output of the model.</li>
</ul>
<!--
BEGIN QUESTION
name: q4a
points: 3
-->
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sin_MSE</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Compute the numerical value of the l2 loss of our sinusoidal model given theta
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Keyword arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta -- the vector of values theta
</span></span></span><span class="line"><span class="cl"><span class="s2">    x     -- the vector of x values
</span></span></span><span class="line"><span class="cl"><span class="s2">    y     -- the vector of y values
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">sin_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sin_MSE_dt1</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Compute the numerical value of the partial of l2 loss with respect to theta_1
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Keyword arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta -- the vector of values theta
</span></span></span><span class="line"><span class="cl"><span class="s2">    x     -- the vector of x values
</span></span></span><span class="line"><span class="cl"><span class="s2">    y     -- the vector of y values
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">sin_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sin_MSE_dt2</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Compute the numerical value of the partial of l2 loss with respect to theta_2
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Keyword arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta -- the vector of values theta
</span></span></span><span class="line"><span class="cl"><span class="s2">    x     -- the vector of x values
</span></span></span><span class="line"><span class="cl"><span class="s2">    y     -- the vector of y values
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">sin_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">))</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1"># This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sin_MSE_gradient</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns the gradient of l2 loss with respect to vector theta
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Keyword arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta -- the vector of values theta
</span></span></span><span class="line"><span class="cl"><span class="s2">    x     -- the vector of x values
</span></span></span><span class="line"><span class="cl"><span class="s2">    y     -- the vector of y values
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sin_MSE_dt1</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">sin_MSE_dt2</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">grader</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&#34;q4a&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h3 id="question-4b-implementing-gradient-descent-and-using-it-to-optimize-the-sin-model">Question 4b: Implementing Gradient Descent and Using It to Optimize the Sin Model</h3>
<p>Let&rsquo;s now implement gradient descent.</p>
<p>Note that the function you&rsquo;re implementing here is somewhat different than the gradient descent function we created in lecture. The version in lecture was <code>gradient_descent(df, initial_guess, alpha, n)</code>, where <code>df</code> was the gradient of the function we are minimizing and <code>initial_guess</code> are the starting parameters for that function. Here our signature is a bit different (described below) than the <code>gradient_descent</code> <a href="https://ds100.org/sp22/resources/assets/lectures/lec12/lec12.html" target="_blank" rel="noopener noreffer ">implementation from lecture</a>.</p>
<!--
BEGIN QUESTION
name: q4b
points: 3
-->
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_theta</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">grad_desc</span><span class="p">(</span><span class="n">loss_f</span><span class="p">,</span> <span class="n">gradient_loss_f</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Run gradient descent update for a finite number of iterations and static learning rate
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Keyword arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    loss_f -- the loss function to be minimized (used for computing loss_history)
</span></span></span><span class="line"><span class="cl"><span class="s2">    gradient_loss_f -- the gradient of the loss function to be minimized
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta -- the vector of values theta to use at first iteration
</span></span></span><span class="line"><span class="cl"><span class="s2">    data -- the data used in the model 
</span></span></span><span class="line"><span class="cl"><span class="s2">    num_iter -- the max number of iterations
</span></span></span><span class="line"><span class="cl"><span class="s2">    alpha -- the learning rate (also called the step size)
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Return:
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta -- the optimal value of theta after num_iter of gradient descent
</span></span></span><span class="line"><span class="cl"><span class="s2">    theta_history -- the series of theta values over each iteration of gradient descent
</span></span></span><span class="line"><span class="cl"><span class="s2">    loss_history -- the series of loss values over each iteration of gradient descent
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta_history</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">theta_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="c1"># ÂÖàappendÊØîËæÉÂ•Ω</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_f</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_b</span> <span class="o">=</span> <span class="n">gradient_loss_f</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">d_b</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">theta_history</span><span class="p">,</span> <span class="n">loss_history</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">theta_start</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">theta_hat</span><span class="p">,</span> <span class="n">thetas_used</span><span class="p">,</span> <span class="n">losses_calculated</span> <span class="o">=</span> <span class="n">grad_desc</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">sin_MSE</span><span class="p">,</span> <span class="n">sin_MSE_gradient</span><span class="p">,</span> <span class="n">theta_start</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">thetas_used</span><span class="p">,</span> <span class="n">losses_calculated</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;theta: </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>theta: [0 0], Loss: 20.859191416422235
theta: [2.60105745 2.60105745], Loss: 9.285008173048666
theta: [0.90342728 2.59100602], Loss: 4.680169273815357
theta: [2.05633644 2.9631291 ], Loss: 2.6242517936325833
theta: [1.15892347 2.86687431], Loss: 1.4765157174727774
theta: [1.79388042 3.07275573], Loss: 0.9073271435862448
theta: [1.32157494 3.00146569], Loss: 0.541531643291128
theta: [1.64954491 3.02910866], Loss: 0.3775841142469479
theta: [1.42325294 2.98820303], Loss: 0.2969750688130759
theta: [1.58295041 3.01033846], Loss: 0.2590425421375732
theta: [1.47097255 2.98926519], Loss: 0.23973439443291833
theta: [1.55040965 3.0017442 ], Loss: 0.23034782416254634
theta: [1.49439132 2.99135194], Loss: 0.2255775832667724
theta: [1.5341564  2.99797824], Loss: 0.22321772191904068
theta: [1.50603995 2.99286671], Loss: 0.22202363967204045
theta: [1.52598919 2.99628665], Loss: 0.22142811500262397
theta: [1.51186655 2.99375531], Loss: 0.22112776381775168
theta: [1.52188208 2.99549617], Loss: 0.22097741373654575
theta: [1.51478773 2.99423497], Loss: 0.22090173185683032
theta: [1.51981739 2.99511516], Loss: 0.2208637810584589
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">grader</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&#34;q4b&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>If you pass the tests above, you&rsquo;re done coding for this lab, though there are some cool visualizations below we&rsquo;d like you to think about.</p>
<p>Let&rsquo;s visually inspect our results of running gradient descent to optimize $\boldsymbol\theta$. The code below plots our $x$-values with our model&rsquo;s predicted $\hat{y}$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\boldsymbol\theta$.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">theta_init</span> <span class="o">=</span> <span class="n">init_theta</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">theta_est</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">grad_desc</span><span class="p">(</span><span class="n">sin_MSE</span><span class="p">,</span> <span class="n">sin_MSE_gradient</span><span class="p">,</span> <span class="n">theta_init</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Plotting our model output over our observaitons shows that gradient descent did  a great job finding both the overall increase (slope) of the data, as well as the oscillation frequency.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">sin_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_est</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model ($\hat</span><span class="si">{y}</span><span class="s1">$)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observation ($y$)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gold&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>&lt;&gt;:4: SyntaxWarning:

invalid escape sequence '\h'

&lt;&gt;:4: SyntaxWarning:

invalid escape sequence '\h'

C:\Users\86135\AppData\Local\Temp\ipykernel_10128\2413075366.py:4: SyntaxWarning:

invalid escape sequence '\h'
</code></pre>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/datalab7/lab07_files/lab07_54_1.png"
        data-srcset="/datalab7/lab07_files/lab07_54_1.png, /datalab7/lab07_files/lab07_54_1.png 1.5x, /datalab7/lab07_files/lab07_54_1.png 2x"
        data-sizes="auto"
        alt="/datalab7/lab07_files/lab07_54_1.png"
        title="png" width="639" height="452" /></p>
<br/>
<hr style="border: 1px solid #fdb515;" />
<h2 id="visualizing-loss-extra">Visualizing Loss (Extra)</h2>
<p>Let&rsquo;s visualize our loss functions and gain some insight as to how gradient descent optimizes our model parameters.</p>
<p>In the previous plot we saw the loss decrease with each iteration. In this part, we&rsquo;ll see the trajectory of the algorithm as it travels the loss surface? Run the following cells to see visualization of this trajectory.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">thetas</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre><code>array([[0.        , 0.        ],
       [2.60105745, 2.60105745],
       [0.90342728, 2.59100602],
       [2.05633644, 2.9631291 ],
       [1.15892347, 2.86687431],
       [1.79388042, 3.07275573],
       [1.32157494, 3.00146569],
       [1.64954491, 3.02910866],
       [1.42325294, 2.98820303],
       [1.58295041, 3.01033846],
       [1.47097255, 2.98926519],
       [1.55040965, 3.0017442 ],
       [1.49439132, 2.99135194],
       [1.5341564 , 2.99797824],
       [1.50603995, 2.99286671],
       [1.52598919, 2.99628665],
       [1.51186655, 2.99375531],
       [1.52188208, 2.99549617],
       [1.51478773, 2.99423497],
       [1.51981739, 2.99511516]])
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Run me to see a 3D plot (gradient descent with static alpha)</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">lab7_utils</span> <span class="kn">import</span> <span class="n">plot_3d</span>
</span></span><span class="line"><span class="cl"><span class="n">plot_3d</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">loss</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">sin_model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">plotly</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">plotly.graph_objs</span> <span class="k">as</span> <span class="nn">go</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">contour_plot</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">theta_history</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    The function takes the following as argument:
</span></span></span><span class="line"><span class="cl"><span class="s2">        theta_history: a (N, 2) array of theta history
</span></span></span><span class="line"><span class="cl"><span class="s2">        loss: a list or array of loss value
</span></span></span><span class="line"><span class="cl"><span class="s2">        loss_function: for example, l2_loss
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: for example, sin_model
</span></span></span><span class="line"><span class="cl"><span class="s2">        x: the original x input
</span></span></span><span class="line"><span class="cl"><span class="s2">        y: the original y output
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta_1_series</span> <span class="o">=</span> <span class="n">theta_history</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># a list or array of theta_1 value</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta_2_series</span> <span class="o">=</span> <span class="n">theta_history</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># a list or array of theta_2 value</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">## In the following block of code, we generate the z value</span>
</span></span><span class="line"><span class="cl">    <span class="c1">## across a 2D grid</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta1_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">theta_1_series</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">theta_1_series</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta2_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">theta_2_series</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">theta_2_series</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x_s</span><span class="p">,</span> <span class="n">y_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">theta1_s</span><span class="p">,</span> <span class="n">theta2_s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x_s</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y_s</span><span class="o">.</span><span class="n">flatten</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">    <span class="n">ls</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">theta1</span><span class="p">,</span> <span class="n">theta2</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">l</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">theta1</span><span class="p">,</span> <span class="n">theta2</span><span class="p">])),</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Create trace of theta point</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create the contour </span>
</span></span><span class="line"><span class="cl">    <span class="n">theta_points</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&#34;theta Values&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                              <span class="n">x</span><span class="o">=</span><span class="n">theta_1_series</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                              <span class="n">y</span><span class="o">=</span><span class="n">theta_2_series</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;lines+markers&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">lr_loss_contours</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Contour</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">theta1_s</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                  <span class="n">y</span><span class="o">=</span><span class="n">theta2_s</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                  <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                  <span class="n">colorscale</span><span class="o">=</span><span class="s1">&#39;Viridis&#39;</span><span class="p">,</span> <span class="n">reversescale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">plotly</span><span class="o">.</span><span class="n">offline</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">lr_loss_contours</span><span class="p">,</span> <span class="n">theta_points</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">title</span><span class="p">}))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">contour_plot</span><span class="p">(</span><span class="s1">&#39;Gradient Descent with Static Learning Rate&#39;</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">sin_model</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s2">&#34;x&#34;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&#34;y&#34;</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>As we can see, gradient descent is able to navigate even this fairly complex loss space and find a nice minimum.</p>
<h1 id="congratulations-you-finished-the-lab">Congratulations! You finished the lab!</h1>
<hr>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-08-13</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/datalab7/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="http://example.org/datalab7/" data-title="DATA100-lab7: Gradient Descent and Feature Engineering" data-hashtags="Numpy,Pandas,Scikit-Learn,SciPy,Plotly,Seaborn"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="http://example.org/datalab7/" data-hashtag="Numpy"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="http://example.org/datalab7/" data-title="DATA100-lab7: Gradient Descent and Feature Engineering"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="http://example.org/datalab7/" data-title="DATA100-lab7: Gradient Descent and Feature Engineering"><i data-svg-src="/lib/simple-icons/icons/line.min.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on ÂæÆÂçö" data-sharer="weibo" data-url="http://example.org/datalab7/" data-title="DATA100-lab7: Gradient Descent and Feature Engineering"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/numpy/">Numpy</a>,&nbsp;<a href="/tags/pandas/">Pandas</a>,&nbsp;<a href="/tags/scikit-learn/">Scikit-Learn</a>,&nbsp;<a href="/tags/scipy/">SciPy</a>,&nbsp;<a href="/tags/plotly/">Plotly</a>,&nbsp;<a href="/tags/seaborn/">Seaborn</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/datalab8/" class="prev" rel="prev" title="DATA100-lab8: Model Selection, Regularization, and Cross-Validation"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>DATA100-lab8: Model Selection, Regularization, and Cross-Validation</a>
            <a href="/datalab9/" class="next" rel="next" title="DATA100-lab9: Probability and Modeling">DATA100-lab9: Probability and Modeling<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.125.1">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">HHZZ</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
