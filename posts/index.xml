<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - HHZZ`s space</title>
        <link>http://example.org/posts/</link>
        <description>All Posts | HHZZ`s space</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 22 Apr 2025 08:27:18 &#43;0800</lastBuildDate><atom:link href="http://example.org/posts/" rel="self" type="application/rss+xml" /><item>
    <title>L16: Image Segmentation</title>
    <link>http://example.org/l16-segmentation/</link>
    <pubDate>Tue, 22 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l16-segmentation/</guid>
    <description><![CDATA[Image Segmentation Goal: to learn about what Fully Convolutional Networks (FCN) are and how they can be used for image segmentation. &#x1f914;
Semantic Segmentation a simple way is to slide the window, creating a slow RNN? another way is to build a Equal size Fully Convolutional Network (but receptive field is limited and hard to generalize to higher resolutions) U-Net like architecture &#x1f609; but how to UPSAMPLE the feature maps?]]></description>
</item>
<item>
    <title>L22-Recap and Conclusion</title>
    <link>http://example.org/l22-recap-and-conclusion/</link>
    <pubDate>Tue, 22 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l22-recap-and-conclusion/</guid>
    <description><![CDATA[Recap and Conclusion Recap What is Computer Vision? building a system that can understand and process visual information
Main approaches to Computer Vision
semantic segmentation using Fully Convolutional Networks (FCN) &#x1f914;
What&rsquo;s next? we will discover interesting new types of deep models Neural ODE NIPS 2018 $$ h_{t+1} = h_t + f(h_t, \theta_t) \Rightarrow \frac{dh}{dt} = f(h(t), t, \theta) $$
Deep Learning will find new applications
AI4S DL4CS, like using DL training a hash function to improve the hash table DL4Mathematics, auto theorem proving Deep Learning will use more data and more computional power Problems and Challenges Models are biased eg: Vector Arithmetic with Word Vectors, fit biased dataset to a model leads to poor performance; Economic Bias (usually train model on the wealthier western family?]]></description>
</item>
<item>
    <title>L20-Generative Model II</title>
    <link>http://example.org/l20-generative-model-ii/</link>
    <pubDate>Mon, 21 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l20-generative-model-ii/</guid>
    <description><![CDATA[Generative Model II for some reason, it seems like that we are backed &#x1f606;
Generative Adversarial Networks (GANs) setup: assume that we have data $x_i$ from a distribution $p_{data}(x)$, all we wanna do is to sample from $p_{data}(x)$
idea: introduce a latent variable $z$ with simple prior $p(z)$, those $z$ can be interpolated,
sample $z$ from $p(z)$, and pass into a Generator $x = G(z)$, then we said that $x$ is a sample from the Generator distribution $p_{G}$,]]></description>
</item>
<item>
    <title>L18-Video</title>
    <link>http://example.org/l18-video/</link>
    <pubDate>Sun, 20 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l18-video/</guid>
    <description><![CDATA[Video in NN Introduction Video: 4D tensor, T x H x W x C, a sequence of frames(images).
Video Classification Input: T x H x W x C Output: K classes, actions instead of nouns. Problems: raw video are BIG, ~1.5GB per minute for SD(640x480x3)
Solutions: on short clips, eg. T=16, H=W=112, low fps
Short Term model Single-Frame CNN so you can use a 2D CNN on each frame independently :)]]></description>
</item>
<item>
    <title>L19-Generative Model I</title>
    <link>http://example.org/l19-generative-model-i/</link>
    <pubDate>Sun, 20 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l19-generative-model-i/</guid>
    <description><![CDATA[Generative Model I supervised learning vs unsupervised learning supervised learning: labeled data, labeled target variable
classification, semantic segmentation, object detection, etc. unsupervised learning: unlabeled data, no target variable
clustering, density estimation, feature extraction/learning, dimensionality reduction, etc. we are trying to learn model the distribution of the data
Discriminative, Generative, and Conditional Generative all three types of learning can be used in a equation, x is the input variable, y is the target variable,]]></description>
</item>
<item>
    <title>L13-Attention</title>
    <link>http://example.org/l13-attention/</link>
    <pubDate>Sat, 19 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l13-attention/</guid>
    <description><![CDATA[Attention Mechanisms in Neural Networks introduction What if Seq to Seq models processed long long sequences?
Attention Mechanisms the core idea is that using weighted sum, and the coefficient can be learned from the model itself
In math, we do not actually care that wether input is a sequence or not.
given hidden states $h_i$ and the context vector $c$, we can calculate the attention weights as follows:
$$ e_{t, i, j} = f_{att}(s_{t-1}, h_{i,j}) \ a_{t, :, :} = softmax(e_{t, :, :}) \ c_{t} = \sum_{i,j} a_{t, i, j} h_{i,j} $$]]></description>
</item>
<item>
    <title>L17-3D Vision</title>
    <link>http://example.org/l17-3d-vision/</link>
    <pubDate>Sat, 19 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l17-3d-vision/</guid>
    <description><![CDATA[3D Vision back to the start &#x1f606;
shape prediction and ingest 3D information 5 data representation
depth map
RGB + Depth image = RGB-D image 2.5D raw 3D sensor can easily capture depth information another type of depth map is surface normal map, which is a 2D image that represents the surface normal (using a normal vector) at each pixel location all the mentioned maps can be learned with Fully Convolutional Networks (FCN) &#x1f914; Voxel grid]]></description>
</item>
<item>
    <title>L21-Reinforcement Learning</title>
    <link>http://example.org/l21-reinforcement-learning/</link>
    <pubDate>Fri, 18 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l21-reinforcement-learning/</guid>
    <description><![CDATA[Reinforcement Learning So far, we have discussed supervised learning, and a little bit unsupervised learning (in UCB-data100 &#x1f609;)
What is Reinforcement Learning at time $t$, env $\rightarrow^{state}$ agent $\rightarrow^{action}$ env $\rightarrow^{reward}$ agent, then env changed, agent learned, then repeated.
state can be partial -&gt; noisy reward can be delayed, implicit and sparse -&gt; noisy AND Nondifferentiable &#x1f632; Nonstationary environment, change over time &#x1f60e; Generative Adversarial Networks (GANs) somehow is a part of Reinforcement Learning.]]></description>
</item>
<item>
    <title>L17-huge GNN</title>
    <link>http://example.org/l17-huge-gnn/</link>
    <pubDate>Wed, 29 Jan 2025 12:19:30 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l17-huge-gnn/</guid>
    <description><![CDATA[Scaling up GNNs 直接load全部nodes又不太可能【naive approach】4090 / A100带不动
neighbor sampling 对hub node的思考 see the paper
cluster-GCN advanced Simplified GCN 舍弃了GCN的non-linearity，直接用linear layer
同质性？但是我想知道和glidar的区别？ ]]></description>
</item>
<item>
    <title>L16-improvedGNN</title>
    <link>http://example.org/l16-improvedgnn/</link>
    <pubDate>Wed, 29 Jan 2025 12:10:50 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l16-improvedgnn/</guid>
    <description><![CDATA[Improved GNN positionally-aware gnn anchor数量 大小 （指数守恒） refer to the paper of Position-aware Graph Neural Networks (P-GNN)]]></description>
</item>
</channel>
</rss>
