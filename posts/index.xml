<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - HHZZ`s space</title>
        <link>http://example.org/posts/</link>
        <description>All Posts | HHZZ`s space</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 21 Apr 2025 08:27:18 &#43;0800</lastBuildDate><atom:link href="http://example.org/posts/" rel="self" type="application/rss+xml" /><item>
    <title>L20-Generative Model II</title>
    <link>http://example.org/l20-generative-model-ii/</link>
    <pubDate>Mon, 21 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l20-generative-model-ii/</guid>
    <description><![CDATA[Generative Model II for some reason, it seems like that we are backed &#x1f606;
Generative Adversarial Networks (GANs) setup: assume that we have data $x_i$ from a distribution $p_{data}(x)$, all we wanna do is to sample from $p_{data}(x)$
idea: introduce a latent variable $z$ with simple prior $p(z)$, those $z$ can be interpolated,
sample $z$ from $p(z)$, and pass into a Generator $x = G(z)$, then we said that $x$ is a sample from the Generator distribution $p_{G}$,]]></description>
</item>
<item>
    <title>L18-Video</title>
    <link>http://example.org/l18-video/</link>
    <pubDate>Sun, 20 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l18-video/</guid>
    <description><![CDATA[Video in NN Introduction Video: 4D tensor, T x H x W x C, a sequence of frames(images).
Video Classification Input: T x H x W x C Output: K classes, actions instead of nouns. Problems: raw video are BIG, ~1.5GB per minute for SD(640x480x3)
Solutions: on short clips, eg. T=16, H=W=112, low fps
Short Term model Single-Frame CNN so you can use a 2D CNN on each frame independently :)]]></description>
</item>
<item>
    <title>L19-Generative Model I</title>
    <link>http://example.org/l19-generative-model-i/</link>
    <pubDate>Sun, 20 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l19-generative-model-i/</guid>
    <description><![CDATA[Generative Model I supervised learning vs unsupervised learning supervised learning: labeled data, labeled target variable
classification, semantic segmentation, object detection, etc. unsupervised learning: unlabeled data, no target variable
clustering, density estimation, feature extraction/learning, dimensionality reduction, etc. we are trying to learn model the distribution of the data
Discriminative, Generative, and Conditional Generative all three types of learning can be used in a equation, x is the input variable, y is the target variable,]]></description>
</item>
<item>
    <title>L13-Attention</title>
    <link>http://example.org/l13-attention/</link>
    <pubDate>Sat, 19 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l13-attention/</guid>
    <description><![CDATA[Attention Mechanisms in Neural Networks introduction What if Seq to Seq models processed long long sequences?
Attention Mechanisms the core idea is that using weighted sum, and the coefficient can be learned from the model itself
In math, we do not actually care that wether input is a sequence or not.
given hidden states $h_i$ and the context vector $c$, we can calculate the attention weights as follows:
$$ e_{t, i, j} = f_{att}(s_{t-1}, h_{i,j}) \ a_{t, :, :} = softmax(e_{t, :, :}) \ c_{t} = \sum_{i,j} a_{t, i, j} h_{i,j} $$]]></description>
</item>
<item>
    <title>L17-3D Vision</title>
    <link>http://example.org/l17-3d-vision/</link>
    <pubDate>Sat, 19 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l17-3d-vision/</guid>
    <description><![CDATA[3D Vision back to the start &#x1f606;
shape prediction and ingest 3D information 5 data representation
depth map
RGB + Depth image = RGB-D image 2.5D raw 3D sensor can easily capture depth information another type of depth map is surface normal map, which is a 2D image that represents the surface normal (using a normal vector) at each pixel location all the mentioned maps can be learned with Fully Convolutional Networks (FCN) &#x1f914; Voxel grid]]></description>
</item>
<item>
    <title>L21-Reinforcement Learning</title>
    <link>http://example.org/l21-reinforcement-learning/</link>
    <pubDate>Fri, 18 Apr 2025 08:27:18 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l21-reinforcement-learning/</guid>
    <description><![CDATA[Reinforcement Learning So far, we have discussed supervised learning, and a little bit unsupervised learning (in UCB-data100 &#x1f609;)
What is Reinforcement Learning at time $t$, env $\rightarrow^{state}$ agent $\rightarrow^{action}$ env $\rightarrow^{reward}$ agent, then env changed, agent learned, then repeated.
state can be partial -&gt; noisy reward can be delayed, implicit and sparse -&gt; noisy AND Nondifferentiable &#x1f632; Nonstationary environment, change over time &#x1f60e; Generative Adversarial Networks (GANs) somehow is a part of Reinforcement Learning.]]></description>
</item>
<item>
    <title>L17-huge GNN</title>
    <link>http://example.org/l17-huge-gnn/</link>
    <pubDate>Wed, 29 Jan 2025 12:19:30 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l17-huge-gnn/</guid>
    <description><![CDATA[Scaling up GNNs 直接load全部nodes又不太可能【naive approach】4090 / A100带不动
neighbor sampling 对hub node的思考 see the paper
cluster-GCN advanced Simplified GCN 舍弃了GCN的non-linearity，直接用linear layer
同质性？但是我想知道和glidar的区别？ ]]></description>
</item>
<item>
    <title>L16-improvedGNN</title>
    <link>http://example.org/l16-improvedgnn/</link>
    <pubDate>Wed, 29 Jan 2025 12:10:50 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l16-improvedgnn/</guid>
    <description><![CDATA[Improved GNN positionally-aware gnn anchor数量 大小 （指数守恒） refer to the paper of Position-aware Graph Neural Networks (P-GNN)]]></description>
</item>
<item>
    <title>L7-GNN t1</title>
    <link>http://example.org/l7-gnn-t1/</link>
    <pubDate>Tue, 28 Jan 2025 13:39:34 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l7-gnn-t1/</guid>
    <description><![CDATA[L7-GNN theory 1 所有gnn的原型在这里
一个layer layers之间的交互 input graph -&gt; computational graph的构建 GO!!!!
a simple layer of GNN msg computations 原神处理dddd
aggregation 核心：order invariance 焯这就是pointnet的核心之一啊！！！
issue： 容易忽略自己节点 some examples of GNNs GCN GraphSAGE 这里的细节在于如何聚合邻居的信息
同时每层来个norm
GAT 首先解释一下什么是attention mechanism 单个attention的计算 多头attention的计算（更容易收敛） GNN layers in practice glidar呼之欲出了 &#x1f631; 有意思的东西&#x1f60b; stacking GNN layers ]]></description>
</item>
<item>
    <title>L6-GNN intro</title>
    <link>http://example.org/l6-gnn-intro/</link>
    <pubDate>Mon, 27 Jan 2025 16:39:56 &#43;0800</pubDate>
    <author>HHZZ</author>
    <guid>http://example.org/l6-gnn-intro/</guid>
    <description><![CDATA[GNN Intro shallow deep graph encoder tmd 终于来了 !!! 借鉴cnn，但是
没有fixed notion or sliding window permutation invariance！ GCNN 直觉上的思考 其实就是3 + 1步走
找到aggregate function 找到loss function train on a set of nodes apply to new nodes 7 8.1 9
16.2 17]]></description>
</item>
</channel>
</rss>
